"Search Keyword","Users","Sessions","Searches"
"genie",43,443,"520"
"genie access request",12,155,"180"
"genie access",22,87,"98"
"data",8,15,"52"
"marketing things",1,1,"47"
"data science & engineering?",1,1,"33"
"annotation",1,1,"30"
"help",3,17,"28"
"dbr migration",1,1,"22"
"databricks case creation",1,1,"18"
"unity catalog",7,7,"18"
"cluster",10,14,"16"
"sas",3,3,"16"
"databricks sql",9,9,"14"
"delete account",13,13,"13"
"cluster not starting",12,12,"12"
"org.apache.spark.sql.analysisexception: nondeterministic expressions are only allowed in",1,9,"12"
"accessibility",1,1,"11"
"greenplum to databricks migration",1,1,"11"
"test",9,10,"11"
"token",7,8,"11"
"amazon s3 bucket access denied",2,8,"10"
"certification",7,7,"10"
"helping center",1,2,"10"
"pymongo",1,1,"10"
"sql",8,8,"10"
"boto3",3,9,"9"
"es-308244",1,1,"9"
"upload file",4,4,"9"
"api",6,6,"8"
"ganglia",1,1,"8"
"invoice",5,6,"8"
"language",2,2,"8"
"login",6,6,"8"
"sign up for databricks community edition",3,3,"8"
"sleep(10)",1,1,"8"
"2206080050000941-genie acess",7,7,"7"
"activity",1,1,"7"
"agent",1,1,"7"
"amazon s3 access denied",1,5,"7"
"azure",4,4,"7"
"calendar",1,1,"7"
"databricks",4,5,"7"
"deprecating support for updating regions on workspaces",4,5,"7"
"h2o",3,3,"7"
"import notebook",5,5,"7"
"intermittent redshift connection issue from job clusters",4,7,"7"
"job error invalid template",2,2,"7"
"move dag is failing",4,7,"7"
"partner connect",2,2,"7"
"power bi",4,4,"7"
"structured streaming ml scoring",1,1,"7"
"upgrade runtime",3,3,"7"
"voucher",6,6,"7"
"voucher  validity",2,2,"7"
"access the admin console",2,2,"6"
"can't specify configuration to sse-kms on a per-bucket basis",4,5,"6"
"cluster can't start",2,2,"6"
"cluster status pending",3,3,"6"
"could not find adls gen2 token",2,4,"6"
"databricks environment highly unstable",2,4,"6"
"delta live tables",4,5,"6"
"gcp",3,3,"6"
"genie 2204080030001854",1,6,"6"
"github",3,3,"6"
"import",6,6,"6"
"knowledge",1,1,"6"
"koalas",1,1,"6"
"log in",4,4,"6"
"marketing",2,2,"6"
"mlflow",6,6,"6"
"model serving",1,1,"6"
"notebook",4,5,"6"
"spark structured streaming job not running",4,4,"6"
"sql endpoint",6,6,"6"
"terminated cluster",1,1,"6"
"1.databricks-log contains no information about causing column in case of type mismatch during synapse-write",5,5,"5"
"3337075389283370 - 6b64b59f-432d-401e-b552-d855f1e1d2e0 - 2204060040002674",2,5,"5"
"access metadata",1,1,"5"
"amazon s3",1,3,"5"
"anonymization",1,1,"5"
"azure databricks notebook execution duration is extremely long (40+ minutes), and it exceeds the 10 minute trigger window",1,3,"5"
"change data feed",2,2,"5"
"cli",5,5,"5"
"cluster not start",4,4,"5"
"cluster terminated",4,5,"5"
"cluster terminated.reason:self bootstrap failure",3,3,"5"
"clusters",4,4,"5"
"db-i-3310",1,1,"5"
"dbfs",4,4,"5"
"describe view",1,1,"5"
"do i need to do associate first before professional",1,1,"5"
"genie access - sr 2204250050001910",1,5,"5"
"genie for #2203180030001191",5,5,"5"
"genie for #2203210030001488",5,5,"5"
"gennie access",3,5,"5"
"hive insert overwrite query error",1,4,"5"
"how can i call notebook in at the time of cluster start",1,3,"5"
"how to complete the certification in databricks",1,1,"5"
"jdbc to sql endpoint",1,1,"5"
"job failure",5,5,"5"
"jobs",5,5,"5"
"marketing stretegies classifications",1,1,"5"
"modified",1,1,"5"
"notebook showing function does not exist",1,5,"5"
"python read database tables",1,1,"5"
"repo",4,5,"5"
"saml2 sso",1,1,"5"
"search",4,5,"5"
"service level",1,1,"5"
"shap",1,1,"5"
"share",2,2,"5"
"shared cluster",1,1,"5"
"spark.databricks.delta.catalog.update.enabled",2,2,"5"
"trash",4,4,"5"
"ui based provisioning tools",1,1,"5"
"unable to access azuredatabricks workspace",2,2,"5"
"unable to load aws credentials",2,3,"5"
"unable to login",5,5,"5"
"unity catalog (preview)",1,2,"5"
"userid",1,1,"5"
"using databricks pool with multiple interactive clusters",3,3,"5"
"view data",1,1,"5"
"workspace",4,4,"5"
"""dbfs_fuse_version=1""",1,1,"4"
"2203180050000737-genie acess",4,4,"4"
"2205100050002029 - genie",1,3,"4"
"5003f00000jklrnaaj",1,2,"4"
"aa",1,1,"4"
"admin",4,4,"4"
"apache spark associate developer certification",1,3,"4"
"base",2,2,"4"
"billing",2,2,"4"
"certain commands are not working on databricks workspaces e.g: display() and %sql select does not work",3,3,"4"
"certification exam",3,3,"4"
"cloud provider launch failure",2,4,"4"
"cluster (10.4) shuts off randomly",1,3,"4"
"cluster not working",3,3,"4"
"cluster pending",2,2,"4"
"cluster terminating",3,4,"4"
"cluster wont start",3,3,"4"
"clusters 'running command' but not doing anything",1,2,"4"
"community edition",3,4,"4"
"copy results to excel",2,2,"4"
"cost",4,4,"4"
"css-arr-s500-sr#2205250030000489-genie access request",1,4,"4"
"css-arr-s500-sr#2205270040003393-genie access request",1,4,"4"
"databricks issue",4,4,"4"
"databricks jobs are failing to trigger the automation account runbooks",1,3,"4"
"databricks partners",2,2,"4"
"databricks runtime",2,2,"4"
"databricks:  when clicking on completed jobs get http error 403",1,4,"4"
"dbutils",3,3,"4"
"delta",3,4,"4"
"disable delta",1,2,"4"
"ecs",1,1,"4"
"error in databricks while creating table or updating partitions",1,2,"4"
"eventhub streaming to databricks delta table stopped due to offset mismatched",1,3,"4"
"excel files",3,3,"4"
"exclud",1,1,"4"
"export csv",4,4,"4"
"facing issue whule querying in r language",1,4,"4"
"frequency",1,1,"4"
"genie for #2202210050001736",4,4,"4"
"genie for #2204060050000418",4,4,"4"
"genie for #2205110050001559",4,4,"4"
"global init script failure | oms agent installation",2,3,"4"
"i got stuck to an error in data bricks , the error is ' command result size exceeds limit: exceeded 20971520 bytes (current = 20972463)'",1,3,"4"
"init script",3,4,"4"
"invalid column 'true'.",1,1,"4"
"job parameter",1,1,"4"
"manage workspace",1,1,"4"
"oss spark history server",1,1,"4"
"pipeline",3,3,"4"
"pricing",3,3,"4"
"r studio",3,3,"4"
"receipt",4,4,"4"
"runtime",1,1,"4"
"schema_of_json",2,2,"4"
"secret scope",4,4,"4"
"service principal scim provisionning is not working",3,4,"4"
"shortcut",4,4,"4"
"sso",3,3,"4"
"stack cli",1,1,"4"
"svg notbook",2,2,"4"
"tensorflow module not found",1,1,"4"
"unable to create db and tables",3,3,"4"
"unable to create empty delta tables in the adls stoarage",1,2,"4"
"update my training date",1,1,"4"
"vnet range",1,1,"4"
"voucher code",3,3,"4"
"vulnerability issue in nodes of databricks cluster",2,2,"4"
"waiting for cluster to start: encountered quota exhaustion issue in your account: azure_error_code",1,1,"4"
"""""""""""""""""""""""""onclick=alert(1)",1,1,"3"
"""><img src=x onerror=javascript:alert(document.cookie)>",2,2,"3"
"""code_path""",1,1,"3"
"""dbfs_fuse_version""",1,1,"3"
"""gc overhead limit exceeded""",1,1,"3"
"""table or view not found""",1,1,"3"
"%run",3,3,"3"
",download data as csv",1,1,"3"
"00d61jgc4._5008y1vr785:ref",1,1,"3"
"10.5 ml",1,1,"3"
"2203170040001201_need genie access",1,3,"3"
"2204110040000576 | private endpoint",1,2,"3"
"2205020050000680-genie acess",3,3,"3"
"<script>/**\x00/javascript:alert(1)///*/</script>",1,1,"3"
"<script>alert(1)</script>",2,2,"3"
"[arr] [sev c] sr-2206170060000071-is it possible to trigger the synapse store procedure from databricks notebook?",1,3,"3"
"[arr][2204070030000183][databricks job not able to complete as the cluster config has reached the max capacity]",1,2,"3"
"[arr][2204260040005094 ][geico]unable to set log destiation to dbfs mnt",1,2,"3"
"academy accreditation",1,1,"3"
"academy login",3,3,"3"
"access control",1,1,"3"
"access filestore",1,3,"3"
"acl permissions",2,2,"3"
"amazon",1,1,"3"
"amazon s3 bucket",1,1,"3"
"arr | some libraries are failing when cluster restarts | 2205170040006180",1,1,"3"
"arr || optimize delta load || 2206150010002628",1,3,"3"
"attach notebook",1,1,"3"
"attach notebook dynamically",1,1,"3"
"auto plot adjustment",1,1,"3"
"automation",1,1,"3"
"aws",3,3,"3"
"azure databricks certified associate platform administrator",1,1,"3"
"azure databricks create pipelines",1,1,"3"
"azure quota exceeded exception",1,1,"3"
"boolean",1,1,"3"
"broadcast",1,1,"3"
"calling rest api with databricks",1,1,"3"
"cannot create cluster",3,3,"3"
"cdf",2,2,"3"
"certificates",1,1,"3"
"certification renewal",1,1,"3"
"changing default timestamp for hive tables and databricks workspace to cet",1,2,"3"
"checkpoint",1,1,"3"
"cluster is not starting",2,2,"3"
"cluster issues while running ml notebbok",1,3,"3"
"cluster lost at least one node. reason: communication lost",2,3,"3"
"combine 2 feilds in sql",1,1,"3"
"communication with at least one worker node was unexpectedly lost.",1,2,"3"
"configure databricks s3 commit service-related settings",1,1,"3"
"connectiviy issues between databricks to cassandra db",2,3,"3"
"css-arr-s500-sr#2205180030001898-genie access request",1,2,"3"
"css-arr-s500-sr#2206010010000682-genie access request",1,3,"3"
"cst now 7:12",1,1,"3"
"cve-2022-22965",2,2,"3"
"dashboard",2,2,"3"
"data management: tips and troubleshooting",1,1,"3"
"data redaction",1,1,"3"
"databases and tables",1,1,"3"
"databricks academy",3,3,"3"
"databricks certification voucher code expiry",1,1,"3"
"databricks display table",1,1,"3"
"databricks exam",3,3,"3"
"databricks job failed with error message",1,2,"3"
"databricks jobs random library installation issues",2,3,"3"
"databricks notebook taking longer time moveing the data from adls to adls",2,2,"3"
"databricks upgrade to 10.5 - spark sql error",1,2,"3"
"dataframes and datasets",1,2,"3"
"dbfs size",1,1,"3"
"dbt",1,1,"3"
"delete my account",3,3,"3"
"delta lake change feed data error on job restart",1,2,"3"
"display",2,2,"3"
"docker wheel",1,1,"3"
"download",3,3,"3"
"email",3,3,"3"
"enable unity catalog",3,3,"3"
"error connecting to trino jdbc",1,3,"3"
"error importing azure devops repositories to databricks workspace",1,3,"3"
"errors running deployment script in databricks",1,2,"3"
"es-314270",1,1,"3"
"exam voucher",3,3,"3"
"exam voucher validity",3,3,"3"
"exception equests.exceptions.httperror: 429 client error: too many requests for url: https://centralus.azuredatabricks.net/api/2.0/jobs/list",2,2,"3"
"export data from databricks aws",1,1,"3"
"export query data into csv",3,3,"3"
"extension failure",1,1,"3"
"failed to connect azure sql server using activedirectoryintegrated in databricks",1,2,"3"
"failed to open delta live tables pipeline page-genie",1,3,"3"
"failure to launch dlt cluster",1,1,"3"
"failure to write to cosmos db",1,3,"3"
"file read from container is taking very long time",1,3,"3"
"filealreadyexistsexception: operation failed: ""the specified path, or an element of the path, exists and its resource type is invalid for this operation."", 409",2,2,"3"
"files in repos",2,2,"3"
"filter data while fetching from parquet file",1,1,"3"
"filters",1,1,"3"
"ganglia historical metrics snapshots issue",1,2,"3"
"geni access request",2,3,"3"
"genie 2205120030000229",1,3,"3"
"genie 2205210030000305",1,2,"3"
"genie for #2203300050000291",3,3,"3"
"genie for #2204110030001092",3,3,"3"
"genie for #2205160050001223",3,3,"3"
"genie request",1,3,"3"
"genie-2204220060001229",1,3,"3"
"genie-2205310030000459",1,3,"3"
"genie: capital group || 2205060040005682 || when upgrading from 8.3 to 9 lts or 10.4 lts we are seeing job run increase",1,1,"3"
"genie:arr:data is getting copied even after the failure of notebook and when the notebook is trying it's getting duplicated",1,1,"3"
"getting started",1,1,"3"
"ggplot size",1,1,"3"
"gpu cluster",1,1,"3"
"gpu usage",2,2,"3"
"how do you change runtime",1,1,"3"
"how to access cluster stats outside of databricks?",1,1,"3"
"how to configure aws machine images to cluster",1,3,"3"
"how to make s3 data available through databases and tables",1,1,"3"
"how to use display the output",1,1,"3"
"import dbc",3,3,"3"
"ingest data into delta lake",1,1,"3"
"install",2,2,"3"
"intelligent assistant",1,1,"3"
"interval",1,1,"3"
"invalid_parameter_value",1,1,"3"
"job aborted",3,3,"3"
"job cluster execution ends up in crash",1,3,"3"
"job clusters",2,2,"3"
"job execution issue",3,3,"3"
"job failed with oom",1,1,"3"
"job failing due to cloudproviderfailure",2,2,"3"
"job fails with atypical errors message",2,2,"3"
"job performance",1,2,"3"
"jobs are failing in adf with modulenotfounderror: no module named 'bcutils'",1,3,"3"
"jobs failing and cluster is being terminated",3,3,"3"
"keyword search",2,2,"3"
"kubernetes engine",1,1,"3"
"library",3,3,"3"
"library issue",2,3,"3"
"login issue",3,3,"3"
"logo",3,3,"3"
"long runtime durations",1,3,"3"
"master",1,1,"3"
"mounting databricks dbfs mount points in storage accounts",1,2,"3"
"need to identify the rootcause of the 'out of memory error', added garbagecollection to notebook but still receiving out of memory error",1,1,"3"
"neo4j",1,1,"3"
"not able to connect to adls with external hms",3,3,"3"
"not able to mount storage account",3,3,"3"
"not able to start cluster server",1,3,"3"
"not able to write to container using pyspark.write method",3,3,"3"
"notebook command shows 'running command' but nothing seems to be happening",2,3,"3"
"notebook template",1,1,"3"
"notebooks are taking more time to complete than average time",1,3,"3"
"office hours",2,2,"3"
"on some occasions the count file is having more numbers than the data file",1,3,"3"
"partner academy",3,3,"3"
"personal access tokens",3,3,"3"
"pip install",2,2,"3"
"pipelines are failing with an error",1,3,"3"
"prefix",1,1,"3"
"prepend",1,1,"3"
"primary key",2,2,"3"
"pyspark udf in spark sql",1,1,"3"
"python",2,2,"3"
"query history",1,1,"3"
"random issue running a query in databricks",1,3,"3"
"read table in sql",1,1,"3"
"recover databricks workspace",1,1,"3"
"redshift",3,3,"3"
"repl issue while running multiple streaming job",1,2,"3"
"request_limit_exceeded",1,1,"3"
"s3",3,3,"3"
"scim api*",2,2,"3"
"share notebook",2,2,"3"
"shortcut keys",2,2,"3"
"sku not available",1,1,"3"
"sla",2,2,"3"
"slack",3,3,"3"
"spark job failure",2,2,"3"
"sql access",2,3,"3"
"start cluster",1,2,"3"
"streaming job crashing with bootstrap url not defined",2,2,"3"
"streamingqueryexception",2,3,"3"
"streamingqueryexception: next on empty iterator",1,1,"3"
"string",1,2,"3"
"termination workspace deletion",1,1,"3"
"training",2,2,"3"
"unable to start cluster",3,3,"3"
"unexpected failure while waiting for the cluster",3,3,"3"
"unsubscribe",2,2,"3"
"upgrade",1,1,"3"
"user usage",1,1,"3"
"username",3,3,"3"
"vaccum",1,2,"3"
"vaccume",1,2,"3"
"vulnerability",3,3,"3"
"whitelist",3,3,"3"
"workload",1,1,"3"
"workspace access | 2205170050002219",1,3,"3"
"workspcaes　50",1,1,"3"
"you do not have access to any running clusters",1,1,"3"
"""""""""""""""""""""""""</><script>alert(1)</script>",1,1,"2"
"""><img src=x onerror=alert(1)>",1,1,"2"
"""\/><img%20s+src+c=x%20on+onerror+%20=""alert(1)""\>",1,1,"2"
"""account owner""",1,1,"2"
"""driver is up but is not responsive, likely due to gc""",1,1,"2"
"""modulenotfounderror""",1,1,"2"
"""secret key""",1,1,"2"
"'/databricks/driver'",1,2,"2"
"/dbfs/mnt/",2,2,"2"
"0457genie",2,2,"2"
"10.4",2,2,"2"
"2112100050001306 genie",1,2,"2"
"2203090050000696 - genie",1,2,"2"
"2203160050001572-genie acess",2,2,"2"
"2203210050001428 || genie access request",1,2,"2"
"2203290040006483 - genie - att",1,1,"2"
"2203310040004917-genie acess",2,2,"2"
"2204110050001767 - genie",1,2,"2"
"2204130040009267 | cluster startup",1,1,"2"
"2204140050001360 | job failure",1,1,"2"
"2204150030000217 genie",1,1,"2"
"2204180040002686 |  myfedex | arr",1,2,"2"
"2204200060001175_need genie access",1,1,"2"
"2205030050001600 genie",1,2,"2"
"2205050050001457 - genie",1,2,"2"
"2205090030000751002-need genie access",1,2,"2"
"2205110030000282 |ihs markit|genie",1,2,"2"
"2205120050000506 genie",1,1,"2"
"2205130030000496 genie",1,2,"2"
"2205130050001633 genie",1,1,"2"
"2205170050002982 || genie access request",1,2,"2"
"2205190030000748_need genie access",1,1,"2"
"2205200030000894 genie",1,2,"2"
"2205200050001146 genie",1,2,"2"
"2206020050001171 - genie",1,1,"2"
"2206020060002128-need genie access",1,2,"2"
"2206060030000742 genie",1,2,"2"
"2206080040001140 informatica jobs running on databricks cluster are failing",1,2,"2"
"2206090030000667 genie",1,1,"2"
"2206170010003168-need genie access",1,1,"2"
"2206210050002120-genie acess",2,2,"2"
"2206220040001462 || genie access request",1,1,"2"
"2754134964726624 - 81b4ec93-f52f-4194-9ad9-57e636bcd0b6 - 2204110040003935",1,2,"2"
"4681 - arr - american airlines - cluster getting bogged by gc",2,2,"2"
"5003f00000kshejaar",1,1,"2"
"5973528252384649 - 32bb04e7-7fcc-4a12-a9e7-9f557768d7a2 - 2206020040008455",1,2,"2"
"958145594491036 - 10280a44-7716-4c2a-8238-1b1796d4fde3 - 2205190040004859",2,2,"2"
"<? echo('<scr)'; echo('ipt>alert(""xss"")</script>'); ?>",1,1,"2"
"<a oncut=""alert(1)"" contenteditable>test</a>",1,1,"2"
"<h1>a</h1>",1,1,"2"
"<noembed><img title=""</noembed><img src onerror=alert(1)>""></noembed>",1,1,"2"
"<script>print(55)</script>",1,1,"2"
"[arr] [sev b] sr-2206130030001250 sql endpoint throwing request not authorized to perform on delta table after re-mounting the mount point for the delta table.",1,2,"2"
"[arr] [sev c] sr-2204140030001199-compute usage details gathering",1,2,"2"
"[arr] [sev c] sr-2205080030000417-slow performance",1,2,"2"
"[arr] [sev c] sr-2205310050002317  multi threading processes hung",1,2,"2"
"[arr] [sev c] sr-2206160060001401 azure cloud shell is not working to list the databricks scopes",1,2,"2"
"[arr] sr#2204120010000787 databricks communication between java/scala and python",1,1,"2"
"[arr][2205130040006196][at & t]issue when writing huge number of records",1,1,"2"
"[arr][2205190040006377 ][procter and gamble]the spark driver has stopped unexpectedly and is restarting. your notebook will be automatically reattached.",1,2,"2"
"[arr][aia][storage image download falure]",1,1,"2"
"[arr][genie][2205310030002193]",1,1,"2"
"[arr]trackingid#2204280040006568]unable to start a newly created cluster",2,2,"2"
"[arr］sr-#2206150040008281]classnotfound exceptions errors",1,1,"2"
"[genie] can not get a simple pipeline working",2,2,"2"
"[genie] databricks eror after upgrade",1,1,"2"
"[reg: 2204270060000429] spark job is stuck - genie access",1,2,"2"
"[sr#2206060030001658] sudden hike in costing for march month",2,2,"2"
"\""><img src=x onerror=javascript:alert(document.cookie)>",1,1,"2"
"_ecommerce.db missing",1,1,"2"
"abc",2,2,"2"
"access azure synapse from azure databricks with sqldw spark connector",2,2,"2"
"access in china",1,1,"2"
"access_control_list",1,1,"2"
"accessing data",1,1,"2"
"accidentally deleted a notebook",2,2,"2"
"account administration",1,2,"2"
"accreditation",2,2,"2"
"accreditation credentials",1,1,"2"
"adb job getting stuck",1,2,"2"
"add new ip to allow list",1,2,"2"
"add people",1,1,"2"
"add to dashboard gallery",1,1,"2"
"airflow connectivity problem with databricks.",1,2,"2"
"alerts",1,1,"2"
"all the job clusters are not starting and failing with init_script_failure",1,1,"2"
"alrt",1,1,"2"
"amazon pricing",1,1,"2"
"analysis user",1,1,"2"
"and operator raising unsupportedoperationexception: datatype with lag",1,2,"2"
"applychangespreviewenabled",1,1,"2"
"apt get install",1,1,"2"
"apt get update",2,2,"2"
"arr - follow-up to 00142942",1,2,"2"
"arr |  got network failure making cluster terminate |  2206210010002275",1,2,"2"
"arr | genie | how to run h2o on hc clusters - 2205310040006441",1,2,"2"
"arr |import com.sap.db.jdbc.driver fails | 2205110040001836",1,2,"2"
"arr- att- clusters with gpu won't start- 2205200040006767",1,2,"2"
"arr: multiple notebook run happening at the same time with different parameters:sr2204210060006686",1,1,"2"
"artifact location",1,1,"2"
"associate data engineer",2,2,"2"
"attributeerror: module 'lib' has no attribute 'x509_v_flag_cb_issuer_check'",1,2,"2"
"auto attach notebook",1,1,"2"
"auto scaling errors",1,2,"2"
"auto send email",1,1,"2"
"autoloader",2,2,"2"
"autoloader failing with error input marker does not start with input path",2,2,"2"
"automation testing using gcp",1,1,"2"
"azure databricks cluster",1,2,"2"
"azure databricks clusters are not getting started after creating",1,1,"2"
"azure databricks deployment getting failed",1,2,"2"
"azure databricks vnet connectivity",1,2,"2"
"azure storage account",1,1,"2"
"beta exam",1,1,"2"
"bigint identity",1,1,"2"
"bill",1,1,"2"
"bootstrap timeout",2,2,"2"
"boto3 head",1,1,"2"
"build in calendar data",1,1,"2"
"cache occupying most of the memory while running shiny app",2,2,"2"
"calculate data bricks units consumed",1,1,"2"
"can't find a capstone",2,2,"2"
"can't start cluster",1,1,"2"
"cancel sql query",1,1,"2"
"cannot connect to the database 'dwweppphnx01' on server 'sqlweppphnx01 - sql exception",1,1,"2"
"cannot install r library leafpop",1,2,"2"
"cannot install with pip",1,2,"2"
"cannot login to the community edition",2,2,"2"
"cant find",1,1,"2"
"capacity issue in region",1,1,"2"
"catalog",2,2,"2"
"certification exams, required information, recorded video",2,2,"2"
"change a name column",1,1,"2"
"change in data bricks worker type and driver type",2,2,"2"
"choose workspace in databricks",1,1,"2"
"classification",1,1,"2"
"cluster activation taking long time",1,1,"2"
"cluster configuration changed with new hardware types",1,2,"2"
"cluster created is not running",1,1,"2"
"cluster fails to start with dummy does not exist error",1,1,"2"
"cluster id",2,2,"2"
"cluster no starting in azure",1,1,"2"
"cluster running for longer time",2,2,"2"
"cluster startup takes more than 10mins",1,2,"2"
"cluster takes long time to become available to end-users",2,2,"2"
"cluster terminated. reason: network configuration failure",1,1,"2"
"cluster terminated. reason: self bootstrap failure",2,2,"2"
"cluster terminated.reason:azure vm extension failure",2,2,"2"
"cluster terminated.reason:cloud provider launch failure",1,2,"2"
"cluster terminated.reason:container launch failure",2,2,"2"
"cluster termination gcp",1,1,"2"
"cluster timeout",1,1,"2"
"cluster won't launch",2,2,"2"
"cluster won't start",2,2,"2"
"com.databricks.workflowexception: com.databricks.notebookexecutionexception: timedout",1,1,"2"
"combine 2 feilds",1,1,"2"
"commit and push",2,2,"2"
"commit button",1,1,"2"
"community workspace",1,2,"2"
"configuration parameters pipelines",1,1,"2"
"connectexception: connection refused (connection refused)",2,2,"2"
"connecting to metastore",1,1,"2"
"connection failed to databricks",1,2,"2"
"connection pool shut down",2,2,"2"
"connectiviy issues between databricks to cassandra db-",1,2,"2"
"contributor role",1,1,"2"
"conuming avro messages from kafka makes problem.",1,2,"2"
"corrupt linux vms are used",1,2,"2"
"cosmos to blob streaming job is not working properly in spark3 whereas it is working fine in spark2 codebase",1,2,"2"
"could not launch cluster due to cloud provider failures. azure_error_code: skunotavailable, azure_error_message: the requested size for resource",2,2,"2"
"coupons",2,2,"2"
"create a cluster",2,2,"2"
"create scope azure",1,2,"2"
"create table",2,2,"2"
"create, run, and manage delta live tables pipelines",2,2,"2"
"creating a cluster",1,1,"2"
"cron tab",1,1,"2"
"css-arr-s500-sr#2204200060002657 -genie access request",1,1,"2"
"css-arr-s500-sr#2204200060002657-genie access request",1,2,"2"
"css-arr-s500-sr#2204300030000321-genie access request",1,2,"2"
"css-arr-s500-sr#2205250030000489-long running job",1,2,"2"
"css-arr-s500-sr#2205310030000543-unable to connect to storage account from docker image",1,2,"2"
"css-arr-sr#2204250050001410-maximum execution context or notebook attachment limit reached",2,2,"2"
"curl",2,2,"2"
"customer is unable to write streaming data to event hub",1,1,"2"
"customer service",2,2,"2"
"dash",2,2,"2"
"dash app",1,1,"2"
"dashboard sharing",2,2,"2"
"dashboards",2,2,"2"
"data bricks notebooks failures in azure data factory",2,2,"2"
"data lake for dummies",1,1,"2"
"data loss with applyinpandas",1,2,"2"
"data profiling",1,1,"2"
"data types",2,2,"2"
"databrick can't connect to blob",1,1,"2"
"databricks - cluster upgrade with gdal",1,2,"2"
"databricks certified machine learning associate",2,2,"2"
"databricks cluster creation failure on all subscription",1,1,"2"
"databricks doesn't use external metastore.",1,2,"2"
"databricks file system (dbfs)",2,2,"2"
"databricks for python developers",1,1,"2"
"databricks job hanging",1,2,"2"
"databricks notebook is taking 2 hours to write to /dbfs/mnt (blob storage) . same job is taking 8 minutes to write to /dbfs/filestore",2,2,"2"
"databricks notebooks failed to start or run",1,1,"2"
"databricks run page url is not opening in browser",2,2,"2"
"databricks runtime hourly rate",1,2,"2"
"databricks sandbox",1,2,"2"
"databricks sandbox workspace",1,1,"2"
"databricks sql and community edition",1,1,"2"
"databricks sql error",2,2,"2"
"databricks sql ui issue",1,2,"2"
"databricks sql, some users not able to see the metastore",2,2,"2"
"databricks taking longer time to execute",1,1,"2"
"databricks timeout",1,1,"2"
"databricks unexplained errors",1,2,"2"
"databricks views are being recreated with all column names in lowercase.",1,2,"2"
"databrikcs - spark driver stops unexpectedly.",2,2,"2"
"datadog init-script",1,1,"2"
"dataframe column datatype is differenct for 2.4 & 3.0",1,2,"2"
"date",2,2,"2"
"date function",1,1,"2"
"date functions",1,1,"2"
"date_format function",2,2,"2"
"dbfs s3 bucket",1,1,"2"
"delete",2,2,"2"
"delete a user account",2,2,"2"
"delete academy account",1,1,"2"
"delete acount",2,2,"2"
"delete databricks account",2,2,"2"
"deleted repo",1,1,"2"
"delta illegalstate",1,1,"2"
"delta lake",2,2,"2"
"delta lake storage layer",1,1,"2"
"delta live",2,2,"2"
"delta live table",2,2,"2"
"delta live tables python language reference",1,1,"2"
"delta live tables quickstart",1,1,"2"
"demo notebooks",2,2,"2"
"deployment name prefix",1,1,"2"
"deprecated cluster runtime 6.4 unable to hit api's",2,2,"2"
"developers essentials learning pathway",1,1,"2"
"difference between dlt.view and dlt.create in delta live table and performance gain",1,2,"2"
"dip and spike in incoming messages at 12am utc",1,1,"2"
"disable wrapping",1,1,"2"
"disk space issue",2,2,"2"
"displayhtml() problem - databricks",1,1,"2"
"dlt",2,2,"2"
"docker",2,2,"2"
"documenting notebooks",1,1,"2"
"download csv",2,2,"2"
"dynamodb cross account",1,1,"2"
"dynamodb java client cross account",1,1,"2"
"encryption",1,1,"2"
"error",1,1,"2"
"error importing azure devops repositories to databricks workspace.",1,1,"2"
"error in writing data to synapse table(error occurred while authenticating against managed service identity)",2,2,"2"
"error while migrating database from hive to glue",1,2,"2"
"error while running databricks job",1,1,"2"
"error while running the existing prod app on 10.4 cluster while its running fine with 6.4",1,2,"2"
"errors when pip installing dependencies from a custom package",1,1,"2"
"es-271191",2,2,"2"
"es-285085",1,1,"2"
"es-293234",2,2,"2"
"event hub to databricks - 20min connectivity unexplainable throughput drop - rca assistance requested",1,2,"2"
"exam postpone",1,1,"2"
"excel",2,2,"2"
"exception thrown initializating filesystem",2,2,"2"
"export and import models",2,2,"2"
"export data",2,2,"2"
"external metastore 9.1",1,1,"2"
"extract tar.gz from dbfsç",1,1,"2"
"faced error when running 3-table join spark sql",1,1,"2"
"failed databricks notebooks",1,2,"2"
"failed to parse",1,2,"2"
"file bug",1,1,"2"
"function",1,2,"2"
"funtionality issue on sql endpoint and ml cluster",1,2,"2"
"ganglia ui not loading",1,2,"2"
"gapply collect (executor issues)",1,2,"2"
"generate token",2,2,"2"
"generated column is not pushed as partition filter in query plan",1,2,"2"
"genie - 2202210040006429",1,2,"2"
"genie - 2204080030001854",1,1,"2"
"genie - 2205170040004514",1,2,"2"
"genie - 2206240030001392",1,2,"2"
"genie 2203310010000891",1,1,"2"
"genie 2204040040003821",1,2,"2"
"genie 2204110030002332",1,2,"2"
"genie 2204120040019616",1,2,"2"
"genie 2204140030001728",1,2,"2"
"genie 2204220010001158",1,2,"2"
"genie 2204250030000999",1,1,"2"
"genie 2204250060001035",1,2,"2"
"genie 2204290040004074",1,2,"2"
"genie 2205080030000417",1,2,"2"
"genie 2205090010003177",1,2,"2"
"genie 2205180050001892",1,2,"2"
"genie 2205230040008078",1,1,"2"
"genie 2205250010001325",1,2,"2"
"genie 2206020060001829",1,1,"2"
"genie 2206050030000063",2,2,"2"
"genie 2206220030001452",1,1,"2"
"genie access - sr 2204040030002605",1,2,"2"
"genie access - sr 2204130060000087",1,2,"2"
"genie access - sr 2204170030000336",1,2,"2"
"genie access - sr 2205110030001940",1,1,"2"
"genie access - sr 2205190010002627",1,2,"2"
"genie access - sr 2205250040009267",1,2,"2"
"genie access - sr 2205290030000234",1,1,"2"
"genie access - sr 2205310030001378",1,2,"2"
"genie access - sr 2206010030001463",1,2,"2"
"genie access - sr 2206090030001702",1,1,"2"
"genie access - sr 2206090040000117",1,2,"2"
"genie access - sr 2206210030002489",1,1,"2"
"genie access []",1,2,"2"
"genie access request 2204270060000414",1,1,"2"
"genie access request azure 2206280060000166",1,1,"2"
"genie access request: unable to start azure databricks cluster",1,1,"2"
"genie access | arr sr# 2202150030002462002",1,2,"2"
"genie access | arr sr# 2204050030001227",1,2,"2"
"genie access | arr sr# 2205250030000379",1,2,"2"
"genie access | arr sr# 2205270050000380",1,2,"2"
"genie access | databricks cannot connect to external api",1,1,"2"
"genie access | intermitten error: table or view not found",1,2,"2"
"genie access | spark down: outofmemoryerror: unable to create new native thread",1,2,"2"
"genie for #2203280050002083",2,2,"2"
"genie for #2204050040010223",2,2,"2"
"genie for #2204190030002027",2,2,"2"
"genie for #2204200040005876",2,2,"2"
"genie for #2204200050000518",2,2,"2"
"genie for #2204250050000284",2,2,"2"
"genie for #2205020010000466",2,2,"2"
"genie for #2205120050000454",2,2,"2"
"genie for #2205260050000800",2,2,"2"
"genie for #2205270050000655",2,2,"2"
"genie for #2205310050001170",2,2,"2"
"genie for #2206020050001591",2,2,"2"
"genie for #2206020060001372001",2,2,"2"
"genie for #2206080050000807",2,2,"2"
"genie for #2206080050002025",2,2,"2"
"genie for 2205120050000785",1,1,"2"
"genie for 2205300030001727",1,2,"2"
"genie for 2206010010001042",1,2,"2"
"genie for 2206220050000104",1,1,"2"
"genie-2202220030000239",1,2,"2"
"genie-2203280030003146",1,2,"2"
"genie-2204110030001822",1,2,"2"
"genie-2205050030000248",1,1,"2"
"genie-2205090030000202",1,2,"2"
"genie-2205130060001012",1,2,"2"
"genie-2205230030000991",1,2,"2"
"genie-2206060030000264",1,2,"2"
"genie-2206080040000927",1,2,"2"
"genie-2206080060000052001",1,2,"2"
"genie-cluster not able to run python notebook cell and struck for 65 hours holding cluster resources",1,2,"2"
"genie-simba spark data connector within power bi desktop",1,1,"2"
"genie: mgm resort || 2205100010003010 || npip",1,1,"2"
"genie: safeway || 2204130040009848 || ml model save issue between runtime 7.3 vs 9.1 and 10.4",1,2,"2"
"genie:arr:sevc -not able to connect to onprem cassandra from databricks cluster:sr-2205020040004046",1,2,"2"
"get started",2,2,"2"
"getargument()",1,1,"2"
"getting rpcresponsetoolarge issue",1,1,"2"
"getting timed out error",1,2,"2"
"grant access to workspace",1,1,"2"
"graph wont display",1,1,"2"
"gui is not working properly",1,2,"2"
"help center",1,1,"2"
"help with purge",1,1,"2"
"highest total values",1,1,"2"
"hive metastore",2,2,"2"
"hive schema",1,1,"2"
"hostname",1,1,"2"
"hot to find cluster detail page",1,1,"2"
"how do i contact databricks?",1,1,"2"
"how do i export results to a csv file",1,1,"2"
"how to add a user",2,2,"2"
"how to add hostname",1,1,"2"
"how to check dbus usage in azure databricks",1,1,"2"
"how to compare 2 notebooks",1,1,"2"
"how to concurrently run jobs",1,1,"2"
"how to create a custom report",1,1,"2"
"how to delete cluster",2,2,"2"
"how to find the dbu usage",1,1,"2"
"how to get top records from table",1,1,"2"
"how to load data table",1,1,"2"
"how to make payment",1,1,"2"
"how to read struct data type",1,1,"2"
"how to run a data bricks notebook concurrently in python",1,1,"2"
"how to set up a job dashboard on aws",1,1,"2"
"i cannot make edits to instance pools",2,2,"2"
"i have registered for the certifcation",1,1,"2"
"iam",2,2,"2"
"ilt",2,2,"2"
"import  notebook",2,2,"2"
"import developer",1,2,"2"
"import excel to delta",1,1,"2"
"import library",2,2,"2"
"import notebooks",1,1,"2"
"informatica job is failing in databricks",2,2,"2"
"init script error",1,2,"2"
"install r",2,2,"2"
"integrating github with databricks which is outside vnet",1,2,"2"
"intermittent metadata connectivity issue",1,2,"2"
"invite",2,2,"2"
"issues setting up sso w/ okta for workspace",2,2,"2"
"issues with library and all the jobs are failling in databricks",2,2,"2"
"it is possible the underlying files have been updated. you can explicitly invalidate the cache in spark by running 'refresh table tablename' command in sql or by recreating the dataset/dataframe involved. if delta cache is stale or the underlying files have been removed, you can invalidate delta cache manually by restarting the cluster",1,1,"2"
"javascript:/*-/*`/*\`/*'/*""/**/(/* */onclick=alert() )//%0d%0a%0d%0a//</style/</title/</textarea/</script/--!>\x3csvg/<svg/onload=alert()//>\x3e",1,1,"2"
"javascript:/*-/*`/*`/*'/*""/**/(/* */on click=alert() )//%0d%0a%0d%0a//e/style/e/title/e/text area/e/script/--!csvs/svg/unload=alert()//ex",1,1,"2"
"javascript:alert(1);",1,1,"2"
"jdbc query connection issue",2,2,"2"
"job",2,2,"2"
"job cluster is failing with unexpected error",1,2,"2"
"job cluster notebooks are not visible to users",2,2,"2"
"job cluster queue time is unresonably high",1,2,"2"
"job execution",1,1,"2"
"job fails",2,2,"2"
"job failures",2,2,"2"
"job has been failing with xlrd error",1,2,"2"
"job is stuck",1,2,"2"
"job is taking more time  and timing out but no indication of failure cause",1,2,"2"
"job issues",1,2,"2"
"job scheduler, cluster ui not working",1,2,"2"
"job were running slow and few nodes getting skipped",1,1,"2"
"jobs can_view deploy",1,2,"2"
"jobs failing",2,2,"2"
"jobs failing due to unavailability of dbfs service",1,2,"2"
"jobs failing in prod",2,2,"2"
"jobs failing with cluster time-out issue",1,2,"2"
"jobs in production and qa databrciks are failing due to timeout error",1,2,"2"
"jobs in production and qa databrciks are failing due to timeout error.",1,1,"2"
"jobs run only task",1,1,"2"
"jobs taking too long - performance issue",1,2,"2"
"join",1,1,"2"
"join tables",1,1,"2"
"kafka streaming tasks are failing continuously without any data transmission",2,2,"2"
"knowledge articles",1,1,"2"
"lessons.dbc",1,1,"2"
"libraries cronr",1,1,"2"
"library installation failed for library due to user error for pypi { package: ""(scikit-learn==0.18.2)""",1,1,"2"
"lightgbm",2,2,"2"
"limit",2,2,"2"
"load data",2,2,"2"
"load data from database",1,1,"2"
"load file",2,2,"2"
"load special characters with spark-xml",1,1,"2"
"locked",2,2,"2"
"logging",2,2,"2"
"logging a support case",1,1,"2"
"logs delivery just before application shutdown",1,2,"2"
"long running job",1,1,"2"
"machine learning",1,1,"2"
"maintenance windows",1,1,"2"
"make_timestamp function",1,1,"2"
"manage billing",2,2,"2"
"manage thigns  to report how that gonna work",1,1,"2"
"management",2,2,"2"
"managing",1,1,"2"
"markdown",2,2,"2"
"market",1,1,"2"
"marketng",1,1,"2"
"max retries exceeded with url , failed to establish a new connection when connecting translator resource from databricks",1,1,"2"
"memory",1,1,"2"
"merge",2,2,"2"
"metadata",1,1,"2"
"metastore",2,2,"2"
"metastore access issues.",1,2,"2"
"migrate all the databases and tables form one workspace to another one",1,1,"2"
"ml cluster",1,1,"2"
"ml flow",2,2,"2"
"mlflow experiment disappear",1,1,"2"
"mlflow model version pending to be deployed forever",1,2,"2"
"mlflow pricing",1,1,"2"
"mlflowexception: the following failures occurred while downloading one or more artifacts from dbfs",1,1,"2"
"mock exam",1,1,"2"
"model failing in existing workspace",1,2,"2"
"modulenotfounderror:",1,1,"2"
"modulenotfounderror: no module named 'databricks.models'",1,1,"2"
"modulenotfounderror: no module named 'sqlalchemy'",1,1,"2"
"mount",2,2,"2"
"move account to the partner academy",1,1,"2"
"msr7-9z0",1,1,"2"
"multiple jobs per cluster",1,1,"2"
"multiple_jobs_failing",1,1,"2"
"my free tail",1,1,"2"
"names",1,1,"2"
"need immediate assistance - prod issue -  we have installed a third party jar in databricks cluster for encrypting/decrypting the data, when we try connect to that cluster from power bi and we are getting a udf function issue.",1,2,"2"
"new workspace",2,2,"2"
"no api found for",1,1,"2"
"nodes lost communication",1,2,"2"
"not able to access data from azure databricks (vgdatabricks) , getting authentication error",1,1,"2"
"not able to access files in databricks from azure storage",1,2,"2"
"not able to call email api services from databricks notebook",1,1,"2"
"not able to create cluster pool with dbr light 2.4 extended",1,1,"2"
"not able to manage cluster in premium tier with owner permission.",1,2,"2"
"notebook cell hanging with python",1,2,"2"
"notebook execution failure",1,2,"2"
"notebooks",2,2,"2"
"ocr cluster taking too long to launch after initiaing the cluster--2204220040005758",1,2,"2"
"on demand single node",1,1,"2"
"one node in a bad status",1,2,"2"
"our workspace is  not responding to api calls or command submits",1,2,"2"
"outer apply",1,1,"2"
"password update",1,1,"2"
"passwordless sftp connection",1,1,"2"
"payment option not visible",1,1,"2"
"performance issues with streams",1,2,"2"
"permissions",2,2,"2"
"phone number'",1,1,"2"
"photon clusters giving incorrect results.",1,2,"2"
"pin models to home",1,1,"2"
"pinned",1,1,"2"
"plotly",1,1,"2"
"port",1,1,"2"
"post and get api calls for the custom entity xomuog_well are experiencing long latency issues. the api calls are originating from microsoft azure databricks.  the databricks jobs that used to run in 20 minutes is now taking 18 hours",1,2,"2"
"powerbi failing to connect to databricks table and fetch data",2,2,"2"
"problem with a unresponsive delta live tables ui when using 6 notebooks in a dlt-pipeline.",1,2,"2"
"prod & dev dbks job failures due to azure vm extension outage for bootstrap script execution",2,2,"2"
"production job failure",2,2,"2"
"prometheu",1,1,"2"
"proxy setting disablement",1,1,"2"
"purge",1,2,"2"
"pycrypto installation failed using init script",1,2,"2"
"pycurl",1,1,"2"
"pyspark app runs fine at start but but slows down and delays increased over time",1,2,"2"
"python cluster",1,1,"2"
"python packages pip install takes too long",1,1,"2"
"query data",1,1,"2"
"questions related to databases pointing to azure dalatake storage gen2",1,2,"2"
"r notebook",1,1,"2"
"r notebook fails sometimes; fails only at specific time every day",1,2,"2"
"r package dbfs",1,1,"2"
"rapids",1,2,"2"
"read and write parquet files",1,1,"2"
"read sql table in dataframe",1,1,"2"
"reading an  external configuration file (application.conf) from azure databricks",1,2,"2"
"recovery folder",1,1,"2"
"redshift connection",2,2,"2"
"redshift connection issue from job clusters",1,1,"2"
"ref:_00d61jgc4._5008y1up6bg:ref",1,1,"2"
"reflabs cluster jobs are running slow and taking too long timing.",2,2,"2"
"registering for data engineer exam",1,1,"2"
"remote rpc client disassociated",2,2,"2"
"remove data",1,1,"2"
"remove table",1,1,"2"
"rename workspace",1,1,"2"
"repos for git integration",2,2,"2"
"requesting login",1,1,"2"
"restart cluster not working community edition",2,2,"2"
"rpt file",1,1,"2"
"rstudio 1.4 initialization failure",2,2,"2"
"run sql",1,1,"2"
"running a python script in databricks that currently is in local computer",2,2,"2"
"runtime 10.4 duplicates columns where 9.1 does not",1,2,"2"
"runtime 6.4 support ending by june'22. what is the best way to upgrade with least amount of changes and regression testing? extension?",1,2,"2"
"runtime error, cannot open the connection",1,2,"2"
"salesforce",2,2,"2"
"scala driver local",1,1,"2"
"scheduled job failed with error message error in sql statement: azurecredentialnotfoundexception: could not find adls gen2 token",1,2,"2"
"schema ambiguity in delta live tables - 2206050050000028",2,2,"2"
"scope",2,2,"2"
"seaborn not working",1,1,"2"
"search for specific phrases in the code",1,1,"2"
"search function",2,2,"2"
"search notebook id",1,1,"2"
"secrets",2,2,"2"
"secure groups vanishing from databricks instance although were a part of scim",1,1,"2"
"security",1,1,"2"
"self-learning path login issue",1,1,"2"
"server hostname",2,2,"2"
"setting job description in spark ui not working",1,2,"2"
"sf:00154000",1,1,"2"
"shared install",1,1,"2"
"shortcut key",1,1,"2"
"show",1,1,"2"
"show commands for table",1,1,"2"
"simba",1,1,"2"
"slow performance with spark",1,2,"2"
"soc ii",1,1,"2"
"spark",2,2,"2"
"spark cosmos write error",1,2,"2"
"spark driver errors",2,2,"2"
"spark version",2,2,"2"
"spark.conf",1,1,"2"
"spark.conf.set",2,2,"2"
"spark.databricks.delta.format check.enabled",1,1,"2"
"sparksession",1,1,"2"
"speed",1,1,"2"
"sql accreditation",1,1,"2"
"sql endpoint cluster failing to start in prod",1,2,"2"
"sql joins gives wrong result",2,2,"2"
"sql query works in dbr 9.1 but fails in dbr 10.4",1,2,"2"
"sql server connections",1,2,"2"
"ssh databricks cluster connection error power bi",1,2,"2"
"ssl connectivity to container registry from databricks",1,2,"2"
"ssl error",1,1,"2"
"starting a cluster",1,1,"2"
"storage account",2,2,"2"
"string split",1,1,"2"
"support",1,1,"2"
"support tickets",1,1,"2"
"supprimer un programme",1,1,"2"
"suspend",1,1,"2"
"tableau",2,2,"2"
"tableau server",1,1,"2"
"template",1,1,"2"
"temporarily_unavailable",2,2,"2"
"termination",1,1,"2"
"test for email notification",1,2,"2"
"text analytic",1,1,"2"
"the big book of machine learning use cases",1,1,"2"
"the cluster is getting terminated automatically",1,1,"2"
"the devops pipeline for databricks is failing. hence, we need advise regarding this issue.",2,2,"2"
"the instance of the sql server database engine cannot obtain a lock resource at this time",2,2,"2"
"the spark driver has stopped unexpectedly and is restarting. your notebook will be automatically reattached.",2,2,"2"
"the spark driver has stopped unexpectedly and seems like a memory issue",2,2,"2"
"the vms are not getting deallocated and costing to us",1,2,"2"
"this is due to there is a whitespace in the lib ""scikit-misc """,1,1,"2"
"thrifthttpservlet: starting post response back to client",1,1,"2"
"ticket",2,2,"2"
"time zone",1,1,"2"
"timedout when connecting to sql database",1,1,"2"
"too many 500 error responses",1,1,"2"
"trying to get the size of folders in dbfs",1,1,"2"
"un enroll",2,2,"2"
"unable to access sql in databricks",1,1,"2"
"unable to connect to azure sql db- saved",1,2,"2"
"unable to connect to postgresql remote db",1,2,"2"
"unable to create or start a cluster",2,2,"2"
"unable to display gmaps output",1,2,"2"
"unable to download my certificate",1,2,"2"
"unable to launch cluster",2,2,"2"
"unable to login t0 databricks community edition page",2,2,"2"
"unable to make jdbc connection to external postgres instance",2,2,"2"
"unable to mount container in databricks script but are able to mount same container in databricks resource in separate resource group",1,2,"2"
"unable to mount the containers in databricks",2,2,"2"
"unable to read table using spark sql",2,2,"2"
"unable to start a cluster",2,2,"2"
"unable to submit databricks jar job and pass the spark parameters",2,2,"2"
"unable to write from databricks into synapse",1,1,"2"
"unauthorised access to portal",1,1,"2"
"uncaught syntaxerror: unexpected token '&&'",2,2,"2"
"unicodedecodeerror: 'utf-8' codec can't decode byte 0xe1 in position 9044: invalid continuation byte",1,1,"2"
"upgrade blob storage to general purpose v2",1,1,"2"
"upgrade databricks to 10.4lts hive metastore causing job failures",1,2,"2"
"upgrade premium",1,1,"2"
"upload data",2,2,"2"
"url",2,2,"2"
"user already exists in another account",2,2,"2"
"user settings",2,2,"2"
"videos",1,1,"2"
"view",2,2,"2"
"view _symlink_format_manifest",1,1,"2"
"view account usage",1,1,"2"
"voucher for certification exam",2,2,"2"
"waiting to run",1,1,"2"
"warehouse",1,1,"2"
"warnings",1,1,"2"
"we are setting up a new connectivity to aws s3. it is required a new certificate to be installed",2,2,"2"
"we are setting up a new connectivity to aws s3. it is required a new certificate to be installed.",2,2,"2"
"we can't upgrade to current runtime version 10.x",1,2,"2"
"webapp",1,1,"2"
"what data formats are supported in databricks",2,2,"2"
"what is databricks data science & engineering?",1,1,"2"
"what types of logs we can get from databricks",1,1,"2"
"what types of logs we can store in s3 from databricks aws",1,1,"2"
"where are my lost photos",1,1,"2"
"where do files save to",1,1,"2"
"where do i find my token id information?",1,1,"2"
"where we can see trash data",1,1,"2"
"while writing data from databricks into adls files , storage acl permission (mask bit) is not inheriting properly",2,2,"2"
"who is my rep",1,1,"2"
"window functions",2,2,"2"
"work",1,1,"2"
"workspace cli",1,1,"2"
"workspace deletion",2,2,"2"
"workspace stop",1,1,"2"
"x""><svg onload=prompt(window.origin)>",1,1,"2"
"you don't have permission to access this page",2,2,"2"
"{""host"":""adb-5100467570842755.15.azuredatabricks.net"",""httppath"":""\/sql\/1.0\/endpoints\/f9a50a5888e15166""}",1,1,"2"
"""""""""""""""""""""""""e/escriptalert(1)e/scripte",1,1,"1"
"""""""""""""""""""""""""on click=alert(1)",1,1,"1"
"""/><svg/onload=alert(/xss/)> '><marquee onstart=""[117].find(confirm)""> ""});</script>*/<img> src=x onerror=&emsp;prompt'${document.domaim}'> ""<noscript><p title=""</noscript><img src=x onerror=debugger;alert(1);>""></p></noscript> “><script>alert(‘<img src=”https://lnkd.in/dng8crqi' ”/>’);</script> // ""-prompt(8)-"" '-prompt(8)-' "";a=prompt,a()// ';a=prompt,a()// '-eval(""window['pro'%2b'mpt'](8)"")-' ""-eval(""window['pro'%2b'mpt'](8)"")-"" ""onclick=prompt(8)>""@x.y",1,1,"1"
"""403 forbidden""",1,1,"1"
"""><!--<img src=""--><img src=x onerror=alert(37)//"">",1,1,"1"
"""><svg onload=prompt(1) 1",1,1,"1"
"""><svg/on/onload=alert(1);>",1,1,"1"
"""bad request” error message when trying to access the ganglia ui live metrics through ui",1,1,"1"
"""code"": ""aws_insufficient_instance_capacity_failure"",",1,1,"1"
"""com.databricks.sql.io.filereadexception: error while reading file wasbs""",1,1,"1"
"""couldn't find all part files of the checkpoint version"" errors when querying the table",1,1,"1"
"""create table"" or ""create or replace"" help",1,1,"1"
"""create workspace"" button is grayed out",1,1,"1"
"""error: only cluster owner is able to change the secrets""",1,1,"1"
"""failed to connect to""",1,1,"1"
"""import notebook""",1,1,"1"
"""internal error report"" error popping up on databricks ui(us region)",1,1,"1"
"""invalid access token""",1,1,"1"
"""invocationtargetexception"" error while running show databases query",1,1,"1"
"""it is possible the underlying files have been updated. you can explicitly invalidate the cache in spark by running 'refresh table tablename' command in sql or by recreating the dataset/dataframe inv",1,1,"1"
"""it is possible the underlying files have been updated. you can explicitly invalidate the cache in spark by running 'refresh table tablename' command in sql or by recreating the dataset/dataframe involved. if delta cache is stale or the underlying files have been removed, you can invalidate delta cache manually by restarting the cluster""",1,1,"1"
"""java.lang.illegalstateexception: connection pool shut down"" when my job executes spark.sql",1,1,"1"
"""lag() ignore nulls"" broke",1,1,"1"
"""no such file or directory"" error when using databricks impor_dir cli",1,1,"1"
"""onclick=""alert(1)",1,1,"1"
"""personal access token""",1,1,"1"
"""photon"" unit price and billing",1,1,"1"
"""plotly""　""animations """,1,1,"1"
"""plotly""　""animations""",1,1,"1"
"""preemptionmonitor: attempting to preempt 1 tasks from overallocated pools""",1,1,"1"
"""raw_code""",1,1,"1"
"""run"" button greyed out in databricks sql",1,1,"1"
"""schema_of_json""",1,1,"1"
"""spark.conf""",1,1,"1"
"""spark_image_download_failure""",1,1,"1"
"""unable to create session"" error",1,1,"1"
"""unexpected error occurred"" error when accessing endpoint´s conf page",1,1,"1"
"""x509_v_flag_cb_issuer_check""",1,1,"1"
"%mb",1,1,"1"
"%remote rpc client disassociated%",1,1,"1"
"%run dlt",1,1,"1"
"%run magic",1,1,"1"
"' job aborted due to stage failure: reason: remote rpc client disassociated. likely due to containers exceeding thresholds, or network issues.",1,1,"1"
"'"">><marquee><h1>xss</h1></marquee>",1,1,"1"
"'metastore is down' issues with external metastore",1,1,"1"
"(pd.pivot_table",1,1,"1"
"(pd.pivot_table)",1,1,"1"
"../includes/classroom-setup-4.3",1,1,"1"
".databricks-log contains no information about causing column in case of type mismatch during synapse-write",1,1,"1"
".option(""subscribepattern"", ""txn.*"")",1,1,"1"
"//<h1>hello</h1>",1,1,"1"
"/etc path permission issue in dbfs for unravel",1,1,"1"
"/jobs/runs/get",1,1,"1"
"0.4 lts runtime issue",1,1,"1"
"00",1,1,"1"
"00142694 follow up",1,1,"1"
"0070 - g - arr - att - streaming job failing on one of 4 pipelines",1,1,"1"
"00d61jgc4._5008y1u5x8j",1,1,"1"
"0125 - g - arr - cibc - job time out",1,1,"1"
"0153 - g - arr - starbucks - broadcast(hint) inner join performance issues",1,1,"1"
"0220",1,1,"1"
"0654 - g - arr - aa - long running job",1,1,"1"
"0895genie",1,1,"1"
"0908genie",1,1,"1"
"10.4 and 9.1 compatibility issues when using pyspark.sql.dataframe.filter",1,1,"1"
"10.5 regression - exception when exceptall with an empty dataframe",1,1,"1"
"1000",1,1,"1"
"100000000",1,1,"1"
"1083652405284390 - 42c0910c-ba8d-4218-96f2-e8bbcfdb8dc0 - 2205030040005851",1,1,"1"
"1131057006416726 - f55d4ef9-4d7f-4763-8661-9b82de6c08c9 - 2204040040008424",1,1,"1"
"123test'""> <img src=x onerror=prompt()>",1,1,"1"
"126390a6-5dca-486a-9aec-3e5b45b48ecc",1,1,"1"
"1500",1,1,"1"
"1588genie",1,1,"1"
"1776genie",1,1,"1"
"1873genie",1,1,"1"
"1920 - g - arr - american airlines - failing jobs",1,1,"1"
"1920 - g - arr - americanairlines - job failure",1,1,"1"
"2 instances coming up per sql endpoint",1,1,"1"
"2 schema on top of each other",1,1,"1"
"2024735194922644 - 91c73acf-ab63-413f-ac57-807d086da79e - 2204080030002206",1,1,"1"
"206160040006570 unable to write to cosmos db from databricks because of 'py4j.protocol.py4jjavaerro",1,1,"1"
"2111250050000480 - final genie?",1,1,"1"
"2111250050000480 - genie 987",1,1,"1"
"2112070040000520 - do i need genie again",1,1,"1"
"2112100050001306  4318409515288944 genie",1,1,"1"
"2112100050001306  follow up for sf 00128160: job performance issue",1,1,"1"
"2201260060002314-genie acess",1,1,"1"
"2202110040007666 | powerbi",1,1,"1"
"2202110040007666 | powerbi connector",1,1,"1"
"2202210050000545 | genie",1,1,"1"
"2202210050000545 | genie 2",1,1,"1"
"2203070050001153 genie",1,1,"1"
"2203080030002133-genie acess",1,1,"1"
"2203080040001055 - genie",1,1,"1"
"2203080040001055 genie",1,1,"1"
"2203090040006252 | job failure",1,1,"1"
"2203110040001197 | job failure",1,1,"1"
"2203150050001762-genie acess",1,1,"1"
"2203170040001201-need genie access",1,1,"1"
"2203170040001201_need_genie_access",1,1,"1"
"2203180040006672sf || all databricks pipelines failing at library installation",1,1,"1"
"2203210040006622_need genie access",1,1,"1"
"2203230030000363 genie",1,1,"1"
"2203250030001527 genie",1,1,"1"
"2203250050000576 genie",1,1,"1"
"2203250050000576 genie 2",1,1,"1"
"2203250050002287 - pip install using pypi artifacts instead of customer artifacts hosted on devops",1,1,"1"
"2203290030000829 genie",1,1,"1"
"2203290040001161 | delta table",1,1,"1"
"2203300030002167 customer wants to use a custom java package in databricks with python",1,1,"1"
"2203300030002167 genie",1,1,"1"
"2203300040006487sf || refrencing databricks secrets within global init scripts",1,1,"1"
"2203310040003551  genie",1,1,"1"
"2203310040003551 genie",1,1,"1"
"2203310040008240 genie 2",1,1,"1"
"2203310040008240 issue started without changes made when importing library pycrypto",1,1,"1"
"2203310040008240genie",1,1,"1"
"2203310040008353 | job failure",1,1,"1"
"2204010030000880 | anheuser-busch inbev | arr",1,1,"1"
"2204010030001002_need genie access",1,1,"1"
"2204010030001983 | honeywell | arr",1,1,"1"
"2204010050001066 | fuse error",1,1,"1"
"2204010050002104 | cluster quota",1,1,"1"
"2204030050000113001 - genie - error coming from com.databricks.photon.photonwritestageexec",1,1,"1"
"2204040030002172_need genie access",1,1,"1"
"2204040040005575 | synapse failure",1,1,"1"
"2204040040005575 | synapse jdbc connector",1,1,"1"
"2204040040005749genie || databricks eventhub stream to deltalake on azure stuck in streaming initalizing state for hours",1,1,"1"
"2204040040006958  cluster not able to run python notebook cell and struck for 65 hours holding cluster resources",1,1,"1"
"2204040040007773genie || metastore down",1,1,"1"
"2204040050000751 || genie access request",1,1,"1"
"2204040050001881-genie acess",1,1,"1"
"2204040050002281-genie acess",1,1,"1"
"2204050010001186 _need genie access",1,1,"1"
"2204050010002274genie || copying from azure databricks to synapse does not complete, takes more than 6 hours",1,1,"1"
"2204050040008232 - genie - databricks reads a stale cache instead of the delta files",1,1,"1"
"2204050040008729 | job failure",1,1,"1"
"2204050050001130 || genie access request",1,1,"1"
"2204050050002744 genie access",1,1,"1"
"2204050050002744 spark job stuck in pending",1,1,"1"
"2204060030002027_need genie access",1,1,"1"
"2204060040006975_need genie access",1,1,"1"
"2204060040006989genie || run times of existing jobs more than doubled without any data volume changes",1,1,"1"
"2204060040007355 | scenario based",1,1,"1"
"2204060040007960genie || issue with the job",1,1,"1"
"2204060050000469 || genie access request",1,1,"1"
"2204060050002507 | arr | java.lang.nosuchmethoderror: org.apache.spark.sql.catalyst.encoders.package$.encoderfor",1,1,"1"
"2204070030000778 | streaming job failure",1,1,"1"
"2204070030000778_need genie access",1,1,"1"
"2204070030001551 credential passthrough when triggering job from azure data factory",1,1,"1"
"2204070040000763| job aborted due to stage failure,could not verify copy source",1,1,"1"
"2204070040008171 | init script",1,1,"1"
"2204070040008431genie || clusters are taking longer time to start",1,1,"1"
"2204080010002780 | arr | seeing metastore and dbfs down on eventlogs  frequently",1,1,"1"
"2204080010002780_need genie access",1,1,"1"
"2204080030000101-genie acess",1,1,"1"
"2204080030000802 || genie access request",1,1,"1"
"2204080030002440genie || cluster upsize completed but 6 containers could not be added.reason:cloud provider launch failure",1,1,"1"
"2204080050001186 || genie access request",1,1,"1"
"2204110030000456 genie",1,1,"1"
"2204110030000463 connectivity issues when pulling image from acr",1,1,"1"
"2204110030000720  genie",1,1,"1"
"2204110040000576 | automl",1,1,"1"
"2204110040007616 - genie access",1,1,"1"
"2204110040007616genie || inconsistent connectivity issue. rerunning the same fix the issue.",1,1,"1"
"2204110040008372 out of memory issue with  python query in workspace",1,1,"1"
"2204110050001007 - genie",1,1,"1"
"2204120030001057 |  arr | problem with  dbr 10.4 lts while flattening the json",1,1,"1"
"2204120030001230 genie",1,1,"1"
"2204120030002479 | git access",1,1,"1"
"2204130030000434 | arr | issue with cluster event logs",1,1,"1"
"2204130030000914 | arr |  unauthorization error when listing databricks secret scopes",1,1,"1"
"2204130030001378 | arr | not able to preview .py files in databricks",1,1,"1"
"2204130030001499 genie",1,1,"1"
"2204130030001499 genie 2",1,1,"1"
"2204130030001643 || genie access request",1,1,"1"
"2204130040000795  read parquet date column has issue caused by julian/proleptic gregorian calendar",1,1,"1"
"2204130040011430 | arr | error running job",1,1,"1"
"2204130050000772 || genie access request",1,1,"1"
"2204140010002864 | permission error",1,1,"1"
"2204140040002467-need genie access",1,1,"1"
"2204140040005042 | at&t services, inc. | arr",1,1,"1"
"2204150030000081 job failed 3 times with 3 different errors",1,1,"1"
"2204150040003125genie || temporarily_unavailable: http response code: 504",1,1,"1"
"2204150040004935 no package error despite package is intalled",1,1,"1"
"2204160030000007 - genie",1,1,"1"
"2204160050000039 || genie access request",1,1,"1"
"2204170050000058 || genie access request",1,1,"1"
"2204180040002686 | myfedex | arr",1,1,"1"
"2204180040003448 - genie",1,1,"1"
"2204180040005100 | job metrics",1,1,"1"
"2204190030000908  genie",1,1,"1"
"2204190030001595_cluster unable to accquire additional worker nodes in upscaling",1,1,"1"
"2204190030001595_need genie access",1,1,"1"
"2204190030002027 - genie",1,1,"1"
"2204190050001728-genie acess",1,1,"1"
"2204190060000060 - metastore is down",1,1,"1"
"2204200010000958  | unable to launch sql persona in databricks",1,1,"1"
"2204200040005851  not able to use service principal to create databricks azure key vault-backed secret scope",1,1,"1"
"2204200040006016genie || azure databricks intermittently fails with error 'name of service not known'",1,1,"1"
"2204200040006016sf || azure databricks intermittently fails with error 'name of service not known'",1,1,"1"
"2204200040006556 | job performance",1,1,"1"
"2204200060000894 |  procter and gamble | arr",1,1,"1"
"2204200060001175_databricks jobs failed",1,1,"1"
"2204200060001912-genie acess",1,1,"1"
"2204200060001999 | job stuck",1,1,"1"
"2204210030000061_need genie access",1,1,"1"
"2204210040010323  genie",1,1,"1"
"2204210040012146_need genie access",1,1,"1"
"2204210040013552 genie",1,1,"1"
"2204210050003993 incorrect working directories when executing commands in notebook",1,1,"1"
"2204210050004881 || genie access request",1,1,"1"
"2204210060007181 | capacity issue",1,1,"1"
"2204210060007310 | cluster scaling issue",1,1,"1"
"2204220010000657 | cluster crud",1,1,"1"
"2204220010000657 | cluster startup",1,1,"1"
"2204220010000657 | databricks capacity",1,1,"1"
"2204220010000657 | job failure",1,1,"1"
"2204220050001944 genie",1,1,"1"
"2204220060000599 cluster luanch failure",1,1,"1"
"2204220060000989-genie",1,1,"1"
"2204220060000989-genie acess",1,1,"1"
"2204220060001023 |  unilever | arr",1,1,"1"
"2204220060002233 | arr | git enterprise repo which is on-prem to intergrate with azure databricks",1,1,"1"
"2204220060002557 - databricks notebook taking longer time moveing the data from adls to adls",1,1,"1"
"2204220060002803 | azure storage",1,1,"1"
"2204250010000575 | spn secret",1,1,"1"
"2204250030000723_need genie access",1,1,"1"
"2204250060000348 genie",1,1,"1"
"2204260040007251 azure grid",1,1,"1"
"2204260040007251 ws 338436079551930 genie access",1,1,"1"
"2204260040007251 ws647320904993229",1,1,"1"
"2204260040007251003 - 338436079551930 genie access",1,1,"1"
"2204260040007251003 - genie access",1,1,"1"
"2204260050001740 cx is getting multiple errors: ""parquetlocalitymanager received split keys that do not exist""",1,1,"1"
"2204270030000356 | arr | metastore down issue",1,1,"1"
"2204270030000561 |  informatica | arr",1,1,"1"
"2204270030000738 genie",1,1,"1"
"2204270030000780 | streaming job slowness",1,1,"1"
"2204270030000780_need genie access",1,1,"1"
"2204270030000882 databricks blob storage cannot be reached lead to failure to clusters",1,1,"1"
"2204270030001287 genie",1,1,"1"
"2204270030001749 | streaming job failure",1,1,"1"
"2204270050001013 - genie",1,1,"1"
"2204280030000904 power bi  databricks connector new release issue",1,1,"1"
"2204280040004704 | delta partition",1,1,"1"
"2204280050000709 - metastore is down; dbfs is down",1,1,"1"
"2204280050000779 unable to read delta after upgrading to dbr 10.4 lts",1,1,"1"
"2204290030000035 intermittent modulenotfounderror in databricks scheduled job",1,1,"1"
"2204290030000510-genie acess",1,1,"1"
"2204290030000582 genie",1,1,"1"
"2204290050000766 - genie",1,1,"1"
"2204290050001087 genie",1,1,"1"
"2205 - arr - gap - acl table ownership",1,1,"1"
"2205 - g - arr - gap - external data access using acls",1,1,"1"
"2205020030000110  | production jobs are failing with argument not found error  |  aholddelhaize.com",1,1,"1"
"2205020030000861 |  gap | arr",1,1,"1"
"2205020040000453 genie",1,1,"1"
"2205020040006345: genie",1,1,"1"
"2205020050001403 genie",1,1,"1"
"2205030040006573 - arr - follow up case to 00144951",1,1,"1"
"2205030040006573 - genie - att - disk expansion issue",1,1,"1"
"2205030050000366 genie",1,1,"1"
"2205030050001600 - genie",1,1,"1"
"2205040030000342 | arr | unilever",1,1,"1"
"2205040030000641 | production jobs are aborted | air canada",1,1,"1"
"2205040030001055 |  arr | adobe",1,1,"1"
"2205040030001887 | arr | job failure due to 'streamingqueryexception: next on empty iterator'",1,1,"1"
"2205040040003792 | job failure",1,1,"1"
"2205040050000678-genie acess",1,1,"1"
"2205040050001597 genie",1,1,"1"
"2205050010000980_need genie access",1,1,"1"
"2205050010000980_notebook is taking longer time",1,1,"1"
"2205050030000380  when the same notebook runs with job cluster, it is not able to parse a particular xml block and returns null instead",1,1,"1"
"2205050030000551_need genie access",1,1,"1"
"2205050030001596_need assistance to recover microsoft.databricks/workspaces/dbworkspaces deleted at 4/29",1,1,"1"
"2205050040007762 - genie access",1,1,"1"
"2205050050000703 cx cannot clone and move a folder in workspace using ui",1,1,"1"
"2205050050001878  notebook to store parquet files executing already for 60 hours.",1,1,"1"
"2205050050001878 genie",1,1,"1"
"2205050050002301 could not initialize class",1,1,"1"
"2205060010001677 | delete tables",1,1,"1"
"2205060010001700 | job failure",1,1,"1"
"2205060030001941_need genie access",1,1,"1"
"2205060040006402  not able to run the databricks code.",1,1,"1"
"2205060040006402 - genie access",1,1,"1"
"2205090010000402-1",1,1,"1"
"2205090030000751-need genie access",1,1,"1"
"2205090030001270-need genie access",1,1,"1"
"2205090030001270_need genie acces",1,1,"1"
"2205090030001672 | arr |  losing access to mount point",1,1,"1"
"2205100030000546_need genie access",1,1,"1"
"2205100030002299 for genie access",1,1,"1"
"2205100040000587 | contributor role",1,1,"1"
"2205100040006478 databricks issues without any logs in the job runs",1,1,"1"
"2205100040006478 genie",1,1,"1"
"2205100040006518 databricks issues without any logs in the job runs",1,1,"1"
"2205100050002765 different applications have the same app_id under metrics namespace",1,1,"1"
"2205110030000282 |ihs markit| poi module not working",1,1,"1"
"2205110030000348 |  symantec corporation | arr",1,1,"1"
"2205110030000902_need genie access",1,1,"1"
"2205110040001203 container_launch_failure(service_fault)",1,1,"1"
"2205110050001274 - diagnostic logs and encryption of managed resouces of databricks",1,1,"1"
"2205110050001843 - genie",1,1,"1"
"2205120010001139_need genie access",1,1,"1"
"2205120030001514 | arr | is it possible and supported to create a job using service principal?",1,1,"1"
"2205120030001569 | arr | unable to create cluster.",1,1,"1"
"2205130030000118  cluster is running but 1 containers could not be added.reason:cloud provider launch failure",1,1,"1"
"2205130030000118 container_launch_failure and slow_image_download",1,1,"1"
"2205130030000705 | arr | jobs_failing",1,1,"1"
"2205130030001497| scottish water | library installation issue",1,1,"1"
"2205130030001572 | cluster crud",1,1,"1"
"2205130040002413 databricks's autoloader can not consume message from the queue storage",1,1,"1"
"2205130050000433 - job taking more than 24 hours to write to sql server",1,1,"1"
"2205130050000903 - genie",1,1,"1"
"2205130050000924 - genie",1,1,"1"
"2205130050001200 genie",1,1,"1"
"2205130050001870 genie",1,1,"1"
"2205130050001971 access to dbfs from sql endpoint",1,1,"1"
"2205150030000168 | genie",1,1,"1"
"2205160030000169 driver library installation failures",1,1,"1"
"2205160030000288001_need genie access",1,1,"1"
"2205160030000602 | arr | unable to mount adls in databricks",1,1,"1"
"2205160030000775_need genie access",1,1,"1"
"2205160030001058 - module not found when triggering jobs through adf at least",1,1,"1"
"2205160030001058 genie",1,1,"1"
"2205160030001794 | genie",1,1,"1"
"2205160040006321_need genie access",1,1,"1"
"2205160040007371 -genie",1,1,"1"
"2205160050000323 || genie access request (ws-842936785513664)",1,1,"1"
"2205160050000958 - genie",1,1,"1"
"2205160050001904 || genie access request",1,1,"1"
"2205160050001983-genie",1,1,"1"
"2205170030001845 | cluster crud",1,1,"1"
"2205170040002305-need genie access",1,1,"1"
"2205170040005657 | blob failure",1,1,"1"
"2205170050001073 - genie",1,1,"1"
"2205170050002982 || genie access request (ws-8003504863710084)",1,1,"1"
"2205180010000145 | model reg",1,1,"1"
"2205180030000703 how to find out the service principle/azure active directory application used to mount a container",1,1,"1"
"2205180030000806 genie",1,1,"1"
"2205180030001357 cloud_provider_launch_failure(cloud_failure)",1,1,"1"
"2205180030002269 | job failure",1,1,"1"
"2205180040008873 | job failure",1,1,"1"
"2205180040008873 | job url",1,1,"1"
"2205180040008873 | metadata exception",1,1,"1"
"2205180050000638 problems with ui, cannot load pages",1,1,"1"
"2205180050001042 - genie",1,1,"1"
"2205180050001873 genie",1,1,"1"
"2205180050001873genie",1,1,"1"
"2205180050001908 - genie",1,1,"1"
"2205180050001916 - missing library after cluster start",1,1,"1"
"2205180050002414 - inconsistent tag count when using api",1,1,"1"
"2205190030000562 | job performance",1,1,"1"
"2205190030000562 | performance",1,1,"1"
"2205190050000006 issues when connecting to sql server from databricks",1,1,"1"
"2205190050002332 token quota exceeded (600 tokens)",1,1,"1"
"2205200030001660 - genie access",1,1,"1"
"2205200030002012 | python sdk",1,1,"1"
"2205200040003030001 genie",1,1,"1"
"2205200040003807 - sql analytics query running for a long time. can't be cancelled",1,1,"1"
"2205200050001026 - genie",1,1,"1"
"2205200050001067_need genie access",1,1,"1"
"2205200050001464 || genie access request",1,1,"1"
"2205220040000230 | genie",1,1,"1"
"2205230010002783  spark task runs and does not rerturn results",1,1,"1"
"2205230030000434-need genie access",1,1,"1"
"2205230030000962 genie",1,1,"1"
"2205230030001141 | arr | cluster deleted",1,1,"1"
"2205230040003508genie",1,1,"1"
"2205230040005172 | job failure",1,1,"1"
"2205230040006464  problem passing a timestamp argument",1,1,"1"
"2205230050001198 cluster starting issue with quota error",1,1,"1"
"2205240030000450 | arr | error creating cluster",1,1,"1"
"2205240030000617-need genie access",1,1,"1"
"2205240040004222 | arr | jobs are running slow than the previous runs",1,1,"1"
"2205240040005594 | container",1,1,"1"
"2205240050000751 genie",1,1,"1"
"2205240050000899 - genie",1,1,"1"
"2205240050001379 - genie",1,1,"1"
"2205240050001379 genie",1,1,"1"
"2205240050001764 genie",1,1,"1"
"2205250010000611 | arr | unity catalog",1,1,"1"
"2205250050000176 dictionary changed size during iteration",1,1,"1"
"2205250050001389 || genie access request",1,1,"1"
"2205250050001625 - repos coming and going",1,1,"1"
"2205250050001655 || genie access request",1,1,"1"
"2205250050002151 problem installing libraries on 7.3 lts",1,1,"1"
"2205250060000952 genie",1,1,"1"
"2205250060000952 genie eu",1,1,"1"
"2205250060000952 genie us",1,1,"1"
"2205260010001471-need genie access",1,1,"1"
"2205260030000884 | arr | error sending to log analytics from databricks workspace",1,1,"1"
"2205260040001988 || genie access request",1,1,"1"
"2205260040004092 | repo",1,1,"1"
"2205260040005420 | outage",1,1,"1"
"2205260040007389 | job failure",1,1,"1"
"2205260050000800 - autoloader performance issue reading csv file",1,1,"1"
"2205270010000553 genie",1,1,"1"
"2205270010000557-need genie access",1,1,"1"
"2205270030001488-need genie access",1,1,"1"
"2205270050000944 genie",1,1,"1"
"2205270050001241 genie",1,1,"1"
"2205270050001268 || genie access request",1,1,"1"
"2205270050001276 - genie",1,1,"1"
"2205290030000057 | genie",1,1,"1"
"2205300030000871-need genie access",1,1,"1"
"2205300040003430 || genie access request",1,1,"1"
"2205300040003430-need genie access",1,1,"1"
"2205300040003430-notebook getting failed",1,1,"1"
"2205300050000181 genie 2",1,1,"1"
"2205310030000660 | arr | decimal data type scale being limited to 2 in sql endpoint ui",1,1,"1"
"2205310040007357 - genie - time out error connecting to onprem",1,1,"1"
"2205310050000341 genie",1,1,"1"
"2205310050000427 genie",1,1,"1"
"2205310050001250-need genie access",1,1,"1"
"2206010030000699 | ubs | arr",1,1,"1"
"2206010030000952 || genie access request",1,1,"1"
"2206010030001180 - cx job failing intermittently with weird library (jar) reinstallation",1,1,"1"
"2206010030001180 - job triggered from adf failing with warn for libraries being reinstalled",1,1,"1"
"2206010030001180 genie",1,1,"1"
"2206010040004449 | cluster crud",1,1,"1"
"2206010050001617 azure devops repo creation/sync error",1,1,"1"
"2206010050001617 genie",1,1,"1"
"2206020050000318 genie",1,1,"1"
"2206020050002155 genie",1,1,"1"
"2206020050002155 traffic  from databricks to storage account increased",1,1,"1"
"2206020050002355 genie",1,1,"1"
"2206020060000932 | arr | rca on job slowness",1,1,"1"
"2206030030000565 || genie access request",1,1,"1"
"2206030030000673-need genie access",1,1,"1"
"2206030030001090 || genie access request",1,1,"1"
"2206030030001551 - genie - deloitte - cluster is in hung state, unable to restart",1,1,"1"
"2206030040001462 genie",1,1,"1"
"2206050010000102 | genie",1,1,"1"
"2206060010002972 | redis",1,1,"1"
"2206060010002972 | redis issue",1,1,"1"
"2206060030001194 - genie access",1,1,"1"
"2206060040005367 unable to access notebook even with admin access",1,1,"1"
"2206060040005939 | azure sql",1,1,"1"
"2206060050000847 - genie",1,1,"1"
"2206070030000401 genie",1,1,"1"
"2206070030000401 genie long running",1,1,"1"
"2206070030001299 | arr | aia group ltd",1,1,"1"
"2206070030001445 genie",1,1,"1"
"2206070030001488-need genie access",1,1,"1"
"2206070040000816 | delta table",1,1,"1"
"2206070040005297 - genie - streaming job failing with 503",1,1,"1"
"2206070040006687-need genie access",1,1,"1"
"2206070040007523 - arr - job using job cluster pointing to incorrect run id",1,1,"1"
"2206070040007523 - genie - cluster shows different run logs",1,1,"1"
"2206070050002327 job that runs 30 minutes took 48h",1,1,"1"
"2206080030000564001 genie",1,1,"1"
"2206080030001383 | jnj | arr",1,1,"1"
"2206080030001461: cluster got triggered automatically",1,1,"1"
"2206080030002199-need genie access",1,1,"1"
"2206080040000669|  job failure:  ""could not reach driver of cluster 1222-065636-pdkarugp"" and rca required",1,1,"1"
"2206080040001140 genie",1,1,"1"
"2206080040002822| job has degraded performance when ingest data from databricks to adx",1,1,"1"
"2206080040008958 - genie - xml streaming conversion to json",1,1,"1"
"2206080050000881 | parallel writes failed in databricks orc table",1,1,"1"
"2206080050002070 || genie access request",1,1,"1"
"2206090030000397 || genie access request",1,1,"1"
"2206090030000569 genie",1,1,"1"
"2206090040007208 | eh connectivity",1,1,"1"
"2206090050000314 genie",1,1,"1"
"2206090050001430 - permission denied when running a job manually",1,1,"1"
"2206090050001814 creating keyvault-backed secrets scopes",1,1,"1"
"2206090050001878 job that usually takes less then 10 minutes was stuck and took over an hour",1,1,"1"
"2206090050002068-need genie access",1,1,"1"
"2206100030001207- need genie acces",1,1,"1"
"2206100030001207-need genie access",1,1,"1"
"2206100050000530 how does databricks authenticate to regional hive metastore?",1,1,"1"
"2206100050002271 || genie access request",1,1,"1"
"2206130010002322 - databricks delta - excessive file accumulation",1,1,"1"
"2206130030001115-need genie access",1,1,"1"
"2206130030001276 | error at data bricks job that happens only when run on job and not on interactive mode",1,1,"1"
"2206130040007377-need genie access",1,1,"1"
"2206130040007750  | jobs api",1,1,"1"
"2206130050000200 - shiny app not working for multiple users",1,1,"1"
"2206130050001255 || genie access request",1,1,"1"
"2206130050001721 limit of secret scopes for a workspace",1,1,"1"
"2206130050002304 - workspace api request quota limit reached",1,1,"1"
"2206140040001526 | workspace ui",1,1,"1"
"2206140060000403  genie",1,1,"1"
"2206140060000403 streaming job failes with error ""cluster became unreachable during run cause: timed out after 15 seconds""""",1,1,"1"
"2206150020000275 - some cluster jobs failling randomly",1,1,"1"
"2206150020002412 genie",1,1,"1"
"2206150030000237 - genie - pepsico - jobs appear to be running but get stuck",1,1,"1"
"2206160040004965 - cluster terminated. reason: container launch failure",1,1,"1"
"2206160040005265 - genie - ubs - control plane request failure",1,1,"1"
"2206160040006065 || enable 'dbutils.secrets.get' on databricks-connect for workspace '6871852184727182'",1,1,"1"
"2206160040008005  spark driver has stopped unexpectedly and is restarting. your notebook will be automatically reattached",1,1,"1"
"2206160050000293  - databricks sql endpoint wont start due to auth problem",1,1,"1"
"2206160050001219 genie",1,1,"1"
"2206170010004570 | job failure",1,1,"1"
"2206170030001002 || genie access request",1,1,"1"
"2206170030001520 - where the json files are not created/refreshed for the recent records in iac.parts_details table.",1,1,"1"
"2206170050000329 performance issues on workspace",1,1,"1"
"2206170050000652  users not able to login to databricks services",1,1,"1"
"2206170050001591 genie",1,1,"1"
"2206170050001662 - genie",1,1,"1"
"2206170050001928 | ip address",1,1,"1"
"2206200030000375 genie",1,1,"1"
"2206200060001223-genie acess",1,1,"1"
"2206210040000762| do we have any api to get ganglia metrics snapshot in databricks directly",1,1,"1"
"2206210040002353-conenctivity issue on databricks cluster",1,1,"1"
"2206210040002353-need genie access",1,1,"1"
"2206210050000539 || genie access request",1,1,"1"
"2206210050001037 - delta live tables cluster dbr version",1,1,"1"
"2206210050001037 genie",1,1,"1"
"2206210050001768 genie",1,1,"1"
"2206210050001814-genie acess",1,1,"1"
"2206220040001688-need genie access",1,1,"1"
"2206220050000104 genie",1,1,"1"
"2206230030001503-trying to install geomesa libraries",1,1,"1"
"2206230040001667-need genie access",1,1,"1"
"2206230040002023 | is there a system table or any method which canrecord all user's permissions in all tables?",1,1,"1"
"2206230050002163-genie acess",1,1,"1"
"2206240030000569-need genie access",1,1,"1"
"2206240050000357 - genie",1,1,"1"
"2206240050000672-genie acess",1,1,"1"
"2206240050001160 jobs are failing when one notebook calls another with dbutils.notebook.run()",1,1,"1"
"2206240050001514 genie",1,1,"1"
"2206250040000042 - genie access",1,1,"1"
"2206260050000236 library dependency issue",1,1,"1"
"2206270050001341 genie",1,1,"1"
"2206270050002115 || genie access request",1,1,"1"
"2206280030001543-need genie access",1,1,"1"
"2206280050000468 genie",1,1,"1"
"2206280050001610 - genie",1,1,"1"
"2206290050002144 - genie",1,1,"1"
"2206300040005453 - genie - deloitte - users ui issue",1,1,"1"
"2206300050000777 - genie",1,1,"1"
"2250genie",1,1,"1"
"2495 - g - arr - cummins - failed job",1,1,"1"
"256496764439556 - 10280a44-7716-4c2a-8238-1b1796d4fde3 - 2204220040003095",1,1,"1"
"2567 - arr - exxonmobile - unable to access external storage from deltalive pipeline",1,1,"1"
"2567 - g - arr - exxonmobile - unable to output delta live table to azure storage account",1,1,"1"
"2576137007542488 - 10280a44-7716-4c2a-8238-1b1796d4fde3 - 2204290040002695",1,1,"1"
"2650234915343663 - c450f3d1-583c-495f-b5d3-0b38b99e70c0 - 2203020010003431",1,1,"1"
"2650234915343663 - c450f3d1-583c-495f-b5d3-0b38b99e70c0 - 2204220010001255",1,1,"1"
"2650234915343663 - c450f3d1-583c-495f-b5d3-0b38b99e70c0 - 2205270040006933",1,1,"1"
"2754134964726624 - 81b4ec93-f52f-4194-9ad9-57e636bcd0b6 - 2202230040008781",1,1,"1"
"2754134964726624 - 81b4ec93-f52f-4194-9ad9-57e636bcd0b6 - 2205100040007765",1,1,"1"
"2754134964726624 - 81b4ec93-f52f-4194-9ad9-57e636bcd0b6 - 2205110040011261",1,1,"1"
"2777928729223047 - d4199cb9-58c6-44c0-901f-9824ed868a0c - 2204200040003791",1,1,"1"
"2777928729223047 - d4199cb9-58c6-44c0-901f-9824ed868a0c - 2205070040001063",1,1,"1"
"2787931338254557 - 135d2085-e95f-4550-b69c-4b5ec0c17f9a - 2204300010000278",1,1,"1"
"2793632992259305 - e756ddea-d52c-404b-abea-81ee9b4d9164 - 2204130040006794",1,1,"1"
"3.10",1,1,"1"
"330372711545732 access",1,1,"1"
"3432 - g - arr - t-mobile - long-running jobs",1,1,"1"
"3670587557554624 - 286d7b79-4849-4b63-a7ca-5dd839c53e86 - 2203180040004316",1,1,"1"
"3962936883739740 - c450f3d1-583c-495f-b5d3-0b38b99e70c0 - 2204210040011633",1,1,"1"
"3978284543780171 genie",1,1,"1"
"3986454804076847 - f55d4ef9-4d7f-4763-8661-9b82de6c08c9 - 2204040040008424",1,1,"1"
"4001genie",1,1,"1"
"403",1,1,"1"
"403 error",1,1,"1"
"403 forbidden you don't have permission to access this page",1,1,"1"
"4031723673524963 - f42342d1-2d01-49b5-beee-364183ed8af5 - 2204020040000225",1,1,"1"
"4206 - arr - humana - databricks support for 3.1.3 to address cve cve-2021-38296",1,1,"1"
"4310827714638474 - 934970a2-fedc-42dd-9240-1793428f4211 - 2205050040006329",1,1,"1"
"4681 - g - arr - american airlines - cluster not shutting down as expected",1,1,"1"
"4687776344032114 - db82b489-2461-4983-9f5a-61a13d431905 - 2204290040005232",1,1,"1"
"500 error",1,1,"1"
"5003f00000jkze7aab",1,1,"1"
"5008y00001u5olg",1,1,"1"
"5008y00001u7uim",1,1,"1"
"5008y00001uplh3qac",1,1,"1"
"504-bad gateway error",1,1,"1"
"5244115429641560 - 77c41218-a808-4734-a49d-b82f9244bc93 - 2205200040005272",1,1,"1"
"5259743963611861 - 33083e0c-20e5-40c8-a77d-557528d9e387 - 2205100010002951",1,1,"1"
"5491164172891362 - b8d79299-dbc4-42b6-b557-01b3a7cf0605 - 2205200040003030",1,1,"1"
"5641 - arr - molinahealthcare - functionality issue on sql editor filter",1,1,"1"
"5641 - g - arr - molinahealthcare - search function not working in databricks sql for filter on data sources.",1,1,"1"
"5829038454671229 - 10280a44-7716-4c2a-8238-1b1796d4fde3 - 2205060040004996",1,1,"1"
"6371889753967149 - 5e4cc064-1d07-4407-a3e7-cce62b3e4237 - 2203280010002683",1,1,"1"
"6398507633548715 - e33df5d1-ae22-417d-b794-8d9b6f338409 - 2204050040008980",1,1,"1"
"6398507633548715 - e33df5d1-ae22-417d-b794-8d9b6f338409 - 2204230040000595",1,1,"1"
"64k",1,1,"1"
"64k rows",1,1,"1"
"6540 - arr - american airlines - stuck job, cancelled after 168 hrs",1,1,"1"
"6540 - g - arr - american airlines - cluster not starting",1,1,"1"
"6649989745882060 - c450f3d1-583c-495f-b5d3-0b38b99e70c0 - 2204260010002364",1,1,"1"
"6706 - arr - at&t - no new databricks secret scopes can be created by users. max limit(100) on the workspace already reached. can we please increase this limit.",1,1,"1"
"6772 - arr - morgan stanley - stderr stdout are not accessible on the executors.",1,1,"1"
"6847458555757558 - 10280a44-7716-4c2a-8238-1b1796d4fde3 - 2204220040003095",1,1,"1"
"6899 - g - arr - anheuser busch - table just created several commands earlier not found",1,1,"1"
"6973325950234402 - 6dfbe157-2219-4313-adfc-6df4829ab651 - 2204050010002796",1,1,"1"
"7070genie",1,1,"1"
"7166646288419298 - 051ce839-3786-4339-b3fd-df3efda185de - 2204270040005587",1,1,"1"
"7173 - g - arr - att - storage account inaccessible",1,1,"1"
"7262183663665710 - ad7b8c5c-3737-4dfa-acad-9078c56e7b95 - 2206280030002105",1,1,"1"
"7262183663665710-need genie access",1,1,"1"
"7325620703232380 - 3377047f-02a8-4316-91eb-99afd67a0352 - 2205030040006573",1,1,"1"
"7325620703232380 - 4da48ca4-4455-422a-be4c-300209649149 - 2204210040012860",1,1,"1"
"7348 - g - arr - h&r block - job failure",1,1,"1"
"7441 - arr - uhg - fairlearn dashboard spins with cors error where databricksusercontent.com content is attempting to retrieve from the azuredatabricks.net",1,1,"1"
"7441 - g - arr - uhg - notebook failure",1,1,"1"
"7443964950314925 - 5e4cc064-1d07-4407-a3e7-cce62b3e4237 - 2204010010002327",1,1,"1"
"7451881325335682 - 55bde1f0-8591-432c-a37f-5c1bdff8c2f8 - 2204280040000761",1,1,"1"
"7464501611136417 - 7cca3be6-ea39-4394-9aab-242213bd98e5 - 2203310030003446",1,1,"1"
"7649genie",1,1,"1"
"7667 - arr - jnj - cluster taking very long time to start, and fails after 40 - 60 minutes",1,1,"1"
"7677 - g - arr - jnj - cluster not starting in time",1,1,"1"
"7763010224164137 - b8080fe8-25bc-47b2-85e7-fd02dd3aee1e - 2204280040004711",1,1,"1"
"7763010224164137 - b8080fe8-25bc-47b2-85e7-fd02dd3aee1e - 2205280040000814",1,1,"1"
"7797454808119526 - f8197865-f5c7-4719-a848-6301201fe96a - 2205140060000097",1,1,"1"
"7896 - g - arr - alaskaair - failed job",1,1,"1"
"8254154577522778 - 6c05363f-2a16-4591-8cf7-5da1b8247e46 - 2205050040007471",1,1,"1"
"8301242989929544 - 33b3c5a4-3db4-4ecc-acfe-bf502b886da6 - 2206210040007194",1,1,"1"
"866851050781084 - f55d4ef9-4d7f-4763-8661-9b82de6c08c9 - 2204120040025469",1,1,"1"
"8817 - arr - cap - databricks job getting hung",1,1,"1"
"8817 - arr - capgroup - databricks job getting hung",1,1,"1"
"8890986345557588_need genie access",1,1,"1"
"8917076304931352 - 09c14ab4-bb01-40d1-9eaa-c49fefd07fb1 - 2205280040000550",1,1,"1"
"9667 - arr - albertsons - clusters not starting with bootstrap errors",1,1,"1"
"9667 - g - arr - albertsons - clusters not starting",1,1,"1"
": i am no longer able to log into https://deere-edl.cloud.databricks.com/.",1,1,"1"
": java.lang.securityexception: to access secrets using databricks connect, please contact support to enable this feature on your workspace.",1,1,"1"
"<<need emea engineer>>  missing mlflow model run",1,1,"1"
"<a ondblclick=""alert(1)"">test</a>",1,1,"1"
"<button><script>alert(""test"")</script></button>",1,1,"1"
"<h1> hello </h1>",1,1,"1"
"<h1><aa/h1>",1,1,"1"
"<h1>aa</h1>",1,1,"1"
"<h1>gi<h1>",1,1,"1"
"<h1>hello</h1>",1,1,"1"
"<h1>hello</h1>--",1,1,"1"
"<h3><font color=red>hello check out new site <h1><sup><hr><mark>evil.com</mark></sup><hr></h1></h3>",1,1,"1"
"<html><body> <?xml:namespace prefix=""t"" ns=""urn:schemas-microsoft-com:time""> <?import namespace=""t"" implementation=""#default#time2""> <t:set attributename=""innerhtml"" to=""xss<script defer>alert(""xss"")</script>""> </body></html>",1,1,"1"
"<iframe><img title=""</iframe><img src onerror=alert(1)>""></iframe>",1,1,"1"
"<img src = x onerror = ""javascript: window.onerror = alert; throw xss"">     <video> <source onerror = ""javascript: alert (xss)"">     <input value = ""xss"" type = text>     <applet code=""javascript:confirm(document.cookie);"">     <isindex x=""javascript:"" onmouseover=""alert(xss)"">     ""></script>”>’><script>alert(string.fromcharcode(88,83,83))</script>     ""><img src=""x:x"" onerror=""alert(xss)"">",1,1,"1"
"<img src=&#0000106&#0000097&#0000118&#0000097&#0000115&#0000099&#0000114&#0000105&#0000112&#0000116&#0000058&#0000097&#0000108&#0000101&#0000114&#0000116&#0000040&#0000039&#0000088&#0000083&#0000083&#0000039&#0000041>",1,1,"1"
"<img src=x onerror=prompt(document.domain)>",1,1,"1"
"<math href=""javascript:javascript:alert(1)"">clickme</math> <math> <maction",1,1,"1"
"<script>alert(""test"")</script>",1,1,"1"
"<script>alert('xss')</script>",1,1,"1"
"<script>confirm('xss')</script>",1,1,"1"
"<xss onclick=""alert(document.cookie)"">click me</xss>",1,1,"1"
"?rmarkdown::pandoc_available",1,1,"1"
"[00157373 follow up] data and repository tab ui slowness issue",1,1,"1"
"[2205050050000182][unable to download adal library]",1,1,"1"
"[5:24 pm] ramya panati (mindtree limited) spark.databricks.enablewsfs",1,1,"1"
"[aia][managed accounts ui advisory]",1,1,"1"
"[airflow task] data of certain conditions are loaded twice on a 'specific date' delta table (insert into)",1,1,"1"
"[airflow task] full data loaded twice in a 'specific date' parquet table (insert overwrite)",1,1,"1"
"[arr#2206210040008860] external metastore cannot access jars",1,1,"1"
"[arr]  calling the global init script of a databricks cluster from automated azure devops pipeline issue",1,1,"1"
"[arr] [sev a] sr-2204100050000057 continue to #00141395 errors execution databricks jobs due to azure_resource_provider_throttling",1,1,"1"
"[arr] [sev a] sr-2204130040012512  scheduled jobs are taking more than usual times. jobs which used to take minutes took hours or running for forever.",1,1,"1"
"[arr] [sev a] sr-2206010010000366 incident regarding job executing",1,1,"1"
"[arr] [sev a] sr-2206090040000117 the performance of request passing through databricks is being degraded",1,1,"1"
"[arr] [sev a] sr-2206210030002489 cpp databricks issue -  da_pos_emer_main long running",1,1,"1"
"[arr] [sev a] sr-4063266763499350  cannot grow bufferholder by size 24 because the size after growing exceeds size limitation 2147483632 from adf",1,1,"1"
"[arr] [sev b] sr- 2205260030000798 notebook execution taking more than the usual time",1,1,"1"
"[arr] [sev b] sr- 2206070030001383 fail to run azure databricks job",1,1,"1"
"[arr] [sev b] sr- 2206080030001852 unable push code to the github repo",1,1,"1"
"[arr] [sev b] sr-2203300030002890 - need to enable table acl on standard clusters",1,1,"1"
"[arr] [sev b] sr-2204040030002605 continue to 00141003 | the impact on spark.databricks.hive.metastore.client.pool.size",1,1,"1"
"[arr] [sev b] sr-2204040030002605 the cause of mestastore, dbfs down",1,1,"1"
"[arr] [sev b] sr-2204040030003249- getting error on performing test connection from databricks to storage account.",1,1,"1"
"[arr] [sev b] sr-2204050050002779-databricks cluster upgrade",1,1,"1"
"[arr] [sev b] sr-2204070010002917   continue to 00141203 how to execute the mounted file for job cluster",1,1,"1"
"[arr] [sev b] sr-2204070010003095-how to install init script ,  python libraries and mq client",1,1,"1"
"[arr] [sev b] sr-2204070010003095-how to install init script , python libraries and mq client",1,1,"1"
"[arr] [sev b] sr-2204080030001854 - executorlostfailure (executor 15 exited caused by one of the running tasks) reason: executor heartbeat timed out after 157195 ms",1,1,"1"
"[arr] [sev b] sr-2204080030001854 executorlostfailure (executor 15 exited caused by one of the running tasks) reason: executor heartbeat timed out after 157195 ms",1,1,"1"
"[arr] [sev b] sr-2204100040000077 rca:unable to acquire lock to files from adls on databricks",1,1,"1"
"[arr] [sev b] sr-2204120030002012  job failing due to executorlostfailure (executor 10 exited caused by one of the running tasks) reason: remote rpc client disassociated",1,1,"1"
"[arr] [sev b] sr-2204130040012512 related #00141930 regarding information of bonecp bug and hikaricp connection pool",1,1,"1"
"[arr] [sev b] sr-2204160040000244",1,1,"1"
"[arr] [sev b] sr-2204180030001121  query is running since 6 hours",1,1,"1"
"[arr] [sev b] sr-2204180030001121 stream delay through optimize with z-order",1,1,"1"
"[arr] [sev b] sr-2204190040005554-jobs running longer than expected frequently",1,1,"1"
"[arr] [sev b] sr-2204200030000408 - databricks cannot access key vault randomly",1,1,"1"
"[arr] [sev b] sr-2204200030000408 -databricks cannot access key vault randomly",1,1,"1"
"[arr] [sev b] sr-2204210040011593-getting this error this operation is not permitted because the blob has snapshots",1,1,"1"
"[arr] [sev b] sr-2204210040011593-issue with our proxy configuration when deploying overwatch in itg workspace",1,1,"1"
"[arr] [sev b] sr-2204220060000915 continue to 00138803 - mount_azure_file sync for",1,1,"1"
"[arr] [sev b] sr-2204240060000475 how to read parquet from sql analytics",1,1,"1"
"[arr] [sev b] sr-2204240060000475 time difference for 30 min happens while reading parquet file",1,1,"1"
"[arr] [sev b] sr-2204250050001910",1,1,"1"
"[arr] [sev b] sr-2205040030001061 dataframereaderror (reason: worker lost)",1,1,"1"
"[arr] [sev b] sr-2205050010002241 java.lang.outofmemoryerror: gc overhead limit exceeded",1,1,"1"
"[arr] [sev b] sr-2205060030001077-postgresql connection failing for metastore because of spark configuration (spark.databricks.pyspark.enableprocessisolation true)",1,1,"1"
"[arr] [sev b] sr-2205110030001940 continue to #00146312 notebook is reverted back previous version automatically",1,1,"1"
"[arr] [sev b] sr-2205110030001940 notebook is reverted back previous version automatically",1,1,"1"
"[arr] [sev b] sr-2205120030001522 disable notebook schedule option",1,1,"1"
"[arr] [sev b] sr-2205130030001214 getting the following error while reading file unable to infer schema for parquet. it must be specified manually",1,1,"1"
"[arr] [sev b] sr-2205190010002627 rca-performance issue for job executing",1,1,"1"
"[arr] [sev b] sr-2205190030001473-streaming is not working for passthrough enabled containers and db's",1,1,"1"
"[arr] [sev b] sr-2205250010001325-few jobs in pii prod databricks workspace are failing continuously with connection reset issue",1,1,"1"
"[arr] [sev b] sr-2205260030001963-cs_new_fix_issue",1,1,"1"
"[arr] [sev b] sr-2205300030001910 title: dbfs api 2.0, 1 mb upload limitation",1,1,"1"
"[arr] [sev b] sr-2205310030001378   notebooks are failing to execute the run command (%run)",1,1,"1"
"[arr] [sev b] sr-2205310030001378 continue to #00182871  request to join the call to explain the solution and issue",1,1,"1"
"[arr] [sev b] sr-2205310030001378 continue to 00182871 additional questions regarding gc",1,1,"1"
"[arr] [sev b] sr-2206010030001463 nosuchmethoderrorcomeswhileinstallingdifferent libraries on databricks cluster",1,1,"1"
"[arr] [sev b] sr-2206020050001074-job failure",1,1,"1"
"[arr] [sev b] sr-2206020060001886 usage_dbfs",1,1,"1"
"[arr] [sev b] sr-2206030050001488 the service at /api/2.0/jobs/runs/submit is temporarily unavailable.",1,1,"1"
"[arr] [sev b] sr-2206050030000063-cluster was stuck in running state had to manually cancel",1,1,"1"
"[arr] [sev b] sr-2206060030001963 abfsrestoperationexception",1,1,"1"
"[arr] [sev b] sr-2206060040005065-transient cluster job failure",1,1,"1"
"[arr] [sev b] sr-2206070030001700  unable to connect service bus with dbr 10.4 lts",1,1,"1"
"[arr] [sev b] sr-2206070030001700 unable to connect service bus with dbr 10.4 lts",1,1,"1"
"[arr] [sev b] sr-2206070030001746 no such method error coming from netty while running couchbase connector from spark",1,1,"1"
"[arr] [sev b] sr-2206070040006772-databricks jobs intermittently failing",1,1,"1"
"[arr] [sev b] sr-2206090030001702 databricks cluster is unable to connect an on premises ip",1,1,"1"
"[arr] [sev b] sr-2206130050001484-sql endpoint failed to launch the cluster",1,1,"1"
"[arr] [sev b] sr-2206140010002319 error for new connection to third party api hit",1,1,"1"
"[arr] [sev b] sr-2206160030000065 unable to craete databrick cluster because of reading secret failure",1,1,"1"
"[arr] [sev b] sr-2206200050001869 emails are not getting triggered through existing webhook",1,1,"1"
"[arr] [sev b] sr-2206210030001843 jobs_performance_issues",1,1,"1"
"[arr] [sev b] sr-2206220030001452-databricks scope pepsecure changed pointed to edap-framework-qa-cmk-kv we would like know the change",1,1,"1"
"[arr] [sev b] sr-2206270030001495 schemaregistryerror while connecting to kafka",1,1,"1"
"[arr] [sev b] sr-2206270030001668-spark driver terminated automatically and restarted",1,1,"1"
"[arr] [sev b]could you tell me the process how to get the executor log from dbinsight?",1,1,"1"
"[arr] [sev c] 2205180030001625-sometimes, we are facing an ssl connect issue when importing the data to powerbi from azure sql endpoint of adb",1,1,"1"
"[arr] [sev c] sr-2110140040008211-intermittent access to the databricks",1,1,"1"
"[arr] [sev c] sr-2204040040005347 rce vulnerability",1,1,"1"
"[arr] [sev c] sr-2204060030000215 the reason why job alert mail did not work",1,1,"1"
"[arr] [sev c] sr-2204110030000414 conf file is not read from global init script",1,1,"1"
"[arr] [sev c] sr-2204110030000414 genie access",1,1,"1"
"[arr] [sev c] sr-2204110030000414 rca  continue to 00141951  cannot evaluate expression current_timestamp()",1,1,"1"
"[arr] [sev c] sr-2204110030000414 rca request- unsupportedoperationexception: datatype issue",1,1,"1"
"[arr] [sev c] sr-2204110030002332-slowness of databricks (pii removal) job",1,1,"1"
"[arr] [sev c] sr-2204170030000336 job was failed because of killed stage",1,1,"1"
"[arr] [sev c] sr-2205190030001704 getting error on doing dbutils.fs.ls('dbfs:/mnt/uad/gokul/')",1,1,"1"
"[arr] [sev c] sr-2205250050001827 quota request for other requests",1,1,"1"
"[arr] [sev c] sr-2205290030000234 rca-jobs running for longer time than usual",1,1,"1"
"[arr] [sev c] sr-2205310030001397-understand utilization of databricks",1,1,"1"
"[arr] [sev c] sr-2205310050002317 multi threading processes hung",1,1,"1"
"[arr] [sev c] sr-2205310050002317 title",1,1,"1"
"[arr] [sev c] sr-2206030050001488 the service at /api/2.0/jobs/runs/submit was temporarily unavailable.",1,1,"1"
"[arr] [sev c] sr-2206090030001603-not able to edit job cluster",1,1,"1"
"[arr] [sev c] sr-2206170030001520  where the json files are not created/refreshed for the recent records in iac.parts_details table.",1,1,"1"
"[arr] [sev c] sr-2206170030001520 where the json files are not created/refreshed for the recent records in iac.parts_details table.",1,1,"1"
"[arr] [sev c] sr-2206170030001661-project related folders deleted inside cluster",1,1,"1"
"[arr] [sev c] sr-2206210030001213-policy issue in databricks cluster",1,1,"1"
"[arr] [sev c] sr-2206230030000970-sql endpoint intermittently timed out when queried by powerbi",1,1,"1"
"[arr] [sev c] sr-2206290050000564-databricks cluster is getting terminated",1,1,"1"
"[arr] [sev c] sr2205090010003177-issues with 9.1 and 10.4lts runtime environments in production",1,1,"1"
"[arr] [sev c] sr2205090010003177-issues with 9.1 and 10.4lts runtime environments in production- followup case",1,1,"1"
"[arr] [sev x] sr- 2205240040005038 operation on target ingest to delta table failed",1,1,"1"
"[arr] [sev x] sr-2204050030002523 related to #00139893 consideration of ""parallel access to the create path detected"" err",1,1,"1"
"[arr] [sev x] sr-c 2206030050001488 the service at /api/2.0/jobs/runs/submit is temporarily unavailable.",1,1,"1"
"[arr] azure credential not found exception- saved",1,1,"1"
"[arr] cannot access external config in artifacts path with dbx",1,1,"1"
"[arr] cannot see the deletion of clusters in log analytics",1,1,"1"
"[arr] cluster starting failure",1,1,"1"
"[arr] connection from databricks to synapse using jdbc intermittent issue-ssl unexpected rethrowing",1,1,"1"
"[arr] databricks jobs page is not loading",1,1,"1"
"[arr] deleted clusters due inactivity not logged in log analytics or other places",1,1,"1"
"[arr] enable dbutils.secrets.get on ws 3385660146604390",1,1,"1"
"[arr] execution of commands  in databaricks cluster stuck",1,1,"1"
"[arr] follow up for sf#00161558",1,1,"1"
"[arr] ganglia metrics not loaded",1,1,"1"
"[arr] how to disable cluster advanced jdbc/odbc",1,1,"1"
"[arr] jobs intermittently failure to write to sql and log analytics",1,1,"1"
"[arr] oserror: errno 95 operation not supported work around other than changing code in prod",1,1,"1"
"[arr] recursive view is not supported in databricks cluster runtime version 10.4?",1,1,"1"
"[arr] sr#2204050040010569-please help me to get the ganglia ui report images to dig into the detailed cluster report",1,1,"1"
"[arr] sr#2204060030003103 the azure databricks – connect to azure synapse - dedicated sql pool issue",1,1,"1"
"[arr] sr#2206070030001639 unable to connect databricks to kafka connector",1,1,"1"
"[arr] sr#2206130030001816 how to change the cluster creator id",1,1,"1"
"[arr] sr-2203310010000891 dspredictionv3 messages are lost while publishing to topic",1,1,"1"
"[arr] sr-2204120030001695 cluster policies not working as expected",1,1,"1"
"[arr] sr-2204120030001695 cluster policies not working as expected for the spark version",1,1,"1"
"[arr] sr-2204150030000252 databricks fails to connect with mysql server (metastrore)",1,1,"1"
"[arr] sr-2204260030001581 need to restores am01-fra-dbw02 in resource group ch-corp-devtest-am01-fra-rg01",1,1,"1"
"[arr] sr-2204270050000908 databricks cluster performance issue",1,1,"1"
"[arr] sr-2205310030002095 need suggestions to improve data ingestion job performance",1,1,"1"
"[arr] tasks workflow based on condition within the task",1,1,"1"
"[arr] undefined function: 'substr'. this function is neither a registered temporary function nor a permanent function",1,1,"1"
"[arr] very long runtime for the job",1,1,"1"
"[arr] whitelist ws to use git enterprize 1102382472621232",1,1,"1"
"[arr]- sr#2206210030002330 i am trying to update  the databricks workspace from public ip to private workspace",1,1,"1"
"[arr][ 2205250030002536][aholddelhaize]we are unable to see clusters in prod adb03 workspace. getting backend service unavailable error.",1,1,"1"
"[arr][2203310040005788 ][pepsico]unable to reach azure load balancer from databricks",1,1,"1"
"[arr][adobe][cluster permissions]",1,1,"1"
"[arr][adobe][job cluster failure]",1,1,"1"
"[arr][aia][storage artifact download failure]",1,1,"1"
"[arr][aia][vm extension failure]",1,1,"1"
"[arr][aircanada]",1,1,"1"
"[arr][at & t][2205240040007193] how to connect to azure file share using adbfss",1,1,"1"
"[arr][citrix][guidance on pbi & sql analytics endpoint connection]",1,1,"1"
"[arr][cluster not able to spin up]",1,1,"1"
"[arr][cluster performance is too slow]",1,1,"1"
"[arr][cluster terminated.reason:azure resource provider throttling]",1,1,"1"
"[arr][cluster terminated.reason:storage download failure]",1,1,"1"
"[arr][compilation error][genie]",1,1,"1"
"[arr][cvs][failure to generate pat token using service principal aad authentication]",1,1,"1"
"[arr][genie][aia]",1,1,"1"
"[arr][genie][eventhub job]",1,1,"1"
"[arr][genie][eventhub performance]",1,1,"1"
"[arr][genie][failed to upload data files from databricks local storage to aml experiment]",1,1,"1"
"[arr][genie][predential][ observing delay while consuming the events from event hub to databricks.]",1,1,"1"
"[arr][genie][spot vm failure]",1,1,"1"
"[arr][genie][storage latency]",1,1,"1"
"[arr][grabtaxi][job report storage performance related]",1,1,"1"
"[arr][ip access list cidr range setup suggestions]",1,1,"1"
"[arr][job aborted due to stage failure]",1,1,"1"
"[arr][job cluster does not start]",1,1,"1"
"[arr][jobs are running for long]",1,1,"1"
"[arr][latency issue in databricks workspace]",1,1,"1"
"[arr][long running databricks notebooks on one of our tenants]",1,1,"1"
"[arr][mars inc][2205170040006218 ]issue with cluster resizing & autoscaling",1,1,"1"
"[arr][metastore down][2206160030000361]",1,1,"1"
"[arr][molina healthcare][2204270040005587 ] in sql endpoint  table created with distinct  and group by are not resulting into records",1,1,"1"
"[arr][pepsico][jobs stuck]",1,1,"1"
"[arr][pg][job failure]",1,1,"1"
"[arr][poor performance of a cluster]",1,1,"1"
"[arr][public analytics][2205250030002283 ] missing notebooks/folders under fiona/rbs in dev adb03",1,1,"1"
"[arr][public analytics][2205250030002283 ]notebooks/folders under fiona/rbs need to restore and audit report",1,1,"1"
"[arr][safeway][no such element]",1,1,"1"
"[arr][storage latency]",1,1,"1"
"[arr][unity catalog enablement][aia]",1,1,"1"
"[arr][walmart][cloud_launch_failure]",1,1,"1"
"[arr][walmart][jobs failing]",1,1,"1"
"[arr][we are getting threshold sparkexception while running the query for more than 5 minutes]",1,1,"1"
"[cheggprod] overwatch investigation",1,1,"1"
"[cheggtest] overwatch investigation",1,1,"1"
"[es-293234]",1,1,"1"
"[external] re: access from synapse spark pool to on-prem oracl... - trackingid#2203010060001556",1,1,"1"
"[feature request] notebook 'find & replace' for each cell unit",1,1,"1"
"[feature request] want to use parameters and dynamic query with sql in an ide (pycharm)",1,1,"1"
"[follow-up 00140889] arr | spark down: outofmemoryerror: unable to create new native thread",1,1,"1"
"[follow-up 00141646] arr | intermitten error: table or view not found",1,1,"1"
"[gc (allocation failure) [psyounggen",1,1,"1"
"[genie access request]",1,1,"1"
"[genie request]",1,1,"1"
"[genie request] dataflow-aee5c8fe-74a1-492e-87d4-557aa09e75ec",1,1,"1"
"[genie request] dbrproductsupplyp101",1,1,"1"
"[genie request] to 7733217811977339",1,1,"1"
"[genie]  error sparklyr: gateway (65529) is shutting down from init() with exception ,java.net.bindexception: address already in use (bind failed)",1,1,"1"
"[genie] custom docker images do not work- saved",1,1,"1"
"[genie] failed job run",1,1,"1"
"[genie] intermittent failure of spark streams with availablenow trigger",1,1,"1"
"[genie] unable to call functions/modules from another notebook in the databricks workspace",1,1,"1"
"[genie] unable to deploy model, stuck at build image",1,1,"1"
"[genie][[job aborted due to stage failure]",1,1,"1"
"[genierequest] dataflow-aee5c8fe-74a1-492e-87d4-557aa09e75ec",1,1,"1"
"[genie} databricks notebook taking longer time moveing the data from adls to adls",1,1,"1"
"[high priority] facing login issues in databricks",1,1,"1"
"[microsoft][hardy] (134)",1,1,"1"
"[mooncake][2205090050002395][the cluster does not terminate after the job finished]",1,1,"1"
"[p1]sql endpoint didn't working at all",1,1,"1"
"[py4jjavaerror ] org.apache.spark.sparkexception: job aborted",1,1,"1"
"[question] install 'jupyterlab-chart-editor' in databricks notebook",1,1,"1"
"[reg: 2205230060000761] cannot understand what error message means - genie access",1,1,"1"
"[reopened case about 00141031, 00141027",1,1,"1"
"[s500] databricks library installations are failing",1,1,"1"
"[sev b] - 2206270030001442 - need to change sharing setting for sql queries",1,1,"1"
"[sev] b 2206210030001843 - jobs_performance_issues",1,1,"1"
"[simba][sparkjdbcdriver](500593) communication link failure. failed to connect to server. reason: java.net.bindexception: address already in use: connect...",1,1,"1"
"[sr#2205310030002095]need suggestions to improve data ingestion job performance",1,1,"1"
"[sr#2206010030001723] databicks not able call azure http function",1,1,"1"
"[sr#2206020060001809] recover the old workspace",1,1,"1"
"[sr: 2205270060000149] consistency of delta lake table when copying data to another location",1,1,"1"
"[sr: 2206070040002938] i got an error on databricks pipelines - genie access",1,1,"1"
"[sr: 2206070060000090] failure to access sql server - genie access",1,1,"1"
"[sr: 2206130060000881] databricks runtime problem - genie access",1,1,"1"
"[sr: 2206200060000835] genie access",1,1,"1"
"[sr: 2206230060000505] genie access",1,1,"1"
"[trackingid#2203310010000891 dspredictionv3 messages are lost while publishing to topic",1,1,"1"
"[urgent] policy violation - user detected downloading bittorrent resulting in dmca complaint",1,1,"1"
"\&#34;+confirm(1)+&#34;",1,1,"1"
"_00d61jgc4._5003fjklrn",1,1,"1"
"_00d61jgc4._5008y1uqgfp:ref",1,1,"1"
"_00d61jgc4._5008y1urnz4",1,1,"1"
"_apply tables are appearing in target  metadata database;",1,1,"1"
"_sqldf",1,1,"1"
"_success file is missing when writing .orc file",1,1,"1"
"a job that typically runs for 6 minutes only, ran for 58 hours this time",1,1,"1"
"a piece of code in one of the notebooks has been behaving erratically",1,1,"1"
"a previously working query stopped working",1,1,"1"
"a retriable error occurred whilea ttempting to download a result file from the cloud store but the retry limi had been exceed",1,1,"1"
"a/b testing on databricks",1,1,"1"
"aaa",1,1,"1"
"aaa 3agest data lake issue",1,1,"1"
"aad token for jdbc odbc",1,1,"1"
"abhilash-dp90",1,1,"1"
"ability to execute table_changes query in sql endpoint",1,1,"1"
"ability to specify different vm instance types based by schedule",1,1,"1"
"aborted job due to connectivity/memory issues",1,1,"1"
"about query speed",1,1,"1"
"about serverless sql",1,1,"1"
"about sql execute error after databricks workspace creation",1,1,"1"
"about the specification to output the audit log to s3",1,1,"1"
"abstractmethoderror while calling an udaf  inherited from collect in pyspark platform",1,1,"1"
"academy",1,1,"1"
"academy 403 error",1,1,"1"
"academy payments not working",1,1,"1"
"acccount admin - additional user",1,1,"1"
"access",1,1,"1"
"access a s3 file by notebook",1,1,"1"
"access all tables under a schema",1,1,"1"
"access and tokens unexpectedly dropped from workspace",1,1,"1"
"access azure data lake storage gen2 container files in  r notebook",1,1,"1"
"access control list input not working for job api",1,1,"1"
"access dbfs default storage path is timing out | arr sr# 2204070050000512",1,1,"1"
"access deere artifactory/repository from databricks",1,1,"1"
"access deleted cluster configuration",1,1,"1"
"access denied error when writing from rdd to s3",1,1,"1"
"access denied to a bucket in databricks",1,1,"1"
"access denied to s3",1,1,"1"
"access denied when writing logs to an s3 bucke",1,1,"1"
"access denied when writing logs to an s3 bucket",1,1,"1"
"access denied when writing to an s3 bucket using rdd",1,1,"1"
"access denied while creating a global init script",1,1,"1"
"access files moved to trash",1,1,"1"
"access free customer training",1,1,"1"
"access issues for s3 mounts",1,1,"1"
"access notebook",1,1,"1"
"access request to riotgames.cloud.databricks.com (allowlist)",1,1,"1"
"access s3 bucket in different accounts",1,1,"1"
"access s3 using spark",1,1,"1"
"access snowflake",1,1,"1"
"access table from power bi root folder name is different for two users",1,1,"1"
"access the account console (e2)",1,1,"1"
"access to kwargs parameters in python wheel job",1,1,"1"
"access to manage account",1,1,"1"
"access to non-admins to ""create and trigger a one-time run""",1,1,"1"
"access token",1,1,"1"
"access workspace",1,1,"1"
"access workspace to check version differences in notebook",1,1,"1"
"accessdeniedexception in one of the query",1,1,"1"
"accessing aws athena from databricks",1,1,"1"
"accessing aws codeartifact",1,1,"1"
"accessing aws location services from databricks using boto3",1,1,"1"
"accessing container reports no credentials error",1,1,"1"
"accessing databricks workspace from power bi through service principle token",1,1,"1"
"accessing s3 data in e2 as tables",1,1,"1"
"account api access",1,1,"1"
"account console cannot log in",1,1,"1"
"account console permission issue",1,1,"1"
"account e1",1,1,"1"
"account id",1,1,"1"
"account level user and service principal lost admin role",1,1,"1"
"account manager",1,1,"1"
"account network cloud configuration unable to delete",1,1,"1"
"account owner emailid change request",1,1,"1"
"account owner transfer",1,1,"1"
"account rep",1,1,"1"
"account sso timeout",1,1,"1"
"account withdrawal",1,1,"1"
"account-level groups ui issues",1,1,"1"
"accounts api authentication",1,1,"1"
"accounts api create workspace call fails every time in new oem databricks account",1,1,"1"
"accounts console sso not working",1,1,"1"
"accreditation credentilas",1,1,"1"
"acessing databases using mlflow projects and databricks",1,1,"1"
"acl - direct files read",1,1,"1"
"acl clusters tcp ports security query",1,1,"1"
"acl datadog",1,1,"1"
"acris_listener_job failed",1,1,"1"
"activate secret in db-connect",1,1,"1"
"activity logs",1,1,"1"
"ad group user not able to access",1,1,"1"
"adb - adb is not working - 2206160060001434",1,1,"1"
"adb clusters not getting on",1,1,"1"
"adb dynamic floating ip's",1,1,"1"
"adb | genie | intermittent delta streaming merge error: cannot broadcast the table that is larger than 8gb - 2204130010003560",1,1,"1"
"add additional ip to whitelist",1,1,"1"
"add admin users to databricks support portal",1,1,"1"
"add awsprofilename for amazon msk library",1,1,"1"
"add comment to column create table as",1,1,"1"
"add comments",1,1,"1"
"add csv from ui",1,1,"1"
"add database in sql",1,1,"1"
"add ghe connectivity to prod workspace",1,1,"1"
"add ip address to allow list",1,1,"1"
"add ip to allow list",1,1,"1"
"add ip to allow list on shard003.cloud.databricks.com",1,1,"1"
"add user",1,1,"1"
"add user to support tickets",1,1,"1"
"add user to workspace",1,1,"1"
"add watcher",1,1,"1"
"adding .jar using init script",1,1,"1"
"adding a user to accounts.cloud.databricks.com",1,1,"1"
"adding libraries using databricks connect",1,1,"1"
"adding service principals to group",1,1,"1"
"adding users through okta resets group entitlements",1,1,"1"
"additional configuration needed on sql analyitics cluster",1,1,"1"
"additional exam questions",1,1,"1"
"additional topic to this s-f 00158380",1,1,"1"
"adf",1,1,"1"
"admin access lost",1,1,"1"
"admin account console sso",1,1,"1"
"admin automatic assigning",1,1,"1"
"admin cannot access ganglia ui in sandard cluster with passthrough [2206220050001608]",1,1,"1"
"admin change",1,1,"1"
"admin users can't login",1,1,"1"
"advana databricks pvc v3.68 upgrade issues",1,1,"1"
"advanced diagnostic information",1,1,"1"
"advanced search",1,1,"1"
"advice for getting adls gen2 container size on databricks",1,1,"1"
"advise on the steps to deploy a model stored in databricks registery in aci",1,1,"1"
"advisory support call",1,1,"1"
"after application restart the job spark ui/logs becomes grayed out and not accessible.",1,1,"1"
"after updating cluster policy spark environment variable, not able to update existing job",1,1,"1"
"aggregate",1,1,"1"
"aircanada - 2206280030002105 - databricks job failures with ""no space left on device""",1,1,"1"
"aircanada - workspace",1,1,"1"
"airflow dags not working when vpn is connected",1,1,"1"
"airflow<>databricks prod http issue",1,1,"1"
"akash.bhat@databricks.com",1,1,"1"
"alais in dlt",1,1,"1"
"alert",1,1,"1"
"alert emails not being sent",1,1,"1"
"alias",1,1,"1"
"all biogen databricks workspaces are down",1,1,"1"
"all clusters in dev environment suddenly failing with bootstrap timeouts",1,1,"1"
"all e2 environments are down.",1,1,"1"
"all envs down with following error",1,1,"1"
"all jobs are failing",1,1,"1"
"all jobs are failing with this new error message",1,1,"1"
"all jobs failing on shard",1,1,"1"
"all jobs in dx2 shard fail as cancelled",1,1,"1"
"all purpose cluster - hangs",1,1,"1"
"all purpose cluster slow on gcp",1,1,"1"
"all scheduled jobs failed this morning & could not start any all-purpose clusters",1,1,"1"
"all users were logged out of databricks workspace and cannot log in",1,1,"1"
"all webui is down",1,1,"1"
"all workflows (jobs) fail immediately with an error message",1,1,"1"
"all-purpose cluster is taking very long to terminate",1,1,"1"
"allocation failed",1,1,"1"
"allocationfailed error | 2204080010002210 | gep",1,1,"1"
"allow access to shutterfly workspace",1,1,"1"
"allow access to workspace",1,1,"1"
"allow auto user creation",1,1,"1"
"alter",1,1,"1"
"alter database  set location",1,1,"1"
"alter feature store",1,1,"1"
"alter group issues",1,1,"1"
"alter table statement failing with configurationinvalid fs.azure.account.key",1,1,"1"
"altim group",1,1,"1"
"ambiguous columns in join operation",1,1,"1"
"ami",1,1,"1"
"ami naming for spark clusters",1,1,"1"
"aml databricksstep failed to submit job to databricks",1,1,"1"
"an error occurred while calling o567.getresult",1,1,"1"
"an error occurred while enumerating the result, check the original exception for details.",1,1,"1"
"an error with streaming",1,1,"1"
"analysis",1,1,"1"
"analysisexception cannot create table",1,1,"1"
"analysisexception: no such struct field id",1,1,"1"
"analysisexception: undefined function: count spark sql",1,1,"1"
"analytics",1,1,"1"
"analytics cluster driver became unresponsive to to gc errors",1,1,"1"
"analyze tables in database",1,1,"1"
"anomalous increase in execution time",1,1,"1"
"another iam exception",1,1,"1"
"anti-virus",1,1,"1"
"any change to the vpc caused databricks terraform to recreate databricks_group_instance_profile and databricks_group_member",1,1,"1"
"any command line or api to replace jar file name in the job?",1,1,"1"
"apache certified 3.0",1,1,"1"
"apache spark associate developer accreditation",1,1,"1"
"apache spark executor memory allocation",1,1,"1"
"apache spark programming - dataframes videos do not work - academy lesson.",1,1,"1"
"api authentication",1,1,"1"
"api calls to dev servicenow from dev databricks are failing with  'user not authenticated' error",1,1,"1"
"api documentation",1,1,"1"
"api endpoint serviceprincipals is extremely slow",1,1,"1"
"api failure for library installation",1,1,"1"
"api health check on nike-ae-hopper.cloud.databricks.com",1,1,"1"
"api returned an unexpected response: 503",1,1,"1"
"api/sql interface to retrieve and terminate long running workloads on interactive clusters",1,1,"1"
"apparent bug in to_timestamp function",1,1,"1"
"append exception",1,1,"1"
"append to existing table with delta live tables",1,1,"1"
"appending parameters to job runs",1,1,"1"
"apt get install not working",1,1,"1"
"aqe",1,1,"1"
"architecture",1,1,"1"
"arda sandbox cluster is not getting started",1,1,"1"
"are gcm cipher suites enabled by default now?_azure china_2205190010000432",1,1,"1"
"argument passed to 'stream' is not supported",1,1,"1"
"arr",1,1,"1"
"arr  = 2202250030001786/ nielseniq coke mastertrack - cluster failure",1,1,"1"
"arr (s500) || aholddelhaize.com || 2206160060001800 || cluster launch failure",1,1,"1"
"arr (s500) || suncor energy inc || 2206150010003640 || internal_error: an internal error occurred during workspace initialization",1,1,"1"
"arr -  2205240040005232 - slow performance on photon cluster",1,1,"1"
"arr -  com.databricks.service.sparkserviceconnectionexception: invalid shard address: ""https://uksouth.azuredatabricks.net/",1,1,"1"
"arr - 2202250030001786 - nielseniq coke mastertrack - cluster failure",1,1,"1"
"arr - 2203180040004316 - walmart - powerbi connectivity to sql endpoint failing with aad",1,1,"1"
"arr - 2204140040007176 - credit-suisse - disable cmk on databricks workspace",1,1,"1"
"arr - 2204210040011633 - delta table write to storage hanging for hours with no errors in logs",1,1,"1"
"arr - 2204210040012860 - at&t -loading maps failing",1,1,"1"
"arr - 2204220010001158 -blobserviceclient importerror: cannot import name 'paramspec' from 'typing_extensions'",1,1,"1"
"arr - 2204220010001255 - americanairlines - job cancellation failing instead cancelling",1,1,"1"
"arr - 2204290040005232 - servicio de administración tributaria - spark job hung for over 6 hours",1,1,"1"
"arr - 2205030030001177",1,1,"1"
"arr - 2206030030001551 - deloitte - cluster hung during terminate state",1,1,"1"
"arr - 2206220040007394/issue with reading a response from databricks api",1,1,"1"
"arr - adobe - 2204080010004175 - increase jobs rest api data retention limits",1,1,"1"
"arr - approx 400 users are not able to login to  both dev and sit adb workspaces",1,1,"1"
"arr - at&t - 2204220040003095 - cluster provisioning failures",1,1,"1"
"arr - at&t - 2204280040000761 - unknown instance type messages in the log",1,1,"1"
"arr - at&t - 2204290040002695 - databricks job failing with sockettimeout error",1,1,"1"
"arr - at&t - 2205030040006573 - disk expansion with jobs",1,1,"1"
"arr - atabricks notebook issues with photon in runtime 10.4 lts",1,1,"1"
"arr - att - invalid fs.azure.account.key error - while trying to save data to adls container from databricks- 2205180040008310",1,1,"1"
"arr - att : failed to establish a new connection: [errno 110] connection timed out'))  : 2204110040003935",1,1,"1"
"arr - bp - 2204040040008424 - sqlconnect on npip workspace is failing",1,1,"1"
"arr - codelco - 2204280040004711 - high data ingestion from eventhubs with no change in messages",1,1,"1"
"arr - connection with tableau",1,1,"1"
"arr - cummins - 2203310030003446 - job hung from 5 months",1,1,"1"
"arr - customer cannot start up clusters",1,1,"1"
"arr - customer getting cluster termination in eastus 2206210030001985",1,1,"1"
"arr - customer getting error on gui when viewing all purpose cluster",1,1,"1"
"arr - customer would like access to private preview feature - private workspaces",1,1,"1"
"arr - cvshealth - 2204050040008980 - databricks job failures",1,1,"1"
"arr - databricks interal storageaccount and redundancy level-",1,1,"1"
"arr - error in the concurrency of jobs (n runids of the same jobsid) that need to call the rest api to the endpoint update and then run-now",1,1,"1"
"arr - ey - 2205200040005272 - databricks jobs failed",1,1,"1"
"arr - failure to launch databricks fro adf - subscriptionrequeststhrottled exceeded the backend storage limit",1,1,"1"
"arr - ganglia metrics not loading in linux machine",1,1,"1"
"arr - gap - 2205200010002443 - diagnostics logs are not available",1,1,"1"
"arr - intermittent error in ml workspace while run/rerun a ml pipeline with databricksstep",1,1,"1"
"arr - job in production taking 5 hours",1,1,"1"
"arr - jobs timing out",1,1,"1"
"arr - log4j 1.2.17 version cve-2019-17571 - in 9.1 lts runtime",1,1,"1"
"arr - mtr - 2205140060000097 - libraries failing to install but shows as installed",1,1,"1"
"arr - nba - 2205120040006710 - data processing is failing",1,1,"1"
"arr - not being able to open folder structures from storage",1,1,"1"
"arr - numerous driver is up but is not responsive, likely due to gc.",1,1,"1"
"arr - recover deleted cluster",1,1,"1"
"arr - sev a - cluster terminated.reason:azure vm extension failure",1,1,"1"
"arr - sev a - databricks notebook is stuck",1,1,"1"
"arr - sev a dbutils.fs.ls taking long to execute and returns 503 error",1,1,"1"
"arr - sev b  - customer not able to run job in dbr 10.x",1,1,"1"
"arr - slowness in delta tables",1,1,"1"
"arr - sql code not working in 10.4",1,1,"1"
"arr - sql using a cte that runs fine in databricks runtime 9.1 lts photon, but gives an error in 10.4 lts photon",1,1,"1"
"arr - uhg - 2204160040000471 - parse_url failing with % at the end of url",1,1,"1"
"arr - walmart - 2204080030002206 - rca - job failure",1,1,"1"
"arr - walmart - 2204080030002206 - rca - job failure on azure databricks workspace",1,1,"1"
"arr - write txt file with headers in the datalake from databricks",1,1,"1"
"arr - z-tech - 2204020040000988 - waf deployment with azure databricks",1,1,"1"
"arr -att- unable to set up external metastore with aad only enabled on the sql metastore- 2205300040000140",1,1,"1"
"arr 2203030050000273 -  customer wants to understand failures and retries on spark tasks",1,1,"1"
"arr 2204110050002238  - not able to connect to databricks cli using aad token",1,1,"1"
"arr 2204110050002238 - update repos not working",1,1,"1"
"arr 2204130050000446  genie",1,1,"1"
"arr 2205040030001503 | empty 4b delta files generation",1,1,"1"
"arr 2205180010001040 - customer's workspace is slow",1,1,"1"
"arr 2205200040003030 - jobs running slow",1,1,"1"
"arr 2205230050000756 - databricks jobs stopped",1,1,"1"
"arr 2206010010001042  -  job failing intermittently when writting to job metadata",1,1,"1"
"arr 2206020060002171- job started failing after dbr upgrade",1,1,"1"
"arr 2206030030000842 - spark timeout failure",1,1,"1"
"arr 2206030030000851 - spark streaming job fails randomly",1,1,"1"
"arr 2206060030001835 - dbr 10.1 reads metastore while dbr9.3 don't",1,1,"1"
"arr 2206150030002085 - job started getting slow",1,1,"1"
"arr 2206220040005881 - spark udf using ray works on dbr9.1 but not on dbr10.4",1,1,"1"
"arr 2206230030000787 - count descriptancy in between read and write operation",1,1,"1"
"arr air canada - jobs randomly failing with module not found.",1,1,"1"
"arr american airlines - customer requests assisance removing duplicates in delta table.",1,1,"1"
"arr customer adobe - access privileges to jobs api (read only)",1,1,"1"
"arr customer aholddelhaize.com : issue - jobs taking long",1,1,"1"
"arr customer air canada - databricks job aborted",1,1,"1"
"arr customer air canada: issue - random dns failures occurring when using the azure firewall using dns proxy enabled, which is azure dns.",1,1,"1"
"arr customer altria: databricks repos sync issues from azure devops",1,1,"1"
"arr customer barracuda - autoloader in databricks automatically defaults to abfss instead of wabs",1,1,"1"
"arr customer chevron - consultation on databricks performance",1,1,"1"
"arr customer chicago trading company: issue - databricks clusters are inaccessible",1,1,"1"
"arr customer chicago trading company: issue - unable to access clusters.",1,1,"1"
"arr customer cibcpte - azure sql procedure execution in databricks notebook failing",1,1,"1"
"arr customer cibcpte - stored sql procedure not working",1,1,"1"
"arr customer credit suisse: issues using databricks connect in their analysis workspace when using intellij.",1,1,"1"
"arr customer credit swiss - unable to run notebooks",1,1,"1"
"arr customer cummins - not able to run scala scripts on 10.4 lts",1,1,"1"
"arr customer cummins: issue -  trying to use import pandas_profiling (pandas-profiling · pypi) but it is failing",1,1,"1"
"arr customer cvs: issue - unable to grant read/list permission to databricks service principal to keyvault, key not found",1,1,"1"
"arr customer daimler ag: issue - workspace not performing correctly",1,1,"1"
"arr customer engie: issue - problems accessing databricks workspace",1,1,"1"
"arr customer ford direct: issue - problem using spark r",1,1,"1"
"arr customer geico - nosuchmethoderror exception suspect to be the classpath issue",1,1,"1"
"arr customer giliead sciences: issue - module os has no attribute pathlike",1,1,"1"
"arr customer jnj: ganglia ui is not working & metrics are not accessible.",1,1,"1"
"arr customer myfedex - i am encountering an error during cluster launch or creation",1,1,"1"
"arr customer navy federal: issue - problem downloading models from the spark nlp library",1,1,"1"
"arr customer nbc universal: issue writing from blob storage to azure synapse dw",1,1,"1"
"arr customer nestle: issue -  time out error - org.apache.spark.sparkexception: environment directory not found at /local_disk0/",1,1,"1"
"arr customer nestle: jobs failing with environment directory not found",1,1,"1"
"arr customer rogers inc: unable to deploy clusters",1,1,"1"
"arr customer safeway: databricks workspace is intermittently inaccessible. this affects them logging into the workspace and attempting to start their notebooks. the main screen loads, but selecting a query fails to bring up that query.",1,1,"1"
"arr customer safeway: issue - th customer has an issue where the databricks workspace is intermittently inaccessible. this affects them logging into the workspace and attempting to start their notebooks. the main screen loads, but selecting a query fails",1,1,"1"
"arr customer starbucks: issue - intermittent issues during auto scaling.",1,1,"1"
"arr customer starbucks: issue - job failed with exception thrown in awaitresult",1,1,"1"
"arr customer starbucks: issue - problems initiating cluster upsize",1,1,"1"
"arr customer uniparks: issue - databricks to storage/synapse connection issue",1,1,"1"
"arr customer vca in: issue - unable to deploy databricks clusters",1,1,"1"
"arr customer: aa - job failing in databricks with concurrent read error",1,1,"1"
"arr customer: deloitte - unable to access databricks cluster",1,1,"1"
"arr customer: gap - issues launching databricks clusters.",1,1,"1"
"arr customer: geico - issue with cluster",1,1,"1"
"arr customer: helmerich & payne, inc. - databricks cluster driver is unhealthy",1,1,"1"
"arr customer: lidl - cluster terminated.reason:azure resource provider throttling",1,1,"1"
"arr customer: mgm resorts - failure in running python script in databricks",1,1,"1"
"arr customer: st joseph - job stuck running",1,1,"1"
"arr customer: st joseph - there is an issue with databricks. we are not able to run any queries in databricks workspace",1,1,"1"
"arr customer: st joseph - there is an issue with databricks. we are not able to run any queries in databricks workspace.",1,1,"1"
"arr customer: td bank - ingestion failures in production",1,1,"1"
"arr customer: unilever - unable to access adb environment",1,1,"1"
"arr customer: uniparks - databricks to storage/synapse connection issue",1,1,"1"
"arr genie access",1,1,"1"
"arr s500 | 2205040030001107 | jobs api issues",1,1,"1"
"arr sev a -    occproductupcconsumer job failure",1,1,"1"
"arr sev a - customer not able to create repo in databricks workspace",1,1,"1"
"arr sev a | 2206300030000564 - spark job failing with metastore issue",1,1,"1"
"arr sev.a 2205120030001265",1,1,"1"
"arr ubs - unable to deploy the workspace with cmk for notebook using the arm template provided by databrick",1,1,"1"
"arr |  2203090040006252 | follow up to 00137308",1,1,"1"
"arr |  data issue on enabling photon",1,1,"1"
"arr |  duplication during delta lake merge",1,1,"1"
"arr |  gc allocation failure |",1,1,"1"
"arr |  job reported time longer than sum of individual cells | 2204190030002165",1,1,"1"
"arr |  nielseniq coke mastertrack - cluster failure",1,1,"1"
"arr |  optimize delta load |  optimize delta load",1,1,"1"
"arr |  t-mobile usa | job stuck at create df | sr: 2205020040000453",1,1,"1"
"arr |  | 2205130040004443",1,1,"1"
"arr | 2203210050001428 | scroll bars are not working in firefox",1,1,"1"
"arr | 2203280010003072 | analyze log analytics logs for activities from databricks notebook",1,1,"1"
"arr | 2203290030003204 | question about the behavior of scim provisioning with aad (preview)",1,1,"1"
"arr | 2204040050000751 | is there a max limit to number of global tables per workspace?",1,1,"1"
"arr | 2204060050000469 | installing python packages fails",1,1,"1"
"arr | 2204070010002917 | does notebooktask in job api support adls paths?",1,1,"1"
"arr | 2204080030000725 | api to attach notebook to cluster",1,1,"1"
"arr | 2204080030001938 | workspace creation is failing | standard chartered bank",1,1,"1"
"arr | 2204080050001186 | databricks cluster failure with message cause null",1,1,"1"
"arr | 2204110030000436 - cluster starting time is took longer time than usual",1,1,"1"
"arr | 2204110040002270 | request access to databricks secrets using dbutils",1,1,"1"
"arr | 2204130030001643 | the job is failing to run multiple notebooks in the same job",1,1,"1"
"arr | 2204140030000156 | the job in adf pipeline needed to cancel due to taking longer than expected.",1,1,"1"
"arr | 2204140030001728 |  jsondecodeerror: expecting value: line 1 column 1 (char 0) when parsing the bytes object",1,1,"1"
"arr | 2204160050000039 | unable to use allocated quota on databricks job",1,1,"1"
"arr | 2204170050000058 | oracle connectivity issue",1,1,"1"
"arr | 2204180030000346 | gep | job failed intermittently with cluster doesn't exist",1,1,"1"
"arr | 2204210050004881 | load sql server data into databricks",1,1,"1"
"arr | 2204220060001879 |",1,1,"1"
"arr | 2204250060001035 | error with union command saying object of type 'nonetype' has no len() only in dbr  9.1 lts",1,1,"1"
"arr | 2204260030001644 | grant access control to mount point",1,1,"1"
"arr | 2204260030002207 |  run result unavailable: job failed with error message unexpected failure while waiting for the cluster (0426-153040-3azvvay4) to be ready.cause cluster 0426-153040-3azvvay4 is unusable since the driver is unhealthy",1,1,"1"
"arr | 2204260030002207 | genie",1,1,"1"
"arr | 2204270030002047 - [needs ist support] resource accidental deletion",1,1,"1"
"arr | 2204280030000731 - delta lake ingestion from spark structured streaming is taking more time",1,1,"1"
"arr | 2204280040001718 | cluster node are getting lost | jnj",1,1,"1"
"arr | 2204280040006201 | reading data from snowflake connection",1,1,"1"
"arr | 2204280060001053 | genie",1,1,"1"
"arr | 2204290030001039 - we are not able find log who deleted the databricks cluster",1,1,"1"
"arr | 2205040040003792 | metastore operations with adls gen2 conf",1,1,"1"
"arr | 2205100040004471 | unexpected failure while waiting for the cluster | jnj",1,1,"1"
"arr | 2205100050002215 | jobs are failing even after adding dependent libs | gsk",1,1,"1"
"arr | 2205110030001559 | genie",1,1,"1"
"arr | 2205110030001559 | please help find instanceid/vmid",1,1,"1"
"arr | 2205120030000229 | was there any change recently in databricks in regard to installing libraries when running a job from adf",1,1,"1"
"arr | 2205140060000097 | job aborted due to stage failure in databricks sql",1,1,"1"
"arr | 2205160030000400 | clusters are not starting | sc",1,1,"1"
"arr | 2205160030001741 | azureadauthenticator.gettokencall threw java.net.sockettimeoutexception : read timed",1,1,"1"
"arr | 2205160030001890 | genie",1,1,"1"
"arr | 2205160040000885 - resource pool management",1,1,"1"
"arr | 2205160040000885 - resource pool management | feature request",1,1,"1"
"arr | 2205160050001904 | cluster tasks were cancelled",1,1,"1"
"arr | 2205180050001892 | unable to access to history of the specific delta table",1,1,"1"
"arr | 2205190030001487 | a user is unable to access to the sql editor even with permission.",1,1,"1"
"arr | 2205200030001371 | job intermittently failing with unexpected_exception | jnj",1,1,"1"
"arr | 2205200040002676 - databricks overwatch proxy configuration issue",1,1,"1"
"arr | 2205200050000441 | databricks security and compliance",1,1,"1"
"arr | 2205210030000268 - adls service principal based access in databricks for shell usage",1,1,"1"
"arr | 2205230030000498 - databricks_api_issue",1,1,"1"
"arr | 2205230030001082 | is access to the delta table using adls credential passthrough with sql endpoints supported?",1,1,"1"
"arr | 2205230040005172 | library installation issue",1,1,"1"
"arr | 2205250060001495 |  pointsbet | outage | unable to start clusters",1,1,"1"
"arr | 2205260030002175 - we are facing socket timeout issues (connection refused) in pyspark while trying to perform the below operation",1,1,"1"
"arr | 2205260040001988 | workspace folders were deleted in production",1,1,"1"
"arr | 2205270010001206 - unity catalog: unable to create an external table at an external location.",1,1,"1"
"arr | 2205270030000850 - slowness in job",1,1,"1"
"arr | 2205310030001011 - databrick cloud provider error",1,1,"1"
"arr | 2206010060001610 | need help to set up gen1 debug log | follow up 00129075",1,1,"1"
"arr | 2206070030000680 - cluster unresponsive due to garbage collector",1,1,"1"
"arr | 2206070030000680 - databricks linux transformation is failing",1,1,"1"
"arr | 2206070040000816| delta live table not refreshing",1,1,"1"
"arr | 2206080030002199 - the cluster is not able to autoscale as per requirment.",1,1,"1"
"arr | 2206080050000999 | ml libraries are not installing | gsk",1,1,"1"
"arr | 2206080050002070 | spark streaming query not restarting from checkpoint",1,1,"1"
"arr | 2206090030000397 | databricks vacuum dry run result incorrect",1,1,"1"
"arr | 2206100050002271 | issue with delta table storage",1,1,"1"
"arr | 2206130030001354 | any ways to run defined udfs or attach jars to run udfs in databricks sql",1,1,"1"
"arr | 2206130050001255 | provide read permissions to all jobs running on job clusters",1,1,"1"
"arr | 2206140010002101 | file pruning is not happening for sql queries | inmobi",1,1,"1"
"arr | 2206160060001294 | request to increase job quota to 1500 in the workspace",1,1,"1"
"arr | 2206160060001434 | adb is not working | unilever",1,1,"1"
"arr | 2206160060001444 - executors get assigned but they do nothing (idle) for most of the time on databricks sql",1,1,"1"
"arr | 2206170030001002 | databricks job is failing with error: importerror",1,1,"1"
"arr | 2206170050000549 | taking too long loading the ui or submitting any command",1,1,"1"
"arr | 2206210030000382 - looking for advisory on 6.4-esr",1,1,"1"
"arr | 2206210030000948 - the driver memory gets and full never gets free",1,1,"1"
"arr | 2898246923673559 | 503 server unavailable for rpc lost when calling /api/2.0/clusters/start",1,1,"1"
"arr | 404 error on sql endpoint  | 2206030030000803",1,1,"1"
"arr | adobe | databricks notebook task parse failure using databricks airflow submit run operator|  2206090010002339",1,1,"1"
"arr | aia | databricks ssl cert expire date | sr: 2204070060000837",1,1,"1"
"arr | aircanada | we need explanation why init script or job dependent libraries are recommended for modulenotfound error",1,1,"1"
"arr | american airlines | cluster launch failed due to unexpected_launch_failure(service_fault) | sr - 2206070010000054",1,1,"1"
"arr | analysisexception: undefined function: ""concat"" this functions is neither a registerd | 2206090030000308",1,1,"1"
"arr | at&t | 2204140040007011 | mlflow registory fails to register model when 9.1 lts non ml cluster is used.",1,1,"1"
"arr | at&t | delta tables showing latest partition alone",1,1,"1"
"arr | at&t | getting this error 'org.apache.hadoop.fs.unsupportedfilesystemexception: no filesystem for scheme",1,1,"1"
"arr | at&t | unable to open the model artifact file from function app using python script",1,1,"1"
"arr | azure databricks error  : id token is not yet valid. make sure your computer's time and time zone are both correct.| 2206090050002728",1,1,"1"
"arr | blackrock-onealaddin| restapi using access token+management token not working for workspace enabled private endpoint| sr:2205190030000177",1,1,"1"
"arr | citrix | need help change the cluster owner | sr:2205250060000952",1,1,"1"
"arr | cluster is in pending state forever | 2204070050000281 | uhg",1,1,"1"
"arr | continuation of 00138590 |cluster failed due to driver unhealthy error - 2203150040007744",1,1,"1"
"arr | customers are not able to create repo in databricks workspace | 2206230040006798 | at&t",1,1,"1"
"arr | cve reports - potential security issue in db runtime 7.3 and 9.1|2206100040004623",1,1,"1"
"arr | databricks insertinto is behaving differently when run from our application vs notebook",1,1,"1"
"arr | databricks job deleted automatically | 2205250030001931",1,1,"1"
"arr | databricks job running slow | 2205030040003955",1,1,"1"
"arr | databricks permissions issue",1,1,"1"
"arr | databricks spark ui show completed jobs as active jobs",1,1,"1"
"arr | delta table question | 2204220050000908",1,1,"1"
"arr | enable dbutils.secrets.get | 2205310050001908",1,1,"1"
"arr | error while conneting the databricks to powerbi/tableau/pyodbc etc | 2206240040006482",1,1,"1"
"arr | exception when doing a vacuum| 2204290050000799",1,1,"1"
"arr | exception when using guave version 23  | 2206210030000985",1,1,"1"
"arr | exception: a master url must be set in your configuration | 2204280030001723",1,1,"1"
"arr | follow up case - 00180734",1,1,"1"
"arr | follow-up sf 00182159 | 2206090030000397 | databricks vacuum dry run result incorrect",1,1,"1"
"arr | gap | job cluster with specific pool launch fail with array index error. | sr: 2205030010000748",1,1,"1"
"arr | gap| error accessing feature store in a remote workspace | 2205090010004670",1,1,"1"
"arr | genie access",1,1,"1"
"arr | genie | 2205110050002089",1,1,"1"
"arr | genie | cluster launch is failing and getting successful on retry - 2204190040007896",1,1,"1"
"arr | genie | cluster not terminated due to inactivity - 2205020040004681",1,1,"1"
"arr | genie | databricks lost connectivity to ftp2.ups.com - 2203290040006520",1,1,"1"
"arr | genie | intermittent databricks errors in the pipeline - 2203210010002555",1,1,"1"
"arr | genie | modulenotfounderror: no module named 'util' - 2204290040004692",1,1,"1"
"arr | genie | slow insert from azure databricks into cosmos db, api cassandra - 2204180040002930",1,1,"1"
"arr | genie | the 'r' package 'prophet' fails to load | 2205090040005472",1,1,"1"
"arr | genie | workspace settings require dbr 8.4+ to access files in repos from clusters | #2205050040003382",1,1,"1"
"arr | genie| jobs failed in azure databricks with sig term issue | 2206210040007194",1,1,"1"
"arr | getting a certificate from keyvault - not key  | 2205310050001908",1,1,"1"
"arr | getting time out error when jobs are triggered",1,1,"1"
"arr | got network failure making cluster terminate | 2206210010002275",1,1,"1"
"arr | impact on vacuum",1,1,"1"
"arr | inditex | job failed due to no permission to secret scope | sr: 2206220050000104",1,1,"1"
"arr | intermitten error: table or view not found",1,1,"1"
"arr | issue reading large tables from powerbi to databrics sql",1,1,"1"
"arr | issue with cluster resizing & autoscaling | 2205170040006218",1,1,"1"
"arr | issues with library importing | 2205020050000332",1,1,"1"
"arr | jdbc error connecting to synapse | 2204190030002165",1,1,"1"
"arr | job api limit | 2204190040001460 | capital group",1,1,"1"
"arr | job cluster error: spark_image_download_failure",1,1,"1"
"arr | jobs are failing with dependent library | 2204220060001123 |shell",1,1,"1"
"arr | jobs are timing out on job-cluster after running approximately 17 mins",1,1,"1"
"arr | library loading failed | 2205260010001471",1,1,"1"
"arr | mlflow project run failure, no logs available | 2203230050002013",1,1,"1"
"arr | module not found error",1,1,"1"
"arr | mount failing",1,1,"1"
"arr | mysql lock problem | 2205200030001217",1,1,"1"
"arr | non-fatal databricks write/ read operations to azure storage via abfsclient |  2204200040006207",1,1,"1"
"arr | not able to read excel file even after adding library | 2206140030001383  | honeywell",1,1,"1"
"arr | notebook failure |  2204040050001708",1,1,"1"
"arr | notebook failure | 2204040050001708",1,1,"1"
"arr | overwatch - null values in jrcp table - workspace 7 - 2204180040004421",1,1,"1"
"arr | overwatch job modules are failing after the upgrade |  2206010040003191",1,1,"1"
"arr | pepsico | databricks cluster node is having issue with garbage collection memory |  2206030030000673",1,1,"1"
"arr | performance issue by upgrading runtime from 9.1 to 10.4",1,1,"1"
"arr | personalized logs at init script | 2205130010001143",1,1,"1"
"arr | production issue : unable to read delta tables | gep | 2205250030000679",1,1,"1"
"arr | request to increase max job limit to 10k -  2206230040007255",1,1,"1"
"arr | s500 | 2205070050000069 | the rate limit increase request which is 100 nodes per minute",1,1,"1"
"arr | sa power networks | need to understand is there any potential security risk if we enable gcm in extra.security file | sr: 2204270030000738",1,1,"1"
"arr | serv a | adobe |job failed due to oom | sr: 2204290030000582",1,1,"1"
"arr | serv a | mars inc | suspect outage related issue | sr: 2206070030000401",1,1,"1"
"arr | seva | 2204280040000761 | job run for more than 9 hrs unexpectedly",1,1,"1"
"arr | seva | 2205160050000323 | databricks jobs are intermittently failing: cannot import name 'leaseproperties' from 'azure.storage.blob'",1,1,"1"
"arr | seva | 2205260040003611 | backed service unavailable in eastus2",1,1,"1"
"arr | seva | 2205300040003430 | databricks jobs are failing after dbr upgrade in prod",1,1,"1"
"arr | seva | 2206010030000952 | databricks job taking longer time",1,1,"1"
"arr | seva | 2206220040001462 | python script is failing from adf",1,1,"1"
"arr | seva | streaming job failing with concurrent timeoutexception",1,1,"1"
"arr | shell | 2204220060001123 | follow up ticket 00143284 | init scripts are not downloading the jar with dependencies |",1,1,"1"
"arr | slow cluster | 2204220040002906",1,1,"1"
"arr | slow job | 2205160040004016",1,1,"1"
"arr | spark dataframe checkpoints under table acl - continuation of 00142979 - 2204200040005509",1,1,"1"
"arr | t-mobile usa | reading to df got stuck | sr: 2205020040000453",1,1,"1"
"arr | the 'r' package 'prophet' fails to load | 2205090040005472",1,1,"1"
"arr | throttling leading to cluster creation failure | 2204220060002439",1,1,"1"
"arr | ucb | the clusters are not running and have been in the starting phase for over 50 mins now |  2206100050002302",1,1,"1"
"arr | unable to access sample datasets on databricks control plane from dtv databricks workspace",1,1,"1"
"arr | unable to connect to azure repos from databricks",1,1,"1"
"arr | username not being identified | 2205200010000014",1,1,"1"
"arr | woolworths limited | dbfs api is not working for a storage with firewall | sr: 2206070030001445",1,1,"1"
"arr | | databricks cluster not being created / :azure vm extension failure | 2205180050001306",1,1,"1"
"arr ||  anheuser-busch inbev || 2205260030002078 || dbe 10.4 issue in creating view.",1,1,"1"
"arr ||  anheuser-busch inbev || 2205260030002078 || dbr 10.4 issue",1,1,"1"
"arr ||  daa-devops || job fails due to outofmemoryerror every 8 hours || 2206290010000004",1,1,"1"
"arr ||  safeway, inc. ||  databricks - driver is up but is not responsive, likely due to gc || 2206160040007720",1,1,"1"
"arr || adobe || 2206130010001905 || bad vm in a databricks workspace",1,1,"1"
"arr || assurant || 2204200040005894 || workspace conf rest api failures",1,1,"1"
"arr || at&t ||  2204010040006385 || self_bootstrap_failure",1,1,"1"
"arr || at&t || 2204010040006385 || rca for cluster terminated due to self bootstrap failure",1,1,"1"
"arr || at&t || 2206220040007241 || need help in benchmarking the streaming job runs",1,1,"1"
"arr || att ||getting an error when listing files in a container in  saplogprdstattloganaly storage account ||2206130040008340",1,1,"1"
"arr || capital group || 2205060040005682 ||  dbr upgrade from 8.3 to 9 lts /10.4 lts causing job performance issue and failures",1,1,"1"
"arr || credit suisse || 2206100040007795 || job fails to connect to key vault",1,1,"1"
"arr || gap || 2205200010002443 || diagnostics logs are not available",1,1,"1"
"arr || geico|| databricks schedule job doesn't retry || 2206220040008314",1,1,"1"
"arr || inditex || 2206150020002427 || fetch secrets in westeurop region failing due to latency",1,1,"1"
"arr || nfcu || 220301004000853 || cluster aad connectivity down",1,1,"1"
"arr || safeway || did_not_expand_disk,failed_to_expand_disk errors on databricks cluster ||  2206230010003162",1,1,"1"
"arr || samsclub || databricks job failing intermittently || 2206150040000044",1,1,"1"
"arr || sev a||  coles supermarket || databricks workspace not loading || 2206210030000029",1,1,"1"
"arr ||morgan stanley || job fails with network configuration failure || 2206280040007729",1,1,"1"
"arr- 2204010040007120- inconsistent results in query",1,1,"1"
"arr- att - 2205190040007411 - job is running more than 18 hours",1,1,"1"
"arr- att - 2205190040007411 - job is running more than 18 hours- followup for 00150434",1,1,"1"
"arr- att - job cluster running streaming job fails - 2204290040004745",1,1,"1"
"arr- att : getting connection error from java code - 2204130040010988",1,1,"1"
"arr- att- not able to read particular partition with service principal credentials- 2206080040008965",1,1,"1"
"arr- att- spark ignoremissingfiles is not working with single node databricks clusters-2205120040001867",1,1,"1"
"arr- att- spark ignoremissingfiles is not working with single node databricks clusters-2205120040001867|| followup of  00157643",1,1,"1"
"arr- att: 2204250040008149 -  databricks mlflow registry cannot communicate with storage account",1,1,"1"
"arr- databricks cluster not loading dependency consistently- 2205160030002086- followup of 00147763",1,1,"1"
"arr- existing cluster does not sync after job cluster finish- 2204260050002691",1,1,"1"
"arr- occp databrciks jobs getting error(pendingpriceconsumer & regularpriceconsumer)",1,1,"1"
"arr- pepsico- modulenotfounderror: no module named 'tabulate'- 2205130040007034",1,1,"1"
"arr- throttling issues",1,1,"1"
"arr- walmart- databricks job failing intermittently - 2206150040000044",1,1,"1"
"arr-2206170010003168-facing slowness in databricks",1,1,"1"
"arr-[reg:2203300030003575]issues in automating workspace setting of databricks",1,1,"1"
"arr-css_sfmc-databricks production job has been failed multiple times",1,1,"1"
"arr-getting time out issues with storage account",1,1,"1"
"arr-sfmc-2204040030002172-quota request for compute-vm (cores-vcpus)",1,1,"1"
"arr-sfmc-2206090050000110-cluster terminated.reason:azure resource provider throttling",1,1,"1"
"arr: jobs are failing intermittently:sr 2204070040000867",1,1,"1"
"arr: query delta table returns no result : sr2206080010000484",1,1,"1"
"arr: sev a:streaming jobs are stuck:sr 2204060040007641",1,1,"1"
"arr: tensorboard won't launch properly from azure databricks notebook:sr2206220010002773",1,1,"1"
"arr: why does resultset take 14+ seconds to filter 1.4m records:sr2206010040005495",1,1,"1"
"arr:'model version is stuck in 'pending' status after enabling databricks online real-time inference.sr2206100040004911",1,1,"1"
"arr:2204200010007619:connection timed out for adb job",1,1,"1"
"arr:2204210040012046  streaming job databricks performance issues",1,1,"1"
"arr:2204210040012046 streaming jobautoscaling issue",1,1,"1"
"arr:cluster usage went down and jobs got delayed and failed on around 04/06/2022 10:02:00 utc.:sr2204060040007331",1,1,"1"
"arr:follow up 00139658:unable to load filestore files in python dataframe. shows filenotfound error though file is present:sr 5356886492280006",1,1,"1"
"arr:follow up sf 00133177: can't connect to cx_oracle.connect:sr2205130040004909",1,1,"1"
"arr:follow up sf00180631:additional question for fix release and root cause:sr2205250040007523",1,1,"1"
"arr:genie:mlflow:sr2206170010005234",1,1,"1"
"arr:identity column not unique,seeing duplicate values in the identity column:sr2205250040005673",1,1,"1"
"arr:job failed on dbr10.4 lts, java.io.filenotfoundexception: operation failed: 'the specified path does not exist.', 404, head:sr205200040003957",1,1,"1"
"arr:mlflow model serving api errors occurred intermittently, repro’d encountered error msg rate has been increased since june 8th 2022:sr2206170010005234",1,1,"1"
"arr:model serving endpoint not coming up with error msg:sr2206200030001878",1,1,"1"
"arr:no stream progress with no-op delta merge:sr2205110040005641",1,1,"1"
"arr:pyodbc driver cannot open lib ‘odbc driver 17 for sql server’:file not found(0) while connecting to azure sql. encountered error msg intermittently:sr2205060040003815",1,1,"1"
"arr:seva: can't read xml from storage container:sr2204110040005543",1,1,"1"
"arr:seva: intermittently failed connectivity between adf pipeline and databricks:sr2204070030001901",1,1,"1"
"array",1,1,"1"
"arrow indexoutofboundsexception",1,1,"1"
"arr|  azure databricks - power bi cluster - degraded performance | 2206280040005838",1,1,"1"
"arr|  error 'azure_resource_provider_throttling' when trying to start cluster | 2206210040008150",1,1,"1"
"arr|  jobs failed",1,1,"1"
"arr|  pointsbet | job stuck at getting secret | sr: 2205180030000806",1,1,"1"
"arr| 2204010050001066 | job failure oserror: [errno 107] transport endpoint is not connected",1,1,"1"
"arr| abnormal behaviour databricks workspace | 2206210040008150",1,1,"1"
"arr| adobe | job api 2.1 response schema doesn't match document |sr: 2205180010000639",1,1,"1"
"arr| azureresourceproviderthrottling during cluster upsize | 2205170040004867",1,1,"1"
"arr| caused by: com.microsoft.azure.eventhubs.receiverdisconnectedexception | 2206100030001353",1,1,"1"
"arr| caused by: com.microsoft.azure.eventhubs.receiverdisconnectedexception| 2206100030001353",1,1,"1"
"arr| cluster termination",1,1,"1"
"arr| com.microsoft.azure.eventhubs.receiverdisconnectedexception:",1,1,"1"
"arr| continuation of 00142833 - cosmosdbspark failure on large files - 2203310040008004",1,1,"1"
"arr| continuation of 00143529 | issue using databricks repo api",1,1,"1"
"arr| continuation of 00147901 - issue using databricks repo api - 2204070030000264",1,1,"1"
"arr| decimal precision error in databricks | 2206220040008977",1,1,"1"
"arr| degraded databricks sql query performance |2206100010001845",1,1,"1"
"arr| erros in event log | 2206060030001859",1,1,"1"
"arr| ey | i am writing a piece of code which can enable direct access via sas token rather than using adls key. this is for automated job clusters || 2206150040008057",1,1,"1"
"arr| fwd group management holdings limited | ml job failed |sr:2205130030000496",1,1,"1"
"arr| genie |  the databricks job fails frequently - 2204250010002866",1,1,"1"
"arr| genie | databricks was unable to connect to vnet injected postgresql server for a couple hours - 2204120010008387001",1,1,"1"
"arr| genie | performance issues - 2204050040007179",1,1,"1"
"arr| genie | restore deleted databricks workspace - 2206270030001681",1,1,"1"
"arr| genie | unable to launch clusters due to azure vm extension failure - 2205170040004703",1,1,"1"
"arr| genie| azure machine learning job failed in databricks -2203310040006132",1,1,"1"
"arr| genie| databricks mounted blobs issue | 2205200050000856",1,1,"1"
"arr| genie| error when install catboost |2206100040005802",1,1,"1"
"arr| hp inc.| databricks-registry-webhooks |sr:2204270030001287",1,1,"1"
"arr| nfcu || cluster aad connectivity down",1,1,"1"
"arr| problem running h2o on hc clusters - 2205310040006441",1,1,"1"
"arr| providence st. joseph health | unable to read and write from cosmos | 2206080010002795",1,1,"1"
"arr| rca for intermittent errors in production environment - 2203210010002555",1,1,"1"
"arr| view() is not working in in databricks r notebook| 2206130040008635",1,1,"1"
"arr|at&t | unable to connect to databricks from powerbi for only some tables.",1,1,"1"
"arr|at&t| running jobs using databricks sql connector-",1,1,"1"
"arr|code push error",1,1,"1"
"arr|dlt cluster stuck in waiting_for_resources | 2206160010003081",1,1,"1"
"arr|genie| unable to use custom container from proxy - 2204080040009286",1,1,"1"
"arr|| mgm resorts international || databricks clusters were removed without notice || 2206210010003378",1,1,"1"
"assigning account level groups to workspace clusters",1,1,"1"
"assistance in tracking a spike in vpc network cost",1,1,"1"
"assistance required in performance tuning for data ingestion from rds",1,1,"1"
"associate data engineer certification",1,1,"1"
"associated sql query",1,1,"1"
"assumedrolecredentialprovider error on 10.4 lts",1,1,"1"
"assumerole time limit",1,1,"1"
"at&t - 2205060040004996 - global-int script failed - rca",1,1,"1"
"at&t escalation case - please check problem description",1,1,"1"
"at&t:arr:r-shinny application time out if no activity for over 5 mins:sr2204210040012118",1,1,"1"
"attribute error: module 'lib' has no attribute 'x509_v_flag_cb_issuer_check''",1,1,"1"
"attributeerror: 'numpy.random.mtrand.randomstate' object has no attribute 'integers'",1,1,"1"
"attributeerror: module 'lib' has no attribute 'x509_v_flag_cd_issuer_check",1,1,"1"
"attributeerror: module ‘lib’ has no attribute ‘x509_v_flag_cb_issuer_check’",1,1,"1"
"audit",1,1,"1"
"audit log e1 account",1,1,"1"
"audit logging",1,1,"1"
"audit logging api error",1,1,"1"
"audit logs configuration details for single tenant workspaces",1,1,"1"
"audit trail for jobs",1,1,"1"
"auditlog設定時のs3:deleteobjectは必須権限となるか。",1,1,"1"
"authentication error while logging to databricks",1,1,"1"
"authentication unavailable",1,1,"1"
"auto  send email",1,1,"1"
"auto optimize target files impact on symlink files for aws presto\athena tables",1,1,"1"
"auto refresh job configurations",1,1,"1"
"auto scaling issue",1,1,"1"
"auto upgrade cluster",1,1,"1"
"autoloader backfillinterval causes stream to pause for long duration.",1,1,"1"
"autoloader can not ingest *.snappy.parquet files",1,1,"1"
"autoloader cloudfiles events by creation timestamp",1,1,"1"
"autoloader failed to load parquet files when wildcard is used",1,1,"1"
"autoloader file notification config issue",1,1,"1"
"autoloader is skipping some files to load",1,1,"1"
"autoloader issues",1,1,"1"
"autoloader not streaming new records",1,1,"1"
"autoloader not working",1,1,"1"
"autoloader performance issues",1,1,"1"
"autoloader populates timestamp column as null",1,1,"1"
"autoloader streaming is not consistent",1,1,"1"
"autoloader streaming query issue",1,1,"1"
"autoloader task getting stuck for 50 hours",1,1,"1"
"autoloader tasks keep getting stuck",1,1,"1"
"automated way to update the job ownership and dbr runtime.",1,1,"1"
"automatic node upgrades is disabled?",1,1,"1"
"automatically run notebook when cluster starts",1,1,"1"
"automation testing",1,1,"1"
"automl experiment giving unknown error",1,1,"1"
"automl fails with retryerror: httpsconnectionpool(host='nvirginia.cloud.databricks.com', port=443)",1,1,"1"
"automl ui",1,1,"1"
"autoscale",1,1,"1"
"autoscaling not working",1,1,"1"
"autoscaling of job clusters failed",1,1,"1"
"average nonzero",1,1,"1"
"avoid concurrent runs based on parameters",1,1,"1"
"aws account 089805728736 cannot find endpoint",1,1,"1"
"aws ami image upgrade",1,1,"1"
"aws api error code: invalidgroup.notfound",1,1,"1"
"aws authentication",1,1,"1"
"aws cloudwatch metrics",1,1,"1"
"aws credentials authenticating error after upgrading to 10.5 runtime",1,1,"1"
"aws credentials not loading on 1st run once a cluster is started",1,1,"1"
"aws cross account s3 access issue",1,1,"1"
"aws glue data catalog delta tabl",1,1,"1"
"aws glue drop table",1,1,"1"
"aws instance types for various use cases",1,1,"1"
"aws instances running non stop",1,1,"1"
"aws insufficient  instance capacity",1,1,"1"
"aws outpost",1,1,"1"
"aws please reduce your request rate.",1,1,"1"
"aws private link",1,1,"1"
"aws quicksight",1,1,"1"
"aws s3 autoloader exception after a few run",1,1,"1"
"aws s3 autoloader returns all null values",1,1,"1"
"aws s3 bucket for databricks artifact is public",1,1,"1"
"aws s3 unknownhostexception",1,1,"1"
"aws security hub問題検出",1,1,"1"
"aws services fail with no region provided error.",1,1,"1"
"aws trial",1,1,"1"
"aws_insufficient_instance_capacity_failure(client_error)",1,1,"1"
"aws_insufficient_instance_capacity_failure(client_error):",1,1,"1"
"azure ad saml certificate expiring 6/14",1,1,"1"
"azure adf pipelines are running longer than expected",1,1,"1"
"azure blob",1,1,"1"
"azure cluster terminated. reason: self bootstrap failure",1,1,"1"
"azure container subscription clean up",1,1,"1"
"azure data bricks notebook is not responding",1,1,"1"
"azure data lake storage gen2",1,1,"1"
"azure databricks",1,1,"1"
"azure databricks - adls connectivity",1,1,"1"
"azure databricks administration: tips and troubleshooting",1,1,"1"
"azure databricks autorun fails",1,1,"1"
"azure databricks billing query.",1,1,"1"
"azure databricks connectivity issue with reporting tools (power bi and tableau).",1,1,"1"
"azure databricks connector for power bi have connectivity issues when communicating to our deere edl",1,1,"1"
"azure databricks create pipelines white screen",1,1,"1"
"azure databricks error  : id token is not yet valid. make sure your computer's time and time zone are both correct.",1,1,"1"
"azure databricks fail to apply an inbuilt function sporadically",1,1,"1"
"azure databricks ip ranges clarity",1,1,"1"
"azure databricks issue 2|sql",1,1,"1"
"azure databricks long jobs fail b/c they can't authenticate to the dbfs to write data at the end of the job",1,1,"1"
"azure databricks sku not available",1,1,"1"
"azure databricks | technical clarity on usage",1,1,"1"
"azure databricks: cannot switch between all workspaces",1,1,"1"
"azure deployment in violation of the microsoft online services acceptable use policy",1,1,"1"
"azure devops",1,1,"1"
"azure devops integration",1,1,"1"
"azure install msodbcsql",1,1,"1"
"azure logic app access databricks via managed identity",1,1,"1"
"azure provider resource throttling",1,1,"1"
"azure services cluster terminated  cloud provider launch failure",1,1,"1"
"azure storage",1,1,"1"
"azure vm extension",1,1,"1"
"azure-key-vault-secrets",1,1,"1"
"azure_185313d2-52ed-4afe-9699-a52268c0db4c’ cluster are running longer than expected--2204290030001366",1,1,"1"
"azuread authentication in databricks after duo mfa implementation",1,1,"1"
"azurecli not found in path",1,1,"1"
"azurerm_databricks_workspace",1,1,"1"
"backend service unavailable",1,1,"1"
"backend service unavailable - unable to see list of clusters",1,1,"1"
"background color",1,1,"1"
"backward compatibility issues",1,1,"1"
"bad configuration in terraform",1,1,"1"
"badge",1,1,"1"
"badges",1,1,"1"
"bai sales random data anomaly",1,1,"1"
"bash",1,1,"1"
"batch",1,1,"1"
"batching",1,1,"1"
"bbb",1,1,"1"
"be able to sink spark accumulators to prometheus",1,1,"1"
"behavior change in %sh when arbitrary files in repos feature is on",1,1,"1"
"behaviour difference in photon",1,1,"1"
"best approach to read append blobs",1,1,"1"
"best practice for images in delta lake",1,1,"1"
"best practices for deploying mlflow model to azure container instance",1,1,"1"
"best practices to connect via simba driver and how to improve performance?",1,1,"1"
"bigquery connection",1,1,"1"
"billing accident",1,1,"1"
"billing account",1,1,"1"
"billing details",1,1,"1"
"billing in aws",1,1,"1"
"billing information",1,1,"1"
"billing invoice",1,1,"1"
"billing question",1,1,"1"
"billing: need help to understand untagged cluster costs",1,1,"1"
"blind",1,1,"1"
"blind xss",1,1,"1"
"block this shit",1,1,"1"
"bodends2 - pd job failure--2204260040006441",1,1,"1"
"bootstrap timeout for job",1,1,"1"
"bootstrap timeout issue.(follow up of 00143432)",1,1,"1"
"boto 3",1,1,"1"
"boto3 access",1,1,"1"
"boto3 client failes with credentail errors",1,1,"1"
"box plot not computed on all data",1,1,"1"
"brand",1,1,"1"
"branding",1,1,"1"
"broadcast join",1,1,"1"
"broadcast join & partitioning",1,1,"1"
"broadcast join exceeds threshold, returns out of memory error",1,1,"1"
"broadcast join's are not effecient on the job",1,1,"1"
"broadcast the table that is larger than 8gb: 12 gb",1,1,"1"
"broadcast the table that is larger than 8gb: 25 gb",1,1,"1"
"bronze dlt pipeline cancelled",1,1,"1"
"browser won't render the mlflow experiment pages and run pages",1,1,"1"
"bug",1,1,"1"
"bug in account ui,  ""add members"" to group page.",1,1,"1"
"bug in updating a scheduled dashboard in databricks sql",1,1,"1"
"bugreport: read sql endpoint from spark jdbc",1,1,"1"
"bulk python dependency list for cluster",1,1,"1"
"bunch of adf jobs ""cancelled"" on single day",1,1,"1"
"business impact field change testing.ignore",1,1,"1"
"by default stats are generated for the delta table while running optimize",1,1,"1"
"c#",1,1,"1"
"cache",1,1,"1"
"cache unexpected ""raw_code""",1,1,"1"
"cal sql file in databricks",1,1,"1"
"calculate business days",1,1,"1"
"calculate business dyas",1,1,"1"
"calculator",1,1,"1"
"call terminal",1,1,"1"
"calledprocesserror get_ipython().run_line_magic",1,1,"1"
"calledprocesserror with pip",1,1,"1"
"calledprocesserror with pip install",1,1,"1"
"calls to databricks api from a lambda are timing out.",1,1,"1"
"can i give certification exam on mac",1,1,"1"
"can i give test on mac",1,1,"1"
"can not ""git clone nauto-repo"" in init script to setup cluster",1,1,"1"
"can not access https url from databricks workspace (privatelink)",1,1,"1"
"can not connetc to dw database from db",1,1,"1"
"can not get databricks certification voucher code",1,1,"1"
"can not load the cluster in workspace",1,1,"1"
"can not produce/consume message from spark to confluent kafka",1,1,"1"
"can not spin databrick cluster",1,1,"1"
"can we deploy databricks e2 workspace in same vpc as databricks pvc",1,1,"1"
"can we have a chance to know which vm type has which kind of disk's tyoe?",1,1,"1"
"can we set  ""spark.databricks.photon.parquetwriter.enabled false"" in cluster policy?",1,1,"1"
"can we set ""spark.sql.mapkeydeduppolicy"" to ""last_win"" on databricks sql(sql endpoint))",1,1,"1"
"can we set auto log off from databricks workspace if inactive in xx mins?",1,1,"1"
"can we use pyspark udf in spark sql like scala udfs ?",1,1,"1"
"can you schedule serving",1,1,"1"
"can't able to access /mnt/advancedanalytics mount files in hc cluster",1,1,"1"
"can't able to install wheel files in the hc clusters",1,1,"1"
"can't access management console to upgrade account",1,1,"1"
"can't access repos after they've been made!",1,1,"1"
"can't cancel queries in sql endpoints",1,1,"1"
"can't connect from powerbi may version behind proxy [2206080050000154]",1,1,"1"
"can't create jobs with acl",1,1,"1"
"can't execute run all utilis for elisa bassani",1,1,"1"
"can't find runtime with target tensorflow version",1,1,"1"
"can't get access to db insight",1,1,"1"
"can't log in",1,1,"1"
"can't read mounted files from dbfs from sh commands",1,1,"1"
"can't see execution plan graph on all-purpose cluster",1,1,"1"
"can't see tables names in the data explorer",1,1,"1"
"cancel",1,1,"1"
"cancel a query",1,1,"1"
"cancel community",1,1,"1"
"cancel publish",1,1,"1"
"cancelled job",1,1,"1"
"cancelling the ilt",1,1,"1"
"cannot access storage account from databricks cluster",1,1,"1"
"cannot access table data in databricks sql",1,1,"1"
"cannot add column to delta table on dbr 9.1 lts",1,1,"1"
"cannot add user to databricks",1,1,"1"
"cannot connect to azure devops repos",1,1,"1"
"cannot connect to metastore",1,1,"1"
"cannot connect using databricks connect",1,1,"1"
"cannot control job creation access.",1,1,"1"
"cannot convert parquet-partitioned table to delta",1,1,"1"
"cannot create a cluste",1,1,"1"
"cannot create and start cluster in azure databricks",1,1,"1"
"cannot create cluster in new databricks e2 deployment",1,1,"1"
"cannot create cluster with cluster policy that has init script in dbfs",1,1,"1"
"cannot create databricks mws endpoints from terraform",1,1,"1"
"cannot create gpu inference cluster",1,1,"1"
"cannot create sql endpoint cluster in region westus",1,1,"1"
"cannot create user: user already exists in another account",1,1,"1"
"cannot edit broken query in databricks sql",1,1,"1"
"cannot enable audit logs for account",1,1,"1"
"cannot get debug log level",1,1,"1"
"cannot import dlt",1,1,"1"
"cannot install cuda",1,1,"1"
"cannot interact with sql view",1,1,"1"
"cannot launch workspace",1,1,"1"
"cannot log in on account page",1,1,"1"
"cannot login",1,1,"1"
"cannot login to accounts.cloud.databricks.com",1,1,"1"
"cannot make user account",1,1,"1"
"cannot modify delta table",1,1,"1"
"cannot mount a storage account",1,1,"1"
"cannot mount gcs bucket to dbfs",1,1,"1"
"cannot put file in dbfs mount folder",1,1,"1"
"cannot read hive tables with serde properties org.apache.hive.hcatalog.data.jsonserde",1,1,"1"
"cannot recognize hive type string",1,1,"1"
"cannot register aws kms customer-managed encryption key",1,1,"1"
"cannot resolve  given input columns",1,1,"1"
"cannot resolve '`[redacted]`' given input columns",1,1,"1"
"cannot restore databricks workspace",1,1,"1"
"cannot retrieve driver log",1,1,"1"
"cannot retrieve job output using api version 2.1 for single task job [2206280050000437]",1,1,"1"
"cannot run job",1,1,"1"
"cannot see the granted permission on the databases when migrated from e1 to e2",1,1,"1"
"cannot select authentication type in azure genie page",1,1,"1"
"cannot send email from databricks",1,1,"1"
"cannot show graphs in notebooks in html format",1,1,"1"
"cannot start a cluster",1,1,"1"
"cannot start a cluster on acxiom-di-datahub-appdev-admin.cloud.databricks.com",1,1,"1"
"cannot start cluster",1,1,"1"
"cannot start cluster on   northcentralus",1,1,"1"
"cannot start cluster on northcentralus",1,1,"1"
"cannot start clusters",1,1,"1"
"cannot start job",1,1,"1"
"cannot start or create clusters",1,1,"1"
"cannot start using delta live tables",1,1,"1"
"cannot store / search mlflow model by tag",1,1,"1"
"cannot use openapi spec yaml from job api in swagger editor.",1,1,"1"
"cant change password",1,1,"1"
"cant connect to local computer/remote vm from databricks workspace using ssh",1,1,"1"
"cant see clone folder option",1,1,"1"
"cant use trial",1,1,"1"
"capcity issue in region",1,1,"1"
"capstone",1,1,"1"
"captcha error",1,1,"1"
"case = 5003f00000jklrnaaj",1,1,"1"
"case by pablo",1,1,"1"
"case no#00142488",1,1,"1"
"case number # 00142833 | 2203310040008004 - cosmosdbspark failure on large files",1,1,"1"
"case number # 00161254",1,1,"1"
"case sensitivity difference in version 10.4 and 9.1",1,1,"1"
"cash sale",1,1,"1"
"caste",1,1,"1"
"catalog namespace is not supported",1,1,"1"
"catalyst",1,1,"1"
"caused by responseerror('too many 500 error responses",1,1,"1"
"caused by: com.databricks.notebookexecutionexception: timedout",1,1,"1"
"caused by: com.databricks.notebookexecutionexception: timedout+sql",1,1,"1"
"caused by: java.lang.illegalaccesserror:",1,1,"1"
"caused by: org.rocksdb.rocksdbexception: block checksum mismatch",1,1,"1"
"cell stuck in ""running command ...""",1,1,"1"
"central model registry communication",1,1,"1"
"certificate",1,1,"1"
"certificate syllabus",1,1,"1"
"certification prep courses",1,1,"1"
"change cloud provider",1,1,"1"
"change cluster ownership",1,1,"1"
"change column name pyspark",1,1,"1"
"change data feed does not work correctly for ""replacewhere"" operation when replacewhere predicate is on non partitioning column",1,1,"1"
"change data feed streaming question",1,1,"1"
"change data feed unexpected behaviour",1,1,"1"
"change databricks owner",1,1,"1"
"change date of certification exam",1,1,"1"
"change email",1,1,"1"
"change email customer academy",1,1,"1"
"change from customer to partners account",1,1,"1"
"change language",1,1,"1"
"change name",1,1,"1"
"change name certification",1,1,"1"
"change of support ownership.",1,1,"1"
"change pyhton  version",1,1,"1"
"change runtime",1,1,"1"
"change security group in databricks",1,1,"1"
"change table owner sql",1,1,"1"
"change vnet ip range",1,1,"1"
"changing/deleting our account owner",1,1,"1"
"char/varchar type length limitation after upgrading to 10.5 runtime",1,1,"1"
"check how to enbale delta live table",1,1,"1"
"check notebook history logs",1,1,"1"
"check python version",1,1,"1"
"check version",1,1,"1"
"checking in notebook to git hub",1,1,"1"
"checking progress of feature request(related case 00135814)",1,1,"1"
"checkpoint data corruption incidents in 2 weeks",1,1,"1"
"checkpoint files not being deleted using foreachbatch",1,1,"1"
"checkpoint not being created",1,1,"1"
"checkpointing not working for some queries",1,1,"1"
"chinese",1,1,"1"
"cifs_utils / mounting windows network file share fails",1,1,"1"
"clarification need on on-spot instances pricing",1,1,"1"
"clarification on databricks managed service encryption & access",1,1,"1"
"clarification required on new r5a instance types",1,1,"1"
"clarity on additional ebs volumes being added",1,1,"1"
"class diagram",1,1,"1"
"class reschedule",1,1,"1"
"classificaion",1,1,"1"
"classnotfoundexception: org.apache.hadoop.fs.s3a.commit.s3acommitterfactory on 10.4 lts runtime",1,1,"1"
"classpath loading on dbr cluster missing conf",1,1,"1"
"classroom setup",1,1,"1"
"clear notifications",1,1,"1"
"cli authentication",1,1,"1"
"clicking run all results in error in notebook",1,1,"1"
"clinical operations -eu region s3 paths access",1,1,"1"
"clone",1,1,"1"
"clone folder",1,1,"1"
"clone notebook",1,1,"1"
"close account",1,1,"1"
"cloud launch provider failure",1,1,"1"
"cloud provider failures during cluster creation east us2",1,1,"1"
"cloud_files",1,1,"1"
"cloud_provider_launch_failure",1,1,"1"
"cloud_provider_launch_failure(cloud_failure):",1,1,"1"
"cloudera connectivity",1,1,"1"
"cloudera manager",1,1,"1"
"cloudwatch-initv2.sh.txt failed",1,1,"1"
"cluster & worker node spin time",1,1,"1"
"cluster activation taking longtime",1,1,"1"
"cluster auto scaling errors in prod env",1,1,"1"
"cluster auto terminated while a job was running",1,1,"1"
"cluster autoscaling issue",1,1,"1"
"cluster bootstrap failure",1,1,"1"
"cluster bootstrap timeout",1,1,"1"
"cluster configuration limitations",1,1,"1"
"cluster configuration to set the default table format to parquet",1,1,"1"
"cluster creating with dbfs init file",1,1,"1"
"cluster creation",1,1,"1"
"cluster creation failed",1,1,"1"
"cluster creation failed (no amis permissions)",1,1,"1"
"cluster creation failing",1,1,"1"
"cluster creation failing with instance profile",1,1,"1"
"cluster creation fails with ""azure vm extension stuck on transitioning state. please try again later"" | arr sr# 2206230050000805",1,1,"1"
"cluster creation failure - bootstrap failure",1,1,"1"
"cluster creation rejected",1,1,"1"
"cluster creation taking a long time even then using nodes from pool",1,1,"1"
"cluster creation taking long time",1,1,"1"
"cluster did not timeout after being unable to aqquire executors",1,1,"1"
"cluster didn't auto terminate",1,1,"1"
"cluster doesn't start",1,1,"1"
"cluster failed to launch",1,1,"1"
"cluster failing",1,1,"1"
"cluster fails to launch with an error",1,1,"1"
"cluster fails to start",1,1,"1"
"cluster failure",1,1,"1"
"cluster failure due to bootstrap failure - dbc-1891663a-774f",1,1,"1"
"cluster formation during pipeline run.",1,1,"1"
"cluster init",1,1,"1"
"cluster init scripts",1,1,"1"
"cluster is in terminated state and not available to receive jobs",1,1,"1"
"cluster is not auto shutdown",1,1,"1"
"cluster is not doing anything",1,1,"1"
"cluster is not running",1,1,"1"
"cluster is not starting its failing.",1,1,"1"
"cluster is very slow",1,1,"1"
"cluster issues",1,1,"1"
"cluster issues with java 17 and jdbc",1,1,"1"
"cluster launch error in a new workspace",1,1,"1"
"cluster launch fails with failure message cloud provider failure azure vm extension stuck on transitioning state",1,1,"1"
"cluster launch failure",1,1,"1"
"cluster launch failure in dev environment",1,1,"1"
"cluster launch failures",1,1,"1"
"cluster launch is failing and getting successful on retry",1,1,"1"
"cluster lg-db-24x7 locked",1,1,"1"
"cluster log4j logs",1,1,"1"
"cluster logging object ownership",1,1,"1"
"cluster logs",1,1,"1"
"cluster logs grows rapidly and uncontrollable, driving high costs, manual purge is unresponsive and unstable",1,1,"1"
"cluster lost at least one node",1,1,"1"
"cluster monitoring enabling jmx sink",1,1,"1"
"cluster never came up",1,1,"1"
"cluster nodes are being available for the other jobs",1,1,"1"
"cluster not creating",1,1,"1"
"cluster not getting started",1,1,"1"
"cluster not getting terminated even when job is getting completed",1,1,"1"
"cluster not launching",1,1,"1"
"cluster not start.",1,1,"1"
"cluster not starting up",1,1,"1"
"cluster on aws keep on shutting down",1,1,"1"
"cluster operation error-",1,1,"1"
"cluster performance issue",1,1,"1"
"cluster poc",1,1,"1"
"cluster policy cluster_log_conf.path",1,1,"1"
"cluster policy for photon dbr doesn't work",1,1,"1"
"cluster policy not working without autoscaling local storage",1,1,"1"
"cluster policy permissions tab missing",1,1,"1"
"cluster problems",1,1,"1"
"cluster queue time is high",1,1,"1"
"cluster resize fails",1,1,"1"
"cluster running issue",1,1,"1"
"cluster running issye",1,1,"1"
"cluster self bootstrap failure",1,1,"1"
"cluster shutdown",1,1,"1"
"cluster spin up issue",1,1,"1"
"cluster spin up issue in prod",1,1,"1"
"cluster spin-up issue",1,1,"1"
"cluster stage looks stuck",1,1,"1"
"cluster starting issue",1,1,"1"
"cluster starting slow",1,1,"1"
"cluster starting with an deregistered instance profile",1,1,"1"
"cluster startup errors",1,1,"1"
"cluster status pending mlflow",1,1,"1"
"cluster stop for a databricks job after 1606h 23m 30s [2206220050000368]",1,1,"1"
"cluster stuck",1,1,"1"
"cluster stuck at terminating...",1,1,"1"
"cluster takes 23 minutes to start",1,1,"1"
"cluster taking long to run after starting",1,1,"1"
"cluster taking more time to start-",1,1,"1"
"cluster temporarily unavailable",1,1,"1"
"cluster temporarily_unavailable",1,1,"1"
"cluster terminated (reason) cloud provider shutdown",1,1,"1"
"cluster terminated - bootstrap timeout",1,1,"1"
"cluster terminated - bootstrap timeout-",1,1,"1"
"cluster terminated -cloud provider shutdown",1,1,"1"
"cluster terminated in databricks, however ec2 instance in aws is still running",1,1,"1"
"cluster terminated. reason: aws insufficient instance capacity failure",1,1,"1"
"cluster terminated. reason: azure vm extension failure",1,1,"1"
"cluster terminated. reason: communication lost",1,1,"1"
"cluster terminated. reason: container launch failure",1,1,"1"
"cluster terminated. reason: unexpected launch failure",1,1,"1"
"cluster terminated.reason:aws insufficient free addresses in subnet failure",1,1,"1"
"cluster terminated.reason:azure quota exceeded exception",1,1,"1"
"cluster terminated.reason:azure quota exceeded exception  error code: invalidtemplatedeployment, error message: the template deployment failed with error:",1,1,"1"
"cluster terminated.reason:communication lost",1,1,"1"
"cluster terminated.reason:network configuration failure",1,1,"1"
"cluster terminated.reason:request throttled",1,1,"1"
"cluster terminated.reason:storage download failure",1,1,"1"
"cluster terminated.reason:unexpected launch failure",1,1,"1"
"cluster terminating reason - unexpected launch failure",1,1,"1"
"cluster terminating with inactivity while command in notebook is still running",1,1,"1"
"cluster termination",1,1,"1"
"cluster termination and autoscaling",1,1,"1"
"cluster termination issue.",1,1,"1"
"cluster type",1,1,"1"
"cluster unable to access delta tables and s3",1,1,"1"
"cluster unable to decrypt a column",1,1,"1"
"cluster unable to fetch secrets",1,1,"1"
"cluster unable to start after april 13th 9pm-2205160040005458",1,1,"1"
"cluster unusual memory and stuck in bad state",1,1,"1"
"cluster upgraded from 9.1 lts to 10.4 and code throwing an error on bit filter",1,1,"1"
"cluster very very slow",1,1,"1"
"cluster will not launch",1,1,"1"
"cluster will not start",1,1,"1"
"cluster with table acl blocks a few classes to be used",1,1,"1"
"cluster/notebook sychronization issues",1,1,"1"
"clustereditinstanceprofilevalidationsuite is not registered to create private workspacee",1,1,"1"
"clusters can't be deleted",1,1,"1"
"clusters cannot be created",1,1,"1"
"clusters disappearing/deleted",1,1,"1"
"clusters failed to start",1,1,"1"
"clusters failing to start in production workspace - unexpected_launch_failure - service_fault",1,1,"1"
"clusters failing with internal failures",1,1,"1"
"clusters in all environments failing with",1,1,"1"
"clusters list api not returning all clusters",1,1,"1"
"clusters never start",1,1,"1"
"clusters not coming up - cannot create elastic disk cluster",1,1,"1"
"clusters not coming up and failing with network issues",1,1,"1"
"clusters not coming up: unexpected launch failure",1,1,"1"
"clusters not launching after changing instance types",1,1,"1"
"clusters not starting",1,1,"1"
"clusters not starting - driver error messages in the event logs",1,1,"1"
"clusters properties are not available in restored workspace",1,1,"1"
"clusters terminates on startup and init script execution because of missing libssl.so",1,1,"1"
"clusters would not start",1,1,"1"
"coalesce causing file already exists error when running a large dataset",1,1,"1"
"code optimization running for hours",1,1,"1"
"code_path",1,1,"1"
"collaboration",1,1,"1"
"collabratrive engagement with databricks direct: assistance with dlt pipeline from our databricks workspace.",1,1,"1"
"collibra unable to acquire metadata from databricks schema via jdbc.",1,1,"1"
"collibra unable to acquire metadata from databricks schema via jdbc. issue missing global-temp.",1,1,"1"
"column alias",1,1,"1"
"column comments",1,1,"1"
"column comments python",1,1,"1"
"column name with % gives an error",1,1,"1"
"column of table datatype",1,1,"1"
"column sorting in table on dashboard",1,1,"1"
"com.databricks.backend.daemon.data.client.dbfsv1.createatomicifabsent(path: path)",1,1,"1"
"com.databricks.common.client.databricksservicehttpclientexception: internal_error",1,1,"1"
"command execution delayed by  """"uploading command""",1,1,"1"
"command executions taking longer  or not executing",1,1,"1"
"command stucks when submitted",1,1,"1"
"commands getting cancelled, spark mssqlconnector failing",1,1,"1"
"commands run very",1,1,"1"
"comment",1,1,"1"
"comment in notebook",1,1,"1"
"comment passer à la version française",1,1,"1"
"commit",1,1,"1"
"commit & push",1,1,"1"
"commit file not generated upon job failure",1,1,"1"
"community",1,1,"1"
"community access token",1,1,"1"
"community edition cluster",1,1,"1"
"community edition log in",1,1,"1"
"community edition repos",1,1,"1"
"community edition with company email",1,1,"1"
"como activo el delta en el databricks",1,1,"1"
"compare",1,1,"1"
"compare notebook",1,1,"1"
"compare notebooks",1,1,"1"
"compute resources bogging down",1,1,"1"
"compute services issue",1,1,"1"
"concurrency control - clarification to fix the multiple update/insert to the table",1,1,"1"
"concurrent append exception",1,1,"1"
"concurrent sessions",1,1,"1"
"concurrent update failure to delta",1,1,"1"
"conda on 9.1 lts ml runtime",1,1,"1"
"conection data lake",1,1,"1"
"conf",1,1,"1"
"configuration hiveservertype is not available",1,1,"1"
"configuration is ambiguously defined. cannot have overlapping suffixes in two rules if the prefixes are overlapping for the same event type.",1,1,"1"
"configuration parameters",1,1,"1"
"configuration parameters access during pipeline runs",1,1,"1"
"configure language",1,1,"1"
"configure log files to custom s3 location",1,1,"1"
"configure self-managed vpc",1,1,"1"
"configuring delta cache for cluster",1,1,"1"
"configuring delta live tables cluster with access role",1,1,"1"
"confirm impact from spring cves",1,1,"1"
"confirmation of databricks jdbc driver",1,1,"1"
"conflict in databricks git",1,1,"1"
"confluent databricks sink connector",1,1,"1"
"confusion of pricing page for delta live table",1,1,"1"
"connect",1,1,"1"
"connect blob container with no credentials",1,1,"1"
"connect databricks service to the common data model (cdm)",1,1,"1"
"connect excel",1,1,"1"
"connect to databricks using the odbc driver",1,1,"1"
"connect to postgresql",1,1,"1"
"connect to snowflake",1,1,"1"
"connectexception: connection refused  datamessages$getmountsv2 since peer boundrpcclient[http://10.139.0.6:7070 with noproxy] is down",1,1,"1"
"connecting from fivetran to databricks  throwing error check version compatibility: failed to read databricks cluster version. error : {""error_code"":""403"",""message"":""unauthorized access to org: 3438608682691136""}",1,1,"1"
"connecting to eventhubs using aad",1,1,"1"
"connecting to postgresql on aws rds using jdbc",1,1,"1"
"connecting.. message keep showing once  connect to databricks",1,1,"1"
"connection between databricks workspace and datalake",1,1,"1"
"connection databricks connect does not work since 06-17-2022",1,1,"1"
"connection error during python library installation",1,1,"1"
"connection issue while auto-scaling of cluster",1,1,"1"
"connection issue with azure databricks using odbc",1,1,"1"
"connection issue with powerbi  ssl read",1,1,"1"
"connection issue with sql endpoints",1,1,"1"
"connection latency to aws secret manager from databricks",1,1,"1"
"connection pool shutdown",1,1,"1"
"connection refused error  from databricks rest api ""/api/2.1/jobs/runs/get""",1,1,"1"
"connection timed out (read failed)",1,1,"1"
"connection timeout",1,1,"1"
"connection to rds mysql server hosted by databricks reset/disconnected error in cluster log",1,1,"1"
"connectivity issue to synapse - from databricks using jdbc",1,1,"1"
"connectivity issue with aws msk cluster from aws databricks",1,1,"1"
"connectivity problem using azure-event-hubs-spark",1,1,"1"
"connectivity to postgresql failing",1,1,"1"
"connectiviy issues between databricks to cassa",1,1,"1"
"consistent error on job",1,1,"1"
"console pricing",1,1,"1"
"constructing-graphs-with-cypher-for-apache-spark",1,1,"1"
"contact details for contingency plan  documentation",1,1,"1"
"container + virtual",1,1,"1"
"continuation of  00143942  | the databricks job fails frequently - 2204250010002866",1,1,"1"
"continuation of 00182772 | arr| error when install catboost |2206100040005802",1,1,"1"
"continuous integration and delivery on azure databricks using azure devops",1,1,"1"
"contributor",1,1,"1"
"control plane request failure",1,1,"1"
"control plane service endpoint configuration for the region`europe-west3`",1,1,"1"
"control r",1,1,"1"
"control workspace access",1,1,"1"
"convert",1,1,"1"
"convert to delta",1,1,"1"
"converting html to string",1,1,"1"
"copy and paste json to fill out job run parameters does not work",1,1,"1"
"copy cell title",1,1,"1"
"copy folder",1,1,"1"
"copy results",1,1,"1"
"core databricks instance down",1,1,"1"
"correct course to learn databricks",1,1,"1"
"cors",1,1,"1"
"cost analysis",1,1,"1"
"costs details",1,1,"1"
"could not deploy and configure databricks workspace",1,1,"1"
"could not fetch cluster list.  no data received for cluster list.",1,1,"1"
"could not find a version that satisfies the requirement",1,1,"1"
"could not find a version that satisfies the requirement tqdm",1,1,"1"
"could not locate executable null\bin\winutils.exe",1,1,"1"
"could not log you in to help",1,1,"1"
"could not reach driver of",1,1,"1"
"could not reach driver of cluster",1,1,"1"
"could not reach driver of cluster 1118-054402-skid528 for 120 seconds.;",1,1,"1"
"could you please help understand the reason while is stuck",1,1,"1"
"couldn't attend the exam",1,1,"1"
"count distinct",1,1,"1"
"count null",1,1,"1"
"count tables and columns",1,1,"1"
"coupon code",1,1,"1"
"create a vpc endpoint on the databricks aws account to enable ghe connectivity",1,1,"1"
"create cluster",1,1,"1"
"create cluster aws",1,1,"1"
"create cluster ends as terminated",1,1,"1"
"create data lake",1,1,"1"
"create delta table statements fail intermittently",1,1,"1"
"create external location (databricks sql)",1,1,"1"
"create file in databricks",1,1,"1"
"create folder",1,1,"1"
"create new admin",1,1,"1"
"create or replace temporary function fails",1,1,"1"
"create pipeline white screen",1,1,"1"
"create primary key",1,1,"1"
"create query",1,1,"1"
"create query in databricks sql is not working",1,1,"1"
"create scope",1,1,"1"
"create secret scope",1,1,"1"
"create sql endpoint",1,1,"1"
"create table as select * from (ctas) statements not working on dbsql endpoints",1,1,"1"
"create table with special characters in column name fails in databricks 9.1",1,1,"1"
"create token azure databricks cluster",1,1,"1"
"create widget fails with block comments in the same cell",1,1,"1"
"createexternallocation owner error",1,1,"1"
"creating a databricks community edition account",1,1,"1"
"creating additional workspace in different aws account.",1,1,"1"
"creating cluster",1,1,"1"
"creating cluster on community edition",1,1,"1"
"creating cluster terminating",1,1,"1"
"creating connection fail issue between sql endpoint with pyodbc",1,1,"1"
"creating sql functions is broken in dbr 10.5",1,1,"1"
"credential passthrough job",1,1,"1"
"credentials",1,1,"1"
"credits",1,1,"1"
"critical findings - cve-2022-22824 - expat",1,1,"1"
"cross",1,1,"1"
"cross account access using assumerole",1,1,"1"
"cross account sqs setup",1,1,"1"
"cross origin resource sharing",1,1,"1"
"cross origin resource sharing azure databricks",1,1,"1"
"crunchbase-company job is failing and also taking long time to fetch data",1,1,"1"
"cs-arr-s500-sr#2204300030000321-genie access request",1,1,"1"
"css arr | 2203230030001649 - adf pipeline job is failing in cart-adf-prod",1,1,"1"
"css arr | 2204040030002558 - unexpected cost spike",1,1,"1"
"css arr | 2204080030000440 - we are not able to get the logs on databricks",1,1,"1"
"css arr | follow up case | 2203210030000403",1,1,"1"
"css arr: follow up 00139476: vm name and id request for 00139476:sr 2203240040003987",1,1,"1"
"css | arr | 2206160030000629 | subnet_exhausted_failure : no more address space to create nic within injected virtual network | tomtom",1,1,"1"
"css | arr | 2206220040008644 | gsk | sqldw load from databricks taking long time",1,1,"1"
"css | arr | databricks notebook took longer | jnj | 2206130040003446",1,1,"1"
"css | arr | need some monitoring report | 2206280030001644 | tata digital",1,1,"1"
"css | arr | s500 | cluster creation delay | 2206270050000084 | inditex",1,1,"1"
"css | arr | tata digital | we are unable to view the output of the databricks job, currently, the job is failing |  2206240030000448",1,1,"1"
"css-2203280040003823-need user details on managed tables created in dbfs path",1,1,"1"
"css-arr-2203170040001201-adb cluster hung issue",1,1,"1"
"css-arr-2203280040003823-need to know hive manage tables creation details in azure databricks",1,1,"1"
"css-arr-2204010030001002-run_result_unavailable",1,1,"1"
"css-arr-2204050010001186-job aborted due to stage failure",1,1,"1"
"css-arr-2204190030001595_cluster unable to accquire additional worker nodes in upscaling",1,1,"1"
"css-arr-2204210030000061-job failure",1,1,"1"
"css-arr-2204260050002175- backward compatibility support for time parser policy and more",1,1,"1"
"css-arr-2204270030000780-spark streaming job taking approx. 800 seconds to execute. when 'full gc' triggerred by the databricks cluster.",1,1,"1"
"css-arr-2205050030001596001-restore databricks workspace",1,1,"1"
"css-arr-2205090030000751002-databricks activity within a timeframe",1,1,"1"
"css-arr-2205090030001270_while trying to “create feature table” – the command is taking more than 8hrs",1,1,"1"
"css-arr-2205100030000546-job aborted/job delay during scheduled run",1,1,"1"
"css-arr-2205100030001515-error when using logintimeout  in sql endpoint  jdbc url",1,1,"1"
"css-arr-2205170040002305-unable to install library sets",1,1,"1"
"css-arr-2205200050001067-jobs are failing with error 'caused by: com.databricks.notebookexecutionexception: failed'",1,1,"1"
"css-arr-2205230030000434-databricks job api calls failing with statuscode: 503",1,1,"1"
"css-arr-2205240030000617-databricks spark job - executorlostfailure (executor 0 exited caused by one of the running tasks) reason: remote rpc client disassociated.",1,1,"1"
"css-arr-2205250030000526-could not able to see cluster list in compute service",1,1,"1"
"css-arr-2205270010000557-pairwise partial correlation in ml model is taking more than expected time",1,1,"1"
"css-arr-2205300030000871-need to get the cluster log details for the given databricks workspace",1,1,"1"
"css-arr-2205310050001250-databricks cluster is getting terminated abruptly",1,1,"1"
"css-arr-2206070040006687-databricks job is failing",1,1,"1"
"css-arr-2206130040007377-cannot write to databricks sql endpoint with jdbc driver",1,1,"1"
"css-arr-2206220010001765-migration tool code issues in databricks dr strategy",1,1,"1"
"css-arr-2206280030001543-car-t cancelled commands in azure databricks",1,1,"1"
"css-arr-2206280030002105-space issue in cluster ((acaze2-it-prd-adb-dataengineering-loyalty_v9)",1,1,"1"
"css-arr-s500-2204180030000820-genie access request",1,1,"1"
"css-arr-s500-2205180030002005-summary” column has more than 10mb of size data when we query or display in databricks we are getting following issue",1,1,"1"
"css-arr-s500-sr#2202110030000677-genie access request",1,1,"1"
"css-arr-s500-sr#2203210040006622-genie access request",1,1,"1"
"css-arr-s500-sr#2203290040005285-genie access request",1,1,"1"
"css-arr-s500-sr#2204080060001080-needs to recover deleted folder in databricks workspace",1,1,"1"
"css-arr-s500-sr#2204120030001534- bulk insert is not working in 10.4 runtime",1,1,"1"
"css-arr-s500-sr#2204120030001534-dbr 10.4 cluster upgrade issues",1,1,"1"
"css-arr-s500-sr#2204200060002657 -genie access",1,1,"1"
"css-arr-s500-sr#2204200060002657-genie access",1,1,"1"
"css-arr-s500-sr#2204220060001892-metastore initialization and clsuter not attached",1,1,"1"
"css-arr-s500-sr#2204250030000325-genie access request",1,1,"1"
"css-arr-s500-sr#2204280030000558-genie access request",1,1,"1"
"css-arr-s500-sr#2204280030000558-unable to read the data from offset for eventhub using spark jobs",1,1,"1"
"css-arr-s500-sr#2204300030000321-databricks job are slow.",1,1,"1"
"css-arr-s500-sr#2205110050000087-databricks jobs don't start with error cloud_provider_launch_failure",1,1,"1"
"css-arr-s500-sr#2205110050000087-genie access request",1,1,"1"
"css-arr-s500-sr#2205130050000700-oserror: [errno 95] operation not supported",1,1,"1"
"css-arr-s500-sr#2205170050000139- genie access request",1,1,"1"
"css-arr-s500-sr#2205170050000139-databricks job getting failed.",1,1,"1"
"css-arr-s500-sr#2205170050002982 -genie access request",1,1,"1"
"css-arr-s500-sr#2205170050002982-cluster start fails with storage download error",1,1,"1"
"css-arr-s500-sr#2205200040003401-unable to launch cluster",1,1,"1"
"css-arr-s500-sr#2205250030000205-genie access request",1,1,"1"
"css-arr-s500-sr#2205250030000548-auto assignment user as admin role",1,1,"1"
"css-arr-s500-sr#2205260030001715-library installed in the cluster",1,1,"1"
"css-arr-s500-sr#2205270040003393-adls2 access issue",1,1,"1"
"css-arr-s500-sr#2205300030000938-registering the model using adls gen2 path",1,1,"1"
"css-arr-s500-sr#2205310030000543-genie access request",1,1,"1"
"css-arr-s500-sr#2206010010000682-job slowness",1,1,"1"
"css-arr-s500-sr#2206010010000682-job stuck",1,1,"1"
"css-arr-s500-sr#2206010040005495-genie access request",1,1,"1"
"css-arr-s500-sr#2206020060000851-genie access request",1,1,"1"
"css-arr-s500-sr#2206090030000569-genie access request",1,1,"1"
"css-arr-s500-sr#2206090030000569-notebook failure",1,1,"1"
"css-arr-s500-sr#2206090050002068-delta table rebuild for migration adls gen1 - gen2 migration",1,1,"1"
"css-arr-s500-sr#2206100030001586-genie access request",1,1,"1"
"css-arr-s500-sr#2206100030001586-jobs list api issue",1,1,"1"
"css-arr-s500-sr#2206100040002569 -genie access request",1,1,"1"
"css-arr-s500-sr#2206120030000066-genie access request",1,1,"1"
"css-arr-s500-sr#2206120030000066-pipelinename  int_ahold_tlog_lastscandate_gen2 running slower than usual-",1,1,"1"
"css-arr-s500-sr#2206130040006805 -genie access request",1,1,"1"
"css-arr-s500-sr#2206130040006805-incremental load of table taking longer and target row count is 0",1,1,"1"
"css-arr-s500-sr#2206220040007041-pricing of standard_d32as_v4 east us 2 region",1,1,"1"
"css-arr-s500-sr#2206220040008750-issue creating and accessing a mount point in databricks pipeline",1,1,"1"
"css-arr-s500-sr2206020060000851 -notebook failures",1,1,"1"
"css-arr-s500sr#2205130050000700-genie access request",1,1,"1"
"css-arr-s500sr#2206100040002569-cannot enable table access control",1,1,"1"
"css-arr-sfmc-2203170040001201-adb cluster hung",1,1,"1"
"css-arr-sfmc-2203170040001201-adb cluster hung issue",1,1,"1"
"css-arr-sfmc-2204060030002027-data bricks notebooks are running are failing",1,1,"1"
"css-arr-sfmc-2204060030002027-data bricks notebooks are running very slow",1,1,"1"
"css-arr-sfmc-2204070030000778-ru is increasing in gr from yesterday night in streming job",1,1,"1"
"css-arr-sfmc-2206020060002128-data copying to datalake storage even though databricks notebook is failing",1,1,"1"
"css-arr-sfmc-2206240030000569-databricks job ‘starbucks_nl_master’ has been failing",1,1,"1"
"css-arr-sr#2203140030000750 azure databricks - scim api user creation adds cluster create permissions by default",1,1,"1"
"css-arr-sr#2204010030001163-not able to provision the all purpose clusters",1,1,"1"
"css-arr-sr#2204010030001753-we are unable to run commands on the cluster tdl-dp-dwh-001",1,1,"1"
"css-arr-sr#2204180030000820-delta table is getting deleted after alter statment",1,1,"1"
"css-arr-sr#2204180030000820-genie access request",1,1,"1"
"css-arr-sr#2205040030000589-jobs failing due to tcp/ip connection error",1,1,"1"
"css-arr-sr#2205200030000589- databricks jobs are slow",1,1,"1"
"css-arr-sr#2205240030000602-support partial csv file reading using copy into command",1,1,"1"
"css-arr-sr#2205270050000347-genie access request",1,1,"1"
"css-arr-sr#2206030030001090-'size of broadcasted table far exceeds estimates and exceeds limit of spark.driver.maxresultsize=4294967296.",1,1,"1"
"css-arr-sr#2206140030001007 -invoice notebook  failed  due to job aborted after 7h 33m 45s",1,1,"1"
"css-arr-sr#2206140030001007- genie access request",1,1,"1"
"css-arr-sr#2206160050001624 -genie access request",1,1,"1"
"css-arr-sr#2206160050001624 -suddenly not able to connect to snowflake account",1,1,"1"
"css-arr-sr#2206220030001709-copy into command not working after truncating target table",1,1,"1"
"css-arr-sr#2206230030001335-genie access request",1,1,"1"
"csv file",1,1,"1"
"csv file load issue using copy into command",1,1,"1"
"ctlr-r",1,1,"1"
"currency",1,1,"1"
"custoemr getting error in databricks jobs- 2204160040000150001",1,1,"1"
"custom ca",1,1,"1"
"custom cells",1,1,"1"
"custom docker images do not work",1,1,"1"
"customer cannot create the job cluster while he is granted the ""allow unrestricted cluster creation"" premission",1,1,"1"
"customer cannot log mlflow models when notebook is triggered from azure synapse",1,1,"1"
"customer free courses",1,1,"1"
"customer got high bill in the month of november 2021",1,1,"1"
"customer has an issue when writing to cosmos cassandra.",1,1,"1"
"customer interested in consultancy services",1,1,"1"
"customer is facing a cloud_provider_launch_failure authorization issue",1,1,"1"
"customer is facing count mismatch issue when reading from cassandra db table",1,1,"1"
"customer is facing job failure and they want to know the root cause",1,1,"1"
"customer is facing throtling issue and production jobs got halted",1,1,"1"
"customer is having intermittent connectivity issue in azure databricks",1,1,"1"
"customer is not able to see spark ui details for a particular stage",1,1,"1"
"customer is reporting a bug when using databricks repos cli",1,1,"1"
"customer is seeing duplicate records",1,1,"1"
"customer is unable to run a jar job",1,1,"1"
"customer needs to delete 2 service principals from standard workspace",1,1,"1"
"customer reports dip and following spike in send activity daily at 12am utc from their databricks.- saved",1,1,"1"
"customer running into intermittent failures due to heartbit timed out",1,1,"1"
"customer stopped being able to access run of an experiment",1,1,"1"
"customer stopped being able to run dbconnect against all his databricks workspaces",1,1,"1"
"customer to partner account",1,1,"1"
"customer unable to upgrade to premium. their free trial has ended and they are unable to upgrade and continue their work",1,1,"1"
"customer wants to know who deleted a folder",1,1,"1"
"customer was overcharged due to autoscaling issue",1,1,"1"
"customer's job is facing could not find adls gen2 token issue",1,1,"1"
"customize containers with databricks container services",1,1,"1"
"customize workspace url",1,1,"1"
"cve 2022-22965 (spring4shell) vulnerability",1,1,"1"
"cve-2022-22965  impact analysis",1,1,"1"
"cve-2022-22965 vulnerability",1,1,"1"
"cx cannot create db cluster even add tunnel.australiaeast.azuredatabricks.net and webapp on azure firewall",1,1,"1"
"cx cannot modify delta table during job run",1,1,"1"
"cx had an issue with repos on databricks",1,1,"1"
"cx is facing cloud_provider_launch_failure(cloud_failure)",1,1,"1"
"cx job failed with the error "" job failed with error message timed out retrying to clustermanager"" | arr sr# 2205270050000380",1,1,"1"
"cx job failing with ""com.databricks.notebookexecutionexception: timedout"" | arr sr# 2203290050000172",1,1,"1"
"cx request private preview for private link",1,1,"1"
"cx uses single node cluster but 2 vms are used",1,1,"1"
"cx would like to know the reason for additional charges for their databricks vms",1,1,"1"
"cypher",1,1,"1"
"dag",1,1,"1"
"daily dbu usage on azure databricks",1,1,"1"
"daily job crashed with an error message",1,1,"1"
"daily workflow suddently requiring more resources",1,1,"1"
"dark background",1,1,"1"
"dark mode",1,1,"1"
"dark theme",1,1,"1"
"dashboard and query return 500",1,1,"1"
"dashboard functionality getting lost",1,1,"1"
"dashboard gallery",1,1,"1"
"dashboard include all filter",1,1,"1"
"dashboard template",1,1,"1"
"dask localcluster with processes fails to start on a databricks privacera-enabled cluster",1,1,"1"
"data access from databricks sql endpoint works incorrecly",1,1,"1"
"data backup",1,1,"1"
"data bricks",1,1,"1"
"data bricks connect pycharm",1,1,"1"
"data bricks deployment failed",1,1,"1"
"data bricks not able to reference current version of key from keyvault",1,1,"1"
"data bricks window",1,1,"1"
"data bricks window case",1,1,"1"
"data bricks window case creation",1,1,"1"
"data bricks window case creation and many more things",1,1,"1"
"data bricks window case creation and many more things marketing",1,1,"1"
"data cluster",1,1,"1"
"data cluster is not",1,1,"1"
"data engineer",1,1,"1"
"data engineer associate",1,1,"1"
"data engineering",1,1,"1"
"data engineering learning path",1,1,"1"
"data enrichment",1,1,"1"
"data files",1,1,"1"
"data generator",1,1,"1"
"data governance gap for streaming workloads",1,1,"1"
"data import timeout",1,1,"1"
"data inconsistency",1,1,"1"
"data lake",1,1,"1"
"data lake multiple hourly job failures due to invalid access token error",1,1,"1"
"data path",1,1,"1"
"data persistency issues",1,1,"1"
"data plane traffic from databricks compute machine to s3 is going via nat gateway which is causing huge bill on nat gateway of aws",1,1,"1"
"data residency with azure databricks",1,1,"1"
"data science path",1,1,"1"
"data security",1,1,"1"
"data source directory structure",1,1,"1"
"data wrangler",1,1,"1"
"data の sample data 取得時のqueryではtableの全件取得が行われ、sample data表示が終わらない。",1,1,"1"
"database_connection_failure(cloud_failure)",1,1,"1"
"databbricks certification /training",1,1,"1"
"databrick cluster issue",1,1,"1"
"databrick failed to load libraries",1,1,"1"
"databrick job failure , unexpected failure while waiting for the cluster to ready",1,1,"1"
"databrick notebook fail intermittently",1,1,"1"
"databrick notebook fail intermittently.",1,1,"1"
"databricka academy",1,1,"1"
"databricks  data engineering",1,1,"1"
"databricks  unxplained errors",1,1,"1"
"databricks & airflow connection timeout issue",1,1,"1"
"databricks - delta live table pipeline hanging",1,1,"1"
"databricks - developer foundation bad ge is not visible",1,1,"1"
"databricks - developer foundation badge is not visible",1,1,"1"
"databricks - developer foundations badge is not visible",1,1,"1"
"databricks - developers foundation badge is not visible",1,1,"1"
"databricks - failed to load directory socket",1,1,"1"
"databricks - no vm skus available",1,1,"1"
"databricks 9.1 lts libraries issue",1,1,"1"
"databricks :  conform_netsuite_prs_transaction notebook is long running then expected",1,1,"1"
"databricks academy 403 error",1,1,"1"
"databricks academy capstone",1,1,"1"
"databricks academy content unavailable for ezra",1,1,"1"
"databricks academy course catalog training links not working",1,1,"1"
"databricks academy faq",1,1,"1"
"databricks access issue",1,1,"1"
"databricks accessing other deployment",1,1,"1"
"databricks account admin",1,1,"1"
"databricks account console refresh loop.",1,1,"1"
"databricks account setup assistance",1,1,"1"
"databricks activity failing",1,1,"1"
"databricks alert emails for job deletions - where does the name come from?",1,1,"1"
"databricks ami updates",1,1,"1"
"databricks api (query history command) not receiving filter properly - saved",1,1,"1"
"databricks api gateway timeout",1,1,"1"
"databricks api support",1,1,"1"
"databricks attempts to create clusters in az's without support for requested nodes.",1,1,"1"
"databricks authentication/connectivity to storage account-",1,1,"1"
"databricks autoloader is causing our s3 costs (list and header requests) to skyrocket",1,1,"1"
"databricks azure key vault communication",1,1,"1"
"databricks certificate",1,1,"1"
"databricks certified associate data analyst (beta)",1,1,"1"
"databricks certified associate developer for apache spark",1,1,"1"
"databricks certified associate developer for apache spark exam voucher",1,1,"1"
"databricks cli does not accept pat to authenticate",1,1,"1"
"databricks cluster - commands are getting cancelled",1,1,"1"
"databricks cluster auto-scale-up failure causing production disruption",1,1,"1"
"databricks cluster creation time increased post fail over from west to east",1,1,"1"
"databricks cluster driver is failing",1,1,"1"
"databricks cluster giving spark exception for pythin packages",1,1,"1"
"databricks cluster is in pending state from long time",1,1,"1"
"databricks cluster is not responding",1,1,"1"
"databricks cluster is not starting. and failed",1,1,"1"
"databricks cluster is running from past 6 months without shutting down",1,1,"1"
"databricks cluster listing",1,1,"1"
"databricks cluster nodes not getting released to pool on time.",1,1,"1"
"databricks cluster not able to start",1,1,"1"
"databricks cluster not starting (could be a network issue)",1,1,"1"
"databricks cluster read csv file very slow",1,1,"1"
"databricks cluster reports modulenotfounderror: no module named 'pmdarima'",1,1,"1"
"databricks cluster restart",1,1,"1"
"databricks cluster slowed down",1,1,"1"
"databricks cluster start up failure",1,1,"1"
"databricks cluster startup",1,1,"1"
"databricks cluster startup - 2204250010002422",1,1,"1"
"databricks cluster takes unreasonably long to be created",1,1,"1"
"databricks cluster terminated",1,1,"1"
"databricks cluster timed-out while reading data from s3",1,1,"1"
"databricks clusters are not starting",1,1,"1"
"databricks clusters not loading environment variables",1,1,"1"
"databricks clusters with dbr 7.3 are not connecting to the external metastore",1,1,"1"
"databricks connect",1,1,"1"
"databricks connect java timeout exception",1,1,"1"
"databricks console is down since 6:05 pm pt",1,1,"1"
"databricks could not access keyvault: https://kv-commercial-prod.vault.azure.net/.",1,1,"1"
"databricks custom cells",1,1,"1"
"databricks dashboard",1,1,"1"
"databricks datadog integration issue",1,1,"1"
"databricks dbu consumption",1,1,"1"
"databricks dbx: issues with deploying job clusters and installing python wheel on it",1,1,"1"
"databricks delta - excessive file accumulation -2206130010002322",1,1,"1"
"databricks deployment failure using automation.",1,1,"1"
"databricks doesn't use external metastore",1,1,"1"
"databricks e2 connectivity with es in another aws account",1,1,"1"
"databricks e2 privatelink endpoints - pending acceptance",1,1,"1"
"databricks errors me out after a short time and kills the browser tab",1,1,"1"
"databricks evironment variable init script issue",1,1,"1"
"databricks exam voucher capture",1,1,"1"
"databricks execution failed with error state: internalerror, error message: library installation failed for library due to infra fault",1,1,"1"
"databricks execution failed with error state: internalerror, error message: library installation failed for library due to infra fault.",1,1,"1"
"databricks execution taking a lot of time",1,1,"1"
"databricks failed to download dependency jar from maven central",1,1,"1"
"databricks failing to connect to snowflake",1,1,"1"
"databricks failing to write file to adls",1,1,"1"
"databricks for sas users",1,1,"1"
"databricks group terraform errors",1,1,"1"
"databricks hc cluster is running extremely slowly(similar to 00145865)",1,1,"1"
"databricks help center  contact us error in sql statement: analysisexception: table or view not found problem cause solution",1,1,"1"
"databricks horovod example",1,1,"1"
"databricks host",1,1,"1"
"databricks iam unstable",1,1,"1"
"databricks image vulnerability issue",1,1,"1"
"databricks ip access list",1,1,"1"
"databricks is failing when we are writing more csv files.",1,1,"1"
"databricks is holding this process for a long time without marking to success",1,1,"1"
"databricks jdbc driver strips list of columns in insert statement",1,1,"1"
"databricks job autoloader degradation of performance",1,1,"1"
"databricks job cancelled",1,1,"1"
"databricks job clusters not starting",1,1,"1"
"databricks job failed due to mysql lock",1,1,"1"
"databricks job failed in production",1,1,"1"
"databricks job failed with mysql lock job-1 error",1,1,"1"
"databricks job failed, driver_unresponsive",1,1,"1"
"databricks job failed, malfunctioning instances",1,1,"1"
"databricks job failing due to spark issue",1,1,"1"
"databricks job failing without any logs",1,1,"1"
"databricks job fails after running 150-180 hours",1,1,"1"
"databricks job fails with noclassdeffounderror when scaling up",1,1,"1"
"databricks job failue oauth",1,1,"1"
"databricks job failure",1,1,"1"
"databricks job failure - npip tunnel setup failure during launch",1,1,"1"
"databricks job failures and hanging with no metrics/logs",1,1,"1"
"databricks job in hung state",1,1,"1"
"databricks job in zombie state",1,1,"1"
"databricks job not working",1,1,"1"
"databricks job ran for hours without any progress",1,1,"1"
"databricks job running more than expected time in production",1,1,"1"
"databricks job running slower than expected",1,1,"1"
"databricks job stuck for 3.9 hrs",1,1,"1"
"databricks job submit and list failure(api-2.0)",1,1,"1"
"databricks job which previously succeeded now fails with 'duplicated pypi package' error",1,1,"1"
"databricks jobs are failing",1,1,"1"
"databricks jobs are failing - please check",1,1,"1"
"databricks jobs are failing intermitantly",1,1,"1"
"databricks jobs are failing to trigger the automation account runbooks.",1,1,"1"
"databricks jobs are failing with error 'driver is up but is not responsive, likely due to gc.'",1,1,"1"
"databricks jobs failed due to accessdeniedexception",1,1,"1"
"databricks jobs failing in production",1,1,"1"
"databricks jobs failure",1,1,"1"
"databricks jobs failure with internal python error in the inspect module.",1,1,"1"
"databricks jobs getting failed due to aws_insufficient_instance_capacity_failure",1,1,"1"
"databricks jobs hanging",1,1,"1"
"databricks jobs seem to have had replica/networking issues",1,1,"1"
"databricks jobs sometimes start when libraries not ready yet",1,1,"1"
"databricks jobs unresponsive while fetching data from cosmosdb",1,1,"1"
"databricks kafka sasl error",1,1,"1"
"databricks lakehouse fundamentals accreditation",1,1,"1"
"databricks latency",1,1,"1"
"databricks libraries installation failed due to network unreachable frequently",1,1,"1"
"databricks libraries missing",1,1,"1"
"databricks library installation fails",1,1,"1"
"databricks log capture user command behavior and table access control question",1,1,"1"
"databricks log check method",1,1,"1"
"databricks login support",1,1,"1"
"databricks logo",1,1,"1"
"databricks long running job stuck in pending state not able to acquire resources",1,1,"1"
"databricks manage account login",1,1,"1"
"databricks managed table on mounted container not deleting files on drop command",1,1,"1"
"databricks memory issues",1,1,"1"
"databricks merge delta table failing",1,1,"1"
"databricks merge statement is creating duplicate rows",1,1,"1"
"databricks migrate lab solution export script is failing while exporting for notebook acl in production workspace",1,1,"1"
"databricks migration tool  bug",1,1,"1"
"databricks ml",1,1,"1"
"databricks mlflow behaviour appears to have changed",1,1,"1"
"databricks mlflow bug",1,1,"1"
"databricks mount point using terraform failed",1,1,"1"
"databricks multi-task jobs are failing in an inconsistent fashion",1,1,"1"
"databricks node lost event and restart but terminated after 30 successive failures. need driver logs to identify issues",1,1,"1"
"databricks not able to access new s3 bucket",1,1,"1"
"databricks not being able to connect to key-vault.",1,1,"1"
"databricks notebook execution issue",1,1,"1"
"databricks notebook failing to create hive table in dbfs.",1,1,"1"
"databricks notebook giving error: rpc response (of 20975702 bytes) exceeds limit of 20971520 bytes",1,1,"1"
"databricks notebook is failing with out of memory error",1,1,"1"
"databricks notebook performance is degraded",1,1,"1"
"databricks notebooks running longer - billing",1,1,"1"
"databricks notebooks to create automation test cases",1,1,"1"
"databricks odbc and jdbc drivers",1,1,"1"
"databricks on safari for ipad",1,1,"1"
"databricks oom issue when trying to save data",1,1,"1"
"databricks optimize table command is failing on oss delta tables",1,1,"1"
"databricks outage 27/06",1,1,"1"
"databricks plan using explain",1,1,"1"
"databricks plugins",1,1,"1"
"databricks pool was not able to get more resources from aws",1,1,"1"
"databricks prd is down",1,1,"1"
"databricks prod issue caused by sparkexception",1,1,"1"
"databricks product roadmap webinars",1,1,"1"
"databricks production is down and not able to access",1,1,"1"
"databricks production job failure",1,1,"1"
"databricks ps team cannot access our workspace",1,1,"1"
"databricks pvc 3.55 configuration query",1,1,"1"
"databricks pvc prod upgrade to 3.60 support",1,1,"1"
"databricks python interpreter not working",1,1,"1"
"databricks qa jobs triggered by tidal scheduler are failing",1,1,"1"
"databricks queries are taking a long time stays stuck at  'uploading command'",1,1,"1"
"databricks related queries",1,1,"1"
"databricks replication for 1 group is stopped",1,1,"1"
"databricks repo authentication error while pushing the changes",1,1,"1"
"databricks repo creating from main to dev creating error",1,1,"1"
"databricks repos - seeing changed/modified files in the repos tab",1,1,"1"
"databricks runtime 10.4 fails emr-dynamodb-connector",1,1,"1"
"databricks runtime 10.4 lts",1,1,"1"
"databricks runtime 10.4lts weird behavior",1,1,"1"
"databricks runtime 10.x docker images",1,1,"1"
"databricks runtime 11.0 doesn't work with  container services",1,1,"1"
"databricks runtime 7.3 lts with external hive metastore 3.1.0 not supporting 'overwrite' method'",1,1,"1"
"databricks runtime 8.4",1,1,"1"
"databricks runtime cost",1,1,"1"
"databricks scala scripts cannot access object store using java module",1,1,"1"
"databricks scheduled job failed, but succeeds when ran manually",1,1,"1"
"databricks scim connector unable to sync certain users",1,1,"1"
"databricks self-learning path login issue",1,1,"1"
"databricks shortcuts",1,1,"1"
"databricks spring core vulnerability impact",1,1,"1"
"databricks sql - dashboard update error presentation",1,1,"1"
"databricks sql api",1,1,"1"
"databricks sql connectivity to excel",1,1,"1"
"databricks sql connector 2.0.0 cannot run on dbr 7.3",1,1,"1"
"databricks sql dashboard",1,1,"1"
"databricks sql dashboard not refreshing on schedule",1,1,"1"
"databricks sql dashboards are scheduled twice and not able to delete one schedule",1,1,"1"
"databricks sql does not open sqleditor - internal server error",1,1,"1"
"databricks sql endpoint access/permissions issue",1,1,"1"
"databricks sql endpoint performance low",1,1,"1"
"databricks sql giving error when querying opencsv table.",1,1,"1"
"databricks sql group access grant",1,1,"1"
"databricks sql is dead",1,1,"1"
"databricks sql not listed in persona",1,1,"1"
"databricks sql page with errors, not able to load saved queries nor the ui.",1,1,"1"
"databricks sql queries failing",1,1,"1"
"databricks sql query error",1,1,"1"
"databricks sql query performance issue",1,1,"1"
"databricks sql redshift",1,1,"1"
"databricks sql will not load at all in safari",1,1,"1"
"databricks sql, some users not able to see the metastore.",1,1,"1"
"databricks sql, some users not anle to see the metastore",1,1,"1"
"databricks sql: clusters are failing to launch",1,1,"1"
"databricks sqlserver jdbc connection setup",1,1,"1"
"databricks sqlserver jdbc connection setup.",1,1,"1"
"databricks sso login issues to shards",1,1,"1"
"databricks streaming job is lagging in processing the data",1,1,"1"
"databricks streaming: checkpointing when using kafka connector as source",1,1,"1"
"databricks support - sql workspace issue",1,1,"1"
"databricks support ticket # 00147690",1,1,"1"
"databricks table column comment cannot decode chinese words",1,1,"1"
"databricks tables restoration",1,1,"1"
"databricks throw error intermittently when connecting to key vault",1,1,"1"
"databricks to application gateway connectivity is not working",1,1,"1"
"databricks to jdbc using ssh tunnel",1,1,"1"
"databricks to tableau",1,1,"1"
"databricks token expiry",1,1,"1"
"databricks ui loading times",1,1,"1"
"databricks ui not loading",1,1,"1"
"databricks ui not responding",1,1,"1"
"databricks ui not responding.",1,1,"1"
"databricks unitycatalog logs are not populating",1,1,"1"
"databricks upgrade v3.55",1,1,"1"
"databricks user within iam passthrough cluster unable to execute query - issue already reported.",1,1,"1"
"databricks very slowly, errors in page",1,1,"1"
"databricks workspace",1,1,"1"
"databricks workspace - security monitoring of misconfigurations",1,1,"1"
"databricks workspace does not return status of all the completed jobs using rest api",1,1,"1"
"databricks workspace guide",1,1,"1"
"databricks workspace properties",1,1,"1"
"databricks workspace-level logging",1,1,"1"
"databricks-cli macos",1,1,"1"
"databricks-cli path",1,1,"1"
"databricks-connect not working",1,1,"1"
"databricks-connect secrets",1,1,"1"
"databricks-connect with token is deprecated",1,1,"1"
"databricks-connect write location",1,1,"1"
"databricks-log contains no information about causing column in case of type mismatch during synapse-write",1,1,"1"
"databricks-sql-access",1,1,"1"
"databricks: issue with simba jdbc connector",1,1,"1"
"databricks: outofmemory error",1,1,"1"
"databricks_permissions in terraform overwriting resource instead of appending it",1,1,"1"
"databricks_util",1,1,"1"
"databricksserviceexception: internal_error: nullpointerexception",1,1,"1"
"databricksthrottledexception thrown in one environment but not another",1,1,"1"
"databricks対応ブラウザのバージョンについて",1,1,"1"
"databricks監査ログ設定におけるs3バケットの暗号化について",1,1,"1"
"databrick環境構築にあたり、dbfsは必須機能となるか",1,1,"1"
"dataframe api",1,1,"1"
"dataframe become empty when try to read a json",1,1,"1"
"dataset refresh failing",1,1,"1"
"datasource",1,1,"1"
"date difference",1,1,"1"
"date en français",1,1,"1"
"date type",1,1,"1"
"date upto last two year",1,1,"1"
"days between",1,1,"1"
"db-004 instructor-led courses are not available for ezra",1,1,"1"
"db-i-205",1,1,"1"
"dbc",1,1,"1"
"dbceventlogginglistener:  error in writing event to log",1,1,"1"
"dbfs cp command taking longer time",1,1,"1"
"dbfs directory can not find",1,1,"1"
"dbfs files in s3",1,1,"1"
"dbfs is down on almost all cluster on 4/1~current",1,1,"1"
"dbfs migration objects :e1 to e2",1,1,"1"
"dbfs path to workspace",1,1,"1"
"dbfs remove",1,1,"1"
"dbfs s3 path",1,1,"1"
"dbfs target directory",1,1,"1"
"dbfs_down",1,1,"1"
"dbfsルートストレージ用のs3バケットのkms暗号化方法について",1,1,"1"
"dbinsights not working",1,1,"1"
"dbr 10.4 query failure",1,1,"1"
"dbr 10.4 upgrade error",1,1,"1"
"dbr 10.x not supporting querying json serde tables and giving snappy compression error",1,1,"1"
"dbr 8.4+",1,1,"1"
"dbr prod -  cannot broadcast the table that is larger than 8gb: 9 gb",1,1,"1"
"dbr version 10.x lts does not work properly with ranger",1,1,"1"
"dbr_cluster_launch_timeout",1,1,"1"
"dbsql cannot query hive table with serde",1,1,"1"
"dbsql is disabled",1,1,"1"
"dbsql/notebook cannot query hive table with hive type string",1,1,"1"
"dbt databricks connection issue",1,1,"1"
"dbt jobs in databricks not completing/failing",1,1,"1"
"dbt workflow",1,1,"1"
"dbu",1,1,"1"
"dbu usage log issue",1,1,"1"
"dbutil",1,1,"1"
"dbutils function",1,1,"1"
"dbutils tutorial",1,1,"1"
"dbutils.fs",1,1,"1"
"dbutils.fs.cp failed with s3 mnt target",1,1,"1"
"dbutils.fs.ls not working",1,1,"1"
"dbutils.fs.mount error",1,1,"1"
"dbutils.notebook.run temp view",1,1,"1"
"dbw-danadf-prod",1,1,"1"
"dbx temporarily unavailable in one of our environments",1,1,"1"
"de 6.1 - incremental data ingestion with auto loader",1,1,"1"
"de 6.3l",1,1,"1"
"debug cluster init script",1,1,"1"
"debugfs on worker nodes not owned by root",1,1,"1"
"debugging",1,1,"1"
"decision tree",1,1,"1"
"decision tree titanic",1,1,"1"
"decision tree titanic python",1,1,"1"
"dedup issue",1,1,"1"
"deep clone sql command needs to be incremental",1,1,"1"
"deep learning using pytorch resulting in oom issues",1,1,"1"
"default r-package repo to be in sync with dbr runtime's microsoft cran",1,1,"1"
"default table format",1,1,"1"
"default table is not present",1,1,"1"
"default table is not present in dat",1,1,"1"
"default table is not present in data",1,1,"1"
"definitive guide",1,1,"1"
"delegated authentication to third-party services",1,1,"1"
"delete a command",1,1,"1"
"delete a notebook",1,1,"1"
"delete cmd from notebook",1,1,"1"
"delete customer account databricks",1,1,"1"
"delete data",1,1,"1"
"delete databricks user",1,1,"1"
"delete doublons",1,1,"1"
"delete exception for streaming job",1,1,"1"
"delete from recents",1,1,"1"
"delete old non-e2 account",1,1,"1"
"delete older data from s3",1,1,"1"
"delete orphan workspace",1,1,"1"
"delete program",1,1,"1"
"delete published notebook",1,1,"1"
"delete recents",1,1,"1"
"delete scv",1,1,"1"
"delete table",1,1,"1"
"delete the account and user",1,1,"1"
"delete uploaded file in dbfs",1,1,"1"
"delete uploaded files",1,1,"1"
"delete user",1,1,"1"
"delete user workspace owne",1,1,"1"
"deleted a cluster",1,1,"1"
"deleted cluster",1,1,"1"
"deleted notebook",1,1,"1"
"deleted objects on dbr metadata",1,1,"1"
"deleted vms continue to write data to loganalytics",1,1,"1"
"deleting model",1,1,"1"
"delta and aws intelligent tiering",1,1,"1"
"delta cache enable",1,1,"1"
"delta catalog",1,1,"1"
"delta lake guide",1,1,"1"
"delta lake issues",1,1,"1"
"delta lake merges & spark order by function issue in production",1,1,"1"
"delta lake stream fails to recover on restart",1,1,"1"
"delta live pipeline job stuck",1,1,"1"
"delta live table - errors in spark logs",1,1,"1"
"delta live table column comments",1,1,"1"
"delta live table interface broken after databrick's code applied to ec2 workspace",1,1,"1"
"delta live table overwrite mode",1,1,"1"
"delta live table pipeline failed in update progress",1,1,"1"
"delta live table pipeline freezes the databricks ui",1,1,"1"
"delta live tables - aggregated quality checks (expectations)",1,1,"1"
"delta live tables apply_changes() ability to update columns based on a matched action vs not_matched action",1,1,"1"
"delta live tables cluster init scripts?",1,1,"1"
"delta live tables libraries",1,1,"1"
"delta live tables not provisioning reqources",1,1,"1"
"delta live tables not showing up under jobs tab in databricks--2204260040007692",1,1,"1"
"delta live tables with autoloader",1,1,"1"
"delta live tables, multiple streaming aggregations",1,1,"1"
"delta live tables, multiple streaming aggregations.",1,1,"1"
"delta log corrupted",1,1,"1"
"delta merge optimization",1,1,"1"
"delta missing checkpoint",1,1,"1"
"delta optimize and structured streaming",1,1,"1"
"delta partation is not working",1,1,"1"
"delta schema merge issue",1,1,"1"
"delta sharing issue",1,1,"1"
"delta streaming job missing data",1,1,"1"
"delta streaming optmization",1,1,"1"
"delta table can't be modified",1,1,"1"
"delta table creation error",1,1,"1"
"delta table merge - server.spotinstancetermination",1,1,"1"
"delta table merge functionality introducing duplicates",1,1,"1"
"delta table merge into command creates lots of files in storage | arr sr# 2203110030000770001",1,1,"1"
"delta table merge performance issue",1,1,"1"
"delta table metastore is corrupted",1,1,"1"
"delta table not able to be modified",1,1,"1"
"delta table optimisations",1,1,"1"
"delta table query running extremely slow, please save logs for further analysis",1,1,"1"
"delta table restore to version took 45 minutes",1,1,"1"
"delta tables",1,1,"1"
"delta upsert new columns mergeschema not working",1,1,"1"
"delta vacuum apparently not working",1,1,"1"
"delta write failures to s3",1,1,"1"
"delta.autooptimize.autocompact doesn't work",1,1,"1"
"delta.formatcheck",1,1,"1"
"demo ticket",1,1,"1"
"demonstration on using data bricks rest api",1,1,"1"
"deploy databricks failed when use a resource group which include gb character in name",1,1,"1"
"deploy through azure devops",1,1,"1"
"deploy to aws using cross account role",1,1,"1"
"deployment is misconfigured",1,1,"1"
"desc query failing with amazons3exception",1,1,"1"
"describe , optimize commands are not working as expected",1,1,"1"
"describe history issues",1,1,"1"
"describe schema extended shows root as owner after changing the owner",1,1,"1"
"describe table differences in dbr 10.4",1,1,"1"
"deserializing json to a class produces strange stdout outputs.",1,1,"1"
"detaching",1,1,"1"
"detaching notebooks from clusters",1,1,"1"
"detail",1,1,"1"
"details for job",1,1,"1"
"detected a data update snappy.parquet in the source table at version 475. this is currently not supported",1,1,"1"
"detected a data update snappy.parquet in the source table at version this is currently not supported",1,1,"1"
"determine monthly dbu consumption",1,1,"1"
"developers essential learning pathway",1,1,"1"
"devops",1,1,"1"
"df.delect",1,1,"1"
"df.select",1,1,"1"
"dictionary changed size during iteration",1,1,"1"
"didnot get [attributeerror: module 'os' has no attribute 'pathlike'] error in interactive cluster but got the error on job cluster",1,1,"1"
"differences between the spark 2.4 and spark 3.0 exams.",1,1,"1"
"different configurations for same databricks runtime version",1,1,"1"
"directquery query folding to databricks producing invalid sql",1,1,"1"
"disable 'create automl experiment' feature",1,1,"1"
"disable and re-enable sso integration for root account with azure ad",1,1,"1"
"disable autocomplete quotes",1,1,"1"
"disable delta feature",1,1,"1"
"disable schedule of dashboard via api",1,1,"1"
"disable the gaining to access to other workspace's",1,1,"1"
"disable the public ip address on the cluster for our aws workspaces",1,1,"1"
"discount code not working",1,1,"1"
"discrepancy in output of same sql query when run in notebook against in dbsql editor",1,1,"1"
"discuss cluster detailed for a sepcific use-case",1,1,"1"
"display command",1,1,"1"
"display dataframe scal",1,1,"1"
"display empty",1,1,"1"
"display gif",1,1,"1"
"display html",1,1,"1"
"display visualizations not working with python notebooks",1,1,"1"
"display(dataframe) never finishes",1,1,"1"
"displaying images in databricks notebook from dbfs or s3",1,1,"1"
"distinct",1,1,"1"
"divergency of jobs status",1,1,"1"
"dlt cluster configuration",1,1,"1"
"dlt complete mode",1,1,"1"
"dlt feature",1,1,"1"
"dlt import custom python module",1,1,"1"
"dlt job keep running without any progress",1,1,"1"
"dlt module import",1,1,"1"
"dlt pipeline - autoloader - rescue column",1,1,"1"
"dlt pipeline with maven packages",1,1,"1"
"dlt python module",1,1,"1"
"dlt s3",1,1,"1"
"dlt taking 30 minutes to start a triggered run",1,1,"1"
"dlt ui reverted to beta interface",1,1,"1"
"dlt.create_streaming_live_table does not work with name property | genie",1,1,"1"
"do i need to do associate first before professionl",1,1,"1"
"do i need to log off",1,1,"1"
"do we have ps options for code review and redesign assistance?",1,1,"1"
"docker image is not working",1,1,"1"
"docker image pull failure ecr",1,1,"1"
"docker job",1,1,"1"
"docker module not found",1,1,"1"
"documentation issues",1,1,"1"
"does 'alter table' can use with 'glue catalog'?",1,1,"1"
"does credential pass-through supported within the job cluster?",1,1,"1"
"does databricks cluster support install jar from private maven repo?",1,1,"1"
"does databricks support folded query in powerbi?",1,1,"1"
"does sql dashboards have filter components?",1,1,"1"
"does the version of databricks runtime must no less than the docker base version for it to run?",1,1,"1"
"does this set up will work on spark kafka connector?",1,1,"1"
"download all records hangs when executing command",1,1,"1"
"download all sql",1,1,"1"
"download azure",1,1,"1"
"download csv local machine",1,1,"1"
"download files to desktop",1,1,"1"
"download invoice",1,1,"1"
"download sql analytics dashboard via api",1,1,"1"
"downtime",1,1,"1"
"downtown for changing cluster node type",1,1,"1"
"drag and drop table fields",1,1,"1"
"driver",1,1,"1"
"driver 'memory leak'  and driver not responsive due to  garbage collection  in us prod for streaming cluster",1,1,"1"
"driver class",1,1,"1"
"driver is not responsive",1,1,"1"
"driver is up but is not responsive errors in high concurrency clusters",1,1,"1"
"driver is up but is not responsive, likely due to gc.",1,1,"1"
"driver logs on ui are not showing correct creation time",1,1,"1"
"driver memory consumption issue",1,1,"1"
"driver nod",1,1,"1"
"driver node",1,1,"1"
"driver node shut down by cloud provider",1,1,"1"
"driver randomly failing on merge job",1,1,"1"
"driver shut down ,while xgboost training",1,1,"1"
"driver-proxy-api",1,1,"1"
"drop and create table queries failing with awsclientioexception",1,1,"1"
"drop column",1,1,"1"
"drop dulicates",1,1,"1"
"drop duplicates",1,1,"1"
"drop_duplicates",1,1,"1"
"dropna(""all"")",1,1,"1"
"dropped a delta table without delete or vacuum and now it's broken",1,1,"1"
"ds-etl-dev and rx-experiement-large-driver spark interactive clusters are not able to query the data lake views",1,1,"1"
"duplicate parameters",1,1,"1"
"duplicates",1,1,"1"
"duplicates when writing from delta live table function",1,1,"1"
"dynamic partition pruning triggering class cast exception for a unary node inserted by cusotm optimiser",1,1,"1"
"dynamic table sames in sql",1,1,"1"
"dynamodb sts java client cross account",1,1,"1"
"e job aborted due to stage failure: reason: remote rpc client disassociated. likely due to containers exceeding thresholds, or network issues.",1,1,"1"
"e-mail notifications when jobs are skipped",1,1,"1"
"e1",1,1,"1"
"e2",1,1,"1"
"e2 migration",1,1,"1"
"e2 usage/billable log",1,1,"1"
"e2 vs classic",1,1,"1"
"earnin databricks console is down",1,1,"1"
"earnin's production databricks console was not accessible for about 20 mins on 4/08",1,1,"1"
"ebs volume on instance",1,1,"1"
"ebs volume(s) - missing required tags",1,1,"1"
"ec2",1,1,"1"
"ec2 fleets cluster pool doesn't follow the region specification",1,1,"1"
"ec2 instances are not stoped after the cluster terminated",1,1,"1"
"ec2 instances are not stopped after the cluster terminated",1,1,"1"
"edit job alert",1,1,"1"
"editing clusters throwing errors",1,1,"1"
"edl - databricks cli access issue",1,1,"1"
"edp testing environment wiped - need to restore-",1,1,"1"
"effective routes and nsg restrictions traffic issue",1,1,"1"
"efficient way to query partitions that have changed from a delta table",1,1,"1"
"eliminate tables from dbfs",1,1,"1"
"email alert for job fail",1,1,"1"
"email alerts to group email",1,1,"1"
"email databricks dashboard has empty content",1,1,"1"
"email notifications doesn't accept email address",1,1,"1"
"empty visualization",1,1,"1"
"enable",1,1,"1"
"enable ""files in repo""",1,1,"1"
"enable databricks sql for a workspace (id: 799967981453871)",1,1,"1"
"enable dbutils.secrets.get",1,1,"1"
"enable dbutils.secrets.get for customer.",1,1,"1"
"enable dbutils.secrets.get on databricks-prod workspace",1,1,"1"
"enable dbutils.secrets.get through databricks connect for workspace",1,1,"1"
"enable delta share ,photon and unity catalog",1,1,"1"
"enable files in repos option no available",1,1,"1"
"enable init script for model serving",1,1,"1"
"enable monitor for model drift and data drift using mlflow",1,1,"1"
"enable prometheus metrics",1,1,"1"
"enable secrets in db-connect",1,1,"1"
"enable serverless sql endpoint in development-workspace",1,1,"1"
"enable shared subnets for aon us account",1,1,"1"
"enable table access control",1,1,"1"
"enable use of dbutils.secrets.get from local ide",1,1,"1"
"enabling customer-managed keys for managed services",1,1,"1"
"enabling ebs ssd gp3 in workspace setting is picking throughput 128mbps by default instead of 125mbps",1,1,"1"
"enabling privatelink for e2 account",1,1,"1"
"enabling spark 3.0 prometheus",1,1,"1"
"enabling the private workspace feature.",1,1,"1"
"encode",1,1,"1"
"end of support / end of life",1,1,"1"
"endpoint monitoring",1,1,"1"
"engineering cluster required restart due to unresponsive notebooks",1,1,"1"
"enterprise features",1,1,"1"
"entry point for pipeline job",1,1,"1"
"environment variables provided by databricks secrets showing different results",1,1,"1"
"eric nechayev",1,1,"1"
"eric.campos@loft.com.br",1,1,"1"
"errno 14 from dbfs",1,1,"1"
"error 'failed to store the result. try rerunning the command'",1,1,"1"
"error adding member to group",1,1,"1"
"error al cargar tablas en el cluster",1,1,"1"
"error calling permissions 2.0 api",1,1,"1"
"error connections from databricks to azure sql",1,1,"1"
"error creating clusters with specific tags",1,1,"1"
"error downloading dbc archive",1,1,"1"
"error encounter in dlt pipeline while drop and recreate",1,1,"1"
"error in a table",1,1,"1"
"error in aws s3",1,1,"1"
"error in delta table merge from sql endpoint",1,1,"1"
"error in job pip-yape-users- 2206230040006088",1,1,"1"
"error in job run event - job run doesnt exist in console",1,1,"1"
"error in jobs after upgrade from 10.3 to 10.4 lts, until 2 days ago",1,1,"1"
"error in jobs: outofmemorysparkexception",1,1,"1"
"error in sql statement: amazons3exception: access denied; request: put https://amgen-edl-databricks-e2-gco-bkt.s3-us-west-2.amazonaws.com oregon-prod/972620489536740/user/hive/warehouse/pharmaace_tblndc_gastric_0406/",1,1,"1"
"error in sql statement: analysisexception: can not create the managed table('`spark_catalog`.`default`.`devicedata`'). the associated location('dbfs:/user/hive/warehouse/devicedata') already exists.",1,1,"1"
"error in sql statement: sparkexception",1,1,"1"
"error loading to synapse",1,1,"1"
"error log",1,1,"1"
"error log in sql",1,1,"1"
"error message occured when query 'display', 'topandas'",1,1,"1"
"error message on main dbx dashboard",1,1,"1"
"error message “automated clusters do not support autotermination”.",1,1,"1"
"error message: 'javapackage' object is not callable",1,1,"1"
"error message: 'javapackage' object is not callable-",1,1,"1"
"error messager when export hive catalog metadata from databricks workspace",1,1,"1"
"error occured deserializing arrow data  allocating buffer while running pd.dataframe(cursor.fetchall())",1,1,"1"
"error occured deserializing arrow data  allocating buffer while running pd.dataframe(cursor.fetchall()))",1,1,"1"
"error on azureadauthenticator.gettokencall",1,1,"1"
"error on creating table with deltashare",1,1,"1"
"error on metastore when using uploaded jar for db connection with aws privatelink",1,1,"1"
"error on scheduled job on azure databricks",1,1,"1"
"error processing query/statement. error code: 0, sql state: error running query: java.io.ioexception: no space left on device",1,1,"1"
"error reading file from absolute path in spark facing issue",1,1,"1"
"error reading info/refs",1,1,"1"
"error reading streaming state file of hdfsstatestoreprovider",1,1,"1"
"error requesting queries api endpoint",1,1,"1"
"error running apt-get to install packages",1,1,"1"
"error running queries on coreviz cluster in live(450)",1,1,"1"
"error running query shaded.databricks.org.apache.hadoop.fs.azure.azureexception: hadoop_azure_shaded.com.microsoft.azure.storage.storageexception: server failed to authenticate the request. make sure the value of authorization header is formed correctly including the signature.",1,1,"1"
"error starting the 'rstudio' cluster.",1,1,"1"
"error starting the 'rstudio' cluster. message is 'rstudio-install.sh failed: script exit status is non-zero'",1,1,"1"
"error to install poppler-utils using apt",1,1,"1"
"error transportrequesthandler",1,1,"1"
"error updating additional subnet",1,1,"1"
"error when connecting powerbi with databricks",1,1,"1"
"error when connecting to acl enabled cluster databases through jdbc",1,1,"1"
"error when creating cluster",1,1,"1"
"error when displaying in a chart",1,1,"1"
"error when mounting file system and trying to read file on spark",1,1,"1"
"error when running a job containing mlflow",1,1,"1"
"error when running job",1,1,"1"
"error when running query on view in notebook",1,1,"1"
"error when running spark-submit job",1,1,"1"
"error when trying to read from s3",1,1,"1"
"error when using databricks-connect 10.4.0b0 with dbr 10.4-lts",1,1,"1"
"error while connecting to databricks sql endpoint from power b",1,1,"1"
"error while creating database table in azure databricks",1,1,"1"
"error while obtaining a new communication channel",1,1,"1"
"error while reading file dbfs:/mnt/itgp/eng/l6b_array_alr_rpf/_delta_log/00000000000000008570.checkpoint.parquet",1,1,"1"
"error while referring libraries on databricks apc",1,1,"1"
"error while running job",1,1,"1"
"error while trying to modify table",1,1,"1"
"error while viewing the data from parquet viewer once we downloaded from azure blob storage containers.",1,1,"1"
"error with data ingestion jobs",1,1,"1"
"error with db-dtypes package on job",1,1,"1"
"error with mlflow when storing the model",1,1,"1"
"error with model deployment which used to be able to be deployed",1,1,"1"
"error: cannot read grants: got empty permissions list",1,1,"1"
"error: credential size is more than configured size limit",1,1,"1"
"error: driver not responding",1,1,"1"
"error: invalid_parameter_value: invalid notebook_path: logging/sqldbconnection",1,1,"1"
"error: run result unavailable. contact databricks if it is unexpected.",1,1,"1"
"error: unable to instantiate org.apache.hadoop.hive.metastore.hivemetastoreclient",1,1,"1"
"errores de intermitencia con los logs",1,1,"1"
"errors on azure databricks feature store",1,1,"1"
"errors writing to sqlmi - databricks notebook",1,1,"1"
"es-326665",1,1,"1"
"es-337012",1,1,"1"
"establishing fsx connectivity from databricks",1,1,"1"
"estimating required number of memory",1,1,"1"
"etl failures",1,1,"1"
"etl pipeline",1,1,"1"
"eu-west-2 and eu-central-1 down",1,1,"1"
"eventhub streaming to databricks delta table failure due to checkpoint",1,1,"1"
"eventhub streaming to databricks delta table stopped due to offset mismatched case",1,1,"1"
"eventhub streaming to databricks delta table stopped due to offset mismatched-",1,1,"1"
"eventhub topic",1,1,"1"
"exacting data from a website from databricks notebook using python3",1,1,"1"
"exam",1,1,"1"
"exam questions and answers",1,1,"1"
"exam suspended",1,1,"1"
"exam(s) not associated with promotion code.",1,1,"1"
"exceeded max number of parallel requests",1,1,"1"
"exception thrown initialization filesystem",1,1,"1"
"exception-thrown-initializating-filesystem",1,1,"1"
"exception: could not open socket: ['tried to connect to ('127.0.0.1', 46229), but an error occured: [errno 111] connection refused']",1,1,"1"
"exception: exception: org.apache.spark.sparkexception: job aborted.",1,1,"1"
"excessive delay to iterate dataframes",1,1,"1"
"exchange rates",1,1,"1"
"exclude",1,1,"1"
"execute a sql",1,1,"1"
"executing code in databricks interactive cluster is very slow.",1,1,"1"
"execution duration showing zero",1,1,"1"
"execution error when accessing dbfs",1,1,"1"
"execution of 2 jars on same cluster sequential with same prod.conf",1,1,"1"
"execution taking longer time",1,1,"1"
"executor lost",1,1,"1"
"executor lost for one job",1,1,"1"
"executor nodes not restarting",1,1,"1"
"executorlostfailure / org.apache.spark.sparkexception: exception thrown in awaitresult:",1,1,"1"
"experiencing high demand - jobs failing",1,1,"1"
"experiments",1,1,"1"
"expired personal acces token (pat) in combination with ip-adress whilelist for databricks workspace",1,1,"1"
"explore paramater for json sizing within databricks",1,1,"1"
"explorer storage",1,1,"1"
"export csv with headers",1,1,"1"
"export data csv file",1,1,"1"
"export data from databricks",1,1,"1"
"export data from delta lake",1,1,"1"
"export data to a table?",1,1,"1"
"export dataset from delta",1,1,"1"
"export dataset from delta lake",1,1,"1"
"export excel",1,1,"1"
"export log",1,1,"1"
"export more that 1000 rows",1,1,"1"
"export more that 1000 rows- excel",1,1,"1"
"export query result",1,1,"1"
"export scala",1,1,"1"
"export sql log",1,1,"1"
"export to csv",1,1,"1"
"expose a model endpoint for external consumption",1,1,"1"
"extended",1,1,"1"
"extension",1,1,"1"
"extension on trial",1,1,"1"
"external aws glue metastore is not accessible from databricks dataplane via assumerole",1,1,"1"
"external data access",1,1,"1"
"external hive metastore error creating views",1,1,"1"
"external hive metastore failing",1,1,"1"
"external users in aad, display name is blank when you add a user to the databricks admin console",1,1,"1"
"extract.py",1,1,"1"
"extreme slowness in the etl jobs.",1,1,"1"
"f.expr(""interval 24 months"")",1,1,"1"
"faced [adb-dp-8081006549057212.12.azuredatabricks.net refused to connect] on ganglia tabs except  [main] table",1,1,"1"
"facing ""permission_denied"" error while configuring audit log delivery",1,1,"1"
"facing error while running job",1,1,"1"
"facing extreme slowness in edl databricks shards",1,1,"1"
"facing import issue in job",1,1,"1"
"facing issue in audit logs",1,1,"1"
"facing issue in deployment of external ml models on databricks",1,1,"1"
"facing issue while adding the user in databricks account level",1,1,"1"
"facing issue while connecting databricks and azure sql db using service principal as authentication mode as token got expired after 1 hr",1,1,"1"
"facing issue while connecting sql endpoint via jdbc connection",1,1,"1"
"facing issue while creating job in databricks e2 shards",1,1,"1"
"facing issue while querying data from sas application",1,1,"1"
"facing issue while trying to connect bigquery using jdbc from databricks notebook",1,1,"1"
"facing issue while writing data to cosmos through spark",1,1,"1"
"facing issue with table metadata",1,1,"1"
"facing issues in databricks sql when downloading the csv file.",1,1,"1"
"facing jackson databind version issue submitting spark job in us-west-2 workspace",1,1,"1"
"facing memory issue while performing etl using spark scripts",1,1,"1"
"facing problem while enabling table acl in high concurrency cluster",1,1,"1"
"facing slowness in data bricks while running through query_fabric",1,1,"1"
"facing slowness while executing queries in databricks cluster",1,1,"1"
"facing with an arrayindexoutofboundsexception and negativearraysizeexception errors",1,1,"1"
"fail clone in same database",1,1,"1"
"fail to create cluster with pool instance 'all spot' in  azure mooncake",1,1,"1"
"fail to install python library glpk",1,1,"1"
"failed installation of event hub connector library from maven repo in databricks cluster.",1,1,"1"
"failed integrity checks",1,1,"1"
"failed job on 10.4 lts payflex_boron_s3",1,1,"1"
"failed library installation blocks cluster",1,1,"1"
"failed network validation checks when creating workspace",1,1,"1"
"failed to access cloud storage: abfsrestoperationexception",1,1,"1"
"failed to acquire mysql lock job-5",1,1,"1"
"failed to create databricks job cluster",1,1,"1"
"failed to download dependency from maven central",1,1,"1"
"failed to find data source",1,1,"1"
"failed to find data source: com.microsoft.sqlserver.jdbc.spark",1,1,"1"
"failed to find data source: tahoe.",1,1,"1"
"failed to import custom library inside mlflow.run()",1,1,"1"
"failed to import git on cluster with docker image and ml flow",1,1,"1"
"failed to launch job",1,1,"1"
"failed to load directory: socket closed - when accessing the workspace",1,1,"1"
"failed to open  delta live tables pipeline page.",1,1,"1"
"failed to open delta live tables pipeline page.ui related issue.(continuation  of 00143631)",1,1,"1"
"failed to store the result. try rerunning the command",1,1,"1"
"failed to upload data into the databricks bucket using this databricks load generator",1,1,"1"
"failed/successful job on two clusters",1,1,"1"
"failing spark streaming job",1,1,"1"
"failure starting repl on adf pipeline job.",1,1,"1"
"failure to clean up existing notifications on s3 bucket",1,1,"1"
"failure to create cluster",1,1,"1"
"failure to initialize configuration databricks endpoint azure",1,1,"1"
"failure to start cluster workspace authorization request",1,1,"1"
"falha na conexão do databricks com o banco sql 'sqldb-coe-pucpr'",1,1,"1"
"feature",1,1,"1"
"feature enablement",1,1,"1"
"feature extraction has become slower",1,1,"1"
"feature request",1,1,"1"
"feature request - misleading error message",1,1,"1"
"feature request : add spark conf to handle bad line/corrupt file of hive external table",1,1,"1"
"feature request follow up for: 00168806",1,1,"1"
"feature request for redshift native driver",1,1,"1"
"feature request(db connect using secret scopes)  case",1,1,"1"
"feature request(db connect using secret scopes) case",1,1,"1"
"feature store api in docker",1,1,"1"
"feature store apis take 2x longer on dbr 10.4 and 10.5",1,1,"1"
"feature store doesn't allow nullable fields",1,1,"1"
"feature store publish into online store not working",1,1,"1"
"feature store: training set load joining failure",1,1,"1"
"feature/hotfix for databricks   mlflow breaking change with protobuf",1,1,"1"
"fetched",1,1,"1"
"few s3 folders were dropped in production",1,1,"1"
"few users unable to sso to our new workspace",1,1,"1"
"file",1,1,"1"
"file cannot inherit from folder when create table",1,1,"1"
"file corruption when saving to s3",1,1,"1"
"file in repo",1,1,"1"
"file metadata column name collision",1,1,"1"
"file not found",1,1,"1"
"file permissions",1,1,"1"
"file read from container is taking very long time -2",1,1,"1"
"file storage",1,1,"1"
"filealreadyexistsexception",1,1,"1"
"filealreadyexistsexception:",1,1,"1"
"filenotfounderror in spark execution",1,1,"1"
"files",1,1,"1"
"files in repos issue",1,1,"1"
"files save to",1,1,"1"
"filter function",1,1,"1"
"find azure server host name",1,1,"1"
"find existing notebook",1,1,"1"
"find exported file",1,1,"1"
"find server host name",1,1,"1"
"find server name",1,1,"1"
"find server name & http",1,1,"1"
"find the rca for databricks scheduled job failure with below issue",1,1,"1"
"find userid",1,1,"1"
"find_in_set function (databricks sql)",1,1,"1"
"finding the right datafile",1,1,"1"
"first genie access request",1,1,"1"
"first redirect to help center does not work, second does",1,1,"1"
"flush-hosts",1,1,"1"
"flyway support",1,1,"1"
"folder",1,1,"1"
"folllowup on the sf ticket 00139349",1,1,"1"
"folloup | arr | 2204250060001035 | error with union command saying object of type 'nonetype' has no len() only in dbr 9.1 lts",1,1,"1"
"folloup | arr | 2205040030000589 | metastore connection failure",1,1,"1"
"follow up  sf 00141036",1,1,"1"
"follow up ( 00185810) cluster is not starting its failing.",1,1,"1"
"follow up (00138540) performance issues.",1,1,"1"
"follow up 00132204",1,1,"1"
"follow up 00133956",1,1,"1"
"follow up 00142454",1,1,"1"
"follow up 00143996",1,1,"1"
"follow up 00145019",1,1,"1"
"follow up 00161597",1,1,"1"
"follow up 00169057",1,1,"1"
"follow up case #00139520",1,1,"1"
"follow up case - 00139453",1,1,"1"
"follow up case 00137808",1,1,"1"
"follow up case 00141072 | call is needed with the customer | please advise on availability",1,1,"1"
"follow up case 00142110",1,1,"1"
"follow up case [00147283]",1,1,"1"
"follow up case [00157500]",1,1,"1"
"follow up case case number # 00140608  - job failing with the are due to no space left on the device.",1,1,"1"
"follow up case for 00123483 | additional question from the customer",1,1,"1"
"follow up case for 00185410 | 2206160060001434",1,1,"1"
"follow up case of 00185993",1,1,"1"
"follow up case on 00144834 | arr sr# 2205050030001519",1,1,"1"
"follow up case on sf #00139547. need a call back to discuss | arr sr# 2202150030002462",1,1,"1"
"follow up case | 00143287",1,1,"1"
"follow up case: 00140430",1,1,"1"
"follow up case: 00143248",1,1,"1"
"follow up for (00181061) repl issue while running multiple streaming job",1,1,"1"
"follow up for 00127479 : odbc error unable to continue fetch after reconnect. retry limit exceeded",1,1,"1"
"follow up for 00148100 arr serva",1,1,"1"
"follow up for 00182232",1,1,"1"
"follow up of  (00141972)significant performance degradation when upgrating from 9.1 ml lts to 10.4 ml lts. 'process test performance",1,1,"1"
"follow up of (00143088)",1,1,"1"
"follow up of (00145024)",1,1,"1"
"follow up of (00146728)databricks - cluster upgrade with gdal",1,1,"1"
"follow up of (00147562)  error importing azure devops repositories to databricks workspace",1,1,"1"
"follow up of (00180660)",1,1,"1"
"follow up of (00181445) event hub to databricks - 20min connectivity unexplainable throughput drop - rca assistance requested",1,1,"1"
"follow up of (00185509) hive insert overwrite query error",1,1,"1"
"follow up of (cluster temporarily unavailabl)",1,1,"1"
"follow up of - 00143612",1,1,"1"
"follow up of - 00146348",1,1,"1"
"follow up of 00122446. (streaming delta table with changedatafeed enabled breaks when zero rows are merged)",1,1,"1"
"follow up of 00138365",1,1,"1"
"follow up of 00139396",1,1,"1"
"follow up of 00141209.",1,1,"1"
"follow up of 00141502",1,1,"1"
"follow up of 00142853",1,1,"1"
"follow up of 00144241.",1,1,"1"
"follow up of 00146306",1,1,"1"
"follow up of 00161717",1,1,"1"
"follow up of 00183584",1,1,"1"
"follow up of sf ticket 00139230",1,1,"1"
"follow up of sf ticket 00143272",1,1,"1"
"follow up of the sf ticket 00138870",1,1,"1"
"follow up of ticket (00138914)",1,1,"1"
"follow up of(00140747) cluster configuration changed with new hardware types.",1,1,"1"
"follow up of(00157421) databrick notebook fail intermittently.",1,1,"1"
"follow up of- 00131007",1,1,"1"
"follow up of- 00140935",1,1,"1"
"follow up of: 00157415. error when connecting powerbi with databricks",1,1,"1"
"follow up on 00143511 || databricks job stalled || 2204250040008063",1,1,"1"
"follow up on case 00143345 | arr sr# 2203040030000703",1,1,"1"
"follow up on case 00145094 | arr sr# 2205060030001100",1,1,"1"
"follow up sf 00139211",1,1,"1"
"follow up sf 00143076",1,1,"1"
"follow up ticket 00141210 and we'd like to confirm the flow of creating secret scope",1,1,"1"
"follow up ticket for 00137952 | 2203140030000472",1,1,"1"
"follow up ticket for 00142310",1,1,"1"
"follow up ticket for 00143692 | msft case 2204270030000780",1,1,"1"
"follow up ticket on the case 00158007",1,1,"1"
"follow up to 00171909, classnotfoundexception",1,1,"1"
"follow up | arr | jnj | 2940060147565980 | the job failed only once due to sockettimeout triggered by adf",1,1,"1"
"follow up || 00180510 || character missing from the string value",1,1,"1"
"follow up: databricks pvc control plane log4j v1 vuln",1,1,"1"
"follow-up  of sf ticket 00147446",1,1,"1"
"follow-up case for - 00144864",1,1,"1"
"follow-up case of 00124226: mllib persistence fails on rt 10.1 ml",1,1,"1"
"follow-up case..00140794",1,1,"1"
"follow-up for 00137221",1,1,"1"
"follow-up for 00138037",1,1,"1"
"follow-up for 00138795",1,1,"1"
"follow-up for 00142710",1,1,"1"
"follow-up for 00142942 , 00145606",1,1,"1"
"follow-up for 00144952",1,1,"1"
"follow-up for sf # 00143521",1,1,"1"
"follow-up for sf # 00143824",1,1,"1"
"follow-up for sf 00146664",1,1,"1"
"follow-up html injection finding (case number # 00148920)",1,1,"1"
"follow-up of   00143621 and 00180696",1,1,"1"
"follow-up of  sf - 00142662",1,1,"1"
"follow-up of 00143245",1,1,"1"
"follow-up of 00180233",1,1,"1"
"follow-up of sf # 00140194",1,1,"1"
"follow-up of sf : 00143621",1,1,"1"
"follow-up of sf : 00157695",1,1,"1"
"follow-up of sf ticket  00144370",1,1,"1"
"follow-up of sf: 00158271",1,1,"1"
"follow-up on 00135798: job fails when worker nodes on specific sku",1,1,"1"
"follow-up on 00136804: spikes in databricks use during downtimes",1,1,"1"
"follow-up ticket on previous 00140987",1,1,"1"
"follow-up to 00146362 - concerns regarding cost increase due to increased dbfs usage",1,1,"1"
"follow-up to this case 00142921",1,1,"1"
"followup 00143872 (credential passthrough is not working with spark submit jobs",1,1,"1"
"followup case for 00184755",1,1,"1"
"followup of sf ticket 00140328",1,1,"1"
"followup of sf ticket 00141611",1,1,"1"
"followup of sf ticket 00145109",1,1,"1"
"followup of sf ticket 00158517",1,1,"1"
"followup of sf ticket 00161349",1,1,"1"
"followup of sf ticket 00180195",1,1,"1"
"followup of sf ticket 00181478",1,1,"1"
"followup of the sf ticket 00138822",1,1,"1"
"followup on the sf ticket 00141602",1,1,"1"
"followup on ticket #00157745",1,1,"1"
"followup | arr | 2203020060001019 | nomodulefound error",1,1,"1"
"followup | arr | 2204140030000156 | the job in adf pipeline needed to cancel due to taking longer than expected.",1,1,"1"
"for multiple clusters we are recieving the following error - 'unexpected failure while waiting for the cluster to be ready'",1,1,"1"
"forbidden request aws abucket",1,1,"1"
"force cluster to use static ip on aws",1,1,"1"
"force job fail",1,1,"1"
"format",1,1,"1"
"found",1,1,"1"
"found 36 partitions",1,1,"1"
"free coupons",1,1,"1"
"free customer/partner introduction learning",1,1,"1"
"free training for ups",1,1,"1"
"french",1,1,"1"
"french date format",1,1,"1"
"frequent failures on some maven package installation failures on our clusters",1,1,"1"
"from_json: space in column name in schema string literal",1,1,"1"
"ganglia metrics per day basis",1,1,"1"
"ganglia not visible for jobs with graviton instances",1,1,"1"
"ganglia stops working",1,1,"1"
"ganglia ui",1,1,"1"
"ganglia ui issue",1,1,"1"
"ganglia ui on customized clusters",1,1,"1"
"gar for 8241605343500592",1,1,"1"
"gar for https://adb-3018353714729944.4.azuredatabricks.net/",1,1,"1"
"gar for sr 2205200050001297",1,1,"1"
"gar for ws 1958239611621368",1,1,"1"
"gar request for 2205200050001297",1,1,"1"
"gateway time-out error intermittently",1,1,"1"
"gc driver - oom error, same code with same data volume worked fine previously",1,1,"1"
"gc issues",1,1,"1"
"gcp cancel account subscription",1,1,"1"
"gcp credit",1,1,"1"
"gcp dont see sql",1,1,"1"
"gcp error message: compute quota exceeded for in region us-east1: quota: ssd_total_gb, used 0.0 and requested 1000.0 out of 500.0",1,1,"1"
"gcp error message: compute quota exceeded for influential-sky-306802 in region us-east1: quota: ssd_total_gb, used 0.0 and requested 1000.0 out of 500.0",1,1,"1"
"gdm prod pcmdty value change issue",1,1,"1"
"ge",1,1,"1"
"ge fdl: production dbr jobs failing",1,1,"1"
"genie  - 2206170010003172",1,1,"1"
"genie  - databricks schedule job doesn't retry - 2206220040008314",1,1,"1"
"genie  2204220010001158",1,1,"1"
"genie  access request",1,1,"1"
"genie - 2204050030003013",1,1,"1"
"genie - 2204060030003074",1,1,"1"
"genie - 2204130040007875",1,1,"1"
"genie - 2204190040007561",1,1,"1"
"genie - 2204220010001492",1,1,"1"
"genie - 2204260030001766",1,1,"1"
"genie - 2204260040007229 - init scrip failure",1,1,"1"
"genie - 2204280050000709",1,1,"1"
"genie - 2205020040002742",1,1,"1"
"genie - 2205020040005904",1,1,"1"
"genie - 2205030040003055",1,1,"1"
"genie - 2205030040003686",1,1,"1"
"genie - 2205070050000066",1,1,"1"
"genie - 2205090030001856",1,1,"1"
"genie - 2205100010001009",1,1,"1"
"genie - 2205100010003212",1,1,"1"
"genie - 2205100040004746",1,1,"1"
"genie - 2205130050000790",1,1,"1"
"genie - 2205160010003408",1,1,"1"
"genie - 2205160040004484",1,1,"1"
"genie - 2205240040005232",1,1,"1"
"genie - 2205250040007523",1,1,"1"
"genie - 2205300040004457",1,1,"1"
"genie - 2206020050002114",1,1,"1"
"genie - 2206070040006197",1,1,"1"
"genie - 2206140040005681",1,1,"1"
"genie - 2206160040004965 - citrix",1,1,"1"
"genie - 2206240040006635",1,1,"1"
"genie - adls gen2 resource is unavailable from june 4th- 2206060040004687",1,1,"1"
"genie -2206220040007394",1,1,"1"
"genie 2112070040000520 for the 2^oo time",1,1,"1"
"genie 2201280040004720",1,1,"1"
"genie 2202250030001786",1,1,"1"
"genie 2203240050000815",1,1,"1"
"genie 2203300030002890",1,1,"1"
"genie 2203300040007683",1,1,"1"
"genie 2204040030003249",1,1,"1"
"genie 2204050040008227",1,1,"1"
"genie 2204140030000156",1,1,"1"
"genie 2204160040000244",1,1,"1"
"genie 2204180030000764",1,1,"1"
"genie 2204190040004473",1,1,"1"
"genie 2204190040005554",1,1,"1"
"genie 2204190040005619",1,1,"1"
"genie 2204190040007561",1,1,"1"
"genie 2204200030000408",1,1,"1"
"genie 2204210040009652",1,1,"1"
"genie 2204210040009652 | 2",1,1,"1"
"genie 2204260030001644",1,1,"1"
"genie 2204280040000761",1,1,"1"
"genie 2205020040002742",1,1,"1"
"genie 2205030040002817",1,1,"1"
"genie 2205050040007788",1,1,"1"
"genie 2205060030001077",1,1,"1"
"genie 2205110040000799",1,1,"1"
"genie 2205110040011175",1,1,"1"
"genie 2205110050002089",1,1,"1"
"genie 2205120050000556",1,1,"1"
"genie 2205120050001524",1,1,"1"
"genie 2205130010002901",1,1,"1"
"genie 2205130050000121",1,1,"1"
"genie 2205130050000207",1,1,"1"
"genie 2205160030001741",1,1,"1"
"genie 2205160030001890",1,1,"1"
"genie 2205170040004514",1,1,"1"
"genie 2205170040006808",1,1,"1"
"genie 2205180030001625",1,1,"1"
"genie 2205190040007992",1,1,"1"
"genie 2205190050000485",1,1,"1"
"genie 2205230030001720",1,1,"1"
"genie 2205230040003508",1,1,"1"
"genie 2205240040005232",1,1,"1"
"genie 2205240040007193",1,1,"1"
"genie 2205250040007523",1,1,"1"
"genie 2205250060001505",1,1,"1"
"genie 2205260030001963",1,1,"1"
"genie 2205270030001115",1,1,"1"
"genie 2206010060001610",1,1,"1"
"genie 2206020050001074",1,1,"1"
"genie 2206020060001738",1,1,"1"
"genie 2206030030000673",1,1,"1"
"genie 2206030050001433",1,1,"1"
"genie 2206060040005065",1,1,"1"
"genie 2206060050001782",1,1,"1"
"genie 2206070030001626",1,1,"1"
"genie 2206070040006772",1,1,"1"
"genie 2206080030001381",1,1,"1"
"genie 2206090030001603",1,1,"1"
"genie 2206090030002273",1,1,"1"
"genie 2206100030001320",1,1,"1"
"genie 2206130050000159",1,1,"1"
"genie 2206130050001484",1,1,"1"
"genie 2206140050000745",1,1,"1"
"genie 2206170010004570",1,1,"1"
"genie 2206170030001661",1,1,"1"
"genie 2206200030001533",1,1,"1"
"genie 2206200030001533 ws 5986108424438021",1,1,"1"
"genie 2206200030001814",1,1,"1"
"genie 2206200050000186",1,1,"1"
"genie 2206220030001652",1,1,"1"
"genie 2206230030000970",1,1,"1"
"genie 2206270030001668",1,1,"1"
"genie 2206280030001728",1,1,"1"
"genie 2206300050001234 hsbc",1,1,"1"
"genie 2650234915343663",1,1,"1"
"genie 2940060147565980",1,1,"1"
"genie 808838112345377",1,1,"1"
"genie : 2204050040009446 -  databricks performance issues in preprod adls account",1,1,"1"
"genie : 2204110040008513-   unkown cluster error causing cluster shutdown (node_lost)",1,1,"1"
"genie : 2205030040006687 -  issue with running databricks notebook",1,1,"1"
"genie : 2205180040008310 -  invalid fs.azure.account.key error - while trying to save data to adls container from databricks",1,1,"1"
"genie : 2206060010002468 -  cluster cannot start",1,1,"1"
"genie : 2206080030002199 -  the cluster is not able to autoscale as per requirment",1,1,"1"
"genie : 2206150040008057 - i am writing a piece of code which can enable direct access via sas token rather than using adls key. this is for automated job clusters",1,1,"1"
"genie : 2206160040007720 - databricks - driver is up but is not responsive, likely due to gc",1,1,"1"
"genie : databricks production job has been failed multiple times- 2204060040006975",1,1,"1"
"genie : intermittent timeout on spark ui and slowness on highconcurrency cluster- 2204120040026531",1,1,"1"
"genie acces",1,1,"1"
"genie acces request",1,1,"1"
"genie access  - 5539637243235386",1,1,"1"
"genie access - 2203140030002364",1,1,"1"
"genie access - 2204120050001120",1,1,"1"
"genie access - 2206140050002240",1,1,"1"
"genie access - sr  2205130030001214",1,1,"1"
"genie access - sr 1420256201526416",1,1,"1"
"genie access - sr 2203310030003651",1,1,"1"
"genie access - sr 2204060030000215",1,1,"1"
"genie access - sr 2204070010002917",1,1,"1"
"genie access - sr 2204100040000077",1,1,"1"
"genie access - sr 2204100050000057",1,1,"1"
"genie access - sr 2204110030000414",1,1,"1"
"genie access - sr 2204120030002012",1,1,"1"
"genie access - sr 2204130040012512",1,1,"1"
"genie access - sr 2204150060000342",1,1,"1"
"genie access - sr 2204180030001121",1,1,"1"
"genie access - sr 2204240060000475",1,1,"1"
"genie access - sr 2205040030001061",1,1,"1"
"genie access - sr 2205050010002241",1,1,"1"
"genie access - sr 2205130030001214",1,1,"1"
"genie access - sr 2205190030001704",1,1,"1"
"genie access - sr 2205230030001094",1,1,"1"
"genie access - sr 2206060030001699",1,1,"1"
"genie access - sr 2206070030000078",1,1,"1"
"genie access - sr 2206070030001700",1,1,"1"
"genie access - sr 2206070030001746",1,1,"1"
"genie access - sr 2206160030000065",1,1,"1"
"genie access - sr 2206200050001869",1,1,"1"
"genie access - sr 2206220010003258",1,1,"1"
"genie access - sr 2206220040008856",1,1,"1"
"genie access - sr 2206270030001495",1,1,"1"
"genie access [2206070050001240]",1,1,"1"
"genie access [2206160050000947]",1,1,"1"
"genie access [2206160050001326]",1,1,"1"
"genie access [2206200050001262]",1,1,"1"
"genie access [2206200050001383]",1,1,"1"
"genie access [2206220050000368]",1,1,"1"
"genie access [2206220050001608]",1,1,"1"
"genie access [2206230050000710]",1,1,"1"
"genie access [2206240050000586]",1,1,"1"
"genie access [2206240050001664]",1,1,"1"
"genie access [2206270050001412]",1,1,"1"
"genie access [2206290050000661]",1,1,"1"
"genie access for 2204270060000414",1,1,"1"
"genie access for customers workspace",1,1,"1"
"genie access request  - 2203170050002403",1,1,"1"
"genie access request #2206070060000496",1,1,"1"
"genie access request - 2203140030002364",1,1,"1"
"genie access request || (ws-1123585223302814)",1,1,"1"
"genie access sr: 2204270060000414",1,1,"1"
"genie access | adf pipeline failing due to error - remote rpc client disassociated. likely due to containers exceeding thresholds, or network issues",1,1,"1"
"genie access | adf to adb call failed and eror shows library not installed",1,1,"1"
"genie access | arr sr # 2204060030001258",1,1,"1"
"genie access | arr sr#  2203280040004004",1,1,"1"
"genie access | arr sr#  2205060030001100",1,1,"1"
"genie access | arr sr#  2205170030001300",1,1,"1"
"genie access | arr sr# 2203110030000770",1,1,"1"
"genie access | arr sr# 2203110030000770001",1,1,"1"
"genie access | arr sr# 2203280040004004",1,1,"1"
"genie access | arr sr# 2203290050000172",1,1,"1"
"genie access | arr sr# 2204040030002134",1,1,"1"
"genie access | arr sr# 2204070050000512",1,1,"1"
"genie access | arr sr# 2204080010000649",1,1,"1"
"genie access | arr sr# 2204120030000623",1,1,"1"
"genie access | arr sr# 2204190030001144",1,1,"1"
"genie access | arr sr# 2204270030002067",1,1,"1"
"genie access | arr sr# 2205050030001519",1,1,"1"
"genie access | arr sr# 2205110030000395",1,1,"1"
"genie access | arr sr# 2205110030001742",1,1,"1"
"genie access | arr sr# 2205130030001130",1,1,"1"
"genie access | arr sr# 2205170030001630",1,1,"1"
"genie access | arr sr# 2205260040002506",1,1,"1"
"genie access | arr sr# 2206020060001871",1,1,"1"
"genie access | arr sr# 2206030030000143",1,1,"1"
"genie access | arr sr# 2206110030000120",1,1,"1"
"genie access | arr sr# 2206160040003753",1,1,"1"
"genie access | databricks spark ui show completed jobs as active jobs",1,1,"1"
"genie access | delta table is getting deleted after alter statment",1,1,"1"
"genie access | duplication during delta lake merge",1,1,"1"
"genie access | impact on vacuum",1,1,"1"
"genie access | init script failure",1,1,"1"
"genie access | job cluster error: spark_image_download_failure",1,1,"1"
"genie access | job running for long",1,1,"1"
"genie access | jobs failing",1,1,"1"
"genie access | kafka api",1,1,"1"
"genie access | not able to connect to cosmos db from data bricks",1,1,"1"
"genie access | we are not able to query _dh_ja_pos_2.pos_day_fct in the ama kpi databricks",1,1,"1"
"genie access | we are observing access issue to adls from databricks",1,1,"1"
"genie access | when trying to copy data from adls gen2 to sql dwh(azure synapse) we get the error with max char length issue",1,1,"1"
"genie access| arr sr# 2204070050000512",1,1,"1"
"genie acess",1,1,"1"
"genie authentication",1,1,"1"
"genie case",1,1,"1"
"genie certificate",1,1,"1"
"genie cluster",1,1,"1"
"genie cluster startup",1,1,"1"
"genie delta",1,1,"1"
"genie fair scheduling pool 2",1,1,"1"
"genie for #2203100050002614",1,1,"1"
"genie for #2203150050001762",1,1,"1"
"genie for #2203210030001488.0",1,1,"1"
"genie for #2203280050002083.0",1,1,"1"
"genie for #2204050040010223.0",1,1,"1"
"genie for #2204080050000427",1,1,"1"
"genie for #2204130050001006",1,1,"1"
"genie for #2204200060002689",1,1,"1"
"genie for #2204210050003652",1,1,"1"
"genie for #2204220050001720",1,1,"1"
"genie for #2204250040003356",1,1,"1"
"genie for #2204270050000851",1,1,"1"
"genie for #2205040030001111",1,1,"1"
"genie for #2205040050001376",1,1,"1"
"genie for #2205050050002191",1,1,"1"
"genie for #2205110050001817",1,1,"1"
"genie for #2205120050001043",1,1,"1"
"genie for #2205170040008648",1,1,"1"
"genie for #2205170050001191",1,1,"1"
"genie for #2205180050001027",1,1,"1"
"genie for #2205180050001491",1,1,"1"
"genie for #2205190050000006",1,1,"1"
"genie for #2205200050000126",1,1,"1"
"genie for #2205210050000185",1,1,"1"
"genie for #2205250050000600",1,1,"1"
"genie for #2205310030001555",1,1,"1"
"genie for #2205310050000310",1,1,"1"
"genie for #2206010050000",1,1,"1"
"genie for #2206010050000301",1,1,"1"
"genie for #2206010050001563",1,1,"1"
"genie for #2206070040009328",1,1,"1"
"genie for #2206070050002555",1,1,"1"
"genie for #2206090030001956",1,1,"1"
"genie for #2206090050002180",1,1,"1"
"genie for #2206140050001621",1,1,"1"
"genie for #2206160050001080",1,1,"1"
"genie for #2206210050001768",1,1,"1"
"genie for #2206230050002286",1,1,"1"
"genie for #2206240050000619",1,1,"1"
"genie for #2206240050001473",1,1,"1"
"genie for #case2205300030001172",1,1,"1"
"genie for #edqwe",1,1,"1"
"genie for 2202080060002931",1,1,"1"
"genie for 2203030050000273",1,1,"1"
"genie for 2204060030001864",1,1,"1"
"genie for 2204120030001216",1,1,"1"
"genie for 2204200050002006",1,1,"1"
"genie for 2204270050000937",1,1,"1"
"genie for 2204280050002289",1,1,"1"
"genie for 2204290050001087",1,1,"1"
"genie for 2205110050000419",1,1,"1"
"genie for 2205120050002131",1,1,"1"
"genie for 2205180010001040",1,1,"1"
"genie for 2205200040003007",1,1,"1"
"genie for 2205230050000756",1,1,"1"
"genie for 2205230050001967002",1,1,"1"
"genie for 2205260030001267",1,1,"1"
"genie for 2206030030000842",1,1,"1"
"genie for 2206060030001699",1,1,"1"
"genie for 2206060030001835",1,1,"1"
"genie for 2206150030002085",1,1,"1"
"genie for 2206220040005881",1,1,"1"
"genie for 2206220050000790",1,1,"1"
"genie for 2206230030000787",1,1,"1"
"genie for 2206300040004630",1,1,"1"
"genie for 2770540884322692",1,1,"1"
"genie for 2873683765356306",1,1,"1"
"genie for 341265499768916",1,1,"1"
"genie for 512531045191703",1,1,"1"
"genie for 565094429650515",1,1,"1"
"genie for 6496992717833072",1,1,"1"
"genie for job cluster error using credential passthrough",1,1,"1"
"genie for library installation issue",1,1,"1"
"genie for notebook error",1,1,"1"
"genie fro 2873683765356306",1,1,"1"
"genie google",1,1,"1"
"genie init",1,1,"1"
"genie permission",1,1,"1"
"genie postgres",1,1,"1"
"genie python",1,1,"1"
"genie req bit filter errror",1,1,"1"
"genie request job slow runtime",1,1,"1"
"genie spark command",1,1,"1"
"genie tk",1,1,"1"
"genie- 2204080030001854",1,1,"1"
"genie- cluster upsize completed but 10 containers could not be added.reason:azure quota exceeded exception",1,1,"1"
"genie- multiple clusters/jobs are not starting or installing libraries",1,1,"1"
"genie- not able to create and start azure databricks cluster",1,1,"1"
"genie- powerbi failing to connect to databricks table and fetch data",1,1,"1"
"genie- processos de cluster interativo travaram durante a execução (processo zumbi)",1,1,"1"
"genie- smtp server configuration to our customized app code",1,1,"1"
"genie- workspace_does_not_exist_error when launching databricks workspace",1,1,"1"
"genie-2203300030000354",1,1,"1"
"genie-2203300030003094",1,1,"1"
"genie-2204010030000411",1,1,"1"
"genie-2204080030000101",1,1,"1"
"genie-2204080030000176",1,1,"1"
"genie-2204130030000625",1,1,"1"
"genie-2204140010001013",1,1,"1"
"genie-2204180030000680",1,1,"1"
"genie-2204190040003814",1,1,"1"
"genie-2204190040008032",1,1,"1"
"genie-2204200060002643",1,1,"1"
"genie-2204210060000096001",1,1,"1"
"genie-2204220050000056",1,1,"1"
"genie-2204260030000800",1,1,"1"
"genie-2204260050001841",1,1,"1"
"genie-2205050030000311",1,1,"1"
"genie-2205060030000515",1,1,"1"
"genie-2205130050001870",1,1,"1"
"genie-2205130060000046",1,1,"1"
"genie-2205160030001560",1,1,"1"
"genie-2205170030000037",1,1,"1"
"genie-2205170030000259",1,1,"1"
"genie-2205190060000328",1,1,"1"
"genie-2205240030000330",1,1,"1"
"genie-2205250030000144",1,1,"1"
"genie-2205250030002103",1,1,"1"
"genie-2205250040009377",1,1,"1"
"genie-2205260040000787",1,1,"1"
"genie-2205260060000312",1,1,"1"
"genie-2205290030000057",1,1,"1"
"genie-2205310030001555",1,1,"1"
"genie-2206010030000423",1,1,"1"
"genie-2206030030000534",1,1,"1"
"genie-2206080030001198",1,1,"1"
"genie-2206080040007574",1,1,"1"
"genie-2206090040005436",1,1,"1"
"genie-2206090060000221",1,1,"1"
"genie-2206130040006805",1,1,"1"
"genie-2206140060000804",1,1,"1"
"genie-2206160010002801",1,1,"1"
"genie-2206170010003172",1,1,"1"
"genie-2206170010004397",1,1,"1"
"genie-2206170040000268",1,1,"1"
"genie-2206170040003494",1,1,"1"
"genie-2206200030001369",1,1,"1"
"genie-2206200040000164",1,1,"1"
"genie-2206210040006589",1,1,"1"
"genie-2206210060000335",1,1,"1"
"genie-2206230040006088",1,1,"1"
"genie-2206270040006028",1,1,"1"
"genie-2206270060000664",1,1,"1"
"genie-2206280040006423",1,1,"1"
"genie-2206290030000108",1,1,"1"
"genie-2206290060000801",1,1,"1"
"genie-activating ip filtering blocks the launch of jobs and apis",1,1,"1"
"genie-an error occurred while calling z:org.apache.spark.api.python.pythonrdd.collectandserve",1,1,"1"
"genie-azure quota exceeded exception",1,1,"1"
"genie-azure synapse analytics failed to execute the jdbc query produced by the connector",1,1,"1"
"genie-cluster no longer turns on.",1,1,"1"
"genie-cluster terminated.reason:cloud provider launch failure. azure error code: skunotavailable",1,1,"1"
"genie-com.databricks.spark.sqldw",1,1,"1"
"genie-com.microsoft.sqlserver.jdbc.sqlserverexception ... ssl error: 'unexpected rethrowing'",1,1,"1"
"genie-connectivity between azure databricks and storage account under private endpoint",1,1,"1"
"genie-customer is seeing duplicate records after a running data bricks activity",1,1,"1"
"genie-databricks having trouble connection to blob",1,1,"1"
"genie-for-benz",1,1,"1"
"genie-getting error after turning on storage account firewall in databricks",1,1,"1"
"genie-internal server error on adf to databricks",1,1,"1"
"genie-job failed after 5 hours with timeout",1,1,"1"
"genie-lot of latency in apllying the changes",1,1,"1"
"genie-myownworkspace",1,1,"1"
"genie-not able to launch pipeline cluster",1,1,"1"
"genie-not using a valid access token. update the access token for the linked service",1,1,"1"
"genie-numero de clusters no endpoint do databricks sql",1,1,"1"
"genie-parsing rdd with read_csv behavior changed between 7.3lts and 10.4lts",1,1,"1"
"genie-pipeline failed on running notebook",1,1,"1"
"genie-problem in all process in databricks",1,1,"1"
"genie-repo error",1,1,"1"
"genie-sevc-2205240040008340-unable to select the log file from the web services",1,1,"1"
"genie-this amount may not have enough cpu cores - cannot create virtual machines for standard_e80ids_v4.",1,1,"1"
"genie-unable to access blobs through a storage account firewall",1,1,"1"
"genie-unable to connect/authorize to adls",1,1,"1"
"genie-unable to run begin transaction via synapse spark connector",1,1,"1"
"genie-virtual machines failing to launch",1,1,"1"
"genie-workspace_does_not_exist_error when launching databricks workspace",1,1,"1"
"genie:  2205020040004681",1,1,"1"
"genie:  2205170040008994",1,1,"1"
"genie:  2205300010001729 || soncor|| cluster failure and pipeline failure",1,1,"1"
"genie:  2206010040008599 || pepsico || sev a",1,1,"1"
"genie: 2203300040007683",1,1,"1"
"genie: 2205190010002610 || american airlines || databricks notebook is failing after instal",1,1,"1"
"genie: 2205230040008078 || workspace",1,1,"1"
"genie: 2205270050001268 || abn amro || cluster performing very slowly then as usual",1,1,"1"
"genie: 2206010040008249 ||samsclub || databricks reponse is very slow/delayed",1,1,"1"
"genie: 2206080040006562",1,1,"1"
"genie: aa | 2203220040007166 | prod workspace",1,1,"1"
"genie: aa || 2204190040006761 || invalid parameter valur - cluster id",1,1,"1"
"genie: aa || 2205200010002365 || job failed because of cluster launch failure",1,1,"1"
"genie: aa || 2650234915343663 || cluster terminated when running job",1,1,"1"
"genie: adobe | 2204110040006238 | cannot find fairscheduler file in dbr10.4",1,1,"1"
"genie: adobe || 2206150010003321 || databricks job api 2.1 is  not giving expected results",1,1,"1"
"genie: adobe||  2206150010003321 || restapi",1,1,"1"
"genie: aholddelhaize.com || 2206160060001800 || quota issue",1,1,"1"
"genie: air-canada || collab",1,1,"1"
"genie: aircanada || 2205040030000256 || adf failures with aircanada",1,1,"1"
"genie: aircanada || 2205260040008301 || unable to write in dev storage",1,1,"1"
"genie: anheuser-busch.com || 2205260030002078 || dbr 10.4 - cannot create views",1,1,"1"
"genie: arr: need to define the cause,  high cpu & ram on 1 x node : sr 2206030030000673",1,1,"1"
"genie: ashleyfurniture || 2204270040007342 || repo issue",1,1,"1"
"genie: at&t |",1,1,"1"
"genie: at&t ||  2204150040004192 ||  notebook failing in job cluster, working fine in hc cluster",1,1,"1"
"genie: at&t || 2206220040007241 || streaming job running into out of memory",1,1,"1"
"genie: blackbaud || 2206100040006706 || vm workers having http issues",1,1,"1"
"genie: boeing || 2205040040007574 || control plane exception, getrunbook faliure",1,1,"1"
"genie: boeing || 2205270040005511 || dev setup for repro",1,1,"1"
"genie: boeing|| 2205270040005511 || json file failure",1,1,"1"
"genie: capitalgeoup || 2204140040008817 || databricks job getting hung",1,1,"1"
"genie: cluster not starting",1,1,"1"
"genie: codelco || 2205190040007992 || postgres sql ssl issue",1,1,"1"
"genie: coles|| 2205090030000973 || sev a",1,1,"1"
"genie: credit suisse || 2205120040006840 || connection timedout with storage account.",1,1,"1"
"genie: cvs || 2205240040006373 || cluster metastore setup",1,1,"1"
"genie: geico || 2204250040005555 || intermittent file not found errors when accessing adls through dbfs mounts",1,1,"1"
"genie: geico || 2204260040005094 || mount logging",1,1,"1"
"genie: h&r block || 2203110040006344 || testworkspace",1,1,"1"
"genie: h&r clock || 2203110040006344 || cosmosdb failed to authenticate the request.",1,1,"1"
"genie: hershey || 2206020040006837 || initi script",1,1,"1"
"genie: inditex || 2206150020002427 || cannot fetch secrets referred in the spark configuration",1,1,"1"
"genie: j&j || 2186760551155912 || job failure",1,1,"1"
"genie: j&j || 2206090040006663 || jobs taking unusal loger time and timeing out",1,1,"1"
"genie: limeade|  2204280010002953 || job fails after running for an hour",1,1,"1"
"genie: malformed records job failure",1,1,"1"
"genie: mars || 2205170040006218 || autoscaling issue",1,1,"1"
"genie: mgm || 2203160010002405 || servers performance is reduced to 30% instead of 100% at production",1,1,"1"
"genie: molinahealthcare ||  2205230040008078",1,1,"1"
"genie: morgan stanley || 2205020040005214",1,1,"1"
"genie: nfcu || 2203010040008530 || aad failure",1,1,"1"
"genie: p&g || 2206170030001895 || spark exception received from driver. driver down",1,1,"1"
"genie: pickel error",1,1,"1"
"genie: safeway ||  2205310010002798 || sev a - cluster start delay",1,1,"1"
"genie: safeway || 2204220010001571 || log4j enable debug",1,1,"1"
"genie: safeway || 2204250040008262 || connecting ml repository",1,1,"1"
"genie: safeway || 2204250040008262 || databricks job failed during connecting ml registry",1,1,"1"
"genie: safeway || 2205120010003782 || sql connectivity issue",1,1,"1"
"genie: safeway|| 2206210040007277",1,1,"1"
"genie: sev a - tata",1,1,"1"
"genie: shell db limit 800",1,1,"1"
"genie: shell storage token",1,1,"1"
"genie: slow cell run",1,1,"1"
"genie: t-mobile || 2204010040006385 || rca for cluster terminated due to self bootstrap failure",1,1,"1"
"genie: t-mobile||  2204150010002234 || gitlab repo not found",1,1,"1"
"genie: ubs || 2204080040009045 || scope issue",1,1,"1"
"genie: ubs || 2205050040001186 || connection time out while connecting with spark",1,1,"1"
"genie: uhg || 204200040003831 || dbutils issue",1,1,"1"
"genie: uhg || 2204280040007886 || spark_error(success)",1,1,"1"
"genie: why does resultset take 14+ seconds to filter 1.4m records: sr2206010040005495",1,1,"1"
"genie:2205270010001059",1,1,"1"
"genie:aa || 2204060010002628 || march 13th airflow trigger failure",1,1,"1"
"genie:arr: databricks notebook running failure",1,1,"1"
"genie:arr: hit error (file not found) using high concurrency cluster, but works fine with pass through cluster:sr 2203280030003944",1,1,"1"
"genie:arr: identity column not unique :sr 2205250040005673",1,1,"1"
"genie:arr: issue while connecting to azure sql using pyodbc driver can't open lib 'odbc driver 17 for sql server' : file not found (0):sr2205060040003815",1,1,"1"
"genie:arr: jobs failure: 2205160010002619",1,1,"1"
"genie:arr: library installation failure:sr2206060010001648",1,1,"1"
"genie:arr: query delta table returns no result : sr206080010000484",1,1,"1"
"genie:arr: sr2206020060002128",1,1,"1"
"genie:arr:2202250040005364 | databricks parallel execution performance issues - dts pdw pilot",1,1,"1"
"genie:arr:cluster takes long time to spin up:sr2204140040006002",1,1,"1"
"genie:arr:cluster unable to startup:sr2204130040009267",1,1,"1"
"genie:arr:collab:databricks notebook is not running as it says another notebook is in progress.: sr2204210060006686001",1,1,"1"
"genie:arr:connectivity to sql dw is slow:sr2204280040006632",1,1,"1"
"genie:arr:databricks trying to connect on-prem oracle server: sr2206240040005619",1,1,"1"
"genie:arr:job failed on dbr 10.4 lts",1,1,"1"
"genie:arr:jobs are failing:sr2204070040000867 | cummins | b",1,1,"1"
"genie:arr:mlflow model serving api error:sr2206170010005234",1,1,"1"
"genie:arr:mlflow: sr2206100040004911",1,1,"1"
"genie:arr:model serving endpoint not coming up:sr2206200030001878",1,1,"1"
"genie:arr:outage issue",1,1,"1"
"genie:arr:outage/cluster down:sr204060040007331",1,1,"1"
"genie:arr:seva - can't read xml file:sr2204110040005543",1,1,"1"
"genie:arr:seva: sr 204160040000471",1,1,"1"
"genie:arr:sevb -",1,1,"1"
"genie:arr:sevb -  job cluster is failing on databricks:sr-2204270040005755",1,1,"1"
"genie:arr:sevb - access the api inside azure databricks but failure in name resolution error:sr-2205040030001343",1,1,"1"
"genie:arr:sevb - com.databricks.notebookexecutionexception: timedout:sr-2205110040009992",1,1,"1"
"genie:arr:sevb - connection db cluster by jdbc/odbc with aad token, can we use same token for blob storage authentication:sr-2205120040004860",1,1,"1"
"genie:arr:sevb - connection timed out for adb job:sr-2204200010007619",1,1,"1"
"genie:arr:sevb - databricks performance issues:sr-2204210040012046",1,1,"1"
"genie:arr:sevb - genie:arr:sevb - 2205160040005458",1,1,"1"
"genie:arr:sevb - init script failure:sr-2204200040003542",1,1,"1"
"genie:arr:sevb - job failing to execute-2205240040005960",1,1,"1"
"genie:arr:sevb - need to get public ip of azure databricks workspace:sr-2205060010002214",1,1,"1"
"genie:arr:sevb - notebook commands not getting executed - trackingid#2205120040006755",1,1,"1"
"genie:arr:sevb - notebook crashed:sr-2204150040003912",1,1,"1"
"genie:arr:sevb - online feature store mysql access not working from databricks-2205190040007612",1,1,"1"
"genie:arr:sevb - recover missing notebooks lost in the switch in repos:sr-2204110030002151",1,1,"1"
"genie:arr:sevb - sr-2204220040005384:unable to execute databricks cli command on azure vm",1,1,"1"
"genie:arr:sevb -can't connect to cx_oracle.connect-2205130040004909",1,1,"1"
"genie:arr:sevb -ganglia ui bad request size of a request header field exceeds server limit-2205170050001933",1,1,"1"
"genie:arr:sevb -notebook commands not getting executed- 2205120040006755",1,1,"1"
"genie:arr:sevb -recover databricks workspace dha-mlp-p-ecom-prod-databric:sr-2204200040005478",1,1,"1"
"genie:arr:sevb -unable to connect to databrciks from power bi desktop client-2205170040007237",1,1,"1"
"genie:arr:sevc - special character in column name causing issue with upgraded dbr-2205180040005805",1,1,"1"
"genie:arr:sevc-notebook commands not getting executed:2205120040006755",1,1,"1"
"genie:arr:sr2205110040005641",1,1,"1"
"genie:arr:sr2205130040004909",1,1,"1"
"genie:arr:streaming job failed by encountered error 401:sr2206130040005127",1,1,"1"
"genie:arr:tensorboard doesn't work on databricks:sr2206220010002773",1,1,"1"
"genie:t-mobile : 2204180010001940 || git integration ""gitlabs""",1,1,"1"
"geniee portal is showing different rows to fill",1,1,"1"
"genie| at&t | production logs",1,1,"1"
"genie|| anheuser-busch || 2206100040007795  - cluster policy",1,1,"1"
"genie|| safeway || 2204180010001625 || commitdeniedexception: commit denied for partition 1 errors in the job logs",1,1,"1"
"gennie access|20220525167365",1,1,"1"
"geo pandas",1,1,"1"
"get secret via sql",1,1,"1"
"get started apache",1,1,"1"
"get started with apache spark",1,1,"1"
"get stuck at ""running command"" or ""cancelling command"" with db web interface",1,1,"1"
"get the size of /dbfs filesytme",1,1,"1"
"getting 504 timeout error when using api to get list of groups/users",1,1,"1"
"getting access denied when running job in r studio",1,1,"1"
"getting an error when try to add instance profile!",1,1,"1"
"getting auth error on azure data bricks while running ml models",1,1,"1"
"getting dataset refresh issue in power bi dashboard",1,1,"1"
"getting error ""operationalerror: (psycopg2.operationalerror) ssl syscall error: eof detected """,1,1,"1"
"getting error ""shuffle partition number too small"" when converting from 7.3 to 9.1 lts",1,1,"1"
"getting error 'failure starting repl. try detaching and re-attaching the notebook.'  while running any notebook",1,1,"1"
"getting error during configuration of log delivery",1,1,"1"
"getting error on sql end point",1,1,"1"
"getting error while creating partition for table.",1,1,"1"
"getting error while creating table",1,1,"1"
"getting error while installing library via api",1,1,"1"
"getting java.lang.assertionerror after upgrading dbr version to 10.0",1,1,"1"
"getting random executorlostfailure",1,1,"1"
"getting refuse to connect when launching tensorboard",1,1,"1"
"getting rpc response too large issue",1,1,"1"
"gif",1,1,"1"
"git clone",1,1,"1"
"git dialog",1,1,"1"
"git error",1,1,"1"
"git how to find the relative path of file",1,1,"1"
"git relative path",1,1,"1"
"git repo with large object can't be pulled",1,1,"1"
"git token management api",1,1,"1"
"git version control best practice",1,1,"1"
"gitlab",1,1,"1"
"gitlab enterprise  on-premise supportability internal documentation.",1,1,"1"
"give only read access to tables",1,1,"1"
"gke cluster is not up on the new shared vpc",1,1,"1"
"global_init_script_failure",1,1,"1"
"glue backtick",1,1,"1"
"glue catalog access from passthrough cluster",1,1,"1"
"gneie-sql endpoints are not working",1,1,"1"
"google chrome tabs are listing ""databricks"" where before it was the notebook name",1,1,"1"
"google cloud platform",1,1,"1"
"google cloud storage files ystem",1,1,"1"
"google kubernetes error",1,1,"1"
"got invalid response: 404",1,1,"1"
"goto cell",1,1,"1"
"goto cmd",1,1,"1"
"gpu cluster disconnects while runnign hyperopt",1,1,"1"
"gpu cluster does not start at certain times of the day",1,1,"1"
"gpu cluster terminated - gcp resource stockout",1,1,"1"
"gpu sharing between spark executors",1,1,"1"
"gpu-enabled clusters",1,1,"1"
"grant access for new user",1,1,"1"
"graph",1,1,"1"
"graph database",1,1,"1"
"graviton instances not working",1,1,"1"
"greenplum to databricks",1,1,"1"
"group by",1,1,"1"
"group permissions do not work",1,1,"1"
"grpc-netty-shaded conflict",1,1,"1"
"guardduty finding - dbx prod",1,1,"1"
"guest user aad login issue in azure portal for the workspace enabled with private link",1,1,"1"
"guidance about security vulnerability scanning",1,1,"1"
"guidance in overwatch upgrade",1,1,"1"
"guidance on databricks connection with opensearch",1,1,"1"
"guidance on how to get access logs for when new user is added or removed from dbx.",1,1,"1"
"h1 hello e/h1",1,1,"1"
"h1e hello e/h1e",1,1,"1"
"hadoop",1,1,"1"
"handle errors",1,1,"1"
"handling rows with >20mb of text",1,1,"1"
"hanging notebook. this is related to case number # 00144460",1,1,"1"
"hanging spark command in notebook",1,1,"1"
"having issue connecting to databricks",1,1,"1"
"having parquet as default table format in dbr 10?",1,1,"1"
"having problem creating or deleting views",1,1,"1"
"having some issue while setting up saml auth for e2 account console",1,1,"1"
"hdfs timeout",1,1,"1"
"health check was not dispatched to the driver within 900 seconds",1,1,"1"
"hello123",1,1,"1"
"help center - new authorized contact - unable to log in",1,1,"1"
"help in setting up customer-managed vpc",1,1,"1"
"help investigate a failing job",1,1,"1"
"help investigate a failing job-",1,1,"1"
"help me debug a job run that timed out",1,1,"1"
"help portal button on dbsql persona giving an error",1,1,"1"
"help to figure out derby permissions",1,1,"1"
"help us to install library in 10lts ml",1,1,"1"
"help with",1,1,"1"
"help()",1,1,"1"
"help.databricks.com,",1,1,"1"
"helps",1,1,"1"
"hi my team getting oom error",1,1,"1"
"hi there, when we try to run packrat::init('/databricks') in the databricks notebook, it gives us an error",1,1,"1"
"high concurrency cluster in prod did not terminate for 2 weeks",1,1,"1"
"high concurrency cluster not terminating",1,1,"1"
"high system cpu usage from may 10th-genie",1,1,"1"
"hitting limits on storage and credential configurations",1,1,"1"
"hitting rate limit of 100 nodes per minute.",1,1,"1"
"hive",1,1,"1"
"hive error",1,1,"1"
"hive insert overwrite query not working as expected",1,1,"1"
"hive metastore integration",1,1,"1"
"hive_metastore grants not working, when using unity catalog.",1,1,"1"
"horovod",1,1,"1"
"host",1,1,"1"
"host name",1,1,"1"
"host subnet and a container subnet",1,1,"1"
"host url",1,1,"1"
"hour",1,1,"1"
"how delete data",1,1,"1"
"how delete data dbfs",1,1,"1"
"how do i add a command",1,1,"1"
"how do i add users",1,1,"1"
"how do i attach a cluster",1,1,"1"
"how do i export to see the results",1,1,"1"
"how do i get permission for npd dataset",1,1,"1"
"how do i install databricks",1,1,"1"
"how do i see the data",1,1,"1"
"how do you change the owner of a job?",1,1,"1"
"how do you crate a pipeline",1,1,"1"
"how do you refresh a single table in delta live tables?",1,1,"1"
"how does spark calculate number of tasks/partitions for a query",1,1,"1"
"how restore deleted cluster",1,1,"1"
"how to access dbfs file system as part of init script",1,1,"1"
"how to access local file",1,1,"1"
"how to allocate prefered vpc cidr blocks",1,1,"1"
"how to allow users with ""can restart"" permissions on a cluster to change runtime version and install libraries",1,1,"1"
"how to assess performance of notebook included with %run magic",1,1,"1"
"how to avoid warning: skipped 44535 bytes of output in job output",1,1,"1"
"how to build docker container for dbr 7.0 and above",1,1,"1"
"how to check dbus",1,1,"1"
"how to check dbus usage",1,1,"1"
"how to check the version of databricks runtime for machine learning",1,1,"1"
"how to cite databricks",1,1,"1"
"how to close a notebook",1,1,"1"
"how to collect spark worker logs?",1,1,"1"
"how to complete the certification",1,1,"1"
"how to compute box plots charts on all rows",1,1,"1"
"how to connect to aws quicksight from databricks",1,1,"1"
"how to connect to sftp site from azure databricks",1,1,"1"
"how to create a table",1,1,"1"
"how to create new table",1,1,"1"
"how to create. a ticket",1,1,"1"
"how to define class_weight param on automl",1,1,"1"
"how to delete a notebook",1,1,"1"
"how to delete cluster from community",1,1,"1"
"how to delete databricks account",1,1,"1"
"how to delete recent view",1,1,"1"
"how to download a file that was exported to csv",1,1,"1"
"how to download invoices from portal",1,1,"1"
"how to download workflow",1,1,"1"
"how to easily add spn (app registration) to databricks",1,1,"1"
"how to ensure data disks are encrypted",1,1,"1"
"how to execute a sql in database?",1,1,"1"
"how to execute sql queries using sql endpoints",1,1,"1"
"how to export a folder as dbc archive using databricks cli or workspace api?",1,1,"1"
"how to export my code",1,1,"1"
"how to find cluster detail page",1,1,"1"
"how to find hostname",1,1,"1"
"how to find number of files preset in database",1,1,"1"
"how to find scheduled jobs",1,1,"1"
"how to find the azure databricks dbu usage",1,1,"1"
"how to find the databricks dbu usage",1,1,"1"
"how to generate a parameter for automated notebook",1,1,"1"
"how to generate the access token through service account",1,1,"1"
"how to get the report for table access control list",1,1,"1"
"how to grant access to cluster logs in the workspace",1,1,"1"
"how to grant access to default catalog to all workspace users",1,1,"1"
"how to grant job permission to runs submitted by api",1,1,"1"
"how to import a spark sql doc",1,1,"1"
"how to install older release of python",1,1,"1"
"how to invoke jobs api 2.1 from inside a notebook",1,1,"1"
"how to know the location of data file",1,1,"1"
"how to launch dependent task in case of failure of previous task as well",1,1,"1"
"how to load excel(xlsx) file into dataframe?",1,1,"1"
"how to load file from console",1,1,"1"
"how to look up tables information",1,1,"1"
"how to migrate party data",1,1,"1"
"how to mount bucket with temporary aws credentials",1,1,"1"
"how to move workspaces",1,1,"1"
"how to open .dbc file",1,1,"1"
"how to open a shell terminal on databricks",1,1,"1"
"how to open an existing workbook",1,1,"1"
"how to participate in unity catalog preview",1,1,"1"
"how to prevent unmanaged table read through s3 path",1,1,"1"
"how to provide input to a cell prompt in a databricks notebook",1,1,"1"
"how to provide input to prompt in databricks notebook cell",1,1,"1"
"how to provide sql access to a group for running queries, explore data",1,1,"1"
"how to read a syntax diagram",1,1,"1"
"how to remove logs",1,1,"1"
"how to save code",1,1,"1"
"how to save code in databricks",1,1,"1"
"how to schedule a query on aws",1,1,"1"
"how to set up a job dashboard?",1,1,"1"
"how to set up cse-kms for s3",1,1,"1"
"how to setup utf-16 on databricks cluster ?",1,1,"1"
"how to stop workspace",1,1,"1"
"how to switch from sql to r",1,1,"1"
"how to switch ilt training dates",1,1,"1"
"how to switch languages",1,1,"1"
"how to switch lit training dates",1,1,"1"
"how to sync s3 data to another workspace",1,1,"1"
"how to terminate cluster",1,1,"1"
"how to terminate the slow query of sql",1,1,"1"
"how to throw exception in a job running a shell script?",1,1,"1"
"how to update job to auto-az",1,1,"1"
"how to upload notebook",1,1,"1"
"how to use generic id for job creation and execution",1,1,"1"
"how to use r studio for databricks",1,1,"1"
"how to use s3  to create external location",1,1,"1"
"how worker node allocation is happening for specific session",1,1,"1"
"however, the access is denied because of the deny assignment with name 'system deny assignment created by azure databricks",1,1,"1"
"html",1,1,"1"
"html injection finding",1,1,"1"
"html to string",1,1,"1"
"htmlwidgets",1,1,"1"
"http",1,1,"1"
"http 400",1,1,"1"
"http error 403</h2> <p>problem accessing /api/2.0/ess token",1,1,"1"
"http issues",1,1,"1"
"http path",1,1,"1"
"http_path",1,1,"1"
"https://adb-4xxx.7.azuredatabricks.net/?o=4xxxx#job/6xxxx/run/1xxxxx",1,1,"1"
"https://sportsbet.cloud.databricks.com/ workspace not available",1,1,"1"
"https://support.databricks.com/s/",1,1,"1"
"huge lag in performance in notebook navigation",1,1,"1"
"hyperopt x.x.x.dbx on customized cluster",1,1,"1"
"i am not able to see new workflow feature",1,1,"1"
"i can't able to create cluster",1,1,"1"
"i have account eldonxu@hotmail.com. how to add my company class 6810",1,1,"1"
"i have reigitered for the cerfication",1,1,"1"
"i want to delete account",1,1,"1"
"iam profile isn’t being attached properly to cluster",1,1,"1"
"iamポリシーに紐づけるs3バケットへのアクション権限について",1,1,"1"
"icon",1,1,"1"
"id",1,1,"1"
"id delete",1,1,"1"
"ide",1,1,"1"
"ideas portal",1,1,"1"
"identification requirement",1,1,"1"
"identity column",1,1,"1"
"identity column is generating duplicates",1,1,"1"
"if a column has a comment > 512 characters long the table won't open in dbricks sql",1,1,"1"
"illegalargumentexception: path must be absolute: foo.csv",1,1,"1"
"im-arm aks (aks-aue-p-im-01) jobs are failing",1,1,"1"
"imdsv2 support",1,1,"1"
"img src=x onerror=prompt(document.domain)e",1,1,"1"
"import a xls to delt",1,1,"1"
"import a xls to delta",1,1,"1"
"import data",1,1,"1"
"import db",1,1,"1"
"import dbc file",1,1,"1"
"import excel",1,1,"1"
"import excel with r",1,1,"1"
"import external data source",1,1,"1"
"import html",1,1,"1"
"import lib",1,1,"1"
"import mlflow issue",1,1,"1"
"impossible to connect on some workspaces for a specific user",1,1,"1"
"impossible to create a job",1,1,"1"
"impossible to install xgboost in version 0.6 on databricks runtime 9.1 lts",1,1,"1"
"in spark clusters, tasks are assigned to inactive executors hence queries taking long time",1,1,"1"
"includeheaders",1,1,"1"
"incomplete contacts in help center",1,1,"1"
"inconsistent connectivity issue. rerunning the same fix the issue.",1,1,"1"
"inconsistent file loading error",1,1,"1"
"inconsistent records from different queries",1,1,"1"
"inconsistent subquery issues in dlt pipeline",1,1,"1"
"inconsistent type returned by describe and show table extended for bigint columns",1,1,"1"
"increase input rate for dlt",1,1,"1"
"increase instance limit",1,1,"1"
"increase number of databricks secret scopes",1,1,"1"
"increase number of workspace for adp databricks account",1,1,"1"
"increase pinned cluster limit to 300",1,1,"1"
"increase secret scope in internal account",1,1,"1"
"increase secret scope limit",1,1,"1"
"increase storage on driver /tmp",1,1,"1"
"increase the scope for edl",1,1,"1"
"increased aws data transfer regional costs on clusters",1,1,"1"
"incremental data ingestion with auto loader",1,1,"1"
"incremental table with daily bigquery data with delta live tables",1,1,"1"
"incurring nat gateway charges for data transfer from s3 to clickhouse",1,1,"1"
"indexoutofboundsexception",1,1,"1"
"india",1,1,"1"
"indian mobile number validate",1,1,"1"
"infer or extract columns from file path while using auto loader",1,1,"1"
"information related to databricks workspace migration",1,1,"1"
"information security - aws account 414351767826",1,1,"1"
"ingest data",1,1,"1"
"init script executed twice when starting the cluster",1,1,"1"
"init script failed",1,1,"1"
"init script failing while installing library",1,1,"1"
"init script failure caused many job failures",1,1,"1"
"init script having rstudio server installation failing",1,1,"1"
"init script not getting installed while cluster creation using api",1,1,"1"
"init script sleep",1,1,"1"
"init scripts two times",1,1,"1"
"input rate",1,1,"1"
"inquiry on azure databricks notice tracking id mtm3-bt8",1,1,"1"
"inquiry on cluster downtime for changing node type",1,1,"1"
"inquiry on databricks new function",1,1,"1"
"insert",1,1,"1"
"insert error for hive_dbks_eedc_dev",1,1,"1"
"insert into",1,1,"1"
"insert into delta table failing",1,1,"1"
"insert is taking time",1,1,"1"
"insert overwrite partition",1,1,"1"
"install cli on a shared windows machine",1,1,"1"
"install dependencies",1,1,"1"
"install gaitpy in dbr with python 3.8",1,1,"1"
"install h3 on databricks",1,1,"1"
"install h3 on databrics",1,1,"1"
"install libraries",1,1,"1"
"install libraries from a private gitlab maven repository",1,1,"1"
"install package",1,1,"1"
"install python pip",1,1,"1"
"install r package",1,1,"1"
"install requirements.txt libs using jobs 2.1 api",1,1,"1"
"installation of libraries vaex and modin",1,1,"1"
"installed libraries",1,1,"1"
"installing edge browser does not work reliable",1,1,"1"
"installing pypi packages gets stuck",1,1,"1"
"installing python packages through pip fails on hc clusters with credential passthrough",1,1,"1"
"installing r and python packages on a non-docker cluster",1,1,"1"
"instance fleet is disabled",1,1,"1"
"instance profile on cluster settings",1,1,"1"
"instance type support",1,1,"1"
"instances always running",1,1,"1"
"integration with aws glue and redshft",1,1,"1"
"interactive cluster auto scaling up issue",1,1,"1"
"interactive cluster hanging on ""running command"" (take 2)",1,1,"1"
"interactive cluster processes crash during execution (zombie process)",1,1,"1"
"intermittent",1,1,"1"
"intermittent access denied while write to amazon s3 write encrypted storage",1,1,"1"
"intermittent blob operation not supported",1,1,"1"
"intermittent cluster startup issue - spark_image_download_failure(service_fault)",1,1,"1"
"intermittent connection refused error to databricks from airflow",1,1,"1"
"intermittent connectivity issue from adf to databricks sql",1,1,"1"
"intermittent connectivity issues between qliksense & data bricks",1,1,"1"
"intermittent data load issue- escalate to databricks team",1,1,"1"
"intermittent error ""could not find adls gen2 token"" when running jobs with credentials passthrough",1,1,"1"
"intermittent errors pulling docker containers",1,1,"1"
"intermittent errors while upsizing the nodes.",1,1,"1"
"intermittent file not found error when accessing adls mounted locations",1,1,"1"
"intermittent issue with custom library",1,1,"1"
"intermittent issues when interacting with storage account",1,1,"1"
"intermittent issues while affecting multiple workspaces",1,1,"1"
"intermittent job failures caused by hive metastore instantiation",1,1,"1"
"intermittent job failures with adls gen2 access token error",1,1,"1"
"intermittent library installation issue",1,1,"1"
"intermittent load issues - databricks portal",1,1,"1"
"intermittent redshift",1,1,"1"
"intermittent redshift connection",1,1,"1"
"intermittent redshift connection issue",1,1,"1"
"intermittent redshift connection issue from",1,1,"1"
"intermittent redshift connection issue from job",1,1,"1"
"intermittent udf failures",1,1,"1"
"intermittently job failure with java.lang.noclassdeffounderror: com/microsoft/azure/eventhubs/connectionstringbuilder [2206270050001412]",1,1,"1"
"intermittently permission denied while accessing workspace repros from notebook",1,1,"1"
"internal_error: failed to validate cluster acl path",1,1,"1"
"internalerror: cannot talk to driver",1,1,"1"
"internet access issue through fortigate firewall. where we added the udr on the subnet of databricks",1,1,"1"
"interoperability",1,1,"1"
"invalid access token",1,1,"1"
"invalid access token when using databricks-cli",1,1,"1"
"invalid bucket error",1,1,"1"
"invalid endpoint when calling account api",1,1,"1"
"invalid or unexpected token",1,1,"1"
"invalidprivateworksapceforsubscription ""is not registered to create private workspace""",1,1,"1"
"invalidprivateworksapceforsubscription \""is not registered to create private workspace\""",1,1,"1"
"invalidprivateworksapceforsubscription \\\""is not registered to create private workspace\\\""",1,1,"1"
"invalidrange, 'the range specified is invalid for the current size of the resource",1,1,"1"
"invariantviolationexception: exceeds char/varchar type length limitation when try to create table",1,1,"1"
"investigate table access in logs",1,1,"1"
"invoices",1,1,"1"
"ioerror: [errno 30] read-only file system:",1,1,"1"
"ioerror: [errno 30] read-only file system: '/var/lib/lxc/base-images/release__8.3.x-snapshot-scala2.12__databricks-universe__head__f5e5a29__09cf150__jenkins__a8e3379__format-2.lock'",1,1,"1"
"ip",1,1,"1"
"ip allowlist for `atlassian` workspace",1,1,"1"
"ip range for 51.107.203.195 tunnel.switzerlandnorth.azuredatabricks.net is missing in  databricks service tag",1,1,"1"
"ip whitelist is blocking all logins",1,1,"1"
"ipyleaflet not displaying map",1,1,"1"
"ipython version issue",1,1,"1"
"ipywidgets is broken! not rendering.",1,1,"1"
"is feature store ui provided in azure china?",1,1,"1"
"is it possible to connect databricks sql with aws redshift db?",1,1,"1"
"is there a download api for databricks billing?",1,1,"1"
"is there any connector available for writing  to dynamodb from databricks pyspark/scala spark ?",1,1,"1"
"is there any release on 20th or 21st may could impact the cypher in request socket",1,1,"1"
"is_member function in python",1,1,"1"
"issue accessing databricks cluster",1,1,"1"
"issue accessing file from different aws account!",1,1,"1"
"issue canceling a job stuck in pending state",1,1,"1"
"issue connecting to databricks non-prod environment via webui",1,1,"1"
"issue creating notebooks and workspace",1,1,"1"
"issue in accessing a databricks url",1,1,"1"
"issue in accessing mount path pointing to s3 bucket is aws china region",1,1,"1"
"issue in connecting to aws postgres server proxy using ssl",1,1,"1"
"issue in installing the libraries.",1,1,"1"
"issue installing rstudio on clusters",1,1,"1"
"issue loading data to cosmo db",1,1,"1"
"issue on every endpoints on engie-digital-smartom-data-pprod-ca-central-1 workspace",1,1,"1"
"issue on spark scheduler pools for streaming job",1,1,"1"
"issue parsing json schema",1,1,"1"
"issue pushing logs to splunk from databricks",1,1,"1"
"issue running transformer model",1,1,"1"
"issue triggering notebook from airflow",1,1,"1"
"issue trying to run databricks dash application on databricks cluster",1,1,"1"
"issue using credential passthrough cluster",1,1,"1"
"issue using the databricks feature store on an ephemeral run",1,1,"1"
"issue when filtering for boolean column in dbr 10.4",1,1,"1"
"issue while accessing certain s3 bucktes from databricks",1,1,"1"
"issue while creating db workspace",1,1,"1"
"issue while downloading and uploading a jar file",1,1,"1"
"issue while re-running delta live tables pipeline in dev mode",1,1,"1"
"issue while writing into s3_path",1,1,"1"
"issue with aws data injestion",1,1,"1"
"issue with aws vpc set up",1,1,"1"
"issue with azure token expiry from datasource end while incremental refresh in pbi.",1,1,"1"
"issue with cluster provisioning",1,1,"1"
"issue with cluster scoped init scripts",1,1,"1"
"issue with cluster start time",1,1,"1"
"issue with connection between sas and databricks",1,1,"1"
"issue with databricks auto loader--2205040040007021",1,1,"1"
"issue with databricks training set",1,1,"1"
"issue with distributing code for pandas function api",1,1,"1"
"issue with executors",1,1,"1"
"issue with external table having integrity check fails when copied between prod to dev s3 buckets.",1,1,"1"
"issue with featurestoreclient python api when working with a string type primary key",1,1,"1"
"issue with jobs during cluster version upgrade",1,1,"1"
"issue with length of job id",1,1,"1"
"issue with long running query",1,1,"1"
"issue with metastore connection",1,1,"1"
"issue with optimize running in pipelines (delta live tables) causing job/pipeline to fail",1,1,"1"
"issue with py3openssl on databricks",1,1,"1"
"issue with pyspark code",1,1,"1"
"issue with rendering tfx metrics in databricks notebook",1,1,"1"
"issue with s3 bucket connectivity from workspace",1,1,"1"
"issue with sending sql query to remote via query api",1,1,"1"
"issue with uat instance of databricks ui",1,1,"1"
"issue with unity catalog data resource",1,1,"1"
"issue with updating network of a workspace",1,1,"1"
"issue: problem with spark nlp library",1,1,"1"
"issues accessing clusters in azure databricks instance",1,1,"1"
"issues accessing tables",1,1,"1"
"issues connecting databricks to devops repo",1,1,"1"
"issues connecting sql endpoint in r shiny application",1,1,"1"
"issues deploying databricks workspace using terraform",1,1,"1"
"issues identifying origin of long running processes",1,1,"1"
"issues in viewing the mlflow experiments from databricks mlflow ui",1,1,"1"
"issues reading files written by databricks in s3",1,1,"1"
"issues reading json column from csv",1,1,"1"
"issues related to wait times in retry policy",1,1,"1"
"issues running mlflow through databricks connect",1,1,"1"
"issues serving pyspark recommendations models",1,1,"1"
"issues when adding new user to databricks",1,1,"1"
"issues when interacting with adlsgen2",1,1,"1"
"issues while exporting databricks workspace",1,1,"1"
"issues while switching to 10.4 lts photon enabled cluster, inserting unidentified charachters in delta tables.",1,1,"1"
"issues with delta live apply changes into",1,1,"1"
"issues with library and all the jobs are failling in databricks-",1,1,"1"
"issues with model serve from mlflow",1,1,"1"
"issues with reading file in the resources directory",1,1,"1"
"issues with reconfiguring the audit logging",1,1,"1"
"jar file init script installation",1,1,"1"
"jar not available when job starts",1,1,"1"
"java",1,1,"1"
"java 11",1,1,"1"
"java app fails to establish tls handshake",1,1,"1"
"java error message in the job execution - spark jar task",1,1,"1"
"java gc issues when processing kafka messages",1,1,"1"
"java vulnerabilities in dbr",1,1,"1"
"java.io.ioexception class class com.amazonaws.auth.instanceprofilecredentialsprovider does not implement awscredentialsprovider",1,1,"1"
"java.io.ioexception: error getting access token from metadata server at:",1,1,"1"
"java.io.ioexception: no space left on device",1,1,"1"
"java.io.ioexception: resource temporarily unavailable",1,1,"1"
"java.lang.assertionerror: assertion failed: concurrent update to the commit log. multiple streaming jobs detected for 45",1,1,"1"
"java.lang.classcastexception when we try runtime 10.5",1,1,"1"
"java.lang.classformaterror: truncated class file when running job",1,1,"1"
"java.lang.exception: results too large",1,1,"1"
"java.lang.illegalargumentexception: requirement failed: did not get the first delta file version: 1 to compute snapshot | arr sr# 2204190030001144",1,1,"1"
"java.lang.illegalstateexception: the transaction log has failed integrity checks.",1,1,"1"
"java.lang.nosuchmethoderror",1,1,"1"
"java.lang.nosuchmethoderror in databricks",1,1,"1"
"java.lang.outofmemoryerror: gc overhead limit exceeded",1,1,"1"
"java.lang.outofmemoryerror: java heap space",1,1,"1"
"java.lang.outofmemoryerror: requested array size exceeds vm limit",1,1,"1"
"java.lang.runtimeexception: cannot reserve additional contiguous bytes in the vectorized reader (integer overflow)",1,1,"1"
"java.net.socketexception: socket is closed",1,1,"1"
"java.net.sockettimeoutexception: read timed out",1,1,"1"
"java.util.nosuchelementexception",1,1,"1"
"javascript:/*--></title></style></textarea></script></xmp><svg/onload='+/""/+/onmouseover=1/+/[*/[]/+alert(1)//'>",1,1,"1"
"javascript:/*-/*`/*`/*'/*""/**/(/* */on click=alert() )//%0d%0a%0d%0a//e/style/e/title/e/text area/e/script/--!ecsvs/svg/unload=alert()//ex3",1,1,"1"
"javascript:alert();",1,1,"1"
"javax.net.ssl.sslexception: connection reset",1,1,"1"
"javax.net.ssl.sslhandshakeexception only on 10.4 lts or 10.5.  it works fine on 9.x lts",1,1,"1"
"jdbc",1,1,"1"
"jdbc - databricks",1,1,"1"
"jdbc connection in databricks sql",1,1,"1"
"jdbc connectivity from tableau",1,1,"1"
"jdbc driver - intermittent stuck queries",1,1,"1"
"jdbc driver causes log4j conflict",1,1,"1"
"jdbc driver class",1,1,"1"
"jdbc driver documentation",1,1,"1"
"jdbc driver stripping query comments",1,1,"1"
"jdbc endpoint",1,1,"1"
"jdbc java 17",1,1,"1"
"jdbc log4j conflict",1,1,"1"
"jdbc/odbc server - open sessions for days",1,1,"1"
"jdk 11",1,1,"1"
"jdk support",1,1,"1"
"job aborted due to stage failure",1,1,"1"
"job aborted error: no space left on device",1,1,"1"
"job aborted errors",1,1,"1"
"job alerts doesn't save email address",1,1,"1"
"job api",1,1,"1"
"job api 2.0 misleading error",1,1,"1"
"job api 2.0 retirement date",1,1,"1"
"job appears to be not running for 25+ minutes of execution",1,1,"1"
"job cannot modify delta table",1,1,"1"
"job cluster creation is failing due to being unable to fetch secrets",1,1,"1"
"job cluster did not shut down",1,1,"1"
"job cluster failed to create",1,1,"1"
"job cluster hanging in running states.",1,1,"1"
"job cluster id is missing in azure subscription logs",1,1,"1"
"job cluster launch failure  does not provide full error message",1,1,"1"
"job cluster launch failure due to instance request limit",1,1,"1"
"job cluster notebooks are not visible to users.",1,1,"1"
"job cluster reuse functionality is not available",1,1,"1"
"job cluster takes more time than interactive cluster",1,1,"1"
"job cluster taking more time than before | job id: 241293",1,1,"1"
"job cluster time out",1,1,"1"
"job cluster unable to retrieve information of a table with data from dbfs location",1,1,"1"
"job cluster were idle for couple of hours",1,1,"1"
"job cluster with credential passthrough fails with error ""could not find adls gen2 token""",1,1,"1"
"job clusters do not terminate",1,1,"1"
"job clusters failing to start up",1,1,"1"
"job clusters not auto scaling properly",1,1,"1"
"job clusters rest api",1,1,"1"
"job clusters using terraform",1,1,"1"
"job crashed with py4j.py4jnetworkexception error",1,1,"1"
"job delete",1,1,"1"
"job email alerts",1,1,"1"
"job email notification alert not sending to specific email address",1,1,"1"
"job error  unexpected run state (while still running?)",1,1,"1"
"job error out after few hours of executio",1,1,"1"
"job execution issues",1,1,"1"
"job execution poor performance",1,1,"1"
"job execution time is by far too late",1,1,"1"
"job failed after 5 hours with timeout",1,1,"1"
"job failed but without any detailed error message[prob environment]",1,1,"1"
"job failed due to cluster unavailble",1,1,"1"
"job failed intermittently with ""java.lang.classnotfoundexception: com.crealytics.spark.excel.defaultsource""",1,1,"1"
"job failed to autoscale disk properly",1,1,"1"
"job failed to retry",1,1,"1"
"job failed with ""timed out retrying to clustermanager""",1,1,"1"
"job failed with cluster termination",1,1,"1"
"job failed with error  9ebd42f2154e4498a69cca2c4d8d6bcd-0-scratchvolume is being used by another operation",1,1,"1"
"job failed with error message cluster became unreachable during run",1,1,"1"
"job failed with error message library installation failed for library due to infra fault",1,1,"1"
"job failed with error message library installation failed for library due to user error for pypi",1,1,"1"
"job failed with error message unexpected failure while waiting for the cluster",1,1,"1"
"job failed with timeout error:",1,1,"1"
"job failed with unknown error",1,1,"1"
"job failing",1,1,"1"
"job failing after dbr version changed",1,1,"1"
"job failing because of column not found issue",1,1,"1"
"job failing databricks",1,1,"1"
"job failing in prod",1,1,"1"
"job failing intermittently due to package error",1,1,"1"
"job failing while using runtime 10.4 spark version 3.2.1",1,1,"1"
"job failing with an error",1,1,"1"
"job failing with error: com.databricks.notebookexecutionexception: failed: failed to acquire mysql lock job-1.",1,1,"1"
"job failing with io.delta.exceptions.metadatachangedexception",1,1,"1"
"job failing with library error | 2204070030001149 |  cummins",1,1,"1"
"job failing with with 'spark cluster stopped unexpectedly' error",1,1,"1"
"job fails with ""an error occurred while trying to connect to the java server (127.0.0.1:35291)""",1,1,"1"
"job fails with message to contact databricks  support",1,1,"1"
"job failuer",1,1,"1"
"job failure - unable to download docker image",1,1,"1"
"job failure at seemingly random intervals",1,1,"1"
"job failure due to an internal error",1,1,"1"
"job failure due to broadcast error",1,1,"1"
"job failure due to cluster bootstrap_timeout",1,1,"1"
"job failure due to cluster error",1,1,"1"
"job failure due to driver disconnected",1,1,"1"
"job failure due to exception ""illegalargumentexception""",1,1,"1"
"job failure due to network connectivity",1,1,"1"
"job failure due to node loss",1,1,"1"
"job failure due to shuffle error",1,1,"1"
"job failure due to skewed data in prod",1,1,"1"
"job failure in databricks production due to import statement not found",1,1,"1"
"job failure issue genie request",1,1,"1"
"job failure notification",1,1,"1"
"job failure without underlying cause",1,1,"1"
"job failure/long running without change to job.",1,1,"1"
"job failures even after reties",1,1,"1"
"job failures in prod",1,1,"1"
"job gets stuck on “uploading command""",1,1,"1"
"job getting aborted due to remote rpc client disassociated. likely due to containers exceeding thresholds, or network issues. check driver logs for warn messages",1,1,"1"
"job hung without progress",1,1,"1"
"job id vlaue out of range",1,1,"1"
"job in production workspace has the logs, metrics, and spark ui unaccessable",1,1,"1"
"job is failing",1,1,"1"
"job is failing and also taking long time to fetch data",1,1,"1"
"job is failing due to connection pool shut down",1,1,"1"
"job is failing with invalid parameter value",1,1,"1"
"job is failing with regex expression error",1,1,"1"
"job is getting failed.",1,1,"1"
"job is running forever",1,1,"1"
"job is running from long time",1,1,"1"
"job is running slower than usual and is failing with time out",1,1,"1"
"job issue between 10.4 and 9.1 lts",1,1,"1"
"job keeps failing with fetchfailedexception",1,1,"1"
"job log",1,1,"1"
"job marked finished but output not written.  still",1,1,"1"
"job metrics screenshot interval",1,1,"1"
"job no longer works; showing access key or secret key is null",1,1,"1"
"job not accepting resources",1,1,"1"
"job oom when writing to s3 or snowflake",1,1,"1"
"job outage on march 31",1,1,"1"
"job parameters start_time and previous_run_start_time",1,1,"1"
"job processes hangs after a while at an unknown time and does not move forward.",1,1,"1"
"job ran longer time than usual | arr sr# 2206030030000143",1,1,"1"
"job run failed",1,1,"1"
"job run fails in prod due to rpc timrout",1,1,"1"
"job run longer and manually terminated the cluster",1,1,"1"
"job run took too long and it seems stuck between start the job and starting the first task",1,1,"1"
"job running long time than expected",1,1,"1"
"job runs are getting timed out from time to time",1,1,"1"
"job runs slower than expected. there are no logs to investigate | arr sr# 2205260040002506",1,1,"1"
"job schedule delay for more than 15 minutes",1,1,"1"
"job scheduler, cluster ui not encountered azure resource provider request limit throttling during standard_e32s_v3 single node cluster startup- saved",1,1,"1"
"job should be failed but still running for more than 1 hour",1,1,"1"
"job status view is broken",1,1,"1"
"job stuck",1,1,"1"
"job stuck at first but succeed with re-run",1,1,"1"
"job stuck at simple read command",1,1,"1"
"job stuck in running state",1,1,"1"
"job stuck on simple read from mounted data lake",1,1,"1"
"job stucks in middle",1,1,"1"
"job taking a lot of time to execute a notebook",1,1,"1"
"job tasks failing in databricks workspace with vnet issue",1,1,"1"
"job threw 403 error in the middle of saving files into aws s3",1,1,"1"
"job unexpectedly trying to access parquet files in glacier",1,1,"1"
"job url failure",1,1,"1"
"job-cluster is slower than interactive cluster",1,1,"1"
"job_aborted_error",1,1,"1"
"jobs ""docker""",1,1,"1"
"jobs access control",1,1,"1"
"jobs api",1,1,"1"
"jobs api 2.0 fails with invalid_parameter_value",1,1,"1"
"jobs api 2.1 ""/reset"" endpoint doesn't pick up ""runtime_engine"" photon specification",1,1,"1"
"jobs api downtime ?",1,1,"1"
"jobs api giving error",1,1,"1"
"jobs api setup duration",1,1,"1"
"jobs api timeout",1,1,"1"
"jobs are becoming very slow",1,1,"1"
"jobs are failing after changing dbr version",1,1,"1"
"jobs are failing around the same time",1,1,"1"
"jobs are failing due to ‘too many connections’ error",1,1,"1"
"jobs are failing to access the adls container files",1,1,"1"
"jobs are getting stuck",1,1,"1"
"jobs are not getting canceled",1,1,"1"
"jobs being aborted with could not initialize class com.cloudmed.spark.util$",1,1,"1"
"jobs can't be scheduled on the 30th or 31st of the month",1,1,"1"
"jobs can_view",1,1,"1"
"jobs couldn't be submitted to databricks at regular speed",1,1,"1"
"jobs fail with cloud provider failure",1,1,"1"
"jobs fail, but no error is reported",1,1,"1"
"jobs failed with http 429 - too many requests",1,1,"1"
"jobs failing with ""unexpected failure while waiting for the cluster""",1,1,"1"
"jobs failing with 'error message: allocation failed. we do not have sufficient capacity for the requested vm size in this region'",1,1,"1"
"jobs failing with 'file already exists'",1,1,"1"
"jobs failing with library not being installed on clusters",1,1,"1"
"jobs failing with library not being installed on dataclusters",1,1,"1"
"jobs failing with netty oom in e2 but they are passing in pvc",1,1,"1"
"jobs failing with request_limit_exceeded error",1,1,"1"
"jobs get stuck while writing to a delta table",1,1,"1"
"jobs getting canceeled",1,1,"1"
"jobs hang or fail after upgrading to runtime 10.4",1,1,"1"
"jobs not terminating",1,1,"1"
"jobs of migration are stopped with out rason",1,1,"1"
"jobs preview requests",1,1,"1"
"jobs running for a very long time and not completing",1,1,"1"
"jobs seems to hang with 10.4 lts",1,1,"1"
"jobs spark example",1,1,"1"
"jobs stuck in sql endpoint",1,1,"1"
"jobs take longer than expected",1,1,"1"
"jobs time out (follow up 00138978)",1,1,"1"
"jobs_performance_issues",1,1,"1"
"jod delete",1,1,"1"
"join takes seemingly forever and eventually aborts",1,1,"1"
"journal discipline codes",1,1,"1"
"jproperties library installation issue",1,1,"1"
"json  file read issue",1,1,"1"
"json files without line delimiter",1,1,"1"
"json serde queries not working on 10.5 dbr",1,1,"1"
"jupyter notebook version",1,1,"1"
"jupyter shell %sh variable",1,1,"1"
"jupyter shell variable",1,1,"1"
"k 77",1,1,"1"
"k8s",1,1,"1"
"kafka bootstrap servers tls puerto 9094",1,1,"1"
"kafka consumer script returns null values",1,1,"1"
"kafka groupid and load delta table as a stream source",1,1,"1"
"kafka spark integration",1,1,"1"
"kafka topics are not able to read with spark",1,1,"1"
"keep data table up to data",1,1,"1"
"keplergl library not working on databricks' notebooks",1,1,"1"
"kernel",1,1,"1"
"keyboard",1,1,"1"
"keyword search sql",1,1,"1"
"kinesis streaming job failed yesterday",1,1,"1"
"kubernetes",1,1,"1"
"l2 align xpo pd monthly dag failure -photon ran out of memory",1,1,"1"
"lake house",1,1,"1"
"language settings",1,1,"1"
"large job ids",1,1,"1"
"last fetched",1,1,"1"
"last modified",1,1,"1"
"latency in inserting records to adx",1,1,"1"
"latency to trigger jobs",1,1,"1"
"latest  databricks ami not available",1,1,"1"
"latest run id",1,1,"1"
"latest tensorflow support: 2.9",1,1,"1"
"launch cluster failed on test environment",1,1,"1"
"launch cluster failure on prd",1,1,"1"
"leaflet python",1,1,"1"
"learning path not available in academy",1,1,"1"
"learning path not working",1,1,"1"
"lenght function",1,1,"1"
"libraries",1,1,"1"
"libraries are not installing",1,1,"1"
"libraries are not properly uninstalled from cluster",1,1,"1"
"libraries are not properly uninstalled from clusters",1,1,"1"
"libraries error after restart cluter",1,1,"1"
"libraries link",1,1,"1"
"libraries pending cluster",1,1,"1"
"libraries won't load",1,1,"1"
"library installation failed when jar was present in storage account",1,1,"1"
"library installation failure",1,1,"1"
"library installation failures in many workspace in azure northeurope",1,1,"1"
"library installation related audit info",1,1,"1"
"library not sticking to cluster",1,1,"1"
"library on dbfs failed to install",1,1,"1"
"license",1,1,"1"
"licensing",1,1,"1"
"limit increase: ""the quota for the number of jobs has been reached""",1,1,"1"
"limitations to replace our hive metastore with aws glue catalog",1,1,"1"
"limiting dbr version in databricks cluster policies - question",1,1,"1"
"limiting secrets access",1,1,"1"
"line wrap",1,1,"1"
"lineage",1,1,"1"
"linearregression",1,1,"1"
"link data content to access history on databricks sql",1,1,"1"
"linkedin",1,1,"1"
"linkedin post",1,1,"1"
"linkedin post error",1,1,"1"
"list my schema",1,1,"1"
"list my schema access",1,1,"1"
"list my schemaaccess",1,1,"1"
"list of task failures",1,1,"1"
"listing queries are taking time",1,1,"1"
"load",1,1,"1"
"load data from infludb in delta : looking for examples/advices",1,1,"1"
"load data python",1,1,"1"
"load excel(xlsx) file into dataframe?",1,1,"1"
"load file in filesystem",1,1,"1"
"load filoe",1,1,"1"
"load text file",1,1,"1"
"loading",1,1,"1"
"loading python packages in databricks",1,1,"1"
"loading table",1,1,"1"
"loading tables with billions of records",1,1,"1"
"local issuer certificate",1,1,"1"
"locate http path",1,1,"1"
"locating ip address",1,1,"1"
"lock delta tables and give select on dynamic views",1,1,"1"
"locked out unusual activity",1,1,"1"
"log",1,1,"1"
"log delivery configuration using terraform",1,1,"1"
"log files can be truncated",1,1,"1"
"log files storage path",1,1,"1"
"log level jobs",1,1,"1"
"log out databricks.webapp functionality for the shard.yaml file",1,1,"1"
"log4j libraries still coming up with the latest pvc 3.60.7",1,1,"1"
"log4j logs not appearing",1,1,"1"
"logging the right way with pyspark",1,1,"1"
"login issue for data bricks community",1,1,"1"
"login partner",1,1,"1"
"logos",1,1,"1"
"long microbatches in the ingest pipeline",1,1,"1"
"long microbatches in the ingest pipeline.",1,1,"1"
"long run job spark ui show incorrect active jobs status, and illegalthreadstateexception in databricks code",1,1,"1"
"long running",1,1,"1"
"long running job cluster's which are sitting ideal in more then 600 hours",1,1,"1"
"long running jobs workspace request",1,1,"1"
"long running notebooks",1,1,"1"
"long running production workflow performance issue",1,1,"1"
"long running streaming job optimization",1,1,"1"
"long running time of pymc3 sampling",1,1,"1"
"long time for spinning up a cluster",1,1,"1"
"looking to understand if databricks supports watch for s3 path",1,1,"1"
"lookup",1,1,"1"
"loosing credential in passthrough",1,1,"1"
"losing connection with sql",1,1,"1"
"losing leading zeros",1,1,"1"
"lost admin access to workspace",1,1,"1"
"lost data in job rrun",1,1,"1"
"lost permission to check which notebook is running in our cluster",1,1,"1"
"lot of latency in apllying the changes",1,1,"1"
"low instance availability in aws us-west-2",1,1,"1"
"ls",1,1,"1"
"mac",1,1,"1"
"mac os excel",1,1,"1"
"magic",1,1,"1"
"magic commands",1,1,"1"
"maker",1,1,"1"
"making",1,1,"1"
"malformed",1,1,"1"
"malformed_request: invalid vpc endpoint in the http request body",1,1,"1"
"manage accounts",1,1,"1"
"manage clusters",1,1,"1"
"manage data upload",1,1,"1"
"manage dbfs",1,1,"1"
"manage thigns  to report",1,1,"1"
"manage things",1,1,"1"
"manage your azure databricks account",1,1,"1"
"manage your databricks account",1,1,"1"
"manage your subscription",1,1,"1"
"managed cache",1,1,"1"
"managed mlflow doesn't sanitize inputs",1,1,"1"
"management of notebook",1,1,"1"
"manager",1,1,"1"
"managers",1,1,"1"
"managing ticket page",1,1,"1"
"mandeep singh sachdeva",1,1,"1"
"manu testing",1,1,"1"
"many failures during clusters creation",1,1,"1"
"many jobs fail with ""azurecredentialnotfoundexception: could not find adls gen2 token""""",1,1,"1"
"many jobs fail with placementlocktimeout: could not acquire placement lock context",1,1,"1"
"many jobs on the scheduled jobs cluster failing at the very beginning with 'canceled' messaged",1,1,"1"
"many testing",1,1,"1"
"mark",1,1,"1"
"marker",1,1,"1"
"market specific sql endpoint creation",1,1,"1"
"market user is unable to modify s3 file",1,1,"1"
"marketing thing to check recent search",1,1,"1"
"marketing thins",1,1,"1"
"marking",1,1,"1"
"master node",1,1,"1"
"matching dashboards in separate uat and prod workspaces causing each other to break",1,1,"1"
"matlab",1,1,"1"
"max",1,1,"1"
"max request rate for ec2 causing job failure",1,1,"1"
"max_connect_error",1,1,"1"
"max_user_connections",1,1,"1"
"maxretryerror: httpsconnectionpool(host='chinaeast2.databricks.azure.cn', port=443)",1,1,"1"
"may i know thr training schedule",1,1,"1"
"mcsa agreement",1,1,"1"
"mdh production process is getting failed",1,1,"1"
"mean",1,1,"1"
"memory consumption at the start of the cluster",1,1,"1"
"memory delete dataframe",1,1,"1"
"memory error and deprecated runtime",1,1,"1"
"memory release dataframe",1,1,"1"
"merge into runs very slow",1,1,"1"
"merge legacy e2 account  into current e2 account",1,1,"1"
"merge query performance",1,1,"1"
"merge schema setting not working for nested columns on delta tables",1,1,"1"
"message cluster terminated. reason: cloud provider launch failure",1,1,"1"
"metadata perfomance issue",1,1,"1"
"metadata queries take hours in sql endpoints randomely",1,1,"1"
"metadata.hiveexception: at least one column must be specified for the table",1,1,"1"
"metastore down",1,1,"1"
"metastore is down and activity is stack",1,1,"1"
"metastore is down error",1,1,"1"
"metastore is down- this error is appearing intermittently and during this time externalk hivemetastore is not available",1,1,"1"
"metastore not loading on databricks",1,1,"1"
"metastore uri",1,1,"1"
"metastore_does_not_exist: no metastore assigned for the current workspace",1,1,"1"
"microsoft azure deployment acceptable use policy violation [#sir9501524]",1,1,"1"
"migrate instance profile from databricks e1 to e2 in same aws account",1,1,"1"
"migrate to e2",1,1,"1"
"migrate tool",1,1,"1"
"migrate tool e2",1,1,"1"
"migrating workspace to different vpc and subnets",1,1,"1"
"migration",1,1,"1"
"migration 7.3 lts to 9.1 lts",1,1,"1"
"migration from  db 7.3 lts and 9.1 lts and performance issue.",1,1,"1"
"migration of table and views from old hive version into new using git repository",1,1,"1"
"migration question",1,1,"1"
"migration strategy for azure databricks cluster to move from one azure tennat to other azure tenant",1,1,"1"
"migration tool throwing error while used for metstore migration",1,1,"1"
"minutes",1,1,"1"
"misleading error message when github token is given but expired",1,1,"1"
"missing and duplicate rows when merging data into a delta table",1,1,"1"
"missing april bill",1,1,"1"
"missing cluster policy",1,1,"1"
"missing jobs in qa workspace",1,1,"1"
"missing log entries from azure data factory",1,1,"1"
"missing tables",1,1,"1"
"ml model inference cluster",1,1,"1"
"mlflow autolog sklearn",1,1,"1"
"mlflow cluster logs unavailable",1,1,"1"
"mlflow code_path",1,1,"1"
"mlflow enable serving through code is not working",1,1,"1"
"mlflow exception",1,1,"1"
"mlflow execution runs slower and slower over time",1,1,"1"
"mlflow experiment location changes",1,1,"1"
"mlflow experiment permission denied",1,1,"1"
"mlflow experiments missing",1,1,"1"
"mlflow infinite recursion error when unselecting metrics",1,1,"1"
"mlflow job",1,1,"1"
"mlflow model / experiment unable to load python dependency from dbfs",1,1,"1"
"mlflow model registry not working on cluster with docker image",1,1,"1"
"mlflow model registry staging options",1,1,"1"
"mlflow model servering cannot create cluster",1,1,"1"
"mlflow model serving error 429",1,1,"1"
"mlflow model serving on databricks",1,1,"1"
"mlflow protobuf python version problem",1,1,"1"
"mlflow res",1,1,"1"
"mlflow run on databricks-connect",1,1,"1"
"mlflow server",1,1,"1"
"mlfow params",1,1,"1"
"mlops",1,1,"1"
"mnt",1,1,"1"
"mobile format",1,1,"1"
"mobile number",1,1,"1"
"mode",1,1,"1"
"model loading fails with the error ""attributeerror: can't get attribute 'modelquantilesinterval' on <module '__main__' from '/opt/conda/envs/vxgioq-cfu-counting-ml-env/bin/pytest'> | arr sr# 2204250050000999",1,1,"1"
"model serving cluster issue",1,1,"1"
"model.amazons3exception: the operation is not valid for the object's storage class",1,1,"1"
"models",1,1,"1"
"modifying subnet of databricks workspace",1,1,"1"
"module dlt",1,1,"1"
"module not found egg",1,1,"1"
"module note found",1,1,"1"
"modulenotfounderror: no module named 'dlt'",1,1,"1"
"modulenotfounderror: no module named 'pasde'",1,1,"1"
"modulenotfounderror: no module named 'sparknlp_jsl'",1,1,"1"
"modulenotfounderror: no module named 'tabulate",1,1,"1"
"mongodb",1,1,"1"
"mongodb python",1,1,"1"
"monitor running jobs",1,1,"1"
"mount /workspaces when using custom databricks runtime",1,1,"1"
"mount adls gen2 sas",1,1,"1"
"mount azure storage account container in databricks workspace in different region",1,1,"1"
"mount command is not working in e2 shard",1,1,"1"
"mount data",1,1,"1"
"mount s3",1,1,"1"
"mounting issue",1,1,"1"
"move cell up",1,1,"1"
"move hive metastore to unity catalog",1,1,"1"
"move subscription azure",1,1,"1"
"ms css",1,1,"1"
"msr7-9z)",1,1,"1"
"multi tenant hipaa environments",1,1,"1"
"multi-task jobs require clusterspec to be specified within the task",1,1,"1"
"multiline",1,1,"1"
"multiple  jobs are failing",1,1,"1"
"multiple  jobs are failing due to connection error",1,1,"1"
"multiple clusters show ""dbfs is down""",1,1,"1"
"multiple dags failure due to library installation",1,1,"1"
"multiple intermittent cluster failures in production",1,1,"1"
"multiple issues in accessing keyvault in west us 2 region",1,1,"1"
"multiple issues with clusters changed over night - logging, shared config",1,1,"1"
"multiple job runs failing due to failure in cluster library installation",1,1,"1"
"multiple jobs failed due to the cluster issue",1,1,"1"
"multiple jobs failing for ge finance datalake",1,1,"1"
"multiple jobs got time-out",1,1,"1"
"multiple library installation failure on prod cluster",1,1,"1"
"multiple principals in api",1,1,"1"
"multiple production job failure due to runtimeexception",1,1,"1"
"multiple production jobs are failing",1,1,"1"
"multiple results being returned for same code--2204290040004791",1,1,"1"
"multiple users experienced query failing with: table or view not found: xyz",1,1,"1"
"multitask job failed without retry",1,1,"1"
"mus adv/prem/azure databricks/job is running for a long time",1,1,"1"
"mws credentials malformed",1,1,"1"
"my cluster does not start",1,1,"1"
"my workspace folder being dispalyed for all users.",1,1,"1"
"mysql",1,1,"1"
"mysql too many connection errors",1,1,"1"
"name of notebooks not shown in browser tabs in e2 workspace",1,1,"1"
"names (databricks sql)",1,1,"1"
"nan",1,1,"1"
"nan functions",1,1,"1"
"nans in nested json",1,1,"1"
"nas",1,1,"1"
"navigate the workspace",1,1,"1"
"navigate to cmd",1,1,"1"
"nba: job failed for intermediate pipeline due to ""unknownhostexception"" exception in databricks",1,1,"1"
"need access to create private workspace",1,1,"1"
"need access to genie to look into customer case",1,1,"1"
"need admin access to workspace",1,1,"1"
"need assistance on unsual activity on customers environment",1,1,"1"
"need assistance regarding library issue",1,1,"1"
"need assistance with terraform for aws - issue in managing vpc endpoints and multiple workspaces within the same vpc",1,1,"1"
"need databricks clarification of the spark conguration of ""spark.hadoop.fs.abfss.impl.disable.cache"" and the suggestions of any other configurations to optimize the performance of accessing the",1,1,"1"
"need deeper understanding of delta live tables",1,1,"1"
"need detail rca for a databricks outage",1,1,"1"
"need documentation to integrate bitbucket server with databricks",1,1,"1"
"need emea engineer - databricks sql connection to external metastore is failing [2206290050000744]",1,1,"1"
"need help creating second workspace",1,1,"1"
"need help for databricks notebook configuration for etl operation",1,1,"1"
"need help in debugging high job execution time",1,1,"1"
"need help in setting dns resolver for databricks webpage",1,1,"1"
"need help integrating sso at account level",1,1,"1"
"need help optimizing pandas udfs for deep learning inference",1,1,"1"
"need help setting up new worksapce",1,1,"1"
"need help to check from azure databricks to verify if the trace data has been sent successfully without any data truncate",1,1,"1"
"need help to enable delta live table",1,1,"1"
"need help to implement instana jvm startup agent",1,1,"1"
"need help validating audit logs - cset tool audit is not working for west europe",1,1,"1"
"need help verifying the actions from databricks",1,1,"1"
"need help why dataframe cache is not working",1,1,"1"
"need help with crowdstrike implementation",1,1,"1"
"need help with cx reaching limit",1,1,"1"
"need help with jobs api submiting job type as python",1,1,"1"
"need help with seperate work spaces in the same accout?",1,1,"1"
"need help with show create table in spark higher version",1,1,"1"
"need info from cloud_launch_failure errors",1,1,"1"
"need new firewall rule to public s3 bucket with databricks-datasetes",1,1,"1"
"need oncall engineer to pin intuit prd workspace back to custom image",1,1,"1"
"need onfirmation from sql endpoint side",1,1,"1"
"need public ips to our business intelligence cluster",1,1,"1"
"need separate workspace for prod environment",1,1,"1"
"need some clarification on delta live tables",1,1,"1"
"need some clarifications regarding the ""change data feed""",1,1,"1"
"need storage analysis",1,1,"1"
"need support changing to new idp location for saml sso",1,1,"1"
"need support for upgrading databricks runtime version",1,1,"1"
"need to be able to retrieve metadata statistics for all columns of the table",1,1,"1"
"need to change the dbr version of sql endpoint",1,1,"1"
"need to check the kernel version number in response to cve-2022-0847, cannot currently as permissions don't allow my team to do so",1,1,"1"
"need to check why the model training for so long and there has a good run for this.",1,1,"1"
"need to check why the model training for so long and why it's stuck shown from log4j logs",1,1,"1"
"need to create additional workspace on different aws account.",1,1,"1"
"need to enable secret access in databricks from databricks-connect",1,1,"1"
"need to figure out job cluster and its corresponding ec2 instances id",1,1,"1"
"need to know about connectivity from key vault to azure databricks",1,1,"1"
"need to know the best way to call common function for databricks jobs",1,1,"1"
"need to know which vm type has what kind of disk.",1,1,"1"
"need to lookup databricks job id by name but databricks jobs 2.1 api only supports list which returns up to 25 jobs",1,1,"1"
"need to run dbinsight forjobs however it always fails",1,1,"1"
"need to see which user deleted a cluster",1,1,"1"
"need to share bronze s# bucket for multiple aws account",1,1,"1"
"need to trigger one adb workspace from another adb workspace",1,1,"1"
"need to verify debugfs mounted in job cluster and reduce the logs",1,1,"1"
"need updates feature request #db-i-5027",1,1,"1"
"need vm/networking information for streaming job crash (boostrap url not defined)",1,1,"1"
"need your confirm that if we could check",1,1,"1"
"nested json",1,1,"1"
"network attached storage",1,1,"1"
"network configuration update not working",1,1,"1"
"networking rules, firewalls on the private and public sub-net",1,1,"1"
"new command line",1,1,"1"
"new data/table replication across workspaces",1,1,"1"
"new databricks e2 account",1,1,"1"
"new endpoints pending acceptance from databricks",1,1,"1"
"new failure in automated databricks pipeline",1,1,"1"
"new feature with table creation",1,1,"1"
"new genie 2203300030002645 ws 5259743963611861 subid 33083e0c-20e5-40c8-a77d-557528d9e387",1,1,"1"
"new genie 2205190050000485",1,1,"1"
"new job clusters are not able to start",1,1,"1"
"new user doesn't receive an invite email",1,1,"1"
"new workspace not visible when submitting case",1,1,"1"
"nfs/efs volume mounting",1,1,"1"
"nlp",1,1,"1"
"no ""use your own docker container"" option",1,1,"1"
"no access to partner academy page",1,1,"1"
"no access to unity catalog ui",1,1,"1"
"no active google kubernetes engine",1,1,"1"
"no active google kubernetes engine cluster found for workspace. retrying google kubernetes engine cluster creation.",1,1,"1"
"no api found for 'get /jobs/run-now",1,1,"1"
"no authentication type appeared for databricks azure genie",1,1,"1"
"no cluster",1,1,"1"
"no credentials",1,1,"1"
"no email",1,1,"1"
"no email register",1,1,"1"
"no limit",1,1,"1"
"no module named 'tensorflow'",1,1,"1"
"no public internet access on cluster nodes",1,1,"1"
"no repos icon",1,1,"1"
"noclassdeffounderror",1,1,"1"
"non-admin users unable to create job tasks in single tenant workspace",1,1,"1"
"non-deterministic behaviour from delta discussed with bart samwell from databricks",1,1,"1"
"not abale to create cluster through azure databricks",1,1,"1"
"not able manage account and devops",1,1,"1"
"not able to ""create job""s in databricks ""workflows""",1,1,"1"
"not able to ""reset password"" in e2 admin console",1,1,"1"
"not able to access any courses",1,1,"1"
"not able to access file in azure blob storage",1,1,"1"
"not able to access mounted container files in databricks",1,1,"1"
"not able to access my account",1,1,"1"
"not able to access other region s3 bucket in us",1,1,"1"
"not able to access the adls (cagsensedevdatahgrbhxfi ) container 'cagsense'through databricks",1,1,"1"
"not able to accessible table data with cluster dbr 5.5 version",1,1,"1"
"not able to activate databricks unity catalog",1,1,"1"
"not able to connect to mqsql metastore db from sql endpoint",1,1,"1"
"not able to connect to the databricks ec2 instances on data plane",1,1,"1"
"not able to connect with oracle in e2 shard while it is working in e1",1,1,"1"
"not able to create a cluster community edition",1,1,"1"
"not able to create a high concurrency cluster",1,1,"1"
"not able to create cluster in azure databricks",1,1,"1"
"not able to create databricks secret scope using rest api",1,1,"1"
"not able to create jdbc connection",1,1,"1"
"not able to create the new workspaces - gcp",1,1,"1"
"not able to edit permissions on a job",1,1,"1"
"not able to export users' notebooks in this workspace",1,1,"1"
"not able to find clusters,repos and admin access also removed in hub account",1,1,"1"
"not able to import lightgbm module in new high cuncurrency cluster.",1,1,"1"
"not able to install dplyr in the shard",1,1,"1"
"not able to install libraries tmap and epir",1,1,"1"
"not able to install libsasl2-dev library.",1,1,"1"
"not able to launch cluser in new workspace",1,1,"1"
"not able to load more azure databricks sql endpoint using jdbc",1,1,"1"
"not able to login from manage account",1,1,"1"
"not able to login to manage account",1,1,"1"
"not able to login using sso",1,1,"1"
"not able to mount adls",1,1,"1"
"not able to open sql ui on any other workspace",1,1,"1"
"not able to provide database access in sql endpoint",1,1,"1"
"not able to read from storage when running on a standard cluster",1,1,"1"
"not able to run code, genie",1,1,"1"
"not able to run marquez java library on the cluster",1,1,"1"
"not able to run queries on top of adls stored tables from sql editor",1,1,"1"
"not able to run the code with same dbr version",1,1,"1"
"not able to start a cluster",1,1,"1"
"not able to start any cluster (intermittent issue)",1,1,"1"
"not able to start any cluster due to timeout issue",1,1,"1"
"not able to start cluster",1,1,"1"
"not able to store ml model",1,1,"1"
"not able to use pandas function",1,1,"1"
"not able to view ganglia on graviton instances",1,1,"1"
"not getting databricks account id",1,1,"1"
"not getting sso login page for some environments",1,1,"1"
"not implemented type for arrow list to pandas: map<string, string> on dbr 9.1 lts",1,1,"1"
"not loading",1,1,"1"
"not push repo bitbucket",1,1,"1"
"not receiving notifications emails from databricks help desk",1,1,"1"
"notebook cell stuck on sumbit",1,1,"1"
"notebook commands",1,1,"1"
"notebook commands get hung on spark.read; spark job gets initiated but commands get stuck indefinitely.",1,1,"1"
"notebook commands getting cancelled",1,1,"1"
"notebook commands not getting executed, showing as uploding command on ui, ui bug",1,1,"1"
"notebook concurrent run",1,1,"1"
"notebook continues to appear as running and it cannot be manually stopped",1,1,"1"
"notebook does not execute.",1,1,"1"
"notebook execution failure issue",1,1,"1"
"notebook execution stopped and not resumed automatically",1,1,"1"
"notebook execution taking to much time. need performance tuning",1,1,"1"
"notebook fails with ioexception caused by remoteclassloadererror",1,1,"1"
"notebook id",1,1,"1"
"notebook issues",1,1,"1"
"notebook not connecting to pipeline",1,1,"1"
"notebook submitting unwanted jobs to cluster",1,1,"1"
"notebook takes too long to load",1,1,"1"
"notebook timed out",1,1,"1"
"notebook workflows",1,1,"1"
"notebook workflows when notebooks are in repos and in jupyter format (.ipynb)",1,1,"1"
"notebookid",1,1,"1"
"notebooks are getting detached from the cluster.",1,1,"1"
"notebooks are not working",1,1,"1"
"notebooks getting cancelled and intermittent blob operation not supported when executing deltatable.delete",1,1,"1"
"notebooks/cluster detach and re-attcah issue",1,1,"1"
"notification not able to be disabled on status website",1,1,"1"
"notifications",1,1,"1"
"npip tunnel setup failure in spark job",1,1,"1"
"nuke poc account",1,1,"1"
"nullpointer exception with streamsets pipeline",1,1,"1"
"nullpointer updating a delta table",1,1,"1"
"nullpointerexception",1,1,"1"
"number of business day",1,1,"1"
"number of business dyas",1,1,"1"
"numpy failed to import in notebook",1,1,"1"
"nvirginia.cloud.databricks.com:443 failed to respond",1,1,"1"
"o number",1,1,"1"
"object oriented programming",1,1,"1"
"object sqldb is not a member of package com.microsoft.azure. the latest jar with dependencies is installed but still have the error pop up.",1,1,"1"
"observing a lot of delay and intermittent failures when trying to start a spark cluster in databricks",1,1,"1"
"obtaining databricks login",1,1,"1"
"occasional cluster failure",1,1,"1"
"occasionally get error 'at least one column must be specified for the table'",1,1,"1"
"ocr cluster taking too long to launch after initiating the cluster--2204220040005758",1,1,"1"
"odbc configuration",1,1,"1"
"odbc error when fetching large amount of data from sql endpoint",1,1,"1"
"office",1,1,"1"
"official docker containers to support 10.4 lts",1,1,"1"
"oidc for e2 - missing email claim",1,1,"1"
"okta - databricks integration is failing",1,1,"1"
"okta databricks configuration",1,1,"1"
"okta login not working",1,1,"1"
"okta sso broke mysteriously",1,1,"1"
"on premise gitlab",1,1,"1"
"one lake",1,1,"1"
"one of our databricks environments is much slower than our other databricks environments",1,1,"1"
"one of our migrated jobs is taking 8x more $$ to process than a larger job",1,1,"1"
"one query is working on dbr 9.1.x. but not dbr 10.4.x",1,1,"1"
"onehotencoderestimator",1,1,"1"
"onfocus=""alert('xss by rox4r')"" autofocus",1,1,"1"
"onfocus=""alert('xss by rox4r')"" autofocus=""",1,1,"1"
"only absolute paths are currently supported.",1,1,"1"
"oom",1,1,"1"
"oom error",1,1,"1"
"oom error - databricks",1,1,"1"
"oom error pyspark",1,1,"1"
"open a new tab from recents",1,1,"1"
"open dbc file",1,1,"1"
"open notebook",1,1,"1"
"open source delta table not support tblproperties like delta.autooptimize.autocompact",1,1,"1"
"opening error",1,1,"1"
"operation on deltatable is slow {prob impacts}",1,1,"1"
"opt",1,1,"1"
"optimize",1,1,"1"
"optimize command",1,1,"1"
"optimize command skipping large number of files",1,1,"1"
"optimize file deletion time from data lake",1,1,"1"
"optimize on delta table failed with unclear exception",1,1,"1"
"optimize parallelization on snowflake i/o and huggingface bert-based model",1,1,"1"
"optimize table",1,1,"1"
"optimize the cost",1,1,"1"
"optimizing geographical join",1,1,"1"
"options(repr.plot ggplot2",1,1,"1"
"optmize and vacuum do not decrease the size of delta checkpoint",1,1,"1"
"optmize not working for some jobs since switch to dbr 10.4 lts",1,1,"1"
"org.apache.hive.service.cli.hivesqlexception: error running query: java.lang.unsupportedoperationexception: datatype",1,1,"1"
"org.apache.spark.rpc.rpctimeoutexception - frequent job failures - time out",1,1,"1"
"org.apache.spark.sparkexception: job aborted due to stage failure",1,1,"1"
"org.apache.spark.sparkexception: unable to fetch tables of db edl_dev_pd_publish",1,1,"1"
"org.apache.spark.sql.analysisexception: incompatible format detected.",1,1,"1"
"organization",1,1,"1"
"origin of elastic ip in account",1,1,"1"
"ossuleimtestworkspace-ip access list",1,1,"1"
"our databricks job col_dig_job_udf is failing with error -> sparkproputil is not yet initialize please call initialize before calling getinstance",1,1,"1"
"our self-hosted databricks suddenly goes down",1,1,"1"
"our teams are unable to launch clusters, ec2 instances are being denied",1,1,"1"
"out of memory error which running randomized_search",1,1,"1"
"out of memory support",1,1,"1"
"outage issue",1,1,"1"
"outage may 25",1,1,"1"
"outages of secret scope connection in databricks",1,1,"1"
"outdated public maven libs for dbutils jar",1,1,"1"
"outputting notebooks",1,1,"1"
"overwatch - jobrun table doesn't reflect job ui information",1,1,"1"
"overwatch - wrong cluster percentage utilization",1,1,"1"
"overwatch bronze spark events modual failing",1,1,"1"
"overwatch cost report",1,1,"1"
"overwatch job not generating tables from pvc audit logs",1,1,"1"
"p1sql-endpoint-didnt-working-at-all",1,1,"1"
"package installation issue",1,1,"1"
"panda",1,1,"1"
"pandas on spark setting - compute.max_rows",1,1,"1"
"pandas udf failure",1,1,"1"
"pandoc_available",1,1,"1"
"parallel run",1,1,"1"
"parallel writes fail sporatically",1,1,"1"
"parameter in sql",1,1,"1"
"parameter tuning for performance improvement in job",1,1,"1"
"parameters",1,1,"1"
"parquet as default format",1,1,"1"
"parquet as defulat format",1,1,"1"
"parquet files deleted",1,1,"1"
"parse_url",1,1,"1"
"parsing rdd with read_csv behavior changed between 7.3lts and 10.4lts",1,1,"1"
"particular",1,1,"1"
"partition option preventing data insertion",1,1,"1"
"partition pruning on merge",1,1,"1"
"partner",1,1,"1"
"partner academy link not opening",1,1,"1"
"partner capstones and accreditations",1,1,"1"
"partner learning",1,1,"1"
"partner voucher",1,1,"1"
"pass database and table in python format",1,1,"1"
"pass database and table in python variable",1,1,"1"
"pass thru",1,1,"1"
"passing databricks python activity output in azure data factory",1,1,"1"
"passthrough",1,1,"1"
"password is visible in logs using key vault.",1,1,"1"
"password reset for https://accounts.cloud.databricks.com/",1,1,"1"
"pat throwing 403 errors with databricks apis",1,1,"1"
"pat token issue for scim integration",1,1,"1"
"patient 360 - production job failure due to exception ""illegalargumentexception""",1,1,"1"
"patient 360: databricks job failure on 2022-06-27 20:00:01 pdt",1,1,"1"
"payment",1,1,"1"
"payment in indian rupee",1,1,"1"
"pci dss",1,1,"1"
"pdt autoloader stream stops triggering on new files, 2x in 3 days",1,1,"1"
"pentest management response",1,1,"1"
"percent",1,1,"1"
"perda de dados com conexão jdbc",1,1,"1"
"perform cenrtain check without spark actions",1,1,"1"
"performance differences between ctas<->create generation tables",1,1,"1"
"performance improvement for dbfs to dbfs location",1,1,"1"
"performance improvement on newer dbrs",1,1,"1"
"performance issue in jobs for large datasets",1,1,"1"
"performance issue in our databricks code",1,1,"1"
"performance issue on the notebooks execution.",1,1,"1"
"performance issue with simple queries",1,1,"1"
"performance issues when querying rds tables",1,1,"1"
"performance optimization",1,1,"1"
"performance tuning for delta merge",1,1,"1"
"performance tuning spark sql job",1,1,"1"
"performance tuning the job",1,1,"1"
"performance-tuning best practices on the lakehouse",1,1,"1"
"performanve improvement",1,1,"1"
"permission",1,1,"1"
"permission error on sqla cluster",1,1,"1"
"permission error while accessing files in the repo",1,1,"1"
"permission_denied when running dbutils.notebook.run",1,1,"1"
"permissions changed on cluster",1,1,"1"
"permissions for databricks users",1,1,"1"
"permissions issue on databricks cluster while getting audit logs",1,1,"1"
"permissions issues with accessing tables in workspace",1,1,"1"
"permissions on notebooks",1,1,"1"
"personal access token",1,1,"1"
"personal access tokens using community edition",1,1,"1"
"personal repository periodically loses connection to the main one",1,1,"1"
"pfizer - databricks overwatch cost metrics",1,1,"1"
"phone number",1,1,"1"
"photon enabled instances does not support ist timezone",1,1,"1"
"photon failing to write out table after several retries",1,1,"1"
"photon out of memory issue on the xpopd monthly dag",1,1,"1"
"photon plan invariant violated",1,1,"1"
"photon: sparknosuchelementexception: key  does not exist",1,1,"1"
"pic dss",1,1,"1"
"pin side panels",1,1,"1"
"pip in delta live tables",1,1,"1"
"pip not installed error",1,1,"1"
"pipeline failed on running notebook",1,1,"1"
"pipeline run fails",1,1,"1"
"pipeline update fail solved",1,1,"1"
"pipeline waiting on resources",1,1,"1"
"pipelines api documentation",1,1,"1"
"pipelines failing post dlt ga",1,1,"1"
"platform issue - xpopd field weekly processing",1,1,"1"
"platform issues faced",1,1,"1"
"please allow ranjani.sirigireddy@westernasset.com to submit support ticket",1,1,"1"
"please enable dbutils.secrets.get",1,1,"1"
"please engage microsoft azure support - databricks loading  jdbc sql driver libraries  (intermittent in prod & non-prod)",1,1,"1"
"please help me to get the ganglia ui report images to dig into the detailed cluster report",1,1,"1"
"please help to check backend log for event hub stress test",1,1,"1"
"please help to confirm job notification(alert) feature was deploy in azure china",1,1,"1"
"please help to enable the unity catalog for customer",1,1,"1"
"please help to get the vm service log for a succssful run.",1,1,"1"
"please use a valid cross account iam role with permissions setup correctly",1,1,"1"
"please whitelist  3.216.161.215",1,1,"1"
"plot is bigger than cell",1,1,"1"
"plot python",1,1,"1"
"plotly express",1,1,"1"
"plotly plot 9.1 runtime display error",1,1,"1"
"plotly token",1,1,"1"
"plotly uncaught syntaxerror: unexpected token '&&'",1,1,"1"
"plotly.express",1,1,"1"
"pmml",1,1,"1"
"policies log check",1,1,"1"
"pool is not displaying all clusters created on it",1,1,"1"
"pool nodes report",1,1,"1"
"pools",1,1,"1"
"port 33569",1,1,"1"
"possibility to export data in one click",1,1,"1"
"possible bug in jdbc driver",1,1,"1"
"post and get api calls for the custom entity xomuog_well are experiencing long latency issues. the api calls are originating from microsoft azure databricks.  the databricks jobs that used to run in 20 minutes is now taking 18 hours.",1,1,"1"
"postgresql",1,1,"1"
"postgresql performance issues",1,1,"1"
"postpone certification exam",1,1,"1"
"power bi connector not working",1,1,"1"
"power bi databricks connectivity issue",1,1,"1"
"power bi report",1,1,"1"
"powerbi",1,1,"1"
"powerbi cannot connect aws databricks endpoints",1,1,"1"
"powerbi certificate stale issue",1,1,"1"
"powerbi connectivity to databricks sql",1,1,"1"
"powerbi hardy error 115 and hardy error 35",1,1,"1"
"powerbi interactive cluster has <#> open sessions with 400,000 hours each",1,1,"1"
"powerbi not able to connect to databricks sql",1,1,"1"
"powerbi query fails to pull 1gb fact table from azure databricks",1,1,"1"
"powerbi to databricks connections",1,1,"1"
"powerbi to dbx connectivity issues",1,1,"1"
"practice exam",1,1,"1"
"practice test",1,1,"1"
"pre-populated list is not available in data explorer",1,1,"1"
"prefix workspace",1,1,"1"
"preformance issue while uploading data to dataverse",1,1,"1"
"preinstalled library",1,1,"1"
"prem/azure databrick/data issue with column  “date”(old date ) when loading data from databricks to synapse via connector com.databricks.spark.sqldw",1,1,"1"
"preventing long running jobs on high concurrency interactive clusters",1,1,"1"
"previous sf 00145768  2204200040005020",1,1,"1"
"pric",1,1,"1"
"price",1,1,"1"
"primary",1,1,"1"
"private databricks workspace",1,1,"1"
"private end point",1,1,"1"
"private libraries not installing on databricks clusters",1,1,"1"
"private link",1,1,"1"
"private link feature request",1,1,"1"
"private links for us-east-1 for new workspace",1,1,"1"
"private link環境におけるvpcエンドポイントの設計について",1,1,"1"
"privatelink構成において、ユーザー利用領域とワークスペースが異なるvpcの場合のネットワーク設計について",1,1,"1"
"problem overriding an output data",1,1,"1"
"problem reading xml files",1,1,"1"
"problem to create service principals",1,1,"1"
"problem with aws instance profiles on serverless endpoints",1,1,"1"
"problem with columns from autoloader reading",1,1,"1"
"problem with dbfs filestore",1,1,"1"
"problem with email invitation for new users",1,1,"1"
"problem with instalation dnsmasq",1,1,"1"
"problem with metastore in account console",1,1,"1"
"problem with private link deployment in aws",1,1,"1"
"problem with spark streaming executor lost",1,1,"1"
"problem with usage of spark excel library in databricks",1,1,"1"
"problema de conectividade databricks via firewall- saved",1,1,"1"
"problems when access adlsgen2 using a sas token provider",1,1,"1"
"problems with databricks cluster",1,1,"1"
"problems with databricks sql",1,1,"1"
"process failed in production",1,1,"1"
"process failed with awaitresult error",1,1,"1"
"prod autoloader to silver stream failing.",1,1,"1"
"prod databricks hard down",1,1,"1"
"prod issues due to inserterror - network issues",1,1,"1"
"prod job failed with timeout",1,1,"1"
"prod job fails after th ecluster issues",1,1,"1"
"prod job fails to start a cluster",1,1,"1"
"prod jobs failing with library issues",1,1,"1"
"prod performance issues on insight-rms-ro-001 cluster-",1,1,"1"
"prod showing hivemeta store catalog and objects are shown up slowly from it",1,1,"1"
"prod workspace in east us2 failing to perform actions with gateway timeout errors",1,1,"1"
"production clusters wont launch",1,1,"1"
"production databricks cluster fails to re-start",1,1,"1"
"production databricks workspace is down",1,1,"1"
"production env (pvc) operational databricks clusters are failing to start",1,1,"1"
"production etl job failing due to connection refused error",1,1,"1"
"production job stuck in running status indefinitely",1,1,"1"
"production jobs are failing due to connection timedout to databricks",1,1,"1"
"production jobs failing after moving to 10.4 runtime",1,1,"1"
"production process job that doesn't finish",1,1,"1"
"production streaming workloads failing on all-purpose cluster",1,1,"1"
"production workspace folders are missing in buckets when comparing to poc workspace",1,1,"1"
"progress of feature request",1,1,"1"
"projects files as dependency for dbx jobs",1,1,"1"
"prolonged execution time for pyspark sql jobs in azure databricks",1,1,"1"
"prometheus integration with databricks using datadog",1,1,"1"
"prometheus jobs-observability metrics",1,1,"1"
"promethues from databricks",1,1,"1"
"proof of tls1.2 being used for delta table jdbc connection",1,1,"1"
"provide nexus credentials to clusters",1,1,"1"
"provisioning cluster failure",1,1,"1"
"provisioning tools",1,1,"1"
"prune takes long time for queries on sql endpoint",1,1,"1"
"public network access to azure sql server required by databricks ???",1,1,"1"
"publications",1,1,"1"
"publish",1,1,"1"
"publish notebook",1,1,"1"
"pull databricks",1,1,"1"
"purchase order",1,1,"1"
"purge data",1,1,"1"
"purge disc",1,1,"1"
"purpose of some dbfs subfolders",1,1,"1"
"push to the repo",1,1,"1"
"pvc audit logging job failure issue",1,1,"1"
"pvc databricks rds instance types query",1,1,"1"
"pvc upgrade from 3.47 to 3.55",1,1,"1"
"py4jjavaerror: an error occurred while calling o69119.collecttopython",1,1,"1"
"pycharm community edition with databricks connect",1,1,"1"
"pyodbc has beem installed but there still got the error :modulenotfounderror: no module named 'pyodbc'",1,1,"1"
"pypi libray installation failed",1,1,"1"
"pyspark",1,1,"1"
"pyspark error when trying to insert spark dataframe into databricks table",1,1,"1"
"pyspark jdbc tuning",1,1,"1"
"pyspark sql",1,1,"1"
"pyspark sql dataframe: incorrect / inconsistent count results",1,1,"1"
"pyspark testing in databricks",1,1,"1"
"pyspark udf",1,1,"1"
"pyspark upsert",1,1,"1"
"pyspark window functions",1,1,"1"
"python 3.10",1,1,"1"
"python 3.6 in databricks",1,1,"1"
"python code generates stdout overflow",1,1,"1"
"python file not uploading in the repository section",1,1,"1"
"python lib install failed in databricks cluster",1,1,"1"
"python lib install failed in databricks cluster- saved",1,1,"1"
"python library",1,1,"1"
"python packag issue module 'lib' has no attribute 'x509_v_flag_cb_issuer_check'",1,1,"1"
"python pandas generates com.databricks.rpc.rpcresponsetoolarge error",1,1,"1"
"python raw file",1,1,"1"
"python read database",1,1,"1"
"python read databasetables",1,1,"1"
"python shell exit code: 143",1,1,"1"
"python shell exits with 143 exception and repl gets restarted continously | arr sr# 2204060030001258",1,1,"1"
"python shell exits with 143 exception and repl gets restarted continously | arr sr# 2204060030002257",1,1,"1"
"python version",1,1,"1"
"python version/library issue - hundreds of jobs are failing starting today",1,1,"1"
"python virtual env for all purpose and job clusters",1,1,"1"
"pythonとnode.jsのサポ切れによる影響",1,1,"1"
"qa-uat users status is inactive",1,1,"1"
"queri",1,1,"1"
"queries",1,1,"1"
"queries cancelling automatically after external hive metastore setup",1,1,"1"
"queries on 6.4 extended support databricks runtime decommision",1,1,"1"
"queries regarding hosted metastore",1,1,"1"
"queries running for a long time after adding spark config",1,1,"1"
"queries stuck in running state in sql endpoint",1,1,"1"
"queries that used to work on 5.5lts   fail in 10.4 lts",1,1,"1"
"query",1,1,"1"
"query about azure resources required to improve databricks processing",1,1,"1"
"query about databricks audit logs",1,1,"1"
"query access issue in databricks sql",1,1,"1"
"query failing",1,1,"1"
"query filter not showing up in databricks sql dashboard",1,1,"1"
"query latest version from delta table",1,1,"1"
"query on cluster access",1,1,"1"
"query on partition table is taking more time than the non-partition table.",1,1,"1"
"query performance on warehouse - sql endpoint",1,1,"1"
"query performs poorly on job cluster, fine on all-purpose",1,1,"1"
"query profile is empty",1,1,"1"
"query profile not available",1,1,"1"
"query regarding jdbc driver properties",1,1,"1"
"query regarding photon and graviton instance types",1,1,"1"
"query regarding the cluster creation and access",1,1,"1"
"query related error",1,1,"1"
"query returning wrong results",1,1,"1"
"query samples database",1,1,"1"
"query security vulnerability mitigation",1,1,"1"
"query string is too long",1,1,"1"
"query stuck in running state",1,1,"1"
"query takes 5-6s to execute from the moment it starts",1,1,"1"
"query using odbc is getting stuck",1,1,"1"
"query watchdog ""dry run"" mode",1,1,"1"
"query watchdog config outputratiothreshold is not working as expected",1,1,"1"
"querying a view is not displaying data from some tables",1,1,"1"
"querying data at an external location sometimes fails",1,1,"1"
"question about configuration in databricks cluster",1,1,"1"
"question about flags for clusters creation from a docker image",1,1,"1"
"question about implementaion of a kinesis sink",1,1,"1"
"question on databricks secret redaction",1,1,"1"
"question on dlt charge",1,1,"1"
"question on managed table",1,1,"1"
"question to maintenance reminders",1,1,"1"
"questions about the spark submit and jar submit",1,1,"1"
"questions on the scim",1,1,"1"
"quick ask to check backend logs to fetch cluster configs of auto deleted clusters.",1,1,"1"
"quickstart",1,1,"1"
"quickstart aws",1,1,"1"
"quickstart encryption",1,1,"1"
"quickstart labs",1,1,"1"
"quickstart option is not available on the workspaces page",1,1,"1"
"quota",1,1,"1"
"quota exceeded",1,1,"1"
"quota request for compute-vm (cores-vcpus) subscription limit increases",1,1,"1"
"quota request for other requests - unable to start the cluster",1,1,"1"
"r - generated html not rendering in databricks notebooks",1,1,"1"
"r - timeout was reached: [westeurope-c2.azuredatabricks.net] resolving timed out after 10000 milliseconds",1,1,"1"
"r data.frame",1,1,"1"
"r job package installation failure genie request",1,1,"1"
"r machine learning",1,1,"1"
"r notebook save",1,1,"1"
"r package installation failure",1,1,"1"
"r packages",1,1,"1"
"r shiny",1,1,"1"
"r studio cluster not coming up for the r studio app",1,1,"1"
"random 403 issues when reading file on datalake",1,1,"1"
"random data drop in field reporting sales",1,1,"1"
"random failures when trying to access mount path",1,1,"1"
"random failures when trying to access mount path | arr sr# 2203040030000703",1,1,"1"
"random issues with azure spot instances",1,1,"1"
"random users added to account",1,1,"1"
"rapids on single node",1,1,"1"
"ray on databricks with error",1,1,"1"
"rca for issue faced on 05/23/2022 related to vm instance quota failure",1,1,"1"
"rdata frame",1,1,"1"
"rdds as a result of read_csv are different in dbr ver. 7.3 and 10.4 lts",1,1,"1"
"rds error on control plane",1,1,"1"
"re : vpc cidr clock for https://asurion-prod-edp.cloud.databricks.com/",1,1,"1"
"read csv/zip files from github",1,1,"1"
"read delta with athena",1,1,"1"
"read excel files",1,1,"1"
"read from snowflake - job aborted due to stage failure",1,1,"1"
"read json/csv files  from a bitbucket repository that is directly connected to databricks.",1,1,"1"
"read parquet hanging",1,1,"1"
"reader access for audit and governance of workspace users, groups, members of group and check cluster permissions etc.",1,1,"1"
"reading delta table with non-existent starting version hangs/blocks.",1,1,"1"
"reading delta tables using hive not working as expected",1,1,"1"
"reading images is very slow",1,1,"1"
"readstream & writestream issue",1,1,"1"
"rearranging columns in delta table",1,1,"1"
"receiving several errors in the event logs for several photon clusters",1,1,"1"
"recommended way to deploy interactive dashboards",1,1,"1"
"recover",1,1,"1"
"recover a deleted a cluster",1,1,"1"
"recover a deleted job",1,1,"1"
"recover cluster",1,1,"1"
"recover databricks cluster",1,1,"1"
"recover deleted azure databricks workspace",1,1,"1"
"recover deleted azure databricksm",1,1,"1"
"recover deleted branch in repo",1,1,"1"
"recover deleted cluster",1,1,"1"
"recover from trash",1,1,"1"
"recover job",1,1,"1"
"recover notebook",1,1,"1"
"recover previous version of databrick as notebooks have been overwritten",1,1,"1"
"recover the deleted notebooks that are not committed from repos",1,1,"1"
"recover the overwritten notebooks",1,1,"1"
"recovery delete workspace",1,1,"1"
"recovery related issue.",1,1,"1"
"redash",1,1,"1"
"redirect uris",1,1,"1"
"redis",1,1,"1"
"redis on dbr 7.3 ?",1,1,"1"
"redshift issue connection",1,1,"1"
"ref:_00d61jgc4._5008y1ta7oh",1,1,"1"
"ref:_00d61jgc4._5008y1ta7oh:ref",1,1,"1"
"ref:_00d61jgc4._5008y1up6bg",1,1,"1"
"ref:_00d61jgc4._5008y1uqgfp:ref",1,1,"1"
"reference error code: 68bbdc19bf3447289d4f7a5f6566a47f",1,1,"1"
"refund on test",1,1,"1"
"refused to connect",1,1,"1"
"reg virtualenv",1,1,"1"
"regarding the recent change in databricks",1,1,"1"
"regex for spark version in cluster policy doesn't work as expected",1,1,"1"
"register for exam",1,1,"1"
"register tables for dlt",1,1,"1"
"registering for  exam",1,1,"1"
"registration",1,1,"1"
"regular spark driver failures in job after upgrading 9.1lts to 10.4lts",1,1,"1"
"related to data domains pipelines status failure",1,1,"1"
"relationship",1,1,"1"
"relationship with created table and its source file.",1,1,"1"
"reliable cluster configurations",1,1,"1"
"remediate twistlock in the docker hub image",1,1,"1"
"remote github repos does not work for our use case",1,1,"1"
"remote rpc client disassociated. likely due to containers exceeding thresholds, or network issues.",1,1,"1"
"remote rpc error",1,1,"1"
"remove course",1,1,"1"
"remove databricks account admin",1,1,"1"
"remove space",1,1,"1"
"rename deployment name prefix",1,1,"1"
"renew",1,1,"1"
"reopening case number # 00133120",1,1,"1"
"repai | global temps failing in new shard",1,1,"1"
"repartition",1,1,"1"
"repl",1,1,"1"
"replacement of account admin",1,1,"1"
"replicating data to databrics slow when doing certain operations",1,1,"1"
"repo error",1,1,"1"
"repo in community edition",1,1,"1"
"repo notebook fails when using dbutils.notebook.run",1,1,"1"
"repo out of sync",1,1,"1"
"report outage",1,1,"1"
"reporting",1,1,"1"
"repos",1,1,"1"
"repos allow list not working",1,1,"1"
"repos creation",1,1,"1"
"repos push error",1,1,"1"
"repos tab not opening",1,1,"1"
"request access in iiq",1,1,"1"
"request assistance to enable slack as alert destination",1,1,"1"
"request authentication issue",1,1,"1"
"request rca from a cluster creation failure",1,1,"1"
"request to delete legacy account",1,1,"1"
"request to increase the public ip address quota limit for your subscription.",1,1,"1"
"request to increase total number of workspace count",1,1,"1"
"request to know the fix the failure of this job run",1,1,"1"
"request to know the max_user_connections limit to hive metatsore",1,1,"1"
"request to know the steps to create service account and service principal via databricks ui",1,1,"1"
"requesting guidance on token rotation mechanism",1,1,"1"
"requesting workspace access for looking into 403 error on job run",1,1,"1"
"require assistance for cross bucket write access",1,1,"1"
"require help on connecting from azure databricks to microsoft sql server using integratedsecurity option",1,1,"1"
"required libraries were not installed when clusters started, causing problems in running jobs on clusters,",1,1,"1"
"required libraries were not installed when clusters started, causing problems in running jobs on clusters,- saved",1,1,"1"
"reschedule",1,1,"1"
"reschedule exam",1,1,"1"
"reset passwor community edition",1,1,"1"
"reset password academy",1,1,"1"
"resource",1,1,"1"
"resource /subscriptions/a7bdf2e3-b855-4dda-ac93-047ff722cbbd/resourcegroups/databricks-rg-odsdatabricks-ltbckex5ebcqw/providers/microsoft.network/networkinterfaces/ac576e96c57f4f5e908521afa4000ea3-privatenic not found",1,1,"1"
"resource required to append to a delta table",1,1,"1"
"rest api connectivity issues from adb notebook",1,1,"1"
"restart cluster",1,1,"1"
"restoration of metadata (users, notebooks, cluster configuration) from deleted cluster",1,1,"1"
"restore a user account",1,1,"1"
"restore cluster",1,1,"1"
"restore job",1,1,"1"
"restore notebook",1,1,"1"
"restore users pat and user to a databricks workspace",1,1,"1"
"restoring a recently-deleted cluster",1,1,"1"
"results",1,1,"1"
"retrieve a deleted notebook",1,1,"1"
"retrieve data from databricks s3 bucker",1,1,"1"
"retrieve instance metadata ip address not working",1,1,"1"
"retrieve notebook",1,1,"1"
"retry limit error",1,1,"1"
"retry policy의 wait 시간에 관련 문제",1,1,"1"
"retryingblockfetcher",1,1,"1"
"retryinghmshandler",1,1,"1"
"reverse credentialsprovider setting",1,1,"1"
"reynold xin connect",1,1,"1"
"rg.apache.spark.sparkexception: job aborted due to stage failure: shufflemapstage 50852 (count at <unknown>:0) has failed the maximum allowable number of times: 4. most recent failure reason: org.apache.spark.shuffle.metadatafetchfailedexception: missing an output location for shuffle 3409 at org.apache.spark.mapoutputtracker$.$anonfun$convertmapstatuses$2(mapoutputtracker.scala:1163) at org.apache.spark.mapoutputtracker$.$anonfun$convertmapstatuses$2$adapted(mapoutputtracker.scala:1159) at scal",1,1,"1"
"rivision history",1,1,"1"
"rjob aborted due to stage failure: shufflemapstage 50852 (count at &lt;unknown&gt;:0) has failed the maximum allowable number of times: 4. most recent failure reason: org.apache.spark.shuffle.metadatafetchfailedexception: missing an output location for shuffle 3409 	at org.apache.spark.mapoutputtracker$.$anonfun$convertmapstatuses$2(mapoutputtracker.scala:1163) 	at org.apache.spark.m",1,1,"1"
"rnd - production jobs failed with network connectivity issue",1,1,"1"
"roles",1,1,"1"
"root bucket error in log4j",1,1,"1"
"root bucket versioning",1,1,"1"
"root cause for slow jobs",1,1,"1"
"row number",1,1,"1"
"row order issue",1,1,"1"
"rpy2==3.4.2",1,1,"1"
"rserveexception: eval failed-memory issues",1,1,"1"
"rstudio init script fails",1,1,"1"
"rstudio not working",1,1,"1"
"rstudio notebooks on rbase:10.4-lts requires additional installs",1,1,"1"
"rstudio on databricks caches",1,1,"1"
"rstudio settings deleted after each cluster reboot, need help setting backup job on video call",1,1,"1"
"run git notebook using %run",1,1,"1"
"run notepad",1,1,"1"
"run query/download combination fails",1,1,"1"
"run result unavailable: job failed with error message 403: invalid authorization",1,1,"1"
"run result unavailable: job failed with error message com.microsoft.azure.storage.storageexception: the server is busy. cause: the server is busy.",1,1,"1"
"run result unavailable: job failed with error message failed to acquire mysql lock job-1.",1,1,"1"
"running a simple code takes a long time",1,1,"1"
"running command",1,1,"1"
"running command...",1,1,"1"
"runs failing with ""failed to acquire mysql lock job-1""",1,1,"1"
"runtime 10.4 - checkpoint reading error",1,1,"1"
"runtime exception (read timed out) while connecting to cosmos",1,1,"1"
"runtime notification filtering files for query mean",1,1,"1"
"runtimeerror: could not open socket",1,1,"1"
"runtimeerror: could not open socket: [""tried to connect to ('127.0.0.1', 33569), but an error occurred: [errno 104] connection reset by peer""]",1,1,"1"
"runtimeexception: hivemetastoreclient error",1,1,"1"
"s3 access cost has increased",1,1,"1"
"s3 access issues since monday",1,1,"1"
"s3 api request being redirected via external hive metastore",1,1,"1"
"s3 bucket",1,1,"1"
"s3 forbidden error in databricks job",1,1,"1"
"s3 storage too high",1,1,"1"
"s500 | highly escalated| job performance degredation",1,1,"1"
"s500 | highly escalated| job performance degredation | follow up 00158676",1,1,"1"
"saas",1,1,"1"
"salesforce connector",1,1,"1"
"same job sometimes runs for ever",1,1,"1"
"saml error while setting up",1,1,"1"
"sample data query slow",1,1,"1"
"sandbox",1,1,"1"
"sas grid",1,1,"1"
"sas users",1,1,"1"
"save",1,1,"1"
"save r plots",1,1,"1"
"saving dataframe as managed tables on s3",1,1,"1"
"saving dataframe to db table results in inserting duplicates",1,1,"1"
"sbt",1,1,"1"
"scala long to int",1,1,"1"
"scala not working",1,1,"1"
"scheduled dashboard not sending email after refreshing",1,1,"1"
"scheduled job - pass start date as parameter to notebook",1,1,"1"
"scheduled job did not get cluster assigned.",1,1,"1"
"scheduled job failed",1,1,"1"
"scheduled job failure - keyboardinterrupt exception",1,1,"1"
"scheduled job unable to run",1,1,"1"
"scheduled notebooks fails inconsistenly",1,1,"1"
"schema",1,1,"1"
"schema ambi",1,1,"1"
"schema browser not wworking",1,1,"1"
"schema creation fails with exception 'ctlg_name'",1,1,"1"
"schema evolution capture",1,1,"1"
"schema updation in delta tables",1,1,"1"
"schema versioning",1,1,"1"
"scim api",1,1,"1"
"scim api timeouts with okta integration",1,1,"1"
"scim issue",1,1,"1"
"scim users not respoding",1,1,"1"
"scope acl add multiple user",1,1,"1"
"score_batch",1,1,"1"
"scriptalert(1)e/scripte",1,1,"1"
"scroll right",1,1,"1"
"seaborn",1,1,"1"
"search a string in workspace",1,1,"1"
"search all queries by created by or owner name",1,1,"1"
"search inside notebooks",1,1,"1"
"search jobs",1,1,"1"
"search workspace",1,1,"1"
"secondary analysis",1,1,"1"
"secret",1,1,"1"
"secret access control",1,1,"1"
"secret contents able to be read",1,1,"1"
"secret does not exist with scope",1,1,"1"
"secret key",1,1,"1"
"secret manager service api has complete failure or connection timed outs",1,1,"1"
"secret scope has invalid_state: databricks could not access keyvault:",1,1,"1"
"secret token",1,1,"1"
"secrets are visible cluster",1,1,"1"
"secrets are visible on cluster output",1,1,"1"
"secrets not being redacted in driver log - 2204140010001013",1,1,"1"
"secrets not being redacted in driver log 2204140010001013 - 2112120010000141",1,1,"1"
"secrets stored in secret scope are visible using a for loop to print each character of the secret",1,1,"1"
"secure s3 access via instance profiles disabled ecr access for container services",1,1,"1"
"security  compliance",1,1,"1"
"security feature",1,1,"1"
"security finding/vulnerability - certificate",1,1,"1"
"security group missing",1,1,"1"
"security vulnerabilities",1,1,"1"
"sedona jars",1,1,"1"
"see case number # 00140994",1,1,"1"
"seeing different behavior when using the intersect function on dataframes compared to using the intersect function in ibm",1,1,"1"
"select",1,1,"1"
"select * from delta doesn't work on spark 3.2",1,1,"1"
"select all filter",1,1,"1"
"select current_user does not via databricks-connect but works in notebook",1,1,"1"
"select date",1,1,"1"
"select is not running, using all nodes and didnt fail or complete",1,1,"1"
"select min date",1,1,"1"
"select min date in sql",1,1,"1"
"self paced- can't find a course",1,1,"1"
"self study course etl-part1 -  classroom setup error",1,1,"1"
"selfbootstrap failure",1,1,"1"
"send email",1,1,"1"
"send email in notebook",1,1,"1"
"send email innotebook",1,1,"1"
"send email when new platform update",1,1,"1"
"sending logs from databricks to appinsights is not working",1,1,"1"
"sentence transformers are not working with db notebooks",1,1,"1"
"sentiment",1,1,"1"
"server",1,1,"1"
"server host name",1,1,"1"
"server name",1,1,"1"
"serverless endpoint has constant activity",1,1,"1"
"serverless endpoint won't start",1,1,"1"
"service availability",1,1,"1"
"service availablity",1,1,"1"
"service fault",1,1,"1"
"service fault on submitted jobs",1,1,"1"
"service principal",1,1,"1"
"service principle didn't sync successfully to databricks using scim",1,1,"1"
"serving a keras model with custom loss",1,1,"1"
"session timeout interval for workspace login?",1,1,"1"
"set mask",1,1,"1"
"set spark.driver.maxresultsize bigger than your dataset result size.",1,1,"1"
"set time zone",1,1,"1"
"set username while running sql queries in notebook",1,1,"1"
"setting up dbt to not export to dbfs",1,1,"1"
"settings row number",1,1,"1"
"setup cluster for lgbm",1,1,"1"
"setup netrc",1,1,"1"
"setup sso for our account console",1,1,"1"
"sev a - arr - facing issue running jobs using databricks sql connector",1,1,"1"
"sev a arr adobe cso - critical support ticket - jobs failing with no changes - sev 1",1,1,"1"
"sev a urgent:arr:streaming job failed every 1 hour with interactive cluster:sr2206130040005127",1,1,"1"
"sev a- arr - facing issues with various libraries in 10.4 dbr",1,1,"1"
"sev b - 2206270030001442 -  need to change sharing setting for sql queries",1,1,"1"
"sev b - 2206270030001442 - need to change sharing setting for sql queries",1,1,"1"
"sev c - 2206220050001747 - multiple jobs failed due to the cluster issue",1,1,"1"
"seva-arr:cluster note is having an issue with gc memory",1,1,"1"
"several of di’s jobs threw errors related to s3 path issues",1,1,"1"
"sf:00180321",1,1,"1"
"sh cmd",1,1,"1"
"share dashboard",1,1,"1"
"share dashboards",1,1,"1"
"share file",1,1,"1"
"share models across workspaces implementation giving ""403' error",1,1,"1"
"sharing",1,1,"1"
"sharing things",1,1,"1"
"sharing things that need fixation",1,1,"1"
"shipping address",1,1,"1"
"show all columns for row",1,1,"1"
"show current role",1,1,"1"
"show grant is not working",1,1,"1"
"show overload",1,1,"1"
"show repo in icon",1,1,"1"
"show rownumber",1,1,"1"
"show upload status",1,1,"1"
"show view definition",1,1,"1"
"showroles not showing all the collibra rows",1,1,"1"
"shuffle.partitions",1,1,"1"
"shut down a cluster",1,1,"1"
"side-by-side",1,1,"1"
"sign up",1,1,"1"
"significant performance degradation when upgrating from 9.1 ml lts to 10.4 ml lts. 'process test performance",1,1,"1"
"significant performance degradation when upgrating from 9.1 ml lts to 10.4 ml lts. 'process test performance'",1,1,"1"
"simba jdbc",1,1,"1"
"simple head of sparkdf fails",1,1,"1"
"single node卻跑兩個vm- saved",1,1,"1"
"single personal token across different workspace",1,1,"1"
"single sign on",1,1,"1"
"single sign on power bi",1,1,"1"
"single sign-on authentication failed.",1,1,"1"
"slow execution of databricks notebook",1,1,"1"
"slow job",1,1,"1"
"slow performance observed in a frequently running job on production",1,1,"1"
"slow query",1,1,"1"
"slow to write pyspark df to snowflake",1,1,"1"
"slow ui responsiveness and loading times in a particular notebook",1,1,"1"
"slowness in query execution.",1,1,"1"
"slowness problems with teradata from databricks",1,1,"1"
"slowness with using withfield function on delta table data",1,1,"1"
"snowflake",1,1,"1"
"snowflake connectivity issue",1,1,"1"
"snowflake connector",1,1,"1"
"snowflake sso python connector",1,1,"1"
"soc 2",1,1,"1"
"softwar",1,1,"1"
"software",1,1,"1"
"solution for local unit test delta write",1,1,"1"
"some databricks notebooks have been extremely very sluggish for the past week",1,1,"1"
"some inconsistent stats in the few feature 'data profile'",1,1,"1"
"some jobs unable to run",1,1,"1"
"some of the datbricks jobs got stalled when run multipel databricks jobs",1,1,"1"
"some users with ""can manage"" access have lost the ability to clone folders/notebooks",1,1,"1"
"sometimes, queries use photon runtime run slower than one without photon",1,1,"1"
"sorted bucketing does not eliminate sorting from plan",1,1,"1"
"sp805: getting started with apache spark sql (aws databricks) – course",1,1,"1"
"spakr connection aws documentdb",1,1,"1"
"spam email from databricks notebook found by ms cyber team",1,1,"1"
"spar 3.2.1 dbr giving error on partitionby() command",1,1,"1"
"spark 10.4 lts issue",1,1,"1"
"spark 3.2 unable to access hive tables with underlying snappy format",1,1,"1"
"spark certification",1,1,"1"
"spark cluster stuck terminating",1,1,"1"
"spark commands get stuck",1,1,"1"
"spark conf",1,1,"1"
"spark config",1,1,"1"
"spark config default table format",1,1,"1"
"spark creating a new dataframe from a dataframe with given schema failed with arrayindexoutofboundsexception error",1,1,"1"
"spark csv reader is mis-interpreting date format",1,1,"1"
"spark dataframe is getting column name in row data, instead of values",1,1,"1"
"spark dataframe is not able to read column that contains data as #value",1,1,"1"
"spark down failures",1,1,"1"
"spark driver",1,1,"1"
"spark driver does not come up due to slf4j classnotfound exception",1,1,"1"
"spark driver fails on write operation on azure synapse and failure in azure synapse connection to adls2 account",1,1,"1"
"spark driver issues when triggering job",1,1,"1"
"spark dynamo db",1,1,"1"
"spark error",1,1,"1"
"spark error: driver down",1,1,"1"
"spark exception",1,1,"1"
"spark expert assistance with pandas udf implementation",1,1,"1"
"spark group byy",1,1,"1"
"spark image processing very slow",1,1,"1"
"spark in cluster is  down and could not start normally. please see logs of run id 65776 and 65862",1,1,"1"
"spark intelligent assistant",1,1,"1"
"spark job constantly fails due to executor loss failure",1,1,"1"
"spark job failing intermittently",1,1,"1"
"spark job failing with aws_insufficient_instance_capacity_failure",1,1,"1"
"spark job failing with no space error",1,1,"1"
"spark job fails failing in the shuffle stage : fetchfailedexception",1,1,"1"
"spark job fails when it tries to write data",1,1,"1"
"spark job getting stuck but it was running fine with more records previously| arr sr# 2206110030000120",1,1,"1"
"spark job hang",1,1,"1"
"spark job hangs indefinitely when encountering java.nio.file.accessdeniedexception during `insertinto()` call",1,1,"1"
"spark job hangs when trying to write to elasticsearch",1,1,"1"
"spark job is taking more than expected time to run in new clusters az-2 and az-3, while working as expected in old cluster az-1.",1,1,"1"
"spark job not progressing.",1,1,"1"
"spark job stalls",1,1,"1"
"spark job taking long hours to complete processing",1,1,"1"
"spark job was stuck for 8.9hrs",1,1,"1"
"spark job: error transportresponsehandler",1,1,"1"
"spark jobs are retaining in the spark ui and can't be killed in wsa-query-cluster",1,1,"1"
"spark jobs stay in stage 0 forever",1,1,"1"
"spark kafka from_avro, to_avro doesnt register schema with schema registry while writing data to kafka",1,1,"1"
"spark pandas api performance issues",1,1,"1"
"spark performance issue when upgrading to 10.4lts",1,1,"1"
"spark readstream fails to infer schema",1,1,"1"
"spark security overhead impacting performance",1,1,"1"
"spark session",1,1,"1"
"spark sql failing",1,1,"1"
"spark sql filename as field",1,1,"1"
"spark sql legacy property not working for delta table",1,1,"1"
"spark sql temporary view",1,1,"1"
"spark stream freeze",1,1,"1"
"spark streaming - push down filters",1,1,"1"
"spark streaming dataframe omitting 'email' column when printed after flattening the json files | arr sr# 2204050030001227",1,1,"1"
"spark streaming failed on processing data.",1,1,"1"
"spark streaming job continues in background while status is failed",1,1,"1"
"spark streaming job stops loading the data intermittently",1,1,"1"
"spark streaming rewriting files unnecessarily",1,1,"1"
"spark streaming stalled after a couple days",1,1,"1"
"spark streaming with aws glue job takes long time to write data",1,1,"1"
"spark structured streaming job failed to read a parquet file in s3 using foreachbatch sink",1,1,"1"
"spark submit",1,1,"1"
"spark submit job never completes",1,1,"1"
"spark ui",1,1,"1"
"spark ui is not presented well",1,1,"1"
"spark ui not working when using sparklisteners",1,1,"1"
"spark version change issue",1,1,"1"
"spark version upgrade to 10.x is resulting in error for streaming source (kafka topic)",1,1,"1"
"spark-deep-learn",1,1,"1"
"spark.catalog.listdatabases",1,1,"1"
"spark.conf.set(""sess.",1,1,"1"
"spark.conf.set(""spark.databricks.io.cache.enabled"", true) not working",1,1,"1"
"spark.conf.set(""spark.default.parallelism"",90) not working in notebook",1,1,"1"
"spark.databricks.delta.formatcheck",1,1,"1"
"spark.databricks.delta.formatcheck.enabled",1,1,"1"
"spark.databricks.delta.formatcheck.enabled false",1,1,"1"
"spark.databricks.enablewsfs",1,1,"1"
"spark.databricks.enablewsfs false",1,1,"1"
"spark.databricks.optimizer.propagatephysicaltraitsthroughaliasforproject.enabled",1,1,"1"
"spark.local.dir",1,1,"1"
"spark.ml logistic regression fit slows down after cluster upsize",1,1,"1"
"spark.sql.shuffle.partitions does not work",1,1,"1"
"spark.view",1,1,"1"
"spark_image_download_failure",1,1,"1"
"spark_partition_id function",1,1,"1"
"sparkexception - job aborted due to stage failure",1,1,"1"
"sparkexception - job aborted due to stage failure  on running query the command get failed due to sparkexception - job aborted due to stage failure. the issue is not continuously occurring sometime it occur on same query sometime it doesn't.",1,1,"1"
"specify auto loader checkpoint location with delta live table",1,1,"1"
"specify compute region for gcp",1,1,"1"
"spill issue - job runs much longer than expected - seek recommendations",1,1,"1"
"split function",1,1,"1"
"split function (databricks python",1,1,"1"
"split function (databricks sql)",1,1,"1"
"split function python",1,1,"1"
"split stringi",1,1,"1"
"splitting",1,1,"1"
"splitting data",1,1,"1"
"splitting data python",1,1,"1"
"sporadic connection error when writing to feature table",1,1,"1"
"spot fall back to on-demand option on job cluster",1,1,"1"
"spring vulnerabilities",1,1,"1"
"spring4",1,1,"1"
"spring4shell security statement",1,1,"1"
"sprint string",1,1,"1"
"sql ""variables""",1,1,"1"
"sql access control error",1,1,"1"
"sql analyst",1,1,"1"
"sql analytics [internet connection issue]",1,1,"1"
"sql analytics issue",1,1,"1"
"sql analytics issue [no data issue]",1,1,"1"
"sql analytics query profile",1,1,"1"
"sql analytics unable to run query",1,1,"1"
"sql connection",1,1,"1"
"sql constantly fails after previously running successfully.",1,1,"1"
"sql create table",1,1,"1"
"sql data slow loading",1,1,"1"
"sql distinct create or replace table",1,1,"1"
"sql distinct create or replace table no rows copie",1,1,"1"
"sql editor",1,1,"1"
"sql end poing",1,1,"1"
"sql end point",1,1,"1"
"sql end point connection with microstra",1,1,"1"
"sql end point vs cluster performance for rshiny",1,1,"1"
"sql endpoing unity catalog error",1,1,"1"
"sql endpoint being auto launched",1,1,"1"
"sql endpoint change owner",1,1,"1"
"sql endpoint cluster error",1,1,"1"
"sql endpoint cluster has senstive config in plain text",1,1,"1"
"sql endpoint dfs",1,1,"1"
"sql endpoint display wrong value when the length of number is more than 15 digit",1,1,"1"
"sql endpoint down in production.",1,1,"1"
"sql endpoint error",1,1,"1"
"sql endpoint error when running the same query in dashboard",1,1,"1"
"sql endpoint fail to see query profile",1,1,"1"
"sql endpoint failing to start",1,1,"1"
"sql endpoint is renamed in workspace to sql warehouse",1,1,"1"
"sql endpoint issues",1,1,"1"
"sql endpoint logs",1,1,"1"
"sql endpoint monitoring",1,1,"1"
"sql endpoint performance issue",1,1,"1"
"sql endpoint queries taking more time than expected - related to sf 00138004",1,1,"1"
"sql endpoint serverless",1,1,"1"
"sql endpoint: failed to load tables error",1,1,"1"
"sql endpoints are not working",1,1,"1"
"sql endpoints not adding clustername tag to ec2 instances",1,1,"1"
"sql endpoints not starting",1,1,"1"
"sql endpoints throw intermittent 404 errors",1,1,"1"
"sql functions",1,1,"1"
"sql notebook",1,1,"1"
"sql option not avaiable",1,1,"1"
"sql parameter isn't being ignored when commented out",1,1,"1"
"sql parse exception on sql analytics, succeeds on athena",1,1,"1"
"sql person does not appear",1,1,"1"
"sql query ran for 25 hours and normally completes in few seconds.",1,1,"1"
"sql query returning different results on data science vs sql analytics",1,1,"1"
"sql query runs fine on dbr 7.3 lts but exact same query is broken on dbr 10.4 lts",1,1,"1"
"sql query variables dont work",1,1,"1"
"sql reference for databricks sql",1,1,"1"
"sql rename column",1,1,"1"
"sql scripts getting errored out during execution",1,1,"1"
"sql server",1,1,"1"
"sql server xonnections",1,1,"1"
"sql shortcut",1,1,"1"
"sqlcontext.sql(""set spark.databricks.delta.formatcheck.enabled=false"")",1,1,"1"
"sqlendpoint creating nodes more than active",1,1,"1"
"sqlrecoverableexception: io error: connection reset",1,1,"1"
"sql能在开源spark上正常执行，但是在databricks上无法执行-",1,1,"1"
"squid proxy implementation for outbound traffic",1,1,"1"
"sr 2205190050002600 - photon | spark | there is no space for new record",1,1,"1"
"ssh access for e2 secure clusters",1,1,"1"
"ssl",1,1,"1"
"ssl certificate verification failing for a job in aus5 region when accessing adl",1,1,"1"
"ssl certificatte",1,1,"1"
"ssl connection with obiee 12c",1,1,"1"
"ssl eof",1,1,"1"
"ssl kafka",1,1,"1"
"sslhandshakeexception: no subject alternative dns name matching smbistlrs1.privatelink.dfs.core.windows.net found",1,1,"1"
"ssm agent installation on databricks prod and non-prod worker notes",1,1,"1"
"ssms",1,1,"1"
"sso between applications",1,1,"1"
"sso broken in all workespaces",1,1,"1"
"sso config change from ping to azure is failing in e1.",1,1,"1"
"sso configuration disabled",1,1,"1"
"sso is disabled",1,1,"1"
"sso issue for https://pwc-nonprod.cloud.databricks.com/",1,1,"1"
"sso issue with new databrick e2 implementation",1,1,"1"
"sso keycloak",1,1,"1"
"sso parameters",1,1,"1"
"sso saml",1,1,"1"
"sso saml  authentication issue with users logging into the us prod databricks environment",1,1,"1"
"sso settings removed",1,1,"1"
"sso token error",1,1,"1"
"sso with account level",1,1,"1"
"stage failure while streaming data from parquet to delta format",1,1,"1"
"stardog metastore refresh is slow in databricks cluster",1,1,"1"
"start aclusters",1,1,"1"
"starting 22 march, there is a substantial increase in cost of the dbfs storage account",1,1,"1"
"starting the cluster fails with self_bootstrap_failure consistently.",1,1,"1"
"state_message",1,1,"1"
"status",1,1,"1"
"status of a job that failed to create a cluster not reported correctly",1,1,"1"
"statuscode=409 statusdescription=the specified path already exists.",1,1,"1"
"statuslogger unrecognized",1,1,"1"
"statuslogger unrecognized format",1,1,"1"
"steps to grant read access to a user on databricks scope and secret",1,1,"1"
"stepup dynamo external source",1,1,"1"
"storage",1,1,"1"
"storage access inconsistent",1,1,"1"
"storage class",1,1,"1"
"str' object has no attribute 'sql_ctx'",1,1,"1"
"strange ui in job list",1,1,"1"
"stream ingestion cloudfiles failure",1,1,"1"
"stream jobs with multiple source fails after restart",1,1,"1"
"stream process fail on missing file",1,1,"1"
"stream to delta only generates schema folder",1,1,"1"
"streaming",1,1,"1"
"streaming ""schema_of_json""",1,1,"1"
"streaming \""schema_of_json\""",1,1,"1"
"streaming \\\""schema_of_json\\\""",1,1,"1"
"streaming aggregation not working",1,1,"1"
"streaming change data feed",1,1,"1"
"streaming data processing",1,1,"1"
"streaming data processing getting stuck after few hours",1,1,"1"
"streaming delta table with changedatafeed enabled breaks when zero rows are merged",1,1,"1"
"streaming exception",1,1,"1"
"streaming executors are getting stuck and die",1,1,"1"
"streaming in production",1,1,"1"
"streaming job concurrent append exception issue",1,1,"1"
"streaming job failing in databricks",1,1,"1"
"streaming job is getting failed",1,1,"1"
"streaming job throwing error",1,1,"1"
"streaming to multiple tables",1,1,"1"
"streamingqueryexception detected a data update",1,1,"1"
"string separate",1,1,"1"
"struct",1,1,"1"
"structured streaming repartition",1,1,"1"
"structured streaming stuck as ""stream initializing""",1,1,"1"
"structured streaming: reading from multiple kafka topics",1,1,"1"
"sts token expiry on redshift copy",1,1,"1"
"stting up a new connectivity to aws s3. it is required a new certificate to be installed.",1,1,"1"
"stuck at running command mount",1,1,"1"
"student",1,1,"1"
"style",1,1,"1"
"submit a case",1,1,"1"
"submit a ticket",1,1,"1"
"submit ticket",1,1,"1"
"submitting mutiple spark job task through run submit api",1,1,"1"
"submitting sql notebook using databricks api",1,1,"1"
"subscription is not registered for adb private workspace",1,1,"1"
"substitute parameters dynamically in job json",1,1,"1"
"sudden cluster termination with error message ""cloud_provider_shutdown"" | arr sr# 2205250030000379",1,1,"1"
"suddenly unable to log in to workspaces via sso",1,1,"1"
"suggestion: why don't you use iso 8601 format on consoles?",1,1,"1"
"support access",1,1,"1"
"support case",1,1,"1"
"support for on-prem bitbucket (bitbucket enterprise edition)",1,1,"1"
"support package",1,1,"1"
"support pricing",1,1,"1"
"support service principals at the account level for automation",1,1,"1"
"support sla",1,1,"1"
"support tasks",1,1,"1"
"support ticket",1,1,"1"
"supportability of  rhel 8  in odbc simba driver",1,1,"1"
"supportability of odbc simba driver in rhel 8",1,1,"1"
"suspen notification of workspace",1,1,"1"
"swagger client",1,1,"1"
"switch cloud",1,1,"1"
"swithcing e1 from adfs to azure ad sso - errors",1,1,"1"
"synapse",1,1,"1"
"synapse loading with jdbc url",1,1,"1"
"synapse pipelines failing",1,1,"1"
"sync",1,1,"1"
"synch databricks configured configure unable ad provisionning scim issue enterprise",1,1,"1"
"syncing with dr workspace",1,1,"1"
"system classpath",1,1,"1"
"system deny assignment created by azure databricks",1,1,"1"
"system requirements",1,1,"1"
"system tables",1,1,"1"
"table",1,1,"1"
"table access",1,1,"1"
"table access permissions getting revoked in sql endpoint",1,1,"1"
"table batch reads and writes",1,1,"1"
"table functions can't use parameters in nested sql",1,1,"1"
"table has zero rows in databricks",1,1,"1"
"table or view not found on warehouse sql endpoint",1,1,"1"
"table utility commands",1,1,"1"
"tableau ssl",1,1,"1"
"tableau, ssl_connect: certificate verify failed",1,1,"1"
"tablesample clause",1,1,"1"
"tagging cluster resource",1,1,"1"
"tagging jobs",1,1,"1"
"tags for instances in the pool",1,1,"1"
"take collect",1,1,"1"
"take too much time to complete notebook",1,1,"1"
"tanvir filing test ticket",1,1,"1"
"task failures",1,1,"1"
"temporarily_unavailable: http response code: 504",1,1,"1"
"tensorflow",1,1,"1"
"terminate existing cluster",1,1,"1"
"terminate existing cluster in community edition",1,1,"1"
"terminated",1,1,"1"
"terraform",1,1,"1"
"terraform databricks_spark_version returning aarch64 images when graviton off",1,1,"1"
"terraform error for mounting a bucket",1,1,"1"
"terraform failures",1,1,"1"
"terraform high concurrency cluster with table acl's on",1,1,"1"
"terraform pipeline fails while creating cluster",1,1,"1"
"terraform pipelines failing",1,1,"1"
"terraform provider recreating instance pool when updating tags",1,1,"1"
"terraform script for defining sql permissions",1,1,"1"
"terraform support for catalog, schema and table grants in unity",1,1,"1"
"terraform template limitation",1,1,"1"
"terraform utilize generated token",1,1,"1"
"test case by pablo",1,1,"1"
"test email notification",1,1,"1"
"test rnd - disconnectedclientexception, unexpectedhttpexception while running a notebook",1,1,"1"
"testing",1,1,"1"
"testing case for email notification. ignore. assign it to smijith if found unassigned",1,1,"1"
"testing case for email thread merg",1,1,"1"
"testing notebooks",1,1,"1"
"text files - spark 3.2.1 documentation",1,1,"1"
"text wrapping",1,1,"1"
"the api returns library status as installed but then the cells below fail to find the library",1,1,"1"
"the cluster is not starting",1,1,"1"
"the count result sometimes becomes 0 when select query executed",1,1,"1"
"the cx failed to spin up npip cluster, after they add australiaeast.azuredatabricks.net, it works",1,1,"1"
"the cx wants to know when the build-in function localtimestamp() will be available",1,1,"1"
"the deployment of the resource is failing due to invalidaccesstoken",1,1,"1"
"the error 'modulenotfounderror' occurred, but the libs were installed on databricks",1,1,"1"
"the execution of this command did not finish successfully",1,1,"1"
"the execution of this command did not finish successfully.",1,1,"1"
"the execution of this command did not finish successfully. cancelled.",1,1,"1"
"the function does not support the dbfs fuse mount.",1,1,"1"
"the job failed with init script exception.",1,1,"1"
"the job hung up for long trying to get hms data",1,1,"1"
"the jobs fail when attempting to distribute this workflow into worker nodes",1,1,"1"
"the library named of msodbcsql17 is installed however pyodbc failed with error '('01000', '[01000] [unixodbc] can't open  'odbc driver''",1,1,"1"
"the newly created workspace url for processing-mlorca-prd-bpp is not working",1,1,"1"
"the notebooks are detached from the cluster",1,1,"1"
"the right way to set workflows job with git",1,1,"1"
"the run of a repo notebook is taking 8 minutes to process each cell",1,1,"1"
"the same query is taking 40 min in powerbi however  if we run it directly on the sql endpoint it tooks almost 2 min",1,1,"1"
"the sample query in ""run your first query "" would not work",1,1,"1"
"the spark context has stopped and the driver is restarting",1,1,"1"
"the spark ui link not acccessible",1,1,"1"
"the training process is idle",1,1,"1"
"the user ryan.cox.-nd@disneystreaming.com is unable to login to databricks via sso",1,1,"1"
"them",1,1,"1"
"theme",1,1,"1"
"there is no database named delta",1,1,"1"
"this delta live tables query is syntactically valid, but you must create a pipeline in order to define and populate your table.",1,1,"1"
"this is a follow up ticket for 00146626",1,1,"1"
"this is test - upwork",1,1,"1"
"time",1,1,"1"
"time & date should be in est for our business conditions",1,1,"1"
"time 2022-05-04 10:27:59 ist message cluster terminated.reason:azure quota exceeded exception  error code: invalidtemplatedeployment, error message: the template deployment failed with error: 'the resource with id: '/subscriptions/a7d209b0-55be-4035-82d3-5b781b6ec360/resourcegroups/databricks-rg-databricksworkspace-ax6r6idmmm4gm/providers/microsoft.compute/virtualmachines/38b04bae0ab147809b055a4ee2a690e1' failed validation with message: 'the requested size for resource '/subscriptions/a7d209b0-5",1,1,"1"
"time out error",1,1,"1"
"time out on jobs",1,1,"1"
"timeout error on redshift data copy from databricks",1,1,"1"
"timeout issues--2204220040007031",1,1,"1"
"timeparser policy",1,1,"1"
"timestamp error while reading data",1,1,"1"
"title",1,1,"1"
"to_csv function",1,1,"1"
"to_excel",1,1,"1"
"token id",1,1,"1"
"too many connections error",1,1,"1"
"too many jobs",1,1,"1"
"too much delay between two micro batches",1,1,"1"
"took 15mins to start the cluster",1,1,"1"
"topandas() causes indexoutofboundsexception in apache arrow",1,1,"1"
"totable aws glue",1,1,"1"
"track views on dashboards",1,1,"1"
"training access for ltk employees",1,1,"1"
"training notebook on single node cluster crashes after random intervals of time",1,1,"1"
"transaction control when writing to sql db",1,1,"1"
"transaction log integrity check failure",1,1,"1"
"transpose",1,1,"1"
"trash folder",1,1,"1"
"treating string as json",1,1,"1"
"treenodeexception when querying a view",1,1,"1"
"tried to connect to ('127.0.0.1', 33569",1,1,"1"
"trigger.availablenow causing significant job delay compared to trigger.once",1,1,"1"
"trim",1,1,"1"
"trim sql",1,1,"1"
"trino delta connector",1,1,"1"
"trunc",1,1,"1"
"truncate option doesnt work with db2",1,1,"1"
"try detaching and re-attaching the notebook",1,1,"1"
"try to create a simple table unity catalog, then error occurs",1,1,"1"
"trying to achieve a shuffle-less join with partitionby",1,1,"1"
"turn off delta",1,1,"1"
"turn off email notification  for  workspace",1,1,"1"
"turning off apache logger per the suggestion from microsoft support is not working",1,1,"1"
"tutorial",1,1,"1"
"two factor",1,1,"1"
"two factor aws",1,1,"1"
"two runs and one one just took 2 mins and others took 4h",1,1,"1"
"typeerror when importing feature store",1,1,"1"
"udf-based external function not support on sql endpoints",1,1,"1"
"ui access not working",1,1,"1"
"ui bug in repos following deployment",1,1,"1"
"ui is limited for dbfs",1,1,"1"
"ui issue",1,1,"1"
"ui issues",1,1,"1"
"ui of the ""tables"" tab is not refreshing when navigate the ""data"" explorer",1,1,"1"
"uinty catalog",1,1,"1"
"unable access to experiment in mlflow",1,1,"1"
"unable to access adls containder from azure databricks cluster",1,1,"1"
"unable to access aws glue catalog in new e2 environment",1,1,"1"
"unable to access delta tables via databricks sql",1,1,"1"
"unable to access files from databricks",1,1,"1"
"unable to access hivemetastore using sql endpoint s with unity enabled",1,1,"1"
"unable to access our legacy workspace",1,1,"1"
"unable to access the data from storage account container",1,1,"1"
"unable to access the details page of the jobs runs",1,1,"1"
"unable to access the external data sources blade in databricks sql",1,1,"1"
"unable to access unity catalog data - storage credentials as workspace admin",1,1,"1"
"unable to activate sso with ping federate (on premise)",1,1,"1"
"unable to add instance profile in new workspace",1,1,"1"
"unable to add users to databricks cloud account",1,1,"1"
"unable to assume role",1,1,"1"
"unable to attach notebook to cluster",1,1,"1"
"unable to attach notebooks to cluster.  databricks times out when trying to list available compute.",1,1,"1"
"unable to cancel job run",1,1,"1"
"unable to change network config despite shutting down all ec2s",1,1,"1"
"unable to change network config for a workspace.",1,1,"1"
"unable to change owner of cluster",1,1,"1"
"unable to connect from databricks (dbw-smartfleet) to mongo instance running in int-hal-pe-prd subscription.",1,1,"1"
"unable to connect from okta",1,1,"1"
"unable to connect odbc simba spark connector to databricks sql end point",1,1,"1"
"unable to connect to adls mounts using databricks cli",1,1,"1"
"unable to connect to as400 datasource (worked on single tenant workspace)",1,1,"1"
"unable to connect to aws athena from aws databricks workspace",1,1,"1"
"unable to connect to azure sql db",1,1,"1"
"unable to connect to hive metastore",1,1,"1"
"unable to connect unity catalog to adls gen 2",1,1,"1"
"unable to create a workspace through the admin console ui",1,1,"1"
"unable to create cluster",1,1,"1"
"unable to create cluster with gpu ec2 instances",1,1,"1"
"unable to create cluster: cluster terminated [2206140050002240]",1,1,"1"
"unable to create databricks storage account private endpoint - due to deny assignment",1,1,"1"
"unable to create high concurrency cluster on standard tier workspace | arr sr# 2206150010000827",1,1,"1"
"unable to create linked services with databricks configured with private endpoints",1,1,"1"
"unable to create table. the associated location is not empty but it's not a delta table",1,1,"1"
"unable to create workspace - oauth error",1,1,"1"
"unable to debug failing model inference on large dataset",1,1,"1"
"unable to delete private endpoint connection in azuredatabricksworkspaces- 220429003000055",1,1,"1"
"unable to delete resource",1,1,"1"
"unable to delete users from https://accounts.cloud.databricks.com/users",1,1,"1"
"unable to deploy ml models to sagemaker using mlflow api",1,1,"1"
"unable to drop database because s3 bucket is no longer there",1,1,"1"
"unable to drop hive table",1,1,"1"
"unable to drop table and also couldnt do repair",1,1,"1"
"unable to edit metastore admin",1,1,"1"
"unable to email sql dashboard snapshot to alert destination",1,1,"1"
"unable to establish aad authentication between databricks and synapse dw",1,1,"1"
"unable to execute notebook commands",1,1,"1"
"unable to find audit logs- overwatch",1,1,"1"
"unable to find valid certification path to requested target",1,1,"1"
"unable to get sso working",1,1,"1"
"unable to grant access to database",1,1,"1"
"unable to grant permission to a schema",1,1,"1"
"unable to import data from rds using spark databricks",1,1,"1"
"unable to import named 'azure.storage' and 'azure.identity'",1,1,"1"
"unable to install great_expectations python module",1,1,"1"
"unable to install package from cran repo in r",1,1,"1"
"unable to install package gaitpy",1,1,"1"
"unable to install pythonnet on cluster",1,1,"1"
"unable to install tensorflow library on graviton cluster",1,1,"1"
"unable to integrate external hive metastore in databricks sql",1,1,"1"
"unable to launch a large cluster",1,1,"1"
"unable to launch aws databricks jobs - amazons3exception on databricks bucket",1,1,"1"
"unable to launch clusters",1,1,"1"
"unable to launch multi node cluster",1,1,"1"
"unable to load data from json into sql table via databricks",1,1,"1"
"unable to load ganglia metrics page for all clusters",1,1,"1"
"unable to load python packages",1,1,"1"
"unable to locate credentials (temp cred's for instane profile)",1,1,"1"
"unable to log into  https://riotgames.cloud.databricks.com",1,1,"1"
"unable to login from macbook pro",1,1,"1"
"unable to login to databricks workspace.",1,1,"1"
"unable to login to dbx support login",1,1,"1"
"unable to login to workspace",1,1,"1"
"unable to login to workspace saml/sso error",1,1,"1"
"unable to modify job cluster pool user-defined tags",1,1,"1"
"unable to mount and read adls file system",1,1,"1"
"unable to mount new containers / access exixting mount points",1,1,"1"
"unable to perform jdbc connection to synpase using aad authentication- saved",1,1,"1"
"unable to provision e2 admin account",1,1,"1"
"unable to query data in sql end point after service principal client secret update",1,1,"1"
"unable to query view which has a complex structure column",1,1,"1"
"unable to reach the server. please check your internet connection and try again.",1,1,"1"
"unable to read ""xml"" files from databricks notebook",1,1,"1"
"unable to read data from synapse into databricks",1,1,"1"
"unable to read delta tables using service principal authentication",1,1,"1"
"unable to read from/ write to s3 bucket",1,1,"1"
"unable to read java file",1,1,"1"
"unable to register a scaler model built on my training data | arr sr# 2205060030001100",1,1,"1"
"unable to reset password",1,1,"1"
"unable to run commands with sudo privileges on high concurrency clusters",1,1,"1"
"unable to run databricks workspace migration",1,1,"1"
"unable to run job cluster when attaching a notebook to it.",1,1,"1"
"unable to run notebooks from repo tab in databricks",1,1,"1"
"unable to run spark.read  for .csv file from storage in vnet/npip workspace",1,1,"1"
"unable to run synapse connector code, job fails",1,1,"1"
"unable to save git credentials on git integration tab",1,1,"1"
"unable to schedule jobs",1,1,"1"
"unable to see feature enablement option",1,1,"1"
"unable to see partner portal on databricks page",1,1,"1"
"unable to see the sql persona in databricks instances",1,1,"1"
"unable to setup mountpoint in adls",1,1,"1"
"unable to setup sso for databricks test",1,1,"1"
"unable to sign into partner-academy",1,1,"1"
"unable to spin a new workspace using terraform",1,1,"1"
"unable to spin up cluster from the workspace",1,1,"1"
"unable to spin/terminate clusters when dhcp option is added in aws account",1,1,"1"
"unable to start a cluster.",1,1,"1"
"unable to start cluster - spark image download failure",1,1,"1"
"unable to start clusters",1,1,"1"
"unable to start jobs clusters",1,1,"1"
"unable to start spark cluster",1,1,"1"
"unable to start the cluster",1,1,"1"
"unable to stat any cliusters",1,1,"1"
"unable to store dbutils widget value in both scala and python variables",1,1,"1"
"unable to stream data from google pub sub using spark 3",1,1,"1"
"unable to subscribe to job alerts",1,1,"1"
"unable to update jobs with modified cluster policies without deleting and recreating jobs",1,1,"1"
"unable to use cifs in databricks e2",1,1,"1"
"unable to use databricks-connect to connect to cluster",1,1,"1"
"unable to use grant sql cmd.",1,1,"1"
"unable to use great_expectations python library",1,1,"1"
"unable to use installed wheel cluster library",1,1,"1"
"unable to use nc64as_t4_v3 sport nodes as worker: overconstrainedallocationrequest",1,1,"1"
"unable to use pretrained models in spark-nlp",1,1,"1"
"unable to vectorize pyspark operation using pandaf_udf. facing performance issues",1,1,"1"
"unable to view ""my company open or closed cases""",1,1,"1"
"unable to write data to cosmos db from databricks",1,1,"1"
"unable to write to feature store with existing table",1,1,"1"
"unapproved usage of cluster by databricks",1,1,"1"
"unauthorised access",1,1,"1"
"uncaught referenceerror: dashrenderer is not defined",1,1,"1"
"uncaught syntax error",1,1,"1"
"uncaught syntaxerror: unexpected end of input",1,1,"1"
"uncaught typeerror: cannot read properties of undefined",1,1,"1"
"unclear user deletion workflow",1,1,"1"
"undefined function",1,1,"1"
"undefined function: 'getargument'",1,1,"1"
"undefined function: getargument",1,1,"1"
"undelete repo notebook",1,1,"1"
"understanding on any impacts of enabling ebs ssd gp3",1,1,"1"
"understanding the impact of a health advisory",1,1,"1"
"unexpected behavior in a notebook",1,1,"1"
"unexpected cluster termination",1,1,"1"
"unexpected databricks cluster unavailability time to time",1,1,"1"
"unexpected error while creating the cluster for job - invalid parameter value",1,1,"1"
"unexpected error while running",1,1,"1"
"unexpected error while running query in redshift external data source",1,1,"1"
"unexpected http exception",1,1,"1"
"unexpected job failures in production",1,1,"1"
"unexpected rethrowing",1,1,"1"
"unexpected unity metastore in aws workspace",1,1,"1"
"union with null",1,1,"1"
"unique ticketid: [ [ ref:_00d61jgc4._5003fjklrn:ref ] ]",1,1,"1"
"unit catalogue: unable to create a metastore",1,1,"1"
"unity catalog - create external tables - s3 access denied",1,1,"1"
"unity catalog configuration'",1,1,"1"
"unity catalog data explorer stopped working",1,1,"1"
"unity catalog enablment on azure databricks",1,1,"1"
"unity catalog failed to acquire a sas token",1,1,"1"
"unity catalog integration with third-party tools",1,1,"1"
"unity catalog lineage analysis",1,1,"1"
"unity catalog principle",1,1,"1"
"unity catalog private preview feature",1,1,"1"
"unity metastore",1,1,"1"
"unitycatalogマネージドテーブルに利用するs3の設定について",1,1,"1"
"unitycatalogマネージドテーブルアクセス用iamポリシーに付与する権限について",1,1,"1"
"unknown issue - job runs much longer than expected",1,1,"1"
"unknown user account in workspace",1,1,"1"
"unpinned clusters disappearing after library installation in e2 workspace",1,1,"1"
"unrecognized parameter external_customer_info when creating new oem workspace",1,1,"1"
"unset tblproperties",1,1,"1"
"unsupported access mechanism for ml-flow-artifacts",1,1,"1"
"unusable since the driver is unhealthy",1,1,"1"
"unused users",1,1,"1"
"unusual exit status",1,1,"1"
"upcoming planned change on workspace/user visibility",1,1,"1"
"update affected all rows, restore didn't restore values",1,1,"1"
"update cmk for managed services and storage for ml-us-dev workspace",1,1,"1"
"update cmk for managed services and storage for workspace ml-sandbox",1,1,"1"
"update ip access list",1,1,"1"
"update ip address whitelist for finra-prod shard",1,1,"1"
"update python package",1,1,"1"
"update tags on cluster",1,1,"1"
"update tags on databricks cluster",1,1,"1"
"update to 10.4 lts",1,1,"1"
"update workspace name",1,1,"1"
"update workspace owner email address",1,1,"1"
"update zone_id on instance pools",1,1,"1"
"updated databrick connector on power bi breaks the connectiom",1,1,"1"
"updating cluster to select az automatically.",1,1,"1"
"updating tags fails.",1,1,"1"
"upgrade account to e2",1,1,"1"
"upgrade to enterprise tier",1,1,"1"
"upgrading the preloaded databricks runtime version of a databricks pool",1,1,"1"
"upgrading to dbr 10.4 on pvc brakes delta merge operation",1,1,"1"
"upgrading to gp3 storage",1,1,"1"
"upload a file to a case",1,1,"1"
"upload a file to a support case",1,1,"1"
"upload a file to a ticket",1,1,"1"
"upload a notebook",1,1,"1"
"upload csv",1,1,"1"
"upload csv file",1,1,"1"
"upload dbc",1,1,"1"
"upload excel to databricks",1,1,"1"
"upload link",1,1,"1"
"upload link sql",1,1,"1"
"upload local file",1,1,"1"
"upload table",1,1,"1"
"upload text file",1,1,"1"
"upload to databricks",1,1,"1"
"upsert bug - `_change_type` column name",1,1,"1"
"upsert failing against destination table in unity catalog",1,1,"1"
"upsert to sql",1,1,"1"
"urgent - cluster failing to launch",1,1,"1"
"urgent - issue with running notebook on a new cluster",1,1,"1"
"urgent - unable to launch jobs ""cloud provider failures"" aws us-east-1",1,1,"1"
"urgent customer queries regarding  simba jdbc delta lake",1,1,"1"
"urgent request- whitelist ips",1,1,"1"
"uri schemes",1,1,"1"
"url parameter",1,1,"1"
"url variable",1,1,"1"
"urllib3.connection.httpsconnection",1,1,"1"
"usage",1,1,"1"
"usage cost",1,1,"1"
"usage information for specific users and specific dbs",1,1,"1"
"usage logs not available",1,1,"1"
"usage spike genie request",1,1,"1"
"use cx_oracle to connect to an oracle db",1,1,"1"
"use cx_oracle to connect to an oracle server",1,1,"1"
"use database",1,1,"1"
"use from_avro or to_avro with confluence",1,1,"1"
"use iam instance profile in prod aws account",1,1,"1"
"use log",1,1,"1"
"use nt group",1,1,"1"
"use of credentials passthrough on high-conc clusters causes issue with iptables",1,1,"1"
"use of private pypi mirror for clusters/jobs api",1,1,"1"
"use of unity catalog external locations",1,1,"1"
"use rstudio server in no-ml clusters",1,1,"1"
"use s3 access point",1,1,"1"
"use yolov5 on databricks",1,1,"1"
"user",1,1,"1"
"user access - pentest security finding",1,1,"1"
"user access for tables",1,1,"1"
"user account deleted",1,1,"1"
"user accounts in manage contacts",1,1,"1"
"user accounts went missing - now are corrupt?",1,1,"1"
"user can't access any query",1,1,"1"
"user cannot access folders from databricks",1,1,"1"
"user cannot login after switching to sso enabled login",1,1,"1"
"user getting error while running sql query in notepad",1,1,"1"
"user is getting failure message in a legacy shard when connecting to the metastore.",1,1,"1"
"user is not able to query redshift data",1,1,"1"
"user not able to login",1,1,"1"
"user not able to login to biogen databricks test workspaces",1,1,"1"
"user not authenticated error - workspace access re",1,1,"1"
"user not receiving email",1,1,"1"
"user onboard problem - user already exists in another account",1,1,"1"
"user or user group access granted to all scopes, all jobs, and all notebooks",1,1,"1"
"user stuck in password reset loop",1,1,"1"
"user unable to access workspace",1,1,"1"
"user unable to log in",1,1,"1"
"user unable to login to databricks",1,1,"1"
"user unable to validate git credentials",1,1,"1"
"user was not able to enter databricks workspace",1,1,"1"
"user without access to help center",1,1,"1"
"user-specified schema incompatible with the schema  inferred from its query",1,1,"1"
"users are not able to start the endpoint",1,1,"1"
"users can create databases poinint to adls gen2 storage container on which they do not have permissions in databricks workspace",1,1,"1"
"users have public access to csv files generated through download button on notebook",1,1,"1"
"users unable to view logs for submitted jobs",1,1,"1"
"using arrays in a select statement is causing query errors",1,1,"1"
"using databricks as a power bi data source intermittently fails",1,1,"1"
"using dbconnect but got errors",1,1,"1"
"using geospatial functions in sql endpoints",1,1,"1"
"using java code need to unload /export databricks query response into amazon s3 buckets as csv files.",1,1,"1"
"using petastorm with databricks connect",1,1,"1"
"using s3 bucket in another account for init script",1,1,"1"
"using union along with function can sometimes fail",1,1,"1"
"ustom cells",1,1,"1"
"v",1,1,"1"
"v5 worker",1,1,"1"
"vaccuum command no working properly",1,1,"1"
"vacuum delta table take a long time",1,1,"1"
"vacuum not working",1,1,"1"
"vacuum on delta table failing continuously || 2202280040007216",1,1,"1"
"valueerror: year 0 is out of range when trying to parse json schema using spark.read.json",1,1,"1"
"var",1,1,"1"
"variable $evn para usar en query sql",1,1,"1"
"variable dinamica en sql",1,1,"1"
"variables",1,1,"1"
"vectorized reader - integer overflow",1,1,"1"
"version",1,1,"1"
"very slow metadata query show tables",1,1,"1"
"video",1,1,"1"
"view ""exceeds the maximum view resolution depth (100)""",1,1,"1"
"view : failed to convert the json string 'void' to a data type",1,1,"1"
"view billable usage",1,1,"1"
"view current iam role",1,1,"1"
"view data /mnt/",1,1,"1"
"view data sets",1,1,"1"
"view usage",1,1,"1"
"virtual machine",1,1,"1"
"virtual machine in subnet",1,1,"1"
"visible datalabels in the graphic combo",1,1,"1"
"visualization deep dive in python",1,1,"1"
"visualizations",1,1,"1"
"vm extension",1,1,"1"
"vm id re",1,1,"1"
"vnet injected workspace is slow",1,1,"1"
"vnet injection",1,1,"1"
"voucher valid",1,1,"1"
"vouchers",1,1,"1"
"vpc in some workspaces appear in broken state, please explain what it means",1,1,"1"
"vpcカスタム設定におけるクロスアカウントiamロールに付与するポリシーについての質問",1,1,"1"
"vs code",1,1,"1"
"vulnerability issues on deere-edl-isg",1,1,"1"
"vulnerability report shows a few issues",1,1,"1"
"want to access executor logs for debugging",1,1,"1"
"warn hard killed driver",1,1,"1"
"warning when connecting through jdbc driver",1,1,"1"
"warning: object json in package json is deprecated (since 1.0.6): use the scala library index to find alternatives: https://index.scala-lang.org/",1,1,"1"
"was ""databricks runtime 6.4 extended support"" updated around 27th april 2022 ? - 2204260040007251",1,1,"1"
"we are getting classnotfoundexception again but this time on spark 3",1,1,"1"
"we are not able to drop damaged table",1,1,"1"
"we are noticing secure groups provisioned through scim are missing on databricks side although no change happened on scim side",1,1,"1"
"we are observing a lot of delay and intermittent failures when trying to start a spark cluster in our databricks",1,1,"1"
"we are setting up a new connectivity to aws s3. it is required a new certificate to be installed. -",1,1,"1"
"we are using databricks native connecter to connect with power bi and while refreshing the report we are getting ssl read error",1,1,"1"
"we are using databricks native connecter to connect with power bi and while refreshing the report we are getting ssl read error-------follow up of  00145189",1,1,"1"
"we can't start clusters",1,1,"1"
"we cannot connect to databricks workspaces!",1,1,"1"
"we found error 'failed to launch spark container on instance",1,1,"1"
"we get this event often in cluster [driver is up but is not responsive, likely due to gc] which impacts daily activities",1,1,"1"
"we have a chaduled job on 13th of eacvh month, but the run did not fire on march 13 2022",1,1,"1"
"we have a current issue where in some cases, when a cluster is created, it will have issues making a connection to our redshift database and will timeout. using the same exact permissions/roles on another cluster there is no issue.  <command-186346> in make_cdp_conn(db, port, username, hostname, password, timeout) 18 conn = psycopg2.connect(dbname=db, port=port, 19 user=username, host=hostname, ---> 20 password=password, connect_timeout=timeout) 21 22 return conn  /databricks/python/lib/python3.",1,1,"1"
"we have deleted a resource group, but needs to recover the databricks workspace",1,1,"1"
"we have found a slow merge execution on some databricks jobs",1,1,"1"
"we have requirement for a globally unique, monotonically increasing sequence number generator safe from multiprocessing",1,1,"1"
"we need assistance with databricks scim/sso implementation on azure",1,1,"1"
"we need of our cluster to transfer ownership to noonpaytechadmin@noon.com account insttead of  'noonpay' account",1,1,"1"
"we want to understand job concept according to doc",1,1,"1"
"web app",1,1,"1"
"web app ip addresses doubts",1,1,"1"
"web terminal",1,1,"1"
"webui for notebook has issue with repo files with foreach and foreachbatch",1,1,"1"
"what are the charges for databricks events",1,1,"1"
"what are the ways to upload files?",1,1,"1"
"what aws iam permissions are needed for a cluster to mount a bucket?",1,1,"1"
"what components in databricks pvc will be effected by rds:mysql instance down-time/restart",1,1,"1"
"what does the filtering files for query mean",1,1,"1"
"what if i can't find a ml runtime with desired tensorflow version when setup cluster?",1,1,"1"
"what is a personal access token",1,1,"1"
"what is checkpoint",1,1,"1"
"what is my username",1,1,"1"
"what is the difference between a host subnet and a container subnet when deploying a workspace with vnet injection?",1,1,"1"
"what is the expected accuracy and precision of time services within ntp service",1,1,"1"
"what the exact situation does customer need to set the ""spark.sql.parquet.enablevectorizedreader"" to ""false""",1,1,"1"
"what this message mean  in stdout: gc (allocation failure)",1,1,"1"
"what version am i on?",1,1,"1"
"what's the impact of this spark config",1,1,"1"
"when a file is written using spark cluster in databricks, we get an error when attempting to read it.",1,1,"1"
"when creating and starting clusters on this particular workspace it appears that the worker host does not start.",1,1,"1"
"when do we support verbose audit logs in azure mooncake? do we have eta?",1,1,"1"
"when i run a databricks job from airflow, i get an error",1,1,"1"
"when trying to access the ganglia ui for live metrics on a started cluster i receive a 400 bad request message.  see attachment .  this has been going on for a month or so now.  cluster is running dbr 10.4 lts.",1,1,"1"
"when two jobs are reading from hive table, sometimes one of the jobs fails (randomly)",1,1,"1"
"where",1,1,"1"
"where are tables stored",1,1,"1"
"where is cluster id",1,1,"1"
"where is dbfs ui browser",1,1,"1"
"where is my dashboard",1,1,"1"
"where is my data mounted",1,1,"1"
"where is path",1,1,"1"
"where is quickstart?",1,1,"1"
"where is the repos tab in the community edition",1,1,"1"
"where is the table i created",1,1,"1"
"where is trash",1,1,"1"
"where to file bug",1,1,"1"
"where to find server hostname",1,1,"1"
"which clouds",1,1,"1"
"which features if spark rdd and dataframe are common?",1,1,"1"
"while reading csv files from datalake to databricks it is creating distorted data in notebookreading csv files from datalake to databricks it is creating distorted data in notebook",1,1,"1"
"while using redshift endpoints run button in dbsql is greyed out",1,1,"1"
"whitelist ipaddres in access list for feature store",1,1,"1"
"whitelist ips in databricks waf in single tenant workspaces",1,1,"1"
"whitelist power bi urls",1,1,"1"
"whitelist verisk ip ranges to access databricks endpoint",1,1,"1"
"whitelist vnet in storage account.",1,1,"1"
"whitelisting ip address",1,1,"1"
"who should have portal access",1,1,"1"
"why am i in a ""read-only file system"" after integrate with github?",1,1,"1"
"why am i not able to create cluster with azure free edition",1,1,"1"
"why automl takes only sample dataset?",1,1,"1"
"why can't register",1,1,"1"
"why cannot created table folder and data file inherit default permission by command ""create .... as select"".",1,1,"1"
"why the cluster wasn’t able to scale and became unstable and ultimately terminated and why the addition of nodes to the cluster is taking longer than expected",1,1,"1"
"widget variable in sql query doesnt utilize the input field",1,1,"1"
"will sql endpoint slow down? how to solve this problem?",1,1,"1"
"will unsupported dbr still run",1,1,"1"
"withcolumn",1,1,"1"
"worflow running for indefinitely long time",1,1,"1"
"work with workspace objects",1,1,"1"
"worker cannot be registered to the driver",1,1,"1"
"worker is lost during jobs execution",1,1,"1"
"workflow job pass parameters between tasks",1,1,"1"
"workflows ui tab issues",1,1,"1"
"workspace access - 2203310030000887",1,1,"1"
"workspace access cluster bit filter error",1,1,"1"
"workspace access request | job fails at seemingly random intervals",1,1,"1"
"workspace access request, hc cluster is running slowly",1,1,"1"
"workspace access | 2205200050000742",1,1,"1"
"workspace access | 4729415788177088",1,1,"1"
"workspace cancelled",1,1,"1"
"workspace creation in gcp is going on infinite loop",1,1,"1"
"workspace domain",1,1,"1"
"workspace down",1,1,"1"
"workspace folder color",1,1,"1"
"workspace job limit",1,1,"1"
"workspace limit exceeded, only 20 workspaces are allowed for your account",1,1,"1"
"workspace network configuration warning",1,1,"1"
"workspace notebook executions are inconsistent",1,1,"1"
"workspace okta sso not working",1,1,"1"
"workspace owner",1,1,"1"
"workspace question",1,1,"1"
"workspace recovery",1,1,"1"
"workspace request r package installation",1,1,"1"
"workspace split",1,1,"1"
"workspace sso authentication access and admin access not working",1,1,"1"
"workspace unavailable",1,1,"1"
"workspace url",1,1,"1"
"workspace user login issues after ad username changes.",1,1,"1"
"workspace でのsaml2.0認証エラー原因調査依頼",1,1,"1"
"workspace_access",1,1,"1"
"workspace_does_not_exist_error when launching databricks workspace",1,1,"1"
"workspaces are not reachable",1,1,"1"
"workspcaes split",1,1,"1"
"workspcaes split policy",1,1,"1"
"would it be possible to have a deterministic hash based on the code inside a udf?",1,1,"1"
"wrapping",1,1,"1"
"write dataframe to csv",1,1,"1"
"write error in job cluster",1,1,"1"
"write json data to tables using rest api",1,1,"1"
"write to csv from dataframe",1,1,"1"
"write to s",1,1,"1"
"write to temp table",1,1,"1"
"writestream schema error",1,1,"1"
"writing data from spark to delta table failing",1,1,"1"
"writing data using replacewhere is failing.",1,1,"1"
"writing dataframe to delta table takes long time to complete| arr sr# 2205110030001344",1,1,"1"
"writing from databricks to s3 fails (index out of bound exception)",1,1,"1"
"written pandas data can not be read outside databricks",1,1,"1"
"wrong ebs volume attached to clsuters",1,1,"1"
"x509_v_flag_cb_issuer_check",1,1,"1"
"x\""><svg onload=prompt(window.origin)>",1,1,"1"
"x\\\""><svg onload=prompt(window.origin)>",1,1,"1"
"x\\\\\\\""><svg onload=prompt(window.origin)>",1,1,"1"
"x\\\\\\\\\\\\\\\""><svg onload=prompt(window.origin)>",1,1,"1"
"xgboost run time",1,1,"1"
"xgboost==0.6a1 installation failing in in dbr 7.3 lts ml",1,1,"1"
"xin wang",1,1,"1"
"xls convert to delta",1,1,"1"
"xss on click=""alert(document.cookie)""click mee/xsse",1,1,"1"
"you do not have permission",1,1,"1"
"you haven’t configured the cli yet!",1,1,"1"
"your workspace settings require dbr 8.4+ to access files in repos from clusters.",1,1,"1"
"your workspace settings require dbr 8.4+ to access files in repos from clusters. an admin can update this setting in the admin console. don't show this again",1,1,"1"
"z-order data skipping not working under certain conditions",1,1,"1"
"z-order with autocompact",1,1,"1"
"{""error_code"":""permission_denied"",""message"":""failed to perform putobject operations on s3bucket:databricks-account-logs-global with deliverypathprefix:audi tlogs with the iam role:arn:aws:iam::239100230770:role/databricks-account-logs-cross-ac-role provided. please add all required s3 actions as mentioned in a pi docs to role policy of your iam role.""}",1,1,"1"
"{""error_code"":""temporarily_unavailable"",""message"":""the service at /import/new-table is taking too long to process your request. please try again later or try a faster operation. [traceid: 00-a62c83980b7e84480efa13671d445bab-d3a8064a1d46fd15-00]""}",1,1,"1"
"{csat impacting}cannot add scope for key vault in the databricks side",1,1,"1"
"‘msck repair table fails intermittently",1,1,"1"
"アカウントコンソールsso設定解除時に発生したエラーの原因調査依頼",1,1,"1"
"アカウントコンソールのsaml2.0を利用したsso設定の実現性について",1,1,"1"
"クロスアクセスiamロールに付与する権限に関するご質問",1,1,"1"
"クロスアクセスアカウントiamロールに付与するポリシーに関する質問",1,1,"1"
"人工客服  manual customer service",1,1,"1"
"外部テーブル作成時に必要なiamポリシーについて",1,1,"1"
"如何将workflow导出来",1,1,"1"
"監査ログの構成に利用するs3バケットの設定について",1,1,"1"
"複数workspaceを作成する場合の、private link利用時のネットワーク構成について",1,1,"1"
