{
  "took" : 109,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 309,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/cannot-modify-spark-serializer",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cannot modify the value of an Apache Spark config Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to SET the value of a Spark config in a notebook and get a Cannot modify the value of a Spark config error. For example: SQL Copy SET spark.serializer=org.apache.spark.serializer.KryoSerializer\n Console Copy Error in SQL statement: AnalysisException: Cannot modify the value of a Spark config: spark.serializer;\n Cause The SET command does not work on SparkConf entries. This is by design in Spark 3.0 and above. Solution You should remove SET commands for SparkConf entries from your notebook. You can enter SparkConf values at the cluster level by entering them in the cluster’s Spark configuration and restarting the cluster.",
          "title" : "Cannot modify the value of an Apache Spark config",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/cannot-modify-spark-serializer"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/create-df-from-json-string-python-dictionary",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Create a DataFrame from a JSON string or Python dictionary Article 03/11/2022 2 minutes to read 2 contributors In this article Create a Spark DataFrame from a JSON string Extract a string column with JSON data from a DataFrame and parse it Create a Spark DataFrame from a Python directory Example notebook In this article we are going to review how you can create an Apache Spark DataFrame from a variable containing a JSON string or a Python dictionary. Create a Spark DataFrame from a JSON string Add the JSON content from the variable to a list. Scala Copy import scala.collection.mutable.ListBuffer\nval json_content1 = \"{'json_col1': 'hello', 'json_col2': 32}\"\nval json_content2 = \"{'json_col1': 'hello', 'json_col2': 'world'}\"\n\nvar json_seq = new ListBuffer[String]()\njson_seq += json_content1\njson_seq += json_content2\n Create a Spark dataset from the list. Scala Copy val json_ds = json_seq.toDS()\n Use spark.read.json to parse the Spark dataset. Scala Copy val df= spark.read.json(json_ds)\ndisplay(df)\n Combined sample code These sample code blocks combine the previous steps into individual examples. The Python and Scala samples perform the same tasks. Python Copy json_content1 = \"{'json_col1': 'hello', 'json_col2': 32}\"\njson_content2 = \"{'json_col1': 'hello', 'json_col2': 'world'}\"\n\njson_list = []\njson_list.append(json_content1)\njson_list.append(json_content2)\n\ndf = spark.read.json(sc.parallelize(json_list))\ndisplay(df)\n Scala Copy import scala.collection.mutable.ListBuffer\nval json_content1 = \"{'json_col1': 'hello', 'json_col2': 32}\"\nval json_content2 = \"{'json_col1': 'hello', 'json_col2': 'world'}\"\n\nvar json_seq = new ListBuffer[String]()\njson_seq += json_content1\njson_seq += json_content2\n\nval json_ds = json_seq.toDS()\nval df= spark.read.json(json_ds)\ndisplay(df)\n Extract a string column with JSON data from a DataFrame and parse it Select the JSON column from a DataFrame and convert it to an RDD of type RDD[Row]. Scala Copy import org.apache.spark.sql.functions._\n\nval test_df = Seq((\"1\", \"{'json_col1': 'hello', 'json_col2': 32}\", \"1.0\"),(\"1\", \"{'json_col1': 'hello', 'json_col2': 'world'}\", \"1.0\")).toDF(\"row_number\", \"json\", \"token\")\n\nval row_rdd = test_df.select(col(\"json\")).rdd  // Selecting just the JSON column and converting it to RDD.\n Convert RDD[Row] to RDD[String]. Scala Copy val string_rdd = row_rdd.map(_.mkString(\",\"))\n Use spark.read.json to parse the RDD[String]. Scala Copy val df1= spark.read.json(string_rdd)\n display(df1)\n Combined sample code These sample code block combines the previous steps into a single example. Scala Copy import org.apache.spark.sql.functions._\n\nval test_df = Seq((\"1\", \"{'json_col1': 'hello', 'json_col2': 32}\", \"1.0\"),(\"1\", \"{'json_col1': 'hello', 'json_col2': 'world'}\", \"1.0\")).toDF(\"row_number\", \"json\", \"token\")\n\nval row_rdd = test_df.select(col(\"json\")).rdd\nval string_rdd = row_rdd.map(_.mkString(\",\"))\n\nval df1= spark.read.json(string_rdd)\ndisplay(df1)\n Create a Spark DataFrame from a Python directory Check the data type and confirm that it is of dictionary type. Python Copy jsonDataDict = {\"job_id\":33100,\"run_id\":1048560,\"number_in_job\":1,\"state\":{\"life_cycle_state\":\"PENDING\",\"state_message\":\"Waiting for cluster\"},\"task\":{\"notebook_task\":{\"notebook_path\":\"/Users/user@databricks.com/path/test_notebook\"}},\"cluster_spec\":{\"new_cluster\":{\"spark_version\":\"4.3.x-scala2.11\",\"attributes\":{\"type\":\"fixed_node\",\"memory\":\"8g\"},\"enable_elastic_disk\":\"false\",\"num_workers\":1}},\"cluster_instance\":{\"cluster_id\":\"0000-000000-wares10\"},\"start_time\":1584689872601,\"setup_duration\":0,\"execution_duration\":0,\"cleanup_duration\":0,\"creator_user_name\":\"user@databricks.com\",\"run_name\":\"my test job\",\"run_page_url\":\"https://testurl.databricks.com#job/33100/run/1\",\"run_type\":\"SUBMIT_RUN\"}\n\ntype(jsonDataDict)\n Use json.dumps to convert the Python dictionary into a JSON string. Python Copy import json\njsonData = json.dumps(jsonDataDict)\n Add the JSON content to a list. Python Copy jsonDataList = []\njsonDataList.append(jsonData)\n Convert the list to a RDD and parse it using spark.read.json. Python Copy jsonRDD = sc.parallelize(jsonDataList)\ndf = spark.read.json(jsonRDD)\ndisplay(df)\n Combined sample code These sample code block combines the previous steps into a single example. Python Copy jsonDataDict = {\"job_id\":33100,\"run_id\":1048560,\"number_in_job\":1,\"state\":{\"life_cycle_state\":\"PENDING\",\"state_message\":\"Waiting for cluster\"},\"task\":{\"notebook_task\":{\"notebook_path\":\"/Users/user@databricks.com/path/test_notebook\"}},\"cluster_spec\":{\"new_cluster\":{\"spark_version\":\"4.3.x-scala2.11\",\"attributes\":{\"type\":\"fixed_node\",\"memory\":\"8g\"},\"enable_elastic_disk\":\"false\",\"num_workers\":1}},\"cluster_instance\":{\"cluster_id\":\"0000-000000-wares10\"},\"start_time\":1584689872601,\"setup_duration\":0,\"execution_duration\":0,\"cleanup_duration\":0,\"creator_user_name\":\"user@databricks.com\",\"run_name\":\"my test job\",\"run_page_url\":\"https://testurl.databricks.com#job/33100/run/1\",\"run_type\":\"SUBMIT_RUN\"}\n\ntype(jsonDataDict)\n\nimport json\njsonData = json.dumps(jsonDataDict)\n\njsonDataList = []\njsonDataList.append(jsonData)\n\njsonRDD = sc.parallelize(jsonDataList)\ndf = spark.read.json(jsonRDD)\ndisplay(df)\n Example notebook Parse a JSON string or Python dictionary example Get notebook",
          "title" : "Create a DataFrame from a JSON string or Python dictionary",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/create-df-from-json-string-python-dictionary"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/decimal-is-fractional-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Decimal$DecimalIsFractional assertion error Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are running a job on Databricks Runtime 7.x or above when you get a java.lang.AssertionError: assertion failed: Decimal$DecimalIsFractional error message. Example stack trace: Console Copy java.lang.AssertionError: assertion failed:\n Decimal$DecimalIsFractional\n  while compiling: <notebook>\n   during phase: globalPhase=terminal, enteringPhase=jvm\n  library version: version 2.12.10\n compiler version: version 2.12.10\nreconstructed args: -deprecation -classpath .....\n*** WARNING: skipped 126593 bytes of output ***\n This error message only occurs on the first run of your notebook. Subsequent runs complete without error. Cause There are two common use cases that can trigger this error message. Cause 1: You are trying to use the round() function on a decimal column that contains null values in a notebook. Cause 2: You are casting a double column to a decimal column in a notebook. This example code can be used to reproduce the error: Scala Copy import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.{DataFrame, SparkSession}\nimport org.apache.spark.sql.Column\n\n//Sample data with decimal values\n\nval updateData =  Seq(\n                     Row(BigDecimal.decimal(123.456), 123.456),\n                     Row(BigDecimal.decimal(123.456), 123.456))\n\nval updateSchema = List(\n                    StructField(\"amt_decimal\", DecimalType(14,3), true),\n                    StructField(\"amt_double\", DoubleType, true))\n\nval testDF =  spark.createDataFrame(\n  spark.sparkContext.parallelize(updateData),\n  StructType(updateSchema)\n)\n\n// Cause 1:\n// round() on the Decimal column reproduces the error\n\ntestDF.withColumn(\"round_amt_decimal\",round(col(\"amt_decimal\"),2)).show()\n\n// Cause 2:\n// CAST() on the Double column to Decimal reproduces the error\n\ntestDF.createOrReplaceTempView(\"dec_table\")\nspark.sql(\"select CAST(amt_double AS DECIMAL(3,3)) AS dec_col from dec_table\").show()\n Solution This is a known issue and can be safely ignored. The error message does not halt the notebook run and it should not result in any data loss.",
          "title" : "Decimal$DecimalIsFractional assertion error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/decimal-is-fractional-error"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/flatten-nested-columns-dynamically",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Convert nested JSON to a flattened DataFrame Article 03/11/2022 2 minutes to read 2 contributors In this article Sample JSON file Convert to DataFrame Extract and flatten Example notebook This article shows you how to flatten nested JSON, using only $\"column.*\" and explode methods. Sample JSON file Pass the sample JSON string to the reader. Scala Copy val json =\"\"\"\n{\n        \"id\": \"0001\",\n        \"type\": \"donut\",\n        \"name\": \"Cake\",\n        \"ppu\": 0.55,\n        \"batters\":\n                {\n                        \"batter\":\n                                [\n                                        { \"id\": \"1001\", \"type\": \"Regular\" },\n                                        { \"id\": \"1002\", \"type\": \"Chocolate\" },\n                                        { \"id\": \"1003\", \"type\": \"Blueberry\" },\n                                        { \"id\": \"1004\", \"type\": \"Devil's Food\" }\n                                ]\n                },\n        \"topping\":\n                [\n                        { \"id\": \"5001\", \"type\": \"None\" },\n                        { \"id\": \"5002\", \"type\": \"Glazed\" },\n                        { \"id\": \"5005\", \"type\": \"Sugar\" },\n                        { \"id\": \"5007\", \"type\": \"Powdered Sugar\" },\n                        { \"id\": \"5006\", \"type\": \"Chocolate with Sprinkles\" },\n                        { \"id\": \"5003\", \"type\": \"Chocolate\" },\n                        { \"id\": \"5004\", \"type\": \"Maple\" }\n                ]\n}\n\"\"\"\n Convert to DataFrame Add the JSON string as a collection type and pass it as an input to spark.createDataset. This converts it to a DataFrame. The JSON reader infers the schema automatically from the JSON string. This sample code uses a list collection type, which is represented as json :: Nil. You can also use other Scala collection types, such as Seq (Scala Sequence). Scala Copy import org.apache.spark.sql.functions._\nimport spark.implicits._\nval DF= spark.read.json(spark.createDataset(json :: Nil))\n Extract and flatten Use $\"column.*\" and explode methods to flatten the struct and array types before displaying the flattened DataFrame. Scala Copy display(DF.select($\"id\" as \"main_id\",$\"name\",$\"batters\",$\"ppu\",explode($\"topping\")) // Exploding the topping column using explode as it is an array type\n        .withColumn(\"topping_id\",$\"col.id\") // Extracting topping_id from col using DOT form\n        .withColumn(\"topping_type\",$\"col.type\") // Extracting topping_tytpe from col using DOT form\n        .drop($\"col\")\n        .select($\"*\",$\"batters.*\") // Flattened the struct type batters tto array type which is batter\n        .drop($\"batters\")\n        .select($\"*\",explode($\"batter\"))\n        .drop($\"batter\")\n        .withColumn(\"batter_id\",$\"col.id\") // Extracting batter_id from col using DOT form\n        .withColumn(\"battter_type\",$\"col.type\") // Extracting battter_type from col using DOT form\n        .drop($\"col\")\n       )\n Warning Make sure to use $ for all column names, otherwise you may get an error message: overloaded method value select with alternatives. Example notebook Run the example notebook to view the sample code and results. Nested JSON to DataFrame example Get notebook",
          "title" : "Convert nested JSON to a flattened DataFrame",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/flatten-nested-columns-dynamically"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/administration/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Azure Databricks administration: tips and troubleshooting Article 03/11/2022 2 minutes to read 3 contributors In this article These articles can help you administer your Azure Databricks workspace, including user and group management, access control, and workspace storage. How to discover who deleted a cluster in Azure portal How to discover who deleted a workspace in Azure portal",
          "title" : "Azure Databricks administration: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/administration/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/bi/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Business intelligence tools: tips and troubleshooting Article 03/11/2022 2 minutes to read 4 contributors In this article These articles can help you manage your business intelligence (BI) tool integrations with Azure Databricks. Configure Simba JDBC driver using Azure AD Configure Simba ODBC driver with a proxy in Windows Troubleshooting JDBC and ODBC connections Power BI proxy and SSL configuration",
          "title" : "Business intelligence tools: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/bi/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/abfs-client-hang",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents ABFS client hangs if incorrect client ID or wrong path used Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You are using Azure Data Lake Storage (ADLS) Gen2. When you try to access an Azure Blob File System (ABFS) path from a Azure Databricks cluster, the command hangs. Enable the debug log and you can see the following stack trace in the driver logs: Console Copy Caused by: java.io.IOException: Server returned HTTP response code: 400 for URL: https://login.microsoftonline.com/b9b831a9-6c10-40bf-86f3-489ed83c81e8/oauth2/token\n  at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1894)\n  at sun.net.www.protocol.http.HttpURLConnection.access$200(HttpURLConnection.java:91)\n  at sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1484)\n  at sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1482)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:782)\n  at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1481)\n  at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480)\n  at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347)\n  at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenSingleCall(AzureADAuthenticator.java:254)\n  ... 31 more\n Cause If ABFS is configured on a cluster with a wrong value for property fs.azure.account.oauth2.client.id, or if you try to access an explicit path of the form abfss://myContainer@myStorageAccount.dfs.core.windows.net/... where myStorageAccount does not exist, then the ABFS driver ends up in a retry loop and becomes unresponsive. The command will eventually fail, but because it retries so many times, it appears to be a hung command. If you try to access an incorrect path with an existing storage account, you will see a 404 error message. The system does not hang in this case. Solution You must verify the accuracy of all credentials when accessing ABFS data. You must also verify the ABFS path you are trying to access exists. If either of these are incorrect, the problem occurs.",
          "title" : "ABFS client hangs if incorrect client ID or wrong path used",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/abfs-client-hang"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/job-fails-adls-hour",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Long jobs fail when accessing ADLS Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are using Azure Active Directory (Azure AD) credential passthrough to access Azure Data Lake Storage (ADLS) resources. Jobs that run longer than one hour fail with a HTTP401 error message. Console Copy com.microsoft.azure.datalake.store.ADLException: Error reading from file /local/Users/<path-to-file>\n\nOperation OPEN failed with HTTP401 : null\nLast encountered exception thrown after 5 tries. [HTTP401(null),HTTP401(null),HTTP401(null),HTTP401(null),HTTP401(null)]\n Cause The lifetime of an Azure AD passthrough token is one hour. When a command is sent to the cluster that takes longer than one hour, it fails if an ADLS resource is accessed after the one hour mark. This is a known issue. Solution You must rewrite your queries, so that no single command takes longer than an hour to complete. It is not possible to increase the lifetime of an Azure AD passthrough token. The token is retrieved by the Azure Databricks replicated principal. You cannot edit its properties. Please review the ADLS credential passthrough limitations documentation for more information.",
          "title" : "Long jobs fail when accessing ADLS",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/job-fails-adls-hour"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/json-unicode",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Failure to detect encoding in JSON Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem Spark job fails with an exception containing the message: Console Copy Invalid UTF-32 character 0x1414141(above 10ffff)  at char #1, byte #7)\nAt org.apache.spark.sql.catalyst.json.JacksonParser.parse\n Cause The JSON data source reader is able to automatically detect encoding of input JSON files using BOM at the beginning of the files. However, BOM is not mandatory by Unicode standard and prohibited by RFC 7159. For example, section 8.1: text Copy ...Implementations MUST NOT add a byte order mark to the beginning of a JSON text.\n As a consequence, Spark is not always able to detect the charset correctly and read the JSON file. Solution To solve the issue, disable the charset auto-detection mechanism and explicitly set the charset using the encoding option: Scala Copy .option(\"encoding\", \"UTF-16LE\")",
          "title" : "Failure to detect encoding in JSON",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/json-unicode"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Data management: tips and troubleshooting Article 03/11/2022 2 minutes to read 4 contributors In this article These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Azure Databricks. Access files written by Apache Spark on ADLS Gen1 Append to a DataFrame Spark 2.0.0 cluster takes a long time to append data How to improve performance with bucketing Simplify chained transformations How to dump tables in CSV, JSON, XML, text, or HTML format Get and set Apache Spark configuration properties in a notebook Hive UDFs Prevent duplicated columns when joining two DataFrames Revoke all user privileges How to list and delete files faster in Azure Databricks How to handle corrupted Parquet files with different schema No USAGE permission on database Nulls and empty strings in a partitioned column save as nulls Behavior of the randomSplit method Generate schema from case class How to specify skew hints in dataset and DataFrame-based join commands How to update nested columns Incompatible schema in some files",
          "title" : "Data management: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/optimize-delta-sink-structured-streaming",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Optimize a Delta sink in a structured streaming application Article 03/11/2022 2 minutes to read 2 contributors In this article Use foreachBatch with a mod value You are using a Delta table as the sink for your structured streaming application and you want to optimize the Delta table so that queries are faster. If your structured streaming application has a very frequent trigger interval, it may not create sufficient files that are eligible for compaction in each microbatch. The autoOptimize operation compacts to 128 MB files. An explicit optimize operation compacts Delta Lake files to 1 GB files. If you do not have a sufficient number of eligible files in each microbatch, you should optimize the Delta table files periodically. Use foreachBatch with a mod value One of the easiest ways to periodically optimize the Delta table sink in a structured streaming application is by using foreachBatch with a mod value on the microbatch batchId. Assume that you have a streaming DataFrame that was created from a Delta table. You use foreachBatch when writing the streaming DataFrame to the Delta sink. Within foreachBatch, the mod value of batchId is used so the optimize operation is run after every 10 microbatches, and the zorder operation is run after every 101 microbatches. Scala Copy val df = spark.readStream.format(\"delta\").table(\"<table-name>\")\ndf.writeStream.format(\"delta\")\n  .foreachBatch{ (batchDF: DataFrame, batchId: Long) =>\n    batchDF.persist()\n    if(batchId % 10 == 0){spark.sql(\"optimize <table-name>\")}\n    if(batchId % 101 == 0){spark.sql(\"optimize <table-name> zorder by (<zorder-column-name>)\")}\n    batchDF.write.format(\"delta\").mode(\"append\").saveAsTable(\"<table-name>\")\n  }.outputMode(\"update\")\n  .start()\n You can modify the mod value as appropriate for your structured streaming application.",
          "title" : "Optimize a Delta sink in a structured streaming application",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/optimize-delta-sink-structured-streaming"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/update-query-fails",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Delta Lake UPDATE query fails with IllegalState exception Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Version Cause Solution Problem When you execute a Delta Lake UPDATE, DELETE, or MERGE query that uses Python UDFs in any of its transformations, it fails with the following exception: Console Copy java.lang.UnsupportedOperationException: Error in SQL statement:\nIllegalStateException: File (adl://xxx/table1) to be rewritten not found among candidate files:\nadl://xxx/table1/part-00001-39cae1bb-9406-49d2-99fb-8c865516fbaa-c000.snappy.parquet\n Version This problem occurs on Databricks Runtime 5.5 and below. Cause Delta Lake internally depends on the input_file_name() function for operations like UPDATE, DELETE, and MERGE. input_file_name() returns an empty value if you use it in a SELECT statement that evaluates a Python UDF. UPDATE calls SELECT internally, which then fails to return file names and leads to the error. This error does not occur with Scala UDFs. Solution You have two options: Use Databricks Runtime 6.0 or above, which includes the resolution to this issue: [SPARK-28153]. If you can’t use Databricks Runtime 6.0 or above, use Scala UDFs instead of Python UDFs.",
          "title" : "Delta Lake UPDATE query fails with IllegalState exception",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/update-query-fails"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-fails-spark-finishes",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Job fails, but Apache Spark tasks finish Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem Your Azure Databricks job reports a failed status, but all Spark jobs and tasks have successfully completed. Cause You have explicitly called spark.stop() or System.exit(0) in your code. If either of these are called, the Spark context is stopped, but the graceful shutdown and handshake with the Azure Databricks job service does not happen. Solution Do not call spark.stop() or System.exit(0) in Spark code that is running on an Azure Databricks cluster.",
          "title" : "Job fails, but Apache Spark tasks finish",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-fails-spark-finishes"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-fails-throttled-atypical",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Job fails with atypical errors message Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem Your job run fails with a throttled due to observing atypical errors error message. Console Copy Cluster became unreachable during run Cause: xxx-xxxxxx-xxxxxxx is throttled due to observing atypical errors\n Cause The jobs on this cluster have returned too many large results to the Apache Spark driver node. As a result, the chauffeur service runs out of memory, and the cluster becomes unreachable. This can happen after calling the .collect or .show API. Solution You can either reduce the workload on the cluster or increase the value of spark.memory.chauffeur.size. The chauffeur service runs on the same host as the Spark driver. When you allocate more memory to the chauffeur service, less overall memory will be available for the Spark driver. Set the value of spark.memory.chauffeur.size: Open the cluster configuration page in your workspace. Click Edit. Expand Advanced Options. Enter the value of spark.memory.chauffeur.size in mb in the Spark Config field. Click Confirm and Restart. Note The default value for spark.memory.chauffeur.size is 1024 megabytes. This is written as spark.memory.chauffeur.size 1024mb in the Spark configuration. The maximum value is the lesser of 16 GB or 20% of the driver node’s total memory.",
          "title" : "Job fails with atypical errors message",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-fails-throttled-atypical"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-run-dash",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Monitor running jobs with a Job Run dashboard Article 03/11/2022 2 minutes to read 3 contributors In this article Job Run dashboard notebook Attach the dashboard Run the dashboard as a scheduled job Display dashboard Results listed The Job Run dashboard is a notebook that displays information about all of the jobs currently running in your workspace. To configure the dashboard, you must have permission to attach a notebook to an all-purpose cluster in the workspace you want to monitor. If an all-purpose cluster does not exist, you must have permission to create one. Once the dashboard is configured, you can manage job permissions and assign Can View permissions to users in your organization. These users can view the dashboard, but cannot modify it. Job Run dashboard notebook Get notebook Attach the dashboard Because the Job Run dashboard is a notebook, no special steps are required to attach the notebook to a cluster. Attach it to an all-purpose cluster. Run the dashboard as a scheduled job After attaching the notebook to a cluster in your workspace, configure it to run as a scheduled job that runs every minute. Open the notebook. Click in the notebook toolbar. Click New in the Schedule job pane. Select Every and minute in the Create Schedule dialog box. Click OK. Click Job Run dashboard in the Schedule job pane. Click Edit next to the Cluster option on the job details page. Select an existing all-purpose cluster. Click Confirm. Display dashboard Go to the job details page for the scheduled job. Check to make sure at least one successful run has occurred. Click Latest successful run (refreshes automatically). Select the Job Run Dashboard view. The dashboard is now in presentation mode. It updates automatically after each scheduled run completes. You can share the dashboard URL with any user who has view permissions. Results listed The Job Run dashboard results are split into two sections: Job Runs - Displays all of the scheduled jobs that are currently running. Run Submits - Displays all of the running jobs that were invoked via an API call. The dashboard displays the following components for each job: Job ID - This is the unique ID number for the job. You can use this to view all of the job data by entering it into a job URL. Run Page - This is the ID number of the specific run for a given job. It is formatted as a clickable hyperlink, so you can navigate directly to the run page from the Job Run dashboard. You can access previous run pages by navigating to the job URL and then clicking the specific run page from the list of completed runs. Run Name - This is the name of the notebook associated with the job. Start Time - This is the time the job run began. Time is displayed in DD-MM-YYYY HH:MM:SS format, using a 24 hour clock. Time is in UTC. Created By - This is the email address of the user who owns the job.",
          "title" : "Monitor running jobs with a Job Run dashboard",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-run-dash"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/disable-broadcast-when-broadcastnestedloopjoin",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Disable broadcast when query plan has BroadcastNestedLoopJoin Article 03/11/2022 2 minutes to read 2 contributors In this article Create tables Attempt to disable broadcast Rewrite query using not exists instead of in Explanation Example notebook This article explains how to disable broadcast when the query plan has BroadcastNestedLoopJoin in the physical plan. You expect the broadcast to stop after you disable the broadcast threshold, by setting spark.sql.autoBroadcastJoinThreshold to -1, but Apache Spark tries to broadcast the bigger table and fails with a broadcast error. This behavior is NOT a bug, however it can be unexpected. We are going to review the expected behavior and provide a mitigation option for this issue. Create tables Start by creating two tables, one with null values table_withNull and the other without null values tblA_NoNull. SQL Copy sql(\"SELECT id FROM RANGE(10)\").write.mode(\"overwrite\").saveAsTable(\"tblA_NoNull\")\nsql(\"SELECT id FROM RANGE(50) UNION SELECT NULL\").write.mode(\"overwrite\").saveAsTable(\"table_withNull\")\n Attempt to disable broadcast We attempt to disable broadcast by setting spark.sql.autoBroadcastJoinThreshold for the query, which has a sub-query with an in clause. SQL Copy spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\nsql(\"select * from table_withNull where id not in (select id from tblA_NoNull)\").explain(true)\n If you review the query plan, BroadcastNestedLoopJoin is the last possible fallback in this situation. It appears even after attempting to disable the broadcast. Console Copy == Physical Plan ==\n*(2) BroadcastNestedLoopJoin BuildRight, LeftAnti, ((id#2482L = id#2483L) || isnull((id#2482L = id#2483L)))\n:- *(2) FileScan parquet default.table_withnull[id#2482L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/table_withnull], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>\n+- BroadcastExchange IdentityBroadcastMode, [id=#2586]\n   +- *(1) FileScan parquet default.tbla_nonull[id#2483L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/tbla_nonull], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>\n If the data being processed is large enough, this results in broadcast errors when Spark attempts to broadcast the table. Rewrite query using not exists instead of in You can resolve the issue by rewriting the query with not exists instead of in. SQL Copy // It can be rewritten into a NOT EXISTS, which will become a regular join:\nsql(\"select * from table_withNull where not exists (select 1 from tblA_NoNull where table_withNull.id = tblA_NoNull.id)\").explain(true)\n By using not exists, the query runs with SortMergeJoin. Console Copy == Physical Plan ==\nSortMergeJoin [id#2482L], [id#2483L], LeftAnti\n:- Sort [id#2482L ASC NULLS FIRST], false, 0\n:  +- Exchange hashpartitioning(id#2482L, 200), [id=#2653]\n:     +- *(1) FileScan parquet default.table_withnull[id#2482L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/table_withnull], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>\n+- Sort [id#2483L ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(id#2483L, 200), [id=#2656]\n      +- *(2) Project [id#2483L]\n         +- *(2) Filter isnotnull(id#2483L)\n            +- *(2) FileScan parquet default.tbla_nonull[id#2483L] Batched: true, DataFilters: [isnotnull(id#2483L)], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/tbla_nonull], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>\n Explanation Spark doesn’t do this automatically, because Spark and SQL have slightly different semantics for null handling. In SQL, not in means that if there is any null value in the not in values, the result is empty. This is why it can only be executed with BroadcastNestedLoopJoin. All not in values must be known in order to ensure there is no null value in the set. Example notebook This notebook has a complete example, showing why Spark does not automatically switch BroadcastNestedLoopJoin to SortMergeJoin. BroadcastNestedLoopJoin example notebook Get notebook",
          "title" : "Disable broadcast when query plan has BroadcastNestedLoopJoin",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/disable-broadcast-when-broadcastnestedloopjoin"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/dupe-column-in-metadata",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Duplicate columns in the metadata error Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem Your Apache Spark job is processing a Delta table when the job fails with an error message. Console Copy org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the metadata update: col1, col2...\n Cause There are duplicate column names in the Delta table. Column names that differ only by case are considered duplicate. Delta Lake is case preserving, but case insensitive, when storing a schema. Parquet is case sensitive when storing and returning column information. Spark can be case sensitive, but it is case insensitive by default. In order to avoid potential data corruption or data loss, duplicate column names are not allowed. Solution Delta tables must not contain duplicate column names. Ensure that all column names are unique.",
          "title" : "Duplicate columns in the metadata error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/dupe-column-in-metadata"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/kafka-no-resolvable-bootstrap-urls",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Kafka error: No resolvable bootstrap urls Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to read or write data to a Kafka stream when you get an error message. Console Copy kafkashaded.org.apache.kafka.common.KafkaException: Failed to construct kafka consumer\n\nCaused by: kafkashaded.org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers\n If you are running a notebook, the error message appears in a notebook cell. If you are running a JAR job, the error message appears in the cluster driver and worker logs. Cause This error message occurs when an invalid hostname or IP address is passed to the kafka.bootstrap.servers configuration in readStream. This means a Kafka bootstrap server is not running at the given hostname or IP address. Solution Contact your Kafka admin to determine the correct hostname or IP address for the Kafka bootstrap servers in your environment. Make sure you use the correct hostname or IP address when you establish the connection between Kafka and your Apache Spark structured streaming application.",
          "title" : "Kafka error: No resolvable bootstrap urls",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/kafka-no-resolvable-bootstrap-urls"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/rocksdb-fails-to-acquire-lock",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents RocksDB fails to acquire a lock Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to use RocksDB as a state store for your structured streaming application, when you get an error message saying that the instance could not be acquired. Console Copy Caused by: java.lang.IllegalStateException: RocksDB instance could not be acquired by [ThreadId: 742, task: 140.3 in stage 3152, TID 553193] as it was not released by [ThreadId: 42, task: 140.1 in stage 3152, TID 553083] after 10009 ms\nStateStoreId(opId=0,partId=140,name=default)\n Cause Two concurrent tasks cannot modify the same RocksDBStateStore instance. Concurrent tasks attempting to access the same state store (the state store tied to the same partition of state maintained by flatMapGroupsWithState) should be extremely rare. It can only happen if the task updating the store instance was restarted by the driver before the previous attempt had terminated. Note Abrupt node termination, like when a spot instance terminates, can also cause this error. Solution This error prevents the state from being corrupted. Restart the query if you encounter this error. If zombie tasks are taking too long to clean up their resources, when the next task tries to acquire a lock, it will also fail. In this case, you should allow more time for the thread to clean up. Set the wait time for the thread by configuring rocksdb.lockAcquireTimeoutMs in your SQL configuration. The value is in milliseconds. Scala Copy spark.sql(\"set spark.sql.streaming.stateStore.rocksdb.lockAcquireTimeoutMs = 20000\")",
          "title" : "RocksDB fails to acquire a lock",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/rocksdb-fails-to-acquire-lock"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/visualizations/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Visualizations: tips and troubleshooting Article 03/11/2022 2 minutes to read 3 contributors In this article These articles can help you with your graphs, charts, and other visualizations in Azure Databricks. How to save Plotly files and display From DBFS",
          "title" : "Visualizations: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/visualizations/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbsql/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Databricks SQL: tips and troubleshooting Article 03/11/2022 2 minutes to read 2 contributors In this article These articles can help you with Databricks SQL. Null column values display as NaN Retrieve queries owned by a disabled user",
          "title" : "Databricks SQL: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbsql/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/execution/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Job execution: tips and troubleshooting Article 03/11/2022 2 minutes to read 3 contributors In this article These articles can help you tune and troubleshoot Apache Spark job execution. Increase the number of tasks per stage Maximum execution context or notebook attachment limit reached Serialized task is too large",
          "title" : "Job execution: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/execution/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Machine learning: tips and troubleshooting Article 03/11/2022 2 minutes to read 4 contributors In this article These articles can help you with your machine learning, deep learning, and other data science workflows in Azure Databricks. Conda fails to download packages from Anaconda Download artifacts from MLflow How to extract feature information for tree-based Apache SparkML pipeline models Fitting an Apache SparkML model throws error H2O.ai Sparkling Water cluster not reachable How to perform group K-fold cross validation with Apache Spark Error when importing OneHotEncoderEstimator MLflow project fails to access an Apache Hive table How to speed up cross-validation Hyperopt fails with maxNumConcurrentTasks error Incorrect results when using documents as inputs Errors when accessing MLflow artifacts without using the MLflow client Experiment warning when custom artifact storage location is used Experiment warning when legacy artifact storage location is used KNN model using pyfunc returns ModuleNotFoundError or FileNotFoundError OSError when accessing MLflow experiment artifacts PERMISSION_DENIED error when accessing MLflow experiment artifacts Python commands fail on Machine Learning clusters Runs are not nested when SparkTrials is enabled in Hyperopt X13PATH environmental variable not found",
          "title" : "Machine learning: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/drop-database-no-delete",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Drop database without deletion Article 03/11/2022 2 minutes to read 2 contributors In this article Example code By default, the DROP DATABASE command drops the database and deletes the directory associated with the database from the file system. Sometimes you may want to drop the database, but keep the underlying database directory intact. Example code You can use this example code to drop the database without dropping the underlying storage folder. Scala Copy import scala.collection.JavaConverters._\nimport org.apache.hadoop.hive.ql.metadata.Hive\nimport org.apache.hadoop.hive.conf.HiveConf\nimport org.apache.hadoop.hive.ql.session.SessionState\n\nval hiveConf = new HiveConf(classOf[SessionState])\nsc.hadoopConfiguration.iterator().asScala.foreach { kv =>\nhiveConf.set(kv.getKey, kv.getValue)\n}\nsc.getConf.getAll.foreach {\ncase (k, v) => hiveConf.set(k, v)\n}\n\nhiveConf.setBoolean(\"hive.cbo.enable\", false)\nval state = new SessionState(hiveConf)\nval hive = Hive.get(state.getConf)\nprintln(state.getConf)\n\nhive.dropDatabase(\"<database-name>\", false, false, true)\n For more information on org.apache.hadoop.hive.ql.metadata.Hive, please review the Hive documentation.",
          "title" : "Drop database without deletion",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/drop-database-no-delete"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/set-up-sql-backed-hive-metastore",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to set up Hive metastore on SQL Server with Hive 2.0-2.2 Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Version Solution Problem When you set up a Hive external metastore on SQL Server, it fails due to HIVE-14698. Version Hive 2.0 through 2.2. Solution To resolve this issue, import the notebook and follow the instructions. The instructions in this notebook refer to Hive 2.1. However, the instructions can be used without alteration for Hive 2.0 and 2.2 as well. External metastore setup Hive 2.1 with SQL Server notebook Get notebook",
          "title" : "How to set up Hive metastore on SQL Server with Hive 2.0-2.2",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/set-up-sql-backed-hive-metastore"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/display-file-timestamp-details",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Display file and directory timestamp details Article 03/11/2022 2 minutes to read 2 contributors In this article Use ls command Use Python commands to display creation date and modification date In this article we show you how to display detailed timestamps, including the date and time when a file was created or modified. Use ls command The simplest way to display file timestamps is to use the ls -lt <path> command in a bash shell. For example, this sample command displays basic timestamps for files and directories in the /dbfs/ folder. Bash Copy ls -lt /dbfs/\n Output: Console Copy total 36\ndrwxrwxrwx 2 root root 4096 Jul  1 12:49 FileStore\ndrwxrwxrwx 2 root root 4096 Jul  1 12:49 databricks\ndrwxrwxrwx 2 root root 4096 Jul  1 12:49 databricks-datasets\ndrwxrwxrwx 2 root root 4096 Jul  1 12:49 databricks-results\ndrwxrwxrwx 2 root root 4096 Jul  1 12:49 ml\ndrwxrwxrwx 2 root root 4096 Jul  1 12:49 tmp\ndrwxrwxrwx 2 root root 4096 Jul  1 12:49 user\ndrwxrwxrwx 2 root root 4096 Jun  9  2020 dbfs\ndrwxrwxrwx 2 root root 4096 May 20  2020 local_disk0\n Use Python commands to display creation date and modification date The ls command is an easy way to display basic information. If you want more detailed timestamps, you should use Python API calls. For example, this sample code uses datetime functions to display the creation date and modified date of all listed files and directories in the /dbfs/ folder. Replace /dbfs/ with the full path to the files you want to display. Python Copy import os\nfrom datetime import datetime\npath = '/dbfs/'\nfdpaths = [path+\"/\"+fd for fd in os.listdir(path)]\nprint(\" file_path \" + \" create_date \" + \" modified_date \")\nfor fdpath in fdpaths:\n  statinfo = os.stat(fdpath)\n  create_date = datetime.fromtimestamp(statinfo.st_ctime)\n  modified_date = datetime.fromtimestamp(statinfo.st_mtime)\n  print(fdpath, create_date, modified_date)\n Output: Console Copy  file_path  create_date  modified_date\n/dbfs//FileStore 2021-07-01 12:49:45.264730 2021-07-01 12:49:45.264730\n/dbfs//databricks 2021-07-01 12:49:45.264730 2021-07-01 12:49:45.264730\n/dbfs//databricks-datasets 2021-07-01 12:49:45.264730 2021-07-01 12:49:45.264730\n/dbfs//databricks-results 2021-07-01 12:49:45.264730 2021-07-01 12:49:45.264730\n/dbfs//dbfs 2020-06-09 21:11:24 2020-06-09 21:11:24\n/dbfs//local_disk0 2020-05-20 22:32:05 2020-05-20 22:32:05\n/dbfs//ml 2021-07-01 12:49:45.264730 2021-07-01 12:49:45.264730\n/dbfs//tmp 2021-07-01 12:49:45.264730 2021-07-01 12:49:45.264730\n/dbfs//user 2021-07-01 12:49:45.264730 2021-07-01 12:49:45.264730",
          "title" : "Display file and directory timestamp details",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/display-file-timestamp-details"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/running-c-plus-plus-code",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Running C++ code in Python Article 03/11/2022 2 minutes to read 3 contributors In this article Run C++ from Python notebook Run C++ from Python notebook Get notebook",
          "title" : "Running C++ code in Python",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/running-c-plus-plus-code"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/sparkr-gapply",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to parallelize R code with gapply Article 03/11/2022 2 minutes to read 3 contributors In this article Parallelization of R code is difficult, because R code runs on the driver and R data.frames are not distributed. Often, there is existing R code that is run locally and that is converted to run on Apache Spark. In other cases, some SparkR functions used for advanced statistical analysis and machine learning techniques may not support distributed computing. In such cases, the SparkR UDF API can be used to distribute the desired workload across a cluster. Example use case: You want to train a machine learning model on subsets of a data set, grouped by a key. If the subsets of the data fit on the workers, it may be more efficient to use the SparkR UDF API to train multiple models at once. The gapply and gapplyCollect functions apply a function to each group in a Spark DataFrame. For each group in a Spark DataFrame: Collect each group as an R data.frame. Send the function to the worker and execute. Return the result to the driver as specified by the schema. Note When you call gapply, you must specify the output schema. With gapplyCollect, the result is collected to the driver using an R data.frame for the output. In the following example, a separate support vector machine model is fit on the airquality data for each month. The output is a data.frame with the resulting MSE for each month, shown both with and without specifying the schema. R Copy df <- createDataFrame(na.omit(airquality))\n\nschema <- structType(\n  structField(\"Month\", \"MSE\"),\n  structField(\"integer\", \"Number\"))\n\nresult <- gapply(df, c(\"Month\"), function(key, x) {\n  library(e1071)\n  data.frame(month = key, mse = svm(Ozone ~ ., x, cross = 3)$tot.MSE)\n}, schema)\n R Copy df <- createDataFrame(na.omit(airquality))\n\ngapplyCollect(df, c(\"Month\"), function(key, x) {\n  library(e1071)\n  y <- data.frame(month = key, mse = svm(Ozone ~ ., x, cross = 3)$tot.MSE)\n names(y) <- c(\"Month\", \"MSE\")\n  y\n})\n Note Start with a Spark DataFrame and install packages on all workers.",
          "title" : "How to parallelize R code with gapply",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/sparkr-gapply"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/administration/who-deleted-workspace",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to discover who deleted a workspace in Azure portal Article 03/11/2022 2 minutes to read 3 contributors In this article If your workspace has disappeared or been deleted, you can identify which user deleted it by checking the Activity log in the Azure portal. Go to the Activity log in the Azure portal. Expand the timeline to focus on when the workspace was deleted. Filter the log for a record of the specific event. Click on the event to display information about the event, including the user who initiated the event. The screenshot shows how you can click the Remove Databricks Workspace event in the Operation Name column, and then view detailed information about the event. If you are still unable to find who deleted the workspace, create a support case with Microsoft Support. Provide details such as the workspace id and the time range of the event (including your time zone). Microsoft Support will review the corresponding backend activity logs.",
          "title" : "How to discover who deleted a workspace in Azure portal",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/administration/who-deleted-workspace"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/azure-vnet-jobs-not-progressing",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Jobs are not progressing in the workspace Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem Jobs fail to run on any cluster in the workspace. Cause This can happen if you have changed the VNet of an existing workspace. Changing the VNet of an existing Azure Databricks workspace is not supported. Review Deploy Azure Databricks in your VNet for more details. Solution Open the cluster driver logs in the Azure Databricks UI. Search for the following WARN messages: Console Copy 19/11/19 16:50:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n19/11/19 16:50:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n19/11/19 16:50:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n If this error is present, it is likely that the VNet of the Azure Databricks workspace was changed. Revert the change to restore the original VNet configuration that was used when the Azure Databricks workspace was created. Restart the running cluster. Resubmit your jobs. Verify the jobs are getting resources.",
          "title" : "Jobs are not progressing in the workspace",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/azure-vnet-jobs-not-progressing"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/azure-ip-limit",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents IP address limit prevents cluster creation Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem Cluster creation fails with a message about a cloud provider error when you hover over cluster state. Console Copy Cloud Provider Launch Failure: A cloud provider error was encountered while setting up the cluster.\n When you view the cluster event log to get more details, you see a message about publicIPAddresses limits. Console Copy ResourceQuotaExceeded Azure error message: Creating the resource of type 'Microsoft.Network/publicIPAddresses' would exceed the quota of '800' resources of type 'Microsoft.Network/publicIPAddresses' per resource group. The current resource count is '800', please delete some resources of this type before creating a new one.'\n Cause Azure subscriptions have a public IP address limit which restricts the number of public IP addresses you can use. This is a hard limit. If you try to start a cluster that would result in your account exceeding the public IP address quota the cluster launch will fail. Solution You can either free up resources or request a quota increase for your account. Stop inactive clusters to free up public IP addresses for use. Open an Azure support case with a request to increase the public IP address quota limit for your subscription.",
          "title" : "IP address limit prevents cluster creation",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/azure-ip-limit"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/dump-table",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to dump tables in CSV, JSON, XML, text, or HTML format Article 03/11/2022 2 minutes to read 3 contributors In this article You want to send results of your computations in Azure Databricks outside Azure Databricks. You can use BI tools to connect to your cluster via JDBC and export results from the BI tools, or save your tables in DBFS or blob storage and copy the data via REST API. This article introduces JSpark, a simple console tool for executing SQL queries using JDBC on Spark clusters to dump remote tables to local disk in CSV, JSON, XML, Text, and HTML format. For example: Bash Copy java -Dconfig.file=mycluster.conf -jar jspark.jar -q \"select id, type, priority, status from tickets limit 5\"\n returns: Copy +----+--------+--------+------+\n|  id|type    |priority|status|\n+----+--------+--------+------+\n|9120|problem |urgent  |closed|\n|9121|question|normal  |hold  |\n|9122|incident|normal  |closed|\n|9123|question|normal  |open  |\n|9124|incident|normal  |solved|\n+----+--------+--------+------+\n Instructions for use, example usage, source code, and a link to the assembled JAR is available at the JSpark GitHub repo. You can specify the parameters of JDBC connection using arguments or using a config file, for example: mycluster.conf. To check or troubleshoot JDBC connections, download the fat JAR jspark.jar and launch it as a regular JAR. It includes hive-jdbc 1.2.1 and all required dependencies.",
          "title" : "How to dump tables in CSV, JSON, XML, text, or HTML format",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/dump-table"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/random-split-behavior",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Behavior of the randomSplit method Article 03/11/2022 2 minutes to read 3 contributors In this article Solution When using randomSplit on a DataFrame, you could potentially observe inconsistent behavior. Here is an example: Python Copy df = spark.read.format('inconsistent_data_source').load()\na,b = df.randomSplit([0.5, 0.5])\na.join(broadcast(b), on='id', how='inner').count()\n Typically this query returns 0. However, depending on the underlying data source or input DataFrame, in some cases the query could result in more than 0 records. This unexpected behavior is explained by the fact that data distribution across RDD partitions is not idempotent, and could be rearranged or updated during the query execution, thus affecting the output of the randomSplit method. Note Spark DataFrames and RDDs preserve partitioning order; this problem only exists when query output depends on the actual data distribution across partitions, for example, values from files 1, 2 and 3 always appear in partition 1. The issue could also be observed when using Delta cache. All solutions listed below are still applicable in this case. Solution Do one of the following: Use explicit Apache Spark RDD caching Python Copy df = inputDF.cache()\na,b = df.randomSplit([0.5, 0.5])\n Repartition by a column or a set of columns Python Copy df = inputDF.repartition(100, 'col1')\na,b = df.randomSplit([0.5, 0.5])\n Apply an aggregate function Python Copy df = inputDF.groupBy('col1').count()\na,b = df.randomSplit([0.5, 0.5])\n These operations persist or shuffle data resulting in the consistent data distribution across partitions in Spark jobs.",
          "title" : "Behavior of the randomSplit method",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/random-split-behavior"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbfs/how-to-specify-dbfs-path",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to specify the DBFS path Article 03/11/2022 2 minutes to read 3 contributors In this article Apache Spark DBUtils Shell commands When working with Azure Databricks you will sometimes have to access the Databricks File System (DBFS). Accessing files on DBFS is done with standard filesystem commands, however the syntax varies depending on the language or tool used. For example, take the following DBFS path: Copy dbfs:/mnt/test_folder/test_folder1/\n Apache Spark Under Spark, you should specify the full path inside the Spark read command. Copy spark.read.parquet(“dbfs:/mnt/test_folder/test_folder1/file.parquet”)\n DBUtils When you are using DBUtils, the full DBFS path should be used, just like it is in Spark commands. The language specific formatting around the DBFS path differs depending on the language used. Bash Copy %fs\nls dbfs:/mnt/test_folder/test_folder1/\n Python Copy dbutils.fs.ls(‘dbfs:/mnt/test_folder/test_folder1/’)\n Scala Copy dbutils.fs.ls(“dbfs:/mnt/test_folder/test_folder1/”)\n Note Specifying dbfs: is not required when using DBUtils or Spark commands. The path dbfs:/mnt/test_folder/test_folder1/ is equivalent to /mnt/test_folder/test_folder1/. Shell commands Shell commands do not recognize the DFBS path. Instead, DBFS and the files within, are accessed with the same syntax as any other folder on the file system. Bash Bash Copy ls /dbfs/mnt/test_folder/test_folder1/\ncat /dbfs/mnt/test_folder/test_folder1/file_name.txt\n Python Python Copy import os\nos.listdir('/dbfs/mnt/test_folder/test_folder1/’)\n Scala Scala Copy import java.io.File\nval directory = new File(\"/dbfs/mnt/test_folder/test_folder1/\")\ndirectory.listFiles",
          "title" : "How to specify the DBFS path",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbfs/how-to-specify-dbfs-path"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-cluster-limit-nb-output",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Job cluster limits on notebook output Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are running a notebook on a job cluster and you get an error message indicating that the output is too large. Console Copy The output of the notebook is too large. Cause: rpc response (of 20975548 bytes) exceeds limit of 20971520 bytes\n Cause This error message can occur in a job cluster whenever the notebook output is greater then 20 MB. If you are using multiple display(), displayHTML(), show() commands in your notebook, this increases the amount of output. Once the output exceeds 20 MB, the error occurs. If you are using multiple print() commands in your notebook, this can increase the output to stdout. Once the output exceeds 20 MB, the error occurs. If you are running a streaming job and enable awaitAnyTermination in the cluster’s Spark Config, it tries to fetch the entire output in a single request. If this exceeds 20 MB, the error occurs. Solution Remove any unnecessary display(), displayHTML(), print(), and show(), commands in your notebook. These can be useful for debugging, but they are not recommended for production jobs. If your job output is exceeding the 20 MB limit, try redirecting your logs to log4j or disable stdout by setting spark.databricks.driver.disableScalaOutput true in the cluster’s Spark Config. For more information, please review the documentation on output size limits.",
          "title" : "Job cluster limits on notebook output",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-cluster-limit-nb-output"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/task-deserialization-time-high",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Task deserialization time is high Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem Your tasks are running slower than expected. You review the stage details in the Spark UI on your cluster and see that task deserialization time is high. Cause Cluster-installed libraries are only installed on the driver when the cluster is started. These libraries are only installed on the executors when the first tasks are submitted. The time taken to install the PyPI libraries is included in the task deserialization time. Note Library installation only occurs on an executor where a task is launched. If a second executor is given a task, the installation process is repeated. The more libraries you have installed, the more noticeable the delay time when a new executor is launched. Solution If you are using a large number of PyPI libraries, you should configure your cluster to install the libraries on all the executors when the cluster is started. This results in a slight increase to the cluster launch time, but allows your job tasks to run faster because you don’t have to wait for libraries to install on the executors after the initial launch. Add spark.databricks.libraries.enableSparkPyPI false to the cluster’s Spark Config and restart the cluster.",
          "title" : "Task deserialization time is high",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/task-deserialization-time-high"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/use-internal-ntp",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Configure a cluster to use a custom NTP server Article 03/11/2022 2 minutes to read 3 contributors In this article Update the NTP configuration on a cluster Verify the cluster is using the updated NTP configuration By default Azure Databricks clusters use public NTP servers. This is sufficient for most use cases, however you can configure a cluster to use a custom NTP server. This does not have to be a public NTP server. It can be a private NTP server under your control. A common use case is to minimize the amount of Internet traffic from your cluster. Update the NTP configuration on a cluster Create a ntp.conf file with the following information: Copy # NTP configuration\nserver <ntp-server-hostname> iburst\n where <ntp-server-hostname> is a NTP server hostname or a NTP server IP address. If you have multiple NTP servers to list, add them all to the file. Each server should be listed on its own line. Upload the ntp.conf file to /dbfs/databricks/init_scripts/ on your cluster. Create the script ntp.sh on your cluster: Python Copy dbutils.fs.put(\"/databricks/init_scripts/ntp.sh\",\"\"\"\n#!/bin/bash\necho \"<ntp-server-ip> <ntp-server-hostname>\" >> /etc/hosts\ncp /dbfs/databricks/init_scripts/ntp.conf /etc/\nsudo service ntp restart\"\"\",True)\n Confirm that the script exists: Python Copy display(dbutils.fs.ls(\"dbfs:/databricks/init_scripts/ntp.sh\"))\n Click Clusters, click your cluster name, click Edit, click Advanced Options, click Init Scripts. Select DBFS under Destination. Enter the full path to ntp.sh and click Add. Click Confirm and Restart. A confirmation dialog box appears. Click Confirm and wait for the cluster to restart. Verify the cluster is using the updated NTP configuration Run the following code in a notebook: Bash Copy %sh ntpq -p\n The output displays the NTP servers that are in use.",
          "title" : "Configure a cluster to use a custom NTP server",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/use-internal-ntp"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/dbconnect-incompatible-dbr64",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Databricks Connect reports version error with Databricks Runtime 6.4 Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You are using the Databricks Connect client with Databricks Runtime 6.4 and receive an error message which states that the client does not support the cluster. Console Copy Caused by: java.lang.IllegalArgumentException: The cluster is running server version `dbr-6.4` but this client only supports Set(dbr-5.5). You can find a list of client releases at https://pypi.org/project/databricks-connect/#history, and install the right client version with `pip install -U databricks-connect==<version>`. For example, to install the latest 5.1 release, use `pip install -U databricks-connect==5.1.*`. To ignore this error and continue, set DEBUG_IGNORE_VERSION_MISMATCH=1.\n Cause Improvements were made to Databricks Runtime 6.4 which are incompatible with the Databricks Connect client 6.4.1 and below. Solution Upgrade the Databricks Connect client to 6.4.2. Follow the documentation to set up the client on your local workstation, making sure to set the databricks-connect value to 6.4.2. Console Copy pip install databricks-connect==6.4.2\n Warning Setting DEBUG_IGNORE_VERSION_MISMATCH=1 is not recommended, as it does not resolve the underlying compatibility issues.",
          "title" : "Databricks Connect reports version error with Databricks Runtime 6.4",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/dbconnect-incompatible-dbr64"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/dbconnect-protoserializer-stackoverflow",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents ProtoSerializer stack overflow error in DBConnect Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are using DBConnect to run a PySpark transformation on a DataFrame with more than 100 columns when you get a stack overflow error. Console Copy py4j.protocol.Py4JJavaError: An error occurred while calling o945.count.\n: java.lang.StackOverflowError\n    at java.lang.Class.getEnclosingMethodInfo(Class.java:1072)\n    at java.lang.Class.getEnclosingClass(Class.java:1272)\n    at java.lang.Class.getSimpleBinaryName(Class.java:1443)\n    at java.lang.Class.getSimpleName(Class.java:1309)\n    at org.apache.spark.sql.types.DataType.typeName(DataType.scala:67)\n    at org.apache.spark.sql.types.DataType.simpleString(DataType.scala:82)\n    at org.apache.spark.sql.types.DataType.sql(DataType.scala:90)\n    at org.apache.spark.sql.util.ProtoSerializer.serializeDataType(ProtoSerializer.scala:3207)\n    at org.apache.spark.sql.util.ProtoSerializer.serializeAttrRef(ProtoSerializer.scala:3610)\n    at org.apache.spark.sql.util.ProtoSerializer.serializeAttr(ProtoSerializer.scala:3600)\n    at org.apache.spark.sql.util.ProtoSerializer.serializeNamedExpr(ProtoSerializer.scala:3537)\n    at org.apache.spark.sql.util.ProtoSerializer.serializeExpr(ProtoSerializer.scala:2323)\n    at org.apache.spark.sql.util.ProtoSerializer$$anonfun$$nestedInanonfun$serializeCanonicalizable$1$1.applyOrElse(ProtoSerializer.scala:3001)\n    at org.apache.spark.sql.util.ProtoSerializer$$anonfun$$nestedInanonfun$serializeCanonicalizable$1$1.applyOrElse(ProtoSerializer.scala:2998)\n Performing the same operation in a notebook works correctly and does not produce an error. Example code You can reproduce the error with this sample code. It creates a DataFrame with 200 columns and renames them all. This sample code runs correctly in a notebook, but results in an error when run in DBConnect. Python Copy df = spark.createDataFrame([{str(i) : i for i in range(2000)}])\ndf = spark.createDataFrame([{str(i) : i for i in range(200)}])\nfor col in df.columns:\n  df = df.withColumnRenamed(col, col + \"_a\")\ndf.collect()\n Cause When you run code in DBConnect, some functions are handled on the remote cluster driver, but some are handled locally on the client PC. If enough memory is not allocated on the local PC, you get an error. Solution You should increase the memory allocated to the Apache Spark driver on the local PC. Run databricks-connect get-spark-home on your local PC to get the ${spark_home} value. Navigate to the ${spark_home}/conf/ folder. Open the spark-defaults.conf file. Add the following settings to the spark-defaults.conf file: text Copy spark.driver.memory 4g\nspark.driver.extraJavaOptions -Xss32M\n Save the changes. Restart DBConnect. Important DBConnect only works with supported Databricks Runtime versions. Ensure that you are using a supported runtime on your cluster before using DBConnect.",
          "title" : "ProtoSerializer stack overflow error in DBConnect",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/dbconnect-protoserializer-stackoverflow"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/dbconnect-spark-session-null",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Apache Spark session is null in DBConnect Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to run your code using Databricks Connect when you get a sparkSession is null error message. Console Copy java.lang.AssertionError: assertion failed: sparkSession is null while trying to executeCollectResult\nat scala.Predef$.assert(Predef.scala:170)\nat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:323)\nat org.apache.spark.sql.Dataset$$anonfun$50.apply(Dataset.scala:3351)\nat org.apache.spark.sql.Dataset$$anonfun$50.apply(Dataset.scala:3350)\nat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3485)\nat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3480)\nat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:111)\nat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:240)\nat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:97)\nat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:170)\nat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3480)\nat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3350)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\nat py4j.Gateway.invoke(Gateway.java:295)\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\nat py4j.GatewayConnection.run(GatewayConnection.java:251)\nat java.lang.Thread.run(Thread.java:748)\n Cause You get the sparkSession is null error message if a Spark session is not active on your cluster when you try to run your code using DBConnect. Solution You must ensure that a Spark session is active on your cluster before you attempt to run your code locally using DBConnect. You can use the following Python example code to check for a Spark session and create one if it does not exist. Python Copy from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n Important DBConnect only works with supported Databricks Runtime versions. Ensure that you are using a supported runtime on your cluster before using DBConnect.",
          "title" : "Apache Spark session is null in DBConnect",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/dbconnect-spark-session-null"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/execution/increase-tasks-per-stage",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Increase the number of tasks per stage Article 03/11/2022 2 minutes to read 3 contributors In this article When using the spark-xml package, you can increase the number of tasks per stage by changing the configuration setting spark.hadoop.mapred.max.split.size to a lower value in the cluster’s Spark configuration. This configuration setting controls the input block size. When data is read from DBFS, it is divided into input blocks, which are then sent to different executors. This configuration controls the size of these input blocks. By default, it is 128 MB (128000000 bytes). Setting this value in the notebook with spark.conf.set() is not effective. In the following example, the Spark Config field shows that the input block size is 32 MB.",
          "title" : "Increase the number of tasks per stage",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/execution/increase-tasks-per-stage"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/common-errors-in-notebooks",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Common errors in notebooks Article 03/11/2022 2 minutes to read 3 contributors In this article Spark job fails with java.lang.NoClassDefFoundError Spark job fails with java.lang.UnsupportedOperationException There are some common issues that occur when using notebooks. This section outlines some of the frequently asked questions and best practices that you should follow. Spark job fails with java.lang.NoClassDefFoundError Sometimes you may come across an error like: Scala Copy java.lang.NoClassDefFoundError: Could not initialize class line.....$read$\n This can occur with a Spark Scala 2.11 cluster and a Scala notebook, if you mix together a case class definition and Dataset/DataFrame operations in the same notebook cell, and later use the case class in a Spark job in a different cell. For example, in the first cell, say you define a case class MyClass and also created a Dataset. Scala Copy case class MyClass(value: Int)\n\nval dataset = spark.createDataset(Seq(1))\n Then in a later cell, you create instances of MyClass inside a Spark job. Scala Copy dataset.map { i => MyClass(i) }.count()\n Solution Move the case class definition to a cell of its own. Scala Copy case class MyClass(value: Int)   // no other code in this cell\n Scala Copy val dataset = spark.createDataset(Seq(1))\ndataset.map { i => MyClass(i) }.count()\n Spark job fails with java.lang.UnsupportedOperationException Sometimes you may come across an error like: Console Copy java.lang.UnsupportedOperationException: Accumulator must be registered before send to executor\n This can occur with a Spark Scala 2.10 cluster and a Scala notebook. The reason and solution for this error are same as that of Spark job fails with java.lang.NoClassDefFoundError.",
          "title" : "Common errors in notebooks",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/common-errors-in-notebooks"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/failure-when-mounting-storage",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Failure when accessing or mounting storage Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to access an existing mount point, or create a new mount point, and it fails with an error message. Console Copy Invalid Mount Exception:The backend could not get tokens for path /mnt.\n Cause The root mount path (/mnt) is also mounted to a storage location. You can verify that something is mounted to the root path by listing all mount points with DBUtils. Python Copy dbutils.fs.mounts()\n If /mnt is listed with a source, you have storage incorrectly mounted to the root path.. Solution You should unmount the root mount path. Python Copy dbutils.fs.unmount(\"/mnt\")\n You can now access existing mount points, or create new mount points.",
          "title" : "Failure when accessing or mounting storage",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/failure-when-mounting-storage"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/enable-retry-init-script",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Enable retries in init script Article 03/11/2022 2 minutes to read 2 contributors In this article Example init script Init scripts are commonly used to configure Azure Databricks clusters. There are some scenarios where you may want to implement retries in an init script. Example init script This sample init script shows you how to implement a retry for a basic copy operation. You can use this sample code as a base for implementing retries in your own init script. Scala Copy dbutils.fs.put(\"dbfs:/databricks/<path-to-init-script>/retry-example-init.sh\", \"\"\"#!/bin/bash\n\necho \"starting script at `date`\"\n\nfunction fail {\n  echo $1 >&2\n  exit 1\n}\n\nfunction retry {\n  local n=1\n  local max=5\n  local delay=5\n  while true; do\n    \"$@\" && break || {\n      if [[ $n -lt $max ]]; then\n        ((n++))\n        echo \"Command failed. Attempt $n/$max: `date`\"\n        sleep $delay;\n      else\n        echo \"Collecting additional info for debugging..\"\n        ps aux > /tmp/ps_info.txt\n        debug_log_file=debug_logs_${HOSTNAME}_$(date +\"%Y-%m-%d--%H-%M\").zip\n        zip -r /tmp/${debug_log_file} /var/log/ /tmp/ps_info.txt /databricks/data/logs/\n        cp /tmp/${debug_log_file} /dbfs/tmp/\n        fail \"The command has failed after $n attempts. `date`\"\n      fi\n    }\n  done\n}\n\nsleep 15s\necho \"starting Copying at `date`\"\nretry cp -rv /dbfs/libraries/xyz.jar /databricks/jars/\n\necho \"Finished script at `date`\"\n\"\"\", true)",
          "title" : "Enable retries in init script",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/enable-retry-init-script"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/ip-access-list-update-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents IP access list update returns INVALID_STATE Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to update an IP access list and you get an INVALID_STATE error message. Console Copy {\"error_code\":\"INVALID_STATE\",\"message\":\"Your current IP 3.3.3.3 will not be allowed to access the workspace under current configuration\"}\n Cause The IP access list update that you are trying to commit does not include your current public IP address. If your current IP address is not included in the access list, you are blocked from the environment. If you assume that your current IP is 3.3.3.3, this example API call results in an INVALID_STATE error message. Bash Copy curl -X POST -n \\\n  https://<databricks-instance>/api/2.0/ip-access-lists\n  -d '{\n    \"label\": \"office\",\n    \"list_type\": \"ALLOW\",\n    \"ip_addresses\": [\n        \"1.1.1.1\",\n        \"2.2.2.2/21\"\n      ]\n    }'\n Solution You must always include your current public IP address in the JSON file that is used to update the IP access list. If you assume that your current IP is 3.3.3.3, this example API call results in a successful IP access list update. Bash Copy curl -X POST -n \\\n  https://<databricks-instance>/api/2.0/ip-access-lists\n  -d '{\n    \"label\": \"office\",\n    \"list_type\": \"ALLOW\",\n    \"ip_addresses\": [\n        \"1.1.1.1\",\n        \"2.2.2.2/21\",\n        \"3.3.3.3\"\n      ]\n    }'",
          "title" : "IP access list update returns INVALID_STATE",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/ip-access-list-update-error"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/persist-metrics-csv-sink-dbfs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Persist Apache Spark CSV metrics to a DBFS location Article 03/11/2022 2 minutes to read 2 contributors In this article Create an init script Cluster-scoped init script Verify that CSV metrics are correctly written Spark has a configurable metrics system that supports a number of sinks, including CSV files. In this article, we are going to show you how to configure an Azure Databricks cluster to use a CSV sink and persist those metrics to a DBFS location. Create an init script All of the configuration is done in an init script. The init script does the following three things: Configures the cluster to generate CSV metrics on both the driver and the worker. Writes the CSV metrics to a temporary, local folder. Uploads the CSV metrics from the temporary, local folder to the chosen DBFS location. Note The CSV metrics are saved locally before being uploaded to the DBFS location because DBFS is not designed for a large number of random writes. Customize the sample code and then run it in a notebook to create an init script on your cluster. Sample code to create an init script: Python Copy dbutils.fs.put(\"/<init-path>/metrics.sh\",\"\"\"\n#!/bin/bash\nmkdir /tmp/csv\nsudo bash -c \"cat <<EOF >> /databricks/spark/dbconf/log4j/master-worker/metrics.properties\n*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\nspark.metrics.staticSources.enabled true\nspark.metrics.executorMetricsSource.enabled true\nspark.executor.processTreeMetrics.enabled true\nspark.sql.streaming.metricsEnabled true\nmaster.source.jvm.class org.apache.spark.metrics.source.JvmSource\nworker.source.jvm.class org.apache.spark.metrics.source.JvmSource\n*.sink.csv.period 5\n*.sink.csv.unit seconds\n*.sink.csv.directory /tmp/csv/\nworker.sink.csv.period 5\nworker.sink.csv.unit seconds\nEOF\"\n\nsudo bash -c \"cat <<EOF >> /databricks/spark/conf/metrics.properties\n*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\nspark.metrics.staticSources.enabled true\nspark.metrics.executorMetricsSource.enabled true\nspark.executor.processTreeMetrics.enabled true\nspark.sql.streaming.metricsEnabled true\ndriver.source.jvm.class org.apache.spark.metrics.source.JvmSource\nexecutor.source.jvm.class org.apache.spark.metrics.source.JvmSource\n*.sink.csv.period 5\n*.sink.csv.unit seconds\n*.sink.csv.directory /tmp/csv/\nworker.sink.csv.period 5\nworker.sink.csv.unit seconds\nEOF\"\n\ncat <<'EOF' >> /tmp/asynccode.sh\n#!/bin/bash\nDB_CLUSTER_ID=$(echo $HOSTNAME | awk -F '-' '{print$1\"-\"$2\"-\"$3}')\nMYIP=$(hostname -I)\nif [[ ! -d /dbfs/<metrics-path>/${DB_CLUSTER_ID}/metrics-${MYIP} ]] ; then\nsudo mkdir -p /dbfs/<metrics-path>/${DB_CLUSTER_ID}/metrics-${MYIP}\nfi\nwhile true; do\n    if [ -d \"/tmp/csv\" ]; then\n        sudo cp -r /tmp/csv/* /dbfs/<metrics-path>/$DB_CLUSTER_ID/metrics-$MYIP\n  fi\n  sleep 5\ndone\nEOF\nchmod a+x /tmp/asynccode.sh\n/tmp/asynccode.sh & disown\n\"\"\", True)\n Replace <init-path> with the DBFS location you want to use to save the init script. Replace <metrics-path> with the DBFS location you want to use to save the CSV metrics. Cluster-scoped init script Once you have created the init script on your cluster, you must configure it as a cluster-scoped init script. Verify that CSV metrics are correctly written Restart your cluster and run a sample job. Check the DBFS location that you configured for CSV metrics and verify that they were correctly written.",
          "title" : "Persist Apache Spark CSV metrics to a DBFS location",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/persist-metrics-csv-sink-dbfs"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/nbconvert-wrong-color-assert",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Python command fails with AssertionError: wrong color format Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You run a Python notebook and it fails with an AssertionError: wrong color format message. An example stack trace: Console Copy   File \"/local_disk0/tmp/1599775649524-0/PythonShell.py\", line 39, in <module>\n    from IPython.nbconvert.filters.ansi import ansi2html\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 963, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 902, in _find_spec\n  File \"<frozen importlib._bootstrap>\", line 876, in _find_spec_legacy\n  File \"/databricks/python/lib/python3.7/site-packages/IPython/utils/shimmodule.py\", line 36, in find_module\n    mod = import_item(mirror_name)\n  File \"/databricks/python/lib/python3.7/site-packages/IPython/utils/importstring.py\", line 31, in import_item\n    module = __import__(package, fromlist=[obj])\n  File \"/databricks/python/lib/python3.7/site-packages/nbconvert/__init__.py\", line 4, in <module>\n    from .exporters import *\n  File \"/databricks/python/lib/python3.7/site-packages/nbconvert/exporters/__init__.py\", line 4, in <module>\n    from .slides import SlidesExporter\n  File \"/databricks/python/lib/python3.7/site-packages/nbconvert/exporters/slides.py\", line 12, in <module>\n    from ..preprocessors.base import Preprocessor\n  File \"/databricks/python/lib/python3.7/site-packages/nbconvert/preprocessors/__init__.py\", line 7, in <module>\n    from .csshtmlheader import CSSHTMLHeaderPreprocessor\n  File \"/databricks/python/lib/python3.7/site-packages/nbconvert/preprocessors/csshtmlheader.py\", line 14, in <module>\n    from jupyterlab_pygments import JupyterStyle\n  File \"/databricks/python/lib/python3.7/site-packages/jupyterlab_pygments/__init__.py\", line 4, in <module>\n    from .style import JupyterStyle\n  File \"/databricks/python/lib/python3.7/site-packages/jupyterlab_pygments/style.py\", line 10, in <module>\n    class JupyterStyle(Style):\n  File \"/databricks/python/lib/python3.7/site-packages/pygments/style.py\", line 101, in __new__\n    ndef[0] = colorformat(styledef)\n  File \"/databricks/python/lib/python3.7/site-packages/pygments/style.py\", line 58, in colorformat\n    assert False, \"wrong color format %r\" % text\nAssertionError: wrong color format 'var(--jp-mirror-editor-variable-color)'\n Cause This is caused by an incompatible version of the nbconvert library. If you do not have nbconvert pinned to the correct version, it is possible to accidentally install an incompatible version via PyPI. Solution Manually install nbconvert version 6.0.0rc0 on the cluster. This overrides any incorrect version of the library that may have been installed. Click the clusters icon in the sidebar. Click the cluster name. Click the Libraries tab. Click Install New. In the Library Source button list, select PyPi. Enter nbconvert==6.0.0rc0 in the Package field. Click Install.",
          "title" : "Python command fails with AssertionError: wrong color format",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/nbconvert-wrong-color-assert"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/pystan-fails-dbr64es",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Latest PyStan fails to install on Databricks Runtime 6.4 Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to install the PyStan PyPi package on a Databricks Runtime 6.4 Extended Support cluster and get a ManagedLibraryInstallFailed error message. Console Copy java.lang.RuntimeException: ManagedLibraryInstallFailed: org.apache.spark.SparkException: Process List(/databricks/python/bin/pip, install, pystan, --disable-pip-version-check) exited with code 1.   Could not find a version that satisfies the requirement httpstan<4.5,>=4.4 (from pystan) (from versions: 0.1.0, 0.1.1, 0.2.3, 0.2.5, 0.3.0, 0.3.1, 0.4.0, 0.5.0, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.7.5, 0.7.6, 0.8.0, 0.9.0, 0.10.1, 1.0.0)\nNo matching distribution found for httpstan<4.5,>=4.4 (from pystan)\n for library:PythonPyPiPkgId(pystan,None,None,List()),isSharedLibrary=false\n Cause When you install PyStan via PyPi, it attempts to install the latest version. PyStan 3.0.0 and above are not compatible with Databricks Runtime 6.4 Extended Support. Solution You should use pystan version 2.19.1.1 on Databricks Runtime 6.4 Extended Support. Specify pystan==2.19.1.1 when you install the library on your cluster. This is the most recent version that is compatible with Databricks Runtime 6.4 Extended Support. If you require pystan version 3.0.0 or above, you should upgrade to Databricks Runtime 7.3 LTS or above.",
          "title" : "Latest PyStan fails to install on Databricks Runtime 6.4",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/pystan-fails-dbr64es"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/extract-feature-info",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to extract feature information for tree-based Apache SparkML pipeline models Article 03/11/2022 2 minutes to read 3 contributors In this article When you are fitting a tree-based model, such as a decision tree, random forest, or gradient boosted tree, it is helpful to be able to review the feature importance levels along with the feature names. Typically models in SparkML are fit as the last stage of the pipeline. To extract the relevant feature information from the pipeline with the tree model, you must extract the correct pipeline stage. You can extract the feature names from the VectorAssembler object: Python Copy from pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline(stages=[indexer, assembler, decision_tree)\nDTmodel = pipeline.fit(train)\nva = dtModel.stages[-2]\ntree = DTmodel.stages[-1]\n\ndisplay(tree) #visualize the decision tree model\nprint(tree.toDebugString) #print the nodes of the decision tree model\n\nlist(zip(va.getInputCols(), tree.featureImportances))\n You can also tune a tree-based model using a cross validator in the last stage of the pipeline. To visualize the decision tree and print the feature importance levels, you extract the bestModel from the CrossValidator object: Python Copy from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\ncv = CrossValidator(estimator=decision_tree, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\npipelineCV = Pipeline(stages=[indexer, assembler, cv)\nDTmodelCV = pipelineCV.fit(train)\nva = DTmodelCV.stages[-2]\ntreeCV = DTmodelCV.stages[-1].bestModel\n\ndisplay(treeCV) #visualize the best decision tree model\nprint(treeCV.toDebugString) #print the nodes of the decision tree model\n\nlist(zip(va.getInputCols(), treeCV.featureImportances))\n The display function visualizes decision tree models only. See Machine learning visualizations.",
          "title" : "How to extract feature information for tree-based Apache SparkML pipeline models",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/extract-feature-info"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/knn-model-pyfunc-modulenotfounderror",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents KNN model using pyfunc returns ModuleNotFoundError or FileNotFoundError Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have created a Sklearn model using KNeighborsClassifier and are using pyfunc to run a prediction. For example: Python Copy import mlflow.pyfunc\npyfunc_udf = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri, result_type='string')\npredicted_df = merge.withColumn(\"prediction\", pyfunc_udf(*merge.columns[1:]))\npredicted_df.collect()\n The prediction returns a ModuleNotFoundError: No module named 'sklearn.neighbors._classification' error message. The prediction may also return a FileNotFoundError: [Errno 2] No usable temporary directory found error message. Cause When a KNN model is logged, all of the data points used for training are saved as part of the pickle file. If the model is trained with millions of records, all of that data is added to the model, which can dramatically increase its size. A model trained on millions of records can easily total multiple GBs. pyfunc attempts to load the entire model into the executor’s cache when running a prediction. If the model is too big to fit into memory, it results in one of the above error messages. Solution You should use a tree-based algorithm, such as Random Forest or XGBoost to downsample the data in a KNN model. If you have unbalanced data, attempt a sampling method like SMOTE, when training a tree-based algorithm.",
          "title" : "KNN model using pyfunc returns ModuleNotFoundError or FileNotFoundError",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/knn-model-pyfunc-modulenotfounderror"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/mlflow-artifacts-custom-storage",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Experiment warning when custom artifact storage location is used Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem When you create an MLflow experiment with a custom artifact location, you get the following warning: Cause MLflow experiment permissions are enforced on artifacts in MLflow Tracking, enabling you to easily control access to datasets, models, and other files. MLflow cannot guarantee the enforcement of access controls on artifacts stored in custom locations. Solution Databricks recommends using the default artifact location when creating an MLflow experiment. The default storage location is backed by access controls.",
          "title" : "Experiment warning when custom artifact storage location is used",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/mlflow-artifacts-custom-storage"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/mlflow-artifacts-legacy-storage",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Experiment warning when legacy artifact storage location is used Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem A new icon appears on the MLflow Experiments page with the following open access warning: Cause MLflow experiment permissions are enforced on artifacts in MLflow Tracking, enabling you to easily control access to datasets, models, and other files. In MLflow 1.11 and above, new experiments store artifacts in an MLflow-managed location (dbfs:/databricks/mlflow-tracking/) that enforces experiment access controls. Certain older experiments use a legacy storage location (dbfs:/databricks/mlflow/) that can be accessed by all users of your workspace. This warning indicates that your experiment uses a legacy artifact storage location. Solution You should always use the MLflow-managed DBFS storage locations when logging artifacts to experiments. This protects against unintended or unauthorized access to your MLflow artifacts.",
          "title" : "Experiment warning when legacy artifact storage location is used",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/mlflow-artifacts-legacy-storage"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/mlflow-artifacts-no-client-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Errors when accessing MLflow artifacts without using the MLflow client Article 03/11/2022 2 minutes to read 3 contributors In this article Invalid mount exception FileNotFoundError MLflow experiment permissions are now enforced on artifacts in MLflow Tracking, enabling you to easily control access to your datasets, models, and other files. Invalid mount exception Problem When trying to access an MLflow run artifact using Databricks File System (DBFS) commands, such as dbutils.fs, you get the following error: Console Copy com.databricks.backend.daemon.data.common.InvalidMountException: Error while using path /databricks/mlflow-tracking/<experiment-id>/<run-id>/artifacts for resolving path &#39;/<experiment-id>/<run-id>/artifacts&#39; within mount at &#39;/databricks/mlflow-tracking&#39;.\n Cause With the extension of MLflow experiment permissions to artifacts, DBFS access APIs for run artifacts stored in dbfs:/databricks/mlflow-tracking/ are no longer supported. Solution Upgrade to MLflow client version 1.9.1 or above to download, list, or upload artifacts stored in dbfs:/databricks/mlflow-tracking/. Bash Copy %sh\npip install --upgrade mlflow\n FileNotFoundError Problem When trying to access an MLflow run artifact using %sh/os.listdir(), you get the following error: Console Copy FileNotFoundError: [Errno 2] No such file or directory: '/databricks/mlflow-tracking/'\n Cause With the extension of MLflow experiment permissions to artifacts, run artifacts stored in dbfs:/databricks/mlflow-tracking/ can only be accessed using MLflow client version 1.9.1 or above. Solution Upgrade to MLflow client version 1.9.1 or above to download, list, or upload artifacts stored in dbfs:/databricks/mlflow-tracking/. Bash Copy %sh\npip install --upgrade mlflow",
          "title" : "Errors when accessing MLflow artifacts without using the MLflow client",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/mlflow-artifacts-no-client-error"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/mlflow-artifacts-permission-denied",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents PERMISSION_DENIED error when accessing MLflow experiment artifacts Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You get a PERMISSION_DENIED error when trying to access an MLflow artifact using the MLflow client. Console Copy RestException: PERMISSION_DENIED: User <user> does not have permission to 'View' experiment with id <experiment-id>\n or Console Copy RestException: PERMISSION_DENIED: User <user> does not have permission to 'Edit' experiment with id <experiment-id>\n Cause With the extension of MLflow experiment permissions to artifacts, you must have explicit permission to access artifacts of an MLflow experiment. The error suggests that you do not have permission to access artifacts of the experiment. Solution Ask the experiment owner to give you the appropriate level of permissions to access the experiment. Experiment permissions automatically apply to artifacts of an experiment.",
          "title" : "PERMISSION_DENIED error when accessing MLflow experiment artifacts",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/mlflow-artifacts-permission-denied"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/x13path-not-found-statsmodels-tsa",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents X13PATH environmental variable not found Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are building a Time Series Arima model using statsmodels.tsa.x13 and get an error message. Console Copy x12a and x13as not found on path. Give the path, put them on PATH, or set the X12PATH or X13PATH environmental variable.\n Cause You do not have the required X13 binaries in your path. Solution Download the X13 binaries and declare the location as the X13PATH in your notebook. Download x13asall_V1.1_B39.tar.gz fromhttps://www.census.gov/srd/www/x13as/x13down_unix.html. Untar the file to a folder. In the notebook click File. Click Upload Data. Upload the folder you created from x13asall_V1.1_B39.tar.gz. Verify the files exist on the cluster. Copy %fs ls dbfs:/FileStore/shared_uploads/<path-to-uploaded-X13-folder>/\n Define X13PATH as the path to the uploaded X13 folder. Python Copy X13PATH = \"/dbfs/FileStore/shared_uploads/<path-to-uploaded-X13-folder>/\"\n You can now successfully build the model Example code Python Copy import statsmodels.api  as sm\nfrom statsmodels.tsa.x13 import x13_arima_select_order, _find_x12\nfrom statsmodels.tsa.x13 import x13_arima_analysis\n\nimport pandas as pd\nfrom pandas import Timestamp\nimport scipy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ns = pd.Series(\n    {Timestamp('2013-03-01 00:00:00'): 838.2,\n     Timestamp('2013-04-01 00:00:00'): 865.17,\n     Timestamp('2013-05-01 00:00:00'): 763.0,\n     Timestamp('2013-06-01 00:00:00'): 802.99,\n     Timestamp('2013-07-01 00:00:00'): 875.56,\n     Timestamp('2013-08-01 00:00:00'): 754.4,\n     Timestamp('2013-09-01 00:00:00'): 617.48,\n     Timestamp('2013-10-01 00:00:00'): 994.75,\n     Timestamp('2013-11-01 00:00:00'): 860.86,\n     Timestamp('2013-12-01 00:00:00'): 786.66,\n     Timestamp('2014-01-01 00:00:00'): 908.48,\n     Timestamp('2014-02-01 00:00:00'): 980.88,\n     Timestamp('2014-03-01 00:00:00'): 1453.73,\n     Timestamp('2014-04-01 00:00:00'): 1473.33,\n     Timestamp('2014-05-01 00:00:00'): 1480.44,\n     Timestamp('2014-06-01 00:00:00'): 1433.91,\n     Timestamp('2014-07-01 00:00:00'): 1386.58,\n     Timestamp('2014-08-01 00:00:00'): 1437.35,\n     Timestamp('2014-09-01 00:00:00'): 1207.07,\n     Timestamp('2014-10-01 00:00:00'): 973.3,\n     Timestamp('2014-11-01 00:00:00'): 962.18,\n     Timestamp('2014-12-01 00:00:00'): 717.69,\n     Timestamp('2015-01-01 00:00:00'): 873.06,\n     Timestamp('2015-02-01 00:00:00'): 881.65,\n     Timestamp('2015-03-01 00:00:00'): 1252.92,\n     Timestamp('2015-04-01 00:00:00'): 866.94,\n     Timestamp('2015-05-01 00:00:00'): 1498.05,\n     Timestamp('2015-06-01 00:00:00'): 1282.31,\n     Timestamp('2015-07-01 00:00:00'): 1411.33,\n     Timestamp('2015-08-01 00:00:00'): 1379.05,\n     Timestamp('2015-09-01 00:00:00'): 1334.52,\n     Timestamp('2015-10-01 00:00:00'): 1231.86,\n     Timestamp('2015-11-01 00:00:00'): 1088.14,\n     Timestamp('2015-12-01 00:00:00'): 967.35,\n     Timestamp('2016-01-01 00:00:00'): 1266.37,\n     Timestamp('2016-02-01 00:00:00'): 1278.79,\n     Timestamp('2016-03-01 00:00:00'): 1497.8,\n     Timestamp('2016-04-01 00:00:00'): 1352.27},\n    name='Cost')\n\nres = sm.tsa.x13_arima_analysis(s,x12path=X13PATH)\nres.plot()\nplt.show()\n Example result",
          "title" : "X13PATH environmental variable not found",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/x13path-not-found-statsmodels-tsa"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/best-practice-cache-count-take",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Best practice for cache(), count(), and take() Article 03/11/2022 2 minutes to read 2 contributors In this article Calling cache() and count() separately Calling take() on a cached DataFrame Calling count() on a cached DataFrame Summary cache() is an Apache Spark transformation that can be used on a DataFrame, Dataset, or RDD when you want to perform more than one action. cache() caches the specified DataFrame, Dataset, or RDD in the memory of your cluster’s workers. Since cache() is a transformation, the caching operation takes place only when a Spark action (for example, count(), show(), take(), or write()) is also used on the same DataFrame, Dataset, or RDD in a single action. Calling cache() and count() separately Scala Copy df1=spark.read.parquet(input_path1)\ndf2=spark.read.parquet(input_path2)\ndf1.cache()                                         # Cache DataFrame df1\n\njoined_df = df1.join(df2, df1.id==df2.id, ‘inner’)  # Join DataFrame df1 and df2\nfiltered_df = joined_df.filter(“name == ‘John’”)    # Filter the joined DataFrame for the name “John”\ndf1.count()                                         # Call count() on the cached DataFrame\nfiltered_df.show()                                  # Show the filtered DataFrame filtered_df\n In this example, DataFrame df1 is cached into memory when df1.count() is executed. df1.cache() does not initiate the caching operation on DataFrame df1. Calling take() on a cached DataFrame Scala Copy df=spark.table(“input_table_name”)\ndf.cache.take(5)                   # Call take(5) on the DataFrame df, while also caching it\ndf.count()                         # Call count() on the DataFrame df\n In this example, DataFrame df is cached into memory when take(5) is executed. Only one partition of DataFrame df is cached in this case, because take(5) only processes 5 records. Only the partition from which the records are fetched is processed, and only that processed partition is cached. Other partitions of DataFrame df are not cached. As a result, when df.count() is called, DataFrame df is created again, since only one partition is available in the cluster’s cache. Calling take(5) in the example only caches 14% of the DataFrame. Calling count() on a cached DataFrame Scala Copy df=spark.table(“input_table_name”)\ndf.cache.count()                    # Call count() on the DataFrame df, while also caching it\ndf.count()                          # Call count() on the DataFrame df\ndf.filter(“name==’John’”).count()\n In this example, DataFrame df is cached into memory when df.count() is executed. To return the count of the dataframe, all the partitions are processed. This means that all the partitions are cached. As a result, when df.count() and df.filter(“name==’John'”).count() are called as subsequent actions, DataFrame df is fetched from the cluster’s cache, rather than getting created again. Calling count() in the example caches 100% of the DataFrame. Summary You should call count() or write() immediately after calling cache() so that the entire DataFrame is processed and cached in memory. If you only cache part of the DataFrame, the entire DataFrame may be recomputed when a subsequent action is performed on the DataFrame. Note The advice for cache() also applies to persist().",
          "title" : "Best practice for cache(), count(), and take()",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/best-practice-cache-count-take"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/cannot-import-timestamp-millis-unix-millis",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cannot import timestamp_millis or unix_millis Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to import timestamp_millis or unix_millis into a Scala notebook, but get an error message. Scala Copy import org.apache.spark.sql.functions.{timestamp_millis, unix_millis}\n Console Copy error: value timestamp_millis is not a member of object org.apache.spark.sql.functions\nimport org.apache.spark.sql.functions.{timestamp_millis, unix_millis}\n Cause The functions timestamp_millis and unix_millis are not available in the Apache Spark DataFrame API. These functions are specific to SQL and are included in Spark 3.1.1 and above. Solution You need to use selectExpr() with timestamp_millis or unix_millis if you want to use either one of them with a DataFrame. selectExpr() takes a set of SQL expressions and runs them. For example, this sample code returns an error message when run. Scala Copy import sqlContext.implicits._\nval df = Seq(\n (1, \"First Value\"),\n (2, \"Second Value\")\n).toDF(\"int_column\", \"string_column\")\n\nimport org.apache.spark.sql.functions.{unix_millis}\nimport org.apache.spark.sql.functions.col\ndf.select(unix_millis(col(\"int_column\"))).show()\n Console Copy error: value unix_millis is not a member of object org.apache.spark.sql.functions\nimport org.apache.spark.sql.functions.{unix_millis}\n While this sample code, using selectExpr(), successfully returns timestamp values. Scala Copy import org.apache.spark.sql.functions._\nimport sqlContext.implicits._\nval ndf = Seq(\n (1, \"First Value\"),\n (2, \"Second Value\")\n).toDF(\"int_column\", \"string_column\")\n\ndisplay(ndf.selectExpr(\"timestamp_millis(int_column)\"))\n Example notebook Cannot import timestamp_millis or unix_millis example Get notebook",
          "title" : "Cannot import timestamp_millis or unix_millis",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/cannot-import-timestamp-millis-unix-millis"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/id-duplicate-on-append",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Identify duplicate data on append operations Article 03/11/2022 2 minutes to read 2 contributors In this article Identify columns with duplicate records Identify input files with duplicate data Identify the location table Use the location table results to search for parquet paths Check the Delta history for the impacted versions Example notebook A common issue when performing append operations on Delta tables is duplicate data. For example, assume user 1 performs a write operation on Delta table A. At the same time, user 2 performs an append operation on Delta table A. This can lead to duplicate records in the table. In this article, we review basic troubleshooting steps that you can use to identify duplicate records, as well as the user name, and notebooks or jobs that resulted in the duplicate data. Identify columns with duplicate records SQL Copy select count(*) as count, <column-name> from <table-name> group by <column-name> order by <column-name>\n The output identifies all columns with duplicate data. Identify input files with duplicate data Select a data point from the previous query and use it to determine which files provided duplicate data. SQL Copy select *, input_file_name() as path from <table-name> where <column-name>=<any-duplicated-value>\n The output includes a column called path, which identifies the full path to each input file. Identify the location table SQL Copy describe table extended <table-name>\n Use the location table results to search for parquet paths Bash Copy grep -r 'part-<filename-01>.snappy.parquet' /dbfs/user/hive/warehouse/<path-to-log>/_delta_log\n Bash Copy grep -r 'part-<filename-02.snappy.parquet' /dbfs/user/hive/warehouse/<path-to-log>/_delta_log\n The results allow you to identify the impacted Delta versions. Check the Delta history for the impacted versions SQL Copy select * from (describe history <table-name> ) t where t.version In(0,1)\n The Delta history results provide the user name, as well as the notebook or job id that caused the duplicate to appear in the Delta table. Now that you have identified the source of the duplicate data, you can modify the notebook or job to prevent it from happening. Example notebook Identify duplicate data on append example Get notebook",
          "title" : "Identify duplicate data on append operations",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/id-duplicate-on-append"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/unable-to-cast-string-to-varchar",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Unable to cast string to varchar Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to cast a string type column to varchar but it isn’t working. Note The varchar data type is available in Databricks Runtime 8.0 and above. Create a simple Delta table, with one column as type string. SQL Copy CREATE OR REPLACE TABLE delta_table1 (`col1` string)\nUSING DELTA;\n Use SHOW TABLE on the newly created table and it reports a string type. SQL Copy SHOW CREATE TABLE delta_table1;\n Create a second Delta table, based on the first, and convert the string type column into varchar. Copy CREATE OR REPLACE TABLE delta_varchar_table1\nUSING DELTA\nAS\nSELECT cast(col1 AS VARCHAR(1000)) FROM delta_table1;\n Use SHOW TABLE on the newly created table and it reports that the table got created, but the column is string type. SQL Copy SHOW CREATE TABLE delta_varchar_table1;\n Cause The varchar type can only be used in table schema. It cannot be used in functions or operators. Please review the Spark supported data types documentation for more information. Solution You cannot cast string to varchar, but you can create a varchar Delta table. SQL Copy CREATE OR REPLACE TABLE delta_varchar_table2 (`col1` VARCHAR(1000))\nUSING DELTA;\n Use SHOW TABLE on the newly created table and it reports a varchar type. SQL Copy SHOW CREATE TABLE delta_varchar_table2;\n You can now create another varchar Delta table, based on the first, and it keeps the varchar type. SQL Copy CREATE OR REPLACE TABLE delta_varchar_table3\nUSING DELTA\nAS\nSELECT * FROM delta_varchar_table2;\n Use SHOW TABLE on the newly created table and it reports a varchar type. SQL Copy SHOW CREATE TABLE delta_varchar_table3;",
          "title" : "Unable to cast string to varchar",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/unable-to-cast-string-to-varchar"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/azure-spark-shuffle-fetch-fail",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Jobs failing with shuffle fetch failures Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are seeing intermittent Apache Spark job failures on jobs using shuffle fetch. Console Copy 21/02/01 05:59:55 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 4, 10.79.1.45, executor 0): FetchFailed(BlockManagerId(1, 10.79.1.134, 4048, None), shuffleId=1, mapId=0, reduceId=0, message=\norg.apache.spark.shuffle.FetchFailedException: Failed to connect to /10.79.1.134:4048\nat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:553)\nat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:484)\nat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:63)\n... 1 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedNoRouteToHostException: No route to host: /10.79.1.134:4048\nat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n Cause This can happen if you have modified the Azure Databricks subnet CIDR range after deployment. This behavior is not supported. Assume the below details describe two scenarios: Original Azure Databricks subnet CIDR Private subnet: 10.10.0.0/24 (10.10.0.0 - 10.10.0.255) Public subnet: 10.10.1.0/24 (10.10.1.0 - 10.10.1.255) Modified Azure Databricks subnet CIDR Private subnet: 10.10.0.0/18 (10.10.0.0 - 10.10.63.255) Public subnet: 10.10.64.0/24 (10.10.64.0 - 10.10.127.255) With the original settings, everything works as intended. With the modified settings, if executors are assigned IP addresses in the subnet range 10.10.1.0 - 10.10.63.255 and the driver assigned an IP address in the subnet range 10.10.0.0 - 10.10.0.255, the communication between executors is blocked due to a firewall rule limiting communication in the original CIDR range of 10.10.0.0/24. If the executors and driver are both assigned IP addresses in 10.10.0.0/24, no communication is blocked and the job runs as intended. However, this assignment is not guaranteed under the modified settings. Solution Revert any subnet CIDR changes and restore the original VNet configuration that you used to create the Azure Databricks workspace. Restart your cluster. Resubmit your jobs.",
          "title" : "Jobs failing with shuffle fetch failures",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/azure-spark-shuffle-fetch-fail"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/identify-less-used-jobs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Identify less used jobs Article 03/11/2022 2 minutes to read 2 contributors In this article Jobs Quota notebook Import the Jobs Quota notebook Run the Jobs Quota notebook Review results Delete jobs The workspace has a limit on the number of jobs that can be shown in the UI. The current job limit is 1000. If you exceed the job limit, you receive a QUOTA_EXCEEDED error message. Console Copy 'error_code':'QUOTA_EXCEEDED','message':'The quota for the number of jobs has been reached. The current quota is 1000. This quota is only applied to jobs created through the UI or through the /jobs/create endpoint, which are displayed in the Jobs UI\n To resolve this issue, you must delete jobs from the workspace UI. You can manually select jobs for removal, or you can use the Jobs Quota notebook to identify jobs which are not used very often and delete them. The Jobs Quota notebook identifies jobs for deletion based on: Jobs which have never run. Jobs that have not run in the past 14 days. Scheduled jobs that have been paused for 14 days or more. Jobs Quota notebook Get notebook Import the Jobs Quota notebook You can import the Jobs Quota notebook directly into your workspace and attach the notebook to a cluster. Attach it to an all-purpose cluster. Run the Jobs Quota notebook Open the Jobs Quota notebook and click Run All to run the notebook. Review results After you run the Jobs Quota notebook, a data table is generated. This table displays information about all the running jobs in your workspace, such as the URL of a job, date_of_creation, created_by, last run (days from current time), and other job details. Note If no other jobs are available in your workspace, the Jobs Quota notebook displays the message No Jobs To Display. Delete jobs You can delete the unneeded jobs manually. Pass the job_id of the job you want to delete to the function deleteByJobId() in the Jobs Quota notebook. A confirmation message is shown when a job is successfully deleted. An error message is shown if you attempt to delete a job that does not exist. Keep calling the deleteByJobId() function in the Jobs Quota notebook until all unneeded jobs are deleted.",
          "title" : "Identify less used jobs",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/identify-less-used-jobs"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbfs/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Databricks File System (DBFS): tips and troubleshooting Article 03/11/2022 2 minutes to read 4 contributors In this article These articles can help you with the Databricks File System (DBFS). Cannot read Azure Databricks objects stored in the DBFS root directory How to specify the DBFS path Operation not supported during append Parallelize filesystem operations Remount a storage account after rotating access keys",
          "title" : "Databricks File System (DBFS): tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbfs/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Developer tools: tips and troubleshooting Article 03/11/2022 2 minutes to read 4 contributors In this article These articles can help you with the tools you use to develop and manage Azure Databricks applications outside the Azure Databricks environment. Apache Spark session is null in DBConnect Databricks Connect reports version error with Databricks Runtime 6.4 Failed to create process error with Databricks CLI in Windows GeoSpark undefined function error with DBConnect Get Apache Spark config in DBConnect Invalid Access Token error when running jobs with Airflow Common errors using Azure Data Factory ProtoSerializer stack overflow error in DBConnect Use tcpdump to create pcap files",
          "title" : "Developer tools: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/create-table-error-external-hive",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Error in CREATE TABLE with external Hive metastore Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are connecting to an external MySQL metastore and attempting to create a table when you get an error. Console Copy AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException:\nMetaException(message:An exception was thrown while adding/validating class(es) : (conn=21)\nColumn length too big for column 'PARAM_VALUE' (max = 16383); use BLOB or TEXT instead.\n Cause This is a known issue with MySQL 8.0 when the default charset is utfmb4. You can confirm this by running a query on the database with the error. SQL Copy SELECT default_character_set_name FROM information_schema.SCHEMATA S WHERE schema_name = \"<database-name>\"\n Solution You need to update or recreate the database and set the charset to latin1. Option 1 Manually run create statements in the Hive database with DEFAULT CHARSET=latin1 at the end of each CREATE TABLE statement. SQL Copy CREATE TABLE `TABLE_PARAMS`\n(\n    `TBL_ID` BIGINT NOT NULL,\n    `PARAM_KEY` VARCHAR(256) BINARY NOT NULL,\n    `PARAM_VALUE` VARCHAR(4000) BINARY NULL,\n    CONSTRAINT `TABLE_PARAMS_PK` PRIMARY KEY (`TBL_ID`,`PARAM_KEY`)\n) ENGINE=INNODB DEFAULT CHARSET=latin1;\n Restart the Hive metastore and repeat until all creation errors have been resolved. Option 2 Setup the database and user accounts. Create the database and run alter database hive character set latin1; before you launch the metastore. This command sets the default CHARSET for the database. It is applied when the metastore creates tables.",
          "title" : "Error in CREATE TABLE with external Hive metastore",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/create-table-error-external-hive"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/drop-table-exception-azure-metastore",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents AnalysisException when dropping table on Azure-backed metastore Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem When you try to drop a table in an external Hive version 2.0 or 2.1 metastore that is deployed on Azure SQL Database, Azure Databricks throws the following exception: Console Copy com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Exception thrown when executing query : SELECT 'org.apache.hadoop.hive.metastore.model.MStorageDescriptor' AS NUCLEUS_TYPE,A0.INPUT_FORMAT,A0.IS_COMPRESSED,A0.IS_STOREDASSUBDIRECTORIES,A0.LOCATION,A0.NUM_BUCKETS,A0.OUTPUT_FORMAT,A0.SD_ID FROM SDS A0 WHERE A0.CD_ID = ? OFFSET 0 ROWS FETCH NEXT ROW ONLY );\n  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:107)\n  at org.apache.spark.sql.hive.HiveExternalCatalog.doDropTable(HiveExternalCatalog.scala:483)\n  at org.apache.spark.sql.catalyst.catalog.ExternalCatalog.dropTable(ExternalCatalog.scala:122)\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.dropTable(SessionCatalog.scala:638)\n  at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:212)\n Cause This is a known Hive bug (HIVE-14698), caused by another known bug with the datanucleus-rdbms module in the package. It is fixed in datanucleus-rdbms 4.1.16. However, Hive 2.0 and 2.1 metastores use version 4.1.7 and these versions are affected. Solution Do one of the following: Upgrade the Hive metastore to version 2.3.0. This also resolves problems due to any other Hive bug that is fixed in version 2.3.0. Import the following notebook to your workspace and follow the instructions to replace the datanucleus-rdbms JAR. This notebook is written to upgrade the metastore to version 2.1.1. You might want to have a similar version in your server side. External metastore upgrade notebook Get notebook",
          "title" : "AnalysisException when dropping table on Azure-backed metastore",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/drop-table-exception-azure-metastore"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Notebooks: tips and troubleshooting Article 03/11/2022 2 minutes to read 4 contributors In this article These articles can help you with your Azure Databricks notebooks. How to check if a spark property is modifiable in a notebook Common errors in notebooks display() does not show microseconds correctly Error: Received command c on object id p0 Failure when accessing or mounting storage Item was too large to export Access notebooks owned by a deleted user Notebook autosave fails due to file size limits Cannot run notebook commands after canceling streaming cell Troubleshooting unresponsive Python notebooks or canceled commands Update job permissions for multiple users",
          "title" : "Notebooks: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/update-job-perms-multiple-users",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Update job permissions for multiple users Article 03/11/2022 2 minutes to read 2 contributors In this article Instructions Example code When you are running jobs, you might want to update user permissions for multiple users. You can do this by using the Azure Databricks job permissions API and a bit of Python code. Instructions Copy the example code into a notebook. Enter the <job-id> (or multiple job ids) into the array arr[]. Enter your payload{}. In this example, we are using the <username> and <permission> that we want to grant. Enter the <workspace-url> into the url field. Enter the <token> under Bearer. Run the notebook cell with the updated code. If the update is successful, the code returns a response of 200 (OK). Example code Python Copy import requests\nimport json\n\narr=[<job-id-1>,<job-id-2>]\nfor j in arr :\n  def requestcall():\n      payload = {\"access_control_list\": [{\"user_name\": \"<username>\",\"permission_level\": \"<permission>\"}]}\n      url='https://<workspace-url>/api/2.0/permissions/jobs/'+str(j)\n      myResponse = requests.patch(url=url, headers={'Authorization': 'Bearer <token>'}, verify=True, data=json.dumps(payload))\n      print(myResponse.status_code)\n      print(myResponse.content)\n        # If the API call is successful, the response code is 200 (OK).\n      if myResponse.ok:\n            # Extracting data in JSON format.\n       data = myResponse.json()\n       return data\n  requestcall()",
          "title" : "Update job permissions for multiple users",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/update-job-perms-multiple-users"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Python with Apache Spark: tips and troubleshooting Article 03/11/2022 2 minutes to read 3 contributors In this article These articles can help you to use Python with Apache Spark. AttributeError: ‘function’ object has no attribute Convert Python datetime object to string Create a cluster with Conda Display file and directory timestamp details Install and compile Cython Reading large DBFS-mounted files using Python APIs Use the HDFS API to read files in Python How to import a custom CA certificate Job remains idle before starting List all workspace objects Load special characters with Spark-XML Python commands fail on high concurrency clusters Cluster cancels Python command execution after installing Bokeh Cluster cancels Python command execution due to library conflict Python command execution fails with AttributeError Python REPL fails to start in Docker Running C++ code in Python How to run SQL queries from Python scripts Python 2 sunset status",
          "title" : "Python with Apache Spark: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/cython",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Install and compile Cython Article 03/11/2022 2 minutes to read 3 contributors In this article Add Cython Source Files to Spark Test Cython compilation on the driver node Define the wapper function to compile and import the module Run the Cython example Performance comparison This document will explain how to run Spark code with compiled Cython code. The steps are as follows: Creates an example cython module on DBFS. Adds the file to the SparkSession. Creates a wrapper method to load the module on the executors. Runs the mapper on a sample dataset. Generate a larger dataset and compare the performance with native Python example. Note By default, paths use dbfs:/ if no protocol is referenced. Python Copy # Write an example cython module to /example/cython/fib.pyx in DBFS.\ndbutils.fs.put(\"/example/cython/fib.pyx\", \"\"\"\ndef fib_mapper_cython(n):\n    '''\n    Return the first fibonnaci number > n.\n    '''\n    cdef int a = 0\n    cdef int b = 1\n    cdef int j = int(n)\n    while b<j:\n        a, b  = b, a+b\n    return b, 1\n\"\"\", True)\n\n# Write an example input file to /example/cython/input.txt in DBFS.\n# Every line of this file is an integer.\ndbutils.fs.put(\"/example/cython_input/input.txt\", \"\"\"\n1\n10\n100\n\"\"\", True)\n\n# Take a look at the example input.\ndbutils.fs.head(\"/example/cython_input/input.txt\")\n Add Cython Source Files to Spark To make the Cython source files available across the cluster, we will use sc.addPyFile to add these files to Spark. For example, Python Copy sc.addPyFile(\"dbfs:/example/cython/fib.pyx\")\n Test Cython compilation on the driver node This code will test compilation on the driver node first. Python Copy import pyximport\nimport os\n\npyximport.install()\nimport fib\n Define the wapper function to compile and import the module The print statements will get executed on the executor nodes. You can view the stdout log messages to track the progress of your module. Python Copy import sys, os, shutil, cython\n\ndef spark_cython(module, method):\n  def wrapped(*args, **kwargs):\n    print 'Entered function with: %s' % args\n    global cython_function_\n    try:\n      return cython_function_(*args, **kwargs)\n    except:\n      import pyximport\n      pyximport.install()\n      print 'Cython compilation complete'\n      cython_function_ = getattr(__import__(module), method)\n    print 'Defined function: %s' % cython_function_\n    return cython_function_(*args, **kwargs)\n  return wrapped\n Run the Cython example The below snippet runs the fibonacci example on a few data points. Python Copy # use the CSV reader to generate a Spark DataFrame. Roll back to RDDs from DataFrames and grab the single element from the GenericRowObject\nlines = spark.read.csv(\"/example/cython_input/\").rdd.map(lambda y: y.__getitem__(0))\n\nmapper = spark_cython('fib', 'fib_mapper_cython')\nfib_frequency = lines.map(mapper).reduceByKey(lambda a, b: a+b).collect()\nprint fib_frequency\n Performance comparison Below we’ll test out the speed difference between the 2 implementations. We will use the spark.range() api to generate data points from 10,000 to 100,000,000 with 50 Spark partitions. We will write this output to DBFS as a CSV. For this test, disable autoscaling in order to make sure the cluster has the fixed number of Spark executors. Python Copy dbutils.fs.rm(\"/tmp/cython_input/\", True)\nspark.range(10000, 100000000, 1, 50).write.csv(\"/tmp/cython_input/\")\n Normal PySpark code Python Copy def fib_mapper_python(n):\n  a = 0\n  b = 1\n  print \"Trying: %s\" % n\n  while b < int(n):\n    a, b = b, a+b\n  return (b, 1)\n\nprint fib_mapper_python(2000)\n\nlines = spark.read.csv(\"/tmp/cython_input/\").rdd.map(lambda y: y.__getitem__(0))\nfib_frequency = lines.map(lambda x: fib_mapper_python(x)).reduceByKey(lambda a, b: a+b).collect()\nprint fib_frequency\n Test Cython code Now test the compiled Cython code. Python Copy lines = spark.read.csv(\"/tmp/cython_input/\").rdd.map(lambda y: y.__getitem__(0))\nmapper = spark_cython('fib', 'fib_mapper_cython')\nfib_frequency = lines.map(mapper).reduceByKey(lambda a, b: a+b).collect()\nprint fib_frequency\n The test dataset we generated has 50 Spark partitions, which creates 50 csv files seen below. You can view the dataset with dbutils.fs.ls(\"/tmp/cython_input/\").",
          "title" : "Install and compile Cython",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/cython"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/job-idle-before-start",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Job remains idle before starting Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have an Apache Spark job that is triggered correctly, but remains idle for a long time before starting. You have a Spark job that ran well for awhile, but goes idle for a long time before resuming. Symptoms include: Cluster downscales to the minimum number of worker nodes during idle time. Driver logs don’t show any Spark jobs during idle time, but does have repeated information about metadata. Ganglia shows activity only on the driver node. Executor logs show no activity. After some time passes, cluster scales up and Spark jobs start or resume. Cause These symptoms indicate that there are a lot of file scan operations happening during this period of the job. Tables are read and consumed in downstream operations. You see the file scan operation details when you review the SQL tab in the Spark UI. The queries appear to be completed, which makes it appear as though no work is being performed during this idle time. The driver node is busy because it is performing the file listing and processing data (metadata containing schema and other information). This work only happens on the driver node, which is why you only see driver node activity in the Ganglia metrics during this time. This issue becomes more pronounced if you have a large number of small files. Solution You should control the file size and number of files ingested at the source location by implementing a preprocessing step. You can also break down the ingestion into a number of smaller steps, so a smaller number of files have to be scanned at once. Another option is to migrate your data store to Delta Lake, which uses transactional logs as an index for all the underlying files.",
          "title" : "Job remains idle before starting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/job-idle-before-start"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/python-command-cancelled",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cluster cancels Python command execution due to library conflict Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem The cluster returns Cancelled in a Python notebook. Notebooks in all other languages execute successfully on the same cluster. Cause When you install a conflicting version of a library, such as ipython, ipywidgets, numpy, scipy, or pandas to the PYTHONPATH, then the Python REPL can break, causing all commands to return Cancelled after 30 seconds. This also breaks %sh, the notebook macro that lets you enter shell scripts in Python notebook cells. Note If you’ve recently installed a bokeh library on the cluster, the installation may have included an incompatible tornado library. See Cluster cancels Python command execution after installing Bokeh. If you’ve installed a numpy library, it may be incompatible. See Python command execution fails with AttributeError. Solution To solve this problem, do the following: Identify the conflicting library and uninstall it. Install the correct version of the library in a notebook or with a cluster-scoped init script. Identify the conflicting library Uninstall each library one at a time, and check if the Python REPL still breaks. If the REPL still breaks, reinstall the library you removed and remove the next one. When you find the library that causes the REPL to break, install the correct version of that library using one of the two methods below. You can also inspect the driver log (std.err) for the cluster (on the Cluster Configuration page) for a stack trace and error message that can help identify the library conflict. Install the correct library Do one of the following. Option 1: Install in a notebook using pip3 Python Copy %sh sudo apt-get -y install python3-pip\n  pip3 install <library-name>\n Option 2: Install using a cluster-scoped init script Follow the steps below to create a cluster-scoped init script that installs the correct version of the library. Replace <library-name> in the examples with the filename of the library to install. If the init script does not already exist, create a base directory to store it: Bash Copy dbutils.fs.mkdirs(\"dbfs:/databricks/<directory>/\")\n Create the following script: Bash Copy dbutils.fs.put(\"/databricks/init/cluster-name/<library-name>.sh\",\"\"\"\n #!/bin/bash\n sudo apt-get -y install python3-pip\n sudo pip3 install <library-name>\n \"\"\", True)\n Confirm that the script exists: Bash Copy display(dbutils.fs.ls(\"dbfs:/databricks/<directory>/<library-name>.sh\"))\n Go to the cluster configuration page and click the Advanced Options toggle. At the bottom of the page, click the Init Scripts tab: In the Destination drop-down, select DBFS, provide the file path to the script, and click Add. Restart the cluster.",
          "title" : "Cluster cancels Python command execution due to library conflict",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/python-command-cancelled"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/pin-r-packages",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Fix the version of R packages Article 03/11/2022 2 minutes to read 3 contributors In this article When you use the install.packages() function to install CRAN packages, you cannot specify the version of the package, because the expectation is that you will install the latest version of the package and it should be compatible with the latest version of its dependencies. If you have an outdated dependency installed, it will be updated as well. Sometimes you want to fix the version of an R package. There are several ways to do this: Use the devtools package. Download and install a package file from a CRAN archive. Use a CRAN snapshot. When you use the Libraries UI or API to install R packages on all the instances of a cluster, we recommend the third option. The Microsoft R Application Network maintains a CRAN Time Machine that stores a snapshot of CRAN every night. The snapshots are available at https://cran.microsoft.com/snapshot/<date> where <date> is the date of the desired snapshot, for example, 2019-05-01. To install specific versions of R packages, specify this URL as the repository of your CRAN library when you create the library.",
          "title" : "Fix the version of R packages",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/pin-r-packages"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/sparkr-lapply",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to parallelize R code with spark.lapply Article 03/11/2022 2 minutes to read 3 contributors In this article Parallelization of R code is difficult, because R code runs on the driver and R data.frames are not distributed. Often, there is existing R code that is run locally and that is converted to run on Apache Spark. In other cases, some SparkR functions used for advanced statistical analysis and machine learning techniques may not support distributed computing. In such cases, the SparkR UDF API can be used to distribute the desired workload across a cluster. Example use case: You want to train multiple machine learning models on the same data, for example for hyper parameter tuning. If the data set fits on each worker, it may be more efficient to use the SparkR UDF API to train several versions of the model at once. The spark.lapply function enables you to perform the same task on multiple workers, by running a function over a list of elements. For each element in a list: Send the function to a worker. Execute the function. Return the result of all workers as a list to the driver. In the following example, a support vector machine model is fit on the iris dataset with 3-fold cross validation while the cost is varied from 0.5 to 1 by increments of 0.1. The output is a list with the summary of the models for the various cost parameters. R Copy library(SparkR)\n\nspark.lapply(seq(0.5, 1, by = 0.1), function(x) {\n  library(e1071)\n  model <- svm(Species ~ ., iris, cost = x, cross = 3)\n  summary(model)\n})\n Note You must install packages on all workers.",
          "title" : "How to parallelize R code with spark.lapply",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/sparkr-lapply"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/bi/jdbc-odbc-troubleshooting",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Troubleshooting JDBC and ODBC connections Article 03/11/2022 2 minutes to read 3 contributors In this article Fetching result set is slow after statement execution Timeout/Exception when creating the connection TTransportException Referencing temporary views Other errors This article provides information to help you troubleshoot the connection between your Databricks JDBC/ODBC server and BI tools and data sources. Fetching result set is slow after statement execution After a query execution, you can fetch result rows by calling the next() method on the returned ResultSet repeatedly. This method triggers a request to the driver Thrift server to fetch a batch of rows back if the buffered ones are exhausted. We found the size of the batch significantly affects the performance. The default value in the most of the JDBC/ODBC drivers is too conservative, and we recommend that you set it to at least 100,000. Contact the BI tool provider if you cannot access this configuration. Timeout/Exception when creating the connection Once you have the server hostname, you can run the following tests from a terminal to check for connectivity to the endpoint. Bash Copy curl https://<server-hostname>:<port>/sql/protocolv1/o/0/<cluster-id> -H \"Authorization: Basic $(echo -n 'token:<personal-access-token>' | base64)\"\n If the connection times out, check whether your network settings of the connection are correct. TTransportException If the response contains a TTransportException (the error is expected) like the following, it means that the gateway is functioning properly and you have passed in valid credentials. If you are not able to connect with the same credentials, check that the client you are using is properly configured and is using the latest Simba drivers (version >= 1.2.0): Console Copy <h2>HTTP ERROR: 500</h2>\n<p>Problem accessing /cliservice. Reason:\n<pre> javax.servlet.ServletException: org.apache.thrift.transport.TTransportException</pre></p>\n Referencing temporary views If the response contains the message Table or view not found: SPARK..temp_view it means that a temporary view is not properly referenced in the client application. Simba has an internal configuration parameter called UseNativeQuery that decides whether the query is translated or not before being submitted to the Thrift server. By default, the parameter is set to 0, in which case Simba can modify the query. In particular, Simba creates a custom #temp schema for temporary views and it expects the client application to reference a temporary view with this schema. You can avoid using this special alias by setting UseNativeQuery=1, which prevents Simba from modifying the query. In this case, Simba sends the query directly to the Thrift server. However, the client needs to make sure that the queries are written in the dialect that Spark expects, that is, HiveQL. To sum up, you have the following options to handle temporary views over Simba and Spark: UseNativeQuery=0 and reference the view by prefixing its name with #temp. UseNativeQuery=1 and make sure the query is written in the dialect that Spark expects. Other errors If you get the error 401 Unauthorized, check the credentials you are using: Console Copy <h2>HTTP ERROR: 401</h2>\n<p>Problem accessing /sql/protocolv1/o/0/test-cluster. Reason:\n<pre>    Unauthorized</pre></p>\n Verify that the username is token (not your username) and the password is a personal access token (it should start with dapi). Responses such as 404, Not Found usually indicate problems with locating the specified cluster: Console Copy <h2>HTTP ERROR: 404</h2>\n<p>Problem accessing /sql/protocolv1/o/0/missing-cluster. Reason:\n<pre>    RESOURCE_DOES_NOT_EXIST: No cluster found matching: missing-cluster</pre></p>\n If you see the following errors in your application log4j logs: Console Copy log4j:ERROR A \"org.apache.log4j.FileAppender\" object is not assignable to a \"com.simba.spark.jdbc42.internal.apache.log4j.Appender\" variable.\n You can ignore these errors. The Simba internal log4j library is shaded to avoid conflicts with the log4j library in your application. However, Simba may still load the log4j configuration of your application, and attempt to use some custom log4j appenders. This attempt fails with the shaded library. Relevant information is still captured in the logs.",
          "title" : "Troubleshooting JDBC and ODBC connections",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/bi/jdbc-odbc-troubleshooting"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbfs/remount-storage-after-rotate-access-key",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Remount a storage account after rotating access keys Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have blob storage associated with a storage account mounted, but are unable to access it after access keys are rotated. Cause There are multiple mount points using the same storage account. Remounting some, but not all, of the mount points with new access keys results in access issues. Solution Use dbutils.fs.mounts() to check all mount points. Review the dbutils.fs.mounts() documentation for usage details. Use dbutils.fs.unmount() to unmount all storage accounts. Review the dbutils.fs.unmount() documentation for usage details. Restart the cluster. Remount the storage account with new keys. Review the mount an Azure Blob storage container documentation for usage details.",
          "title" : "Remount a storage account after rotating access keys",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbfs/remount-storage-after-rotate-access-key"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/drop-delta-table",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Best practices for dropping a managed Delta Lake table Article 03/11/2022 2 minutes to read 3 contributors In this article Regardless of how you drop a managed table, it can take a significant amount of time, depending on the data size. Delta Lake managed tables in particular contain a lot of metadata in the form of transaction logs, and they can contain duplicate data files. If a Delta table has been in use for a long time, it can accumulate a very large amount of data. In the Azure Databricks environment, there are two ways to drop tables: Run DROP TABLE in a notebook cell. Click Delete in the UI. Even though you can delete tables in the background without affecting workloads, it is always good to make sure that you run DELETE FROM and VACUUM before you start a drop command on any table. This ensures that the metadata and file sizes are cleaned up before you initiate the actual data deletion. For example, if you are trying to delete the Delta table events, run the following commands before you start the DROP TABLE command: Run DELETE FROM: DELETE FROM events Run VACUUM with an interval of zero: VACUUM events RETAIN 0 HOURS These two steps reduce the amount of metadata and number of uncommitted files that would otherwise increase the data deletion time.",
          "title" : "Best practices for dropping a managed Delta Lake table",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/drop-delta-table"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/file-transaction-log-not-found",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents A file referenced in the transaction log cannot be found Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem Your job fails with an error message: A file referenced in the transaction log cannot be found. Example stack trace: Console Copy Error in SQL statement: SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 106, XXX.XXX.XXX.XXX, executor 0): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/mnt/<path>/part-00000-da504c51-3bb4-4406-bb99-3566c0e2f743-c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.databricks.com/delta/delta-intro.html#frequently-asked-questions ... Caused by: java.io.FileNotFoundException: dbfs:/mnt/<path>/part-00000-da504c51-3bb4-4406-bb99-3566c0e2f743-c000.snappy.parquet ...\n Cause There are three common causes for this error message. Cause 1: You start the Delta streaming job, but before the streaming job starts processing, the underlying data is deleted. Cause 2: You perform updates to the Delta table, but the transaction files are not updated with the latest details. Cause 3: You attempt multi-cluster read or update operations on the same Delta table, resulting in a cluster referring to files on a cluster that was deleted and recreated. Solution Cause 1: You should use a new checkpoint directory, or set the Spark property spark.sql.files.ignoreMissingFiles to true in the cluster’s Spark Config. Cause 2: Wait for the data to load, then refresh the table. You can also run fsck to update the transaction files with the latest details. Note fsck removes any file entries that cannot be found in the underlying file system from the transaction log of a Delta table. Cause 3: When tables have been deleted and recreated, the metadata cache in the driver is incorrect. You should not delete a table, you should always overwrite a table. If you do delete a table, you should clear the metadata cache to mitigate the issue. You can use a Python or Scala notebook command to clear the cache. Python Copy spark._jvm.com.databricks.sql.transaction.tahoe.DeltaLog.clearCache()\n Scala Copy com.databricks.sql.transaction.tahoe.DeltaLog.clearCache()",
          "title" : "A file referenced in the transaction log cannot be found",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/file-transaction-log-not-found"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/spark-ui-wrong-number-jobs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Apache Spark UI shows wrong number of jobs Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You are reviewing the number of active Apache Spark jobs on a cluster in the Spark UI, but the number is too high to be accurate. If you restart the cluster, the number of jobs shown in the Spark UI is correct at first, but over time it grows abnormally high. Cause The Spark UI is not always accurate for large, or long-running, clusters due to event drops. The Spark UI requires termination entries to know when an active job has completed. If a job misses this entry, due to errors or unexpected failure, the job may stop running while incorrectly showing as active in the Spark UI. Note For more information review Apache Spark UI is not in sync with job. Solution You should not use the Spark UI as a source of truth for active jobs on a cluster. The method sc.statusTracker().getActiveJobIds() in the Spark API is a reliable way to track the number of active jobs. Please review the Spark Status Tracker documentation for more information.",
          "title" : "Apache Spark UI shows wrong number of jobs",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/spark-ui-wrong-number-jobs"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/manage-size-delta-table",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Manage the size of Delta tables Article 03/11/2022 2 minutes to read 2 contributors In this article Enable file system versioning Enable bloom filters Review your Delta logRetentionDuration policy VACUUM your Delta table OPTIMIZE your Delta table Delta tables are different than traditional tables. Delta tables include ACID transactions and time travel features, which means they maintain transaction logs and stale data files. These additional features require storage space. In this article we discuss recommendations that can help you manage the size of your Delta tables. Enable file system versioning When you enable file system versioning, you keep multiple variants of your data in the same storage bucket. The file system creates versions of your data, instead of deleting items, which increases the storage space available for your Delta table. Enable bloom filters A Bloom filter index is a space-efficient data structure that enables data skipping on chosen columns, particularly for fields containing arbitrary text. Databricks supports file level Bloom filters; each data file can have a single Bloom filter index file associated with it. Before reading a file Databricks checks the index file and the file is read only if the index indicates that the file might match a data filter. The size of a Bloom filter depends on the number elements in the set for which the Bloom filter has been created and the required false positive probability (FPP). The lower the FPP, the higher the number of used bits per element and the more accurate it will be, at the cost of more storage space. Review your Delta logRetentionDuration policy Log files are retained for 30 days by default. This value is configurable through the delta.logRetentionDuration property. You can set a value for this property with the ALTER TABLE SET TBLPROPERTIES SQL method. The more days you retain, the more storage space you consume. For example if you set delta.logRetentionDuration = '365 days' it keeps the log files for 365 days instead of the default of 30 days. VACUUM your Delta table VACUUM removes data files that are no longer in the latest state of the transaction log for the table and are older than a retention threshold. Files are deleted according to the time they have been logically removed from Delta’s transaction log + retention hours, not their modification timestamps on the storage system. The default threshold is 7 days. Databricks does not automatically trigger VACUUM operations on Delta tables. You must run this command manually. VACUUM helps you delete obsolete files that are no longer needed. OPTIMIZE your Delta table The OPTIMIZE command compacts multiple Delta files into large single files. This improves the overall query speed and performance of your Delta table by helping you avoid having too many small files around. By default, OPTIMIZE creates 1GB files.",
          "title" : "Manage the size of Delta tables",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/manage-size-delta-table"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/pattern-match-files-in-path",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Select files using a pattern match Article 03/11/2022 2 minutes to read 2 contributors In this article Sample files Glob patterns When selecting files, a common requirement is to only read specific files from a folder. For example, if you are processing logs, you may want to read files from a specific month. Instead of enumerating each file and folder to find the desired files, you can use a glob pattern to match multiple files with a single expression. This article uses example patterns to show you how to read specific files from a sample list. Sample files Assume that the following files are located in the root folder. Console Copy //root/1999.txt\n//root/2000.txt\n//root/2001.txt\n//root/2002.txt\n//root/2003.txt\n//root/2004.txt\n//root/2005.txt\n//root/2020/04.txt\n//root/2020/05.txt\n Glob patterns Asterisk * - The asterisk matches one or more characters. It is a wild card for multiple characters. This example matches all files with a .txt extension Scala Copy display(spark.read.format(\"text\").load(\"//root/*.txt\"))\n Question mark ? - The question mark matches a single character. It is a wild card that is limited to replacing a single character. This example matches all files from the root folder, except 1999.txt. It does not search the contents of the 2020 folder. Scala Copy display(spark.read.format(\"text\").load(\"//root/200?.txt\"))\n Character class [ab] - The character class matches a single character from the set. It is represented by the characters you want to match inside a set of brackets. This example matches all files with a 2 or 3 in place of the matched character. It returns 2002.txt and 2003.txt from the sample files. Scala Copy display(spark.read.format(\"text\").load(\"//root/200[23].txt\"))\n Negated character class [^ab] - The negated character class matches a single character that is not in the set. It is represented by the characters you want to exclude inside a set of brackets. This example matches all files except those with a 2 or 3 in place of the matched character. It returns 2000.txt, 2001.txt, 2004.txt, and 2005.txt from the sample files. Scala Copy display(spark.read.format(\"text\").load(\"//root/200[^23].txt\"))\n Character range [a-b] - The character class matches a single character in the range of values. It is represented by the range of characters you want to match inside a set of brackets. This example matches all files with a character within the search range in place of the matched character. It returns 2002.txt, 2003.txt, 2004.txt, and 2005.txt from the sample files. Scala Copy display(spark.read.format(\"text\").load(\"//root/200[2-5].txt\"))\n Negated character range [^a-b] - The negated character class matches a single character that is not in the range of values. It is represented by the range of characters you want to exclude inside a set of brackets. This example matches all files with a character outside the search range in place of the matched character. It returns 2000.txt and 2001.txt from the sample files. Scala Copy display(spark.read.format(\"text\").load(\"//root/200[^2-5].txt\"))\n Alternation {a,b} - Alternation matches either expression. It is represented by the expressions you want to match inside a set of curly brackets. This example matches all files with an expression that matches one of the two selected expressions. It returns 2004.txt and 2005.txt from the sample files. Scala Copy display(spark.read.format(\"text\").load(\"//root/20{04, 05}.txt\"))",
          "title" : "Select files using a pattern match",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/pattern-match-files-in-path"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/spark-jar-job-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Multiple Apache Spark JAR jobs fail when run concurrently Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem If you run multiple Apache Spark JAR jobs concurrently, some of the runs might fail with the error: Console Copy org.apache.spark.sql.AnalysisException: Table or view not found: xxxxxxx; line 1 pos 48\n Cause This error occurs due to a bug in Scala. When an object extends App, its val fields are no longer immutable and they can be changed when the main method is called. If you run JAR jobs multiple times, a val field containing a DataFrame can be changed inadvertently. As a result, when any one of the concurrent runs finishes, it wipes out the temporary views of the other runs. Scala issue 11576 provides more detail. Solution To work around this bug, call the main() method explicitly. As an example, if you have code similar to this: Scala Copy \n  object MainTest extends App {\n    ...\n  }\n You can replace it with code that does not extend App: Scala Copy \n  object MainTest {\n    def main(args: Array[String]) {\n    ......\n    }\n  }",
          "title" : "Multiple Apache Spark JAR jobs fail when run concurrently",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/spark-jar-job-error"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/spark-job-fail-parquet-column-convert",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Apache Spark job fails with Parquet column cannot be converted error Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are reading data in Parquet format and writing to a Delta table when you get a Parquet column cannot be converted error message. The cluster is running Databricks Runtime 7.3 LTS or above. Console Copy org.apache.spark.SparkException: Task failed while writing rows.\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file s3://bucket-name/landing/edw/xxx/part-xxxx-tid-c00.snappy.parquet. Parquet column cannot be converted. Column: [Col1], Expected: DecimalType(10,0), Found: FIXED_LEN_BYTE_ARRAY\n\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException.\n Cause The vectorized Parquet reader is decoding the decimal type column to a binary format. The vectorized Parquet reader is enabled by default in Databricks Runtime 7.3 and above for reading datasets in Parquet files. The read schema uses atomic data types: binary, boolean, date, string, and timestamp. Note This error only occurs if you have decimal type columns in the source data. Solution If you have decimal type columns in your source data, you should disable the vectorized Parquet reader. Set spark.sql.parquet.enableVectorizedReader to false in the cluster’s Spark configuration to disable the vectorized Parquet reader at the cluster level. You can also disable the vectorized Parquet reader at the notebook level by running: Scala Copy spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n Note The vectorized Parquet reader enables native record-level filtering using push-down filters, improving memory locality, and cache utilization. If you disable the vectorized Parquet reader, there may be a minor performance impact. You should only disable it, if you have decimal type columns in your source data.",
          "title" : "Apache Spark job fails with Parquet column cannot be converted error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/spark-job-fail-parquet-column-convert"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents SQL with Apache Spark: tips and troubleshooting Article 03/11/2022 2 minutes to read 3 contributors In this article These articles can help you to use SQL with Apache Spark. Broadcast join exceeds threshold, returns out of memory error Cannot grow BufferHolder; exceeds size limitation Date functions only accept int values in Apache Spark 3.0 Disable broadcast when query plan has BroadcastNestedLoopJoin Duplicate columns in the metadata error Generate unique increasing numeric values Error in SQL statement: AnalysisException: Table or view not found Error when downloading full results after join Error when running MSCK REPAIR TABLE in parallel Find the size of a table Inner join drops records in result JDBC write fails with a PrimaryKeyViolation error Query does not skip header row on external table SHOW DATABASES command returns unexpected column name",
          "title" : "SQL with Apache Spark: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/inner-join-drops-records-in-result",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Inner join drops records in result Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You perform an inner join, but the resulting joined table is missing data. For example, assume you have two tables, orders and models. Python Copy df_orders = spark.createDataFrame([('Nissan','Altima','2-door 2.5 S Coupe'), ('Nissan','Altima','4-door 3.5 SE Sedan'), ('Nissan','Altima',''), ('Nissan','Altima', None)], [\"Company\", \"Model\", \"Info\"])\n Python Copy df_models = spark.createDataFrame([('Nissan','Altima',''), ('Nissan','Altima','2-door 2.5 S Coupe'), ('Nissan','Altima','2-door 3.5 SE Coupe'), ('Nissan','Altima','4-door 2.5 S Sedan'), ('Nissan','Altima','4-door 3.5 SE Sedan'), ('Nissan','Altima','4-door 3.5 SL Sedan'), ('Nissan','Altima','4-door HYBRID Sedan'), ('Nissan','Altima',None)], [\"Company\", \"Model\", \"Info\"])\n You attempt a straight join of the two tables. Python Copy df_orders.createOrReplaceTempView(\"Orders\")\ndf_models.createOrReplaceTempView(\"Models\")\n SQL Copy SELECT *\nMAGIC FROM Orders a\nMAGIC INNER JOIN Models b\nMAGIC ON a.Company = b.Company\nMAGIC AND a.Model = b.Model\nMAGIC AND a.Info = b.Info\n The resulting joined table only includes three of the four records from the orders table. The record with a null value in a column does not appear in the results. Cause Apache Spark does not consider null values when performing a join operation. If you attempt to join tables, and some of the columns contain null values, the null records will not be included in the resulting joined table. Solution If your source tables contain null values, you should use the Spark null safe operator (<=>). When you use <=> Spark processes null values (instead of dropping them) when performing a join. For example, if we modify the sample code with <=>, the resulting table does not drop the null values. SQL Copy SELECT *\nMAGIC FROM Orders a\nMAGIC INNER JOIN Models b\nMAGIC ON a.Company = b.Company\nMAGIC AND a.Model = b.Model\nMAGIC AND a.Info <=> b.Info\n Example notebook Inner join drops null values example Get notebook",
          "title" : "Inner join drops records in result",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/inner-join-drops-records-in-result"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/conflicting-directory-structures-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Conflicting directory structures error Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have an Apache Spark job that is failing with a Java assertion error java.lang.AssertionError: assertion failed: Conflicting directory structures detected.. Example stack trace Console Copy Caused by: org.apache.spark.sql.streaming.StreamingQueryException: There was an error when trying to infer the partition schema of the current batch of files. Please provide your partition columns explicitly by using: .option('cloudFiles.partitionColumns', 'comma-separated-list')\n=== Streaming Query ===\nIdentifier: [id = aabc5549-cb4b-4e4e-9403-4e793f4824a0, runId = 4e743dda-909f-4932-9489-3dd0b364d811]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {CloudFilesSource[<file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt]: {'seqNum':423,'sourceVersion':1}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nCloudFilesSource[<file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt]\nat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:385)\nat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:268)\nCaused by: java.lang.RuntimeException: There was an error when trying to infer the partition schema of the current batch of files. Please provide your partition columns explicitly by using: .option('cloudFiles.partitionColumns', 'comma-separated-list')\nat com.databricks.sql.fileNotification.autoIngest.CloudFilesErrors$.partitionInferenceError(CloudFilesErrors.scala:115)\nat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceFileIndex.liftedTree1$1(CloudFilesSourceFileIndex.scala:65)\nat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceFileIndex.partitionSpec(CloudFilesSourceFileIndex.scala:63)\nat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:50)\nat com.databricks.sql.fileNotification.autoIngest.CloudFilesSource.getBatch(CloudFilesSource.scala:361)\n... 1 more\nCaused by: java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Suspicious paths:\n<file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt\n<file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt/clfy_x_clfy_evt\n\nIf provided paths are partition directories, please set 'basePath' in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.\nat scala.Predef$.assert(Predef.scala:223)\nat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:204)\nat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parseP\n Cause You have conflicting directory paths in the storage location. In the example stack trace, we see two conflicting directory paths. <file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt <file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt/clfy_x_clfy_evt Because these directories appear in the same hierarchy, an update in root or in a branch level can result in a conflict. Solution Avoid multiple concurrent updates in a hierarchical directory structure or updates happening in the same partition. You should make multiple distinct paths for updates once a conflict is detected. Alternatively, you can add more partitions. These example directories do not conflict. <file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt/evt=clfy_x_clfy_evt1 <file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt/evt=clfy_x_clfy_evt2",
          "title" : "Conflicting directory structures error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/conflicting-directory-structures-error"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/ss-read-from-last-offset",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to restart a structured streaming query from last written offset Article 03/11/2022 2 minutes to read 3 contributors In this article Scenario Solution Scenario You have a stream, running a windowed aggregation query, that reads from Apache Kafka and writes files in Append mode. You want to upgrade the application and restart the query with the offset equal to the last written offset. You want to discard all state information that hasn’t been written to the sink, start processing from the earliest offsets that contributed to the discarded state, and modify the checkpoint directory accordingly. However, if you use existing checkpoints after upgrading the application code, old states and objects from the previous application version are re-used, which results in unexpected output such as reading from old sources or processing with old application code. Solution Apache Spark maintains state across the execution and binary objects on checkpoints. Therefore you cannot modify the checkpoint directory. As an alternative, copy and update the offset with the input records and store this in a file or a database. Read it during the initialization of the next restart and use the same value in readStream. Make sure to delete the checkpoint directory. You can get the current offsets by using asynchronous APIs: Scala Copy spark.streams.addListener(new StreamingQueryListener() {\n    override def onQueryStarted(queryStarted: QueryStartedEvent): Unit = {\n        println(\"Query started:\" + queryStarted.id)\n    }\n    override def onQueryTerminated(queryTerminated: QueryTerminatedEvent): Unit = {\n        println(\"Query terminated\" + queryTerminated.id)\n    }\n    override def onQueryProgress(queryProgress: QueryProgressEvent): Unit = {\n     println(\"Query made progress\")\n        println(\"Starting offset:\" + queryProgress.progress.sources(0).startOffset)\n        println(\"Ending offset:\" + queryProgress.progress.sources(0).endOffset)\n        //Logic to save these offsets\n    }\n})\n You can use readStream with the latest offset written by the process shown above: Scala Copy option(\"startingOffsets\",  \"\"\" {\"articleA\":{\"0\":23,\"1\":-1},\"articleB\":{\"0\":-2}} \"\"\")\n The input schema for streaming records is: Copy root\n|-- key: binary (nullable = true)\n|-- value: binary (nullable = true)\n|-- article: string (nullable = true)\n|-- partition: integer (nullable = true)\n|-- offset: long (nullable = true)\n|-- timestamp: timestamp (nullable = true)\n|-- timestampType: integer (nullable = true)\n Also, you can implement logic to save and update the offset to a database and read it at the next restart.",
          "title" : "How to restart a structured streaming query from last written offset",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/ss-read-from-last-offset"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/visualizations/save-plotly-to-dbfs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to save Plotly files and display From DBFS Article 03/11/2022 2 minutes to read 3 contributors In this article You can save a chart generated with Plotly to the driver node as a jpg or png file. Then, you can display it in a notebook by using the displayHTML() method. By default, you save Plotly charts to the /databricks/driver/ directory on the driver node in your cluster. Use the following procedure to display the charts at a later time. Generate a sample plot: Python Copy data = {'data': [{'y': [4, 2, 3, 4]}],\n            'layout': {'title': 'Test Plot',\n                       'font': dict(size=16)}}\np = plot(data,output_type='div')\ndisplayHTML(p)\n Save the generated plot to a file with plotly.io.write_image(): Bash Copy plotly.io.write_image(fig=data,file=\"/databricks/driver/plotly_images/<imageName>.jpg\", format=\"jpeg\",scale=None, width=None, height=None)\n Copy the file from the driver node and save it to DBFS: Bash Copy dbutils.fs.cp(\"file:/databricks/driver/plotly_images/<imageName>.jpg\", \"dbfs:/FileStore/<your_folder_name>/<imageName>.jpg\")\n Display the image using displayHTML(): Bash Copy displayHTML('''<img src=\"/files/<your_folder_name>/<imageName>.jpg\">''')\n See also Plotly in Python and R Notebooks.",
          "title" : "How to save Plotly files and display From DBFS",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/visualizations/save-plotly-to-dbfs"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Azure infrastructure: tips and troubleshooting Article 03/11/2022 2 minutes to read 4 contributors In this article These articles can help you manage the Azure configuration for your Azure Databricks workspaces. How to assign a single public IP for VNet-injected workspaces using Azure Firewall Network configuration of Azure Data Lake Storage Gen1 causes ADLException: Error getting info for file Jobs are not progressing in the workspace Configure custom DNS settings using dnsmasq How to analyze user interface performance issues SAS requires current ABFS client Unable to mount Azure Data Lake Storage Gen1 account",
          "title" : "Azure infrastructure: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Data sources: tips and troubleshooting Article 03/11/2022 2 minutes to read 4 contributors In this article These articles can help you manage your data source integrations. Create tables on JSON datasets Error when reading data from ADLS Gen1 with Sparklyr Failure when mounting or accessing Azure Blob storage Unable to read files and list directories in a WASB filesystem Optimize read performance from JDBC data sources Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2 CosmosDB-Spark connector library conflict Failure to detect encoding in JSON Inconsistent timestamp results with JDBC applications Kafka client terminated with OffsetOutOfRangeException Unable to access Azure Data Lake Storage (ADLS) Gen1 when firewall is enabled ABFS client hangs if incorrect client ID or wrong path used ADLS and WASB writes are being throttled Long jobs fail when accessing ADLS Use cx_Oracle to connect to an Oracle server",
          "title" : "Data sources: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/azure-storage-throttling",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents ADLS and WASB writes are being throttled Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem When accessing data stored on Azure data Lake Storage (ADLS) Windows Azure Storage Blobs (WASB) requests start timing out. You may see an error message indicating that storage is being accessed at too high a rate. Console Copy Files and folders are being created at too high a rate\n Cause Azure storage subscriptions have a limit on how many files and folders can be accessed over time. If too many requests are made in a given time frame, your account will be subject to throttling in order to keep the requests under the subscription limit. Solution To resolve this issue you can either increase the storage limits on your Azure subscription or optimize your Spark code to reduce the number of files created.",
          "title" : "ADLS and WASB writes are being throttled",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/azure-storage-throttling"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/jdbc-optimize-read",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Optimize read performance from JDBC data sources Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Solution Problem Reading data from an external JDBC database is slow. How can I improve read performance? Solution See the detailed discussion in the Azure Databricks documentation on how to optimize performance when reading data from an external JDBC database.",
          "title" : "Optimize read performance from JDBC data sources",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/jdbc-optimize-read"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/azure-core-limit",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents CPU core limit prevents cluster creation Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem Cluster creation fails with a message about a cloud provider error when you hover over cluster state. Console Copy Cloud Provider Launch Failure: A cloud provider error was encountered while setting up the cluster.\n When you view the cluster event log to get more details, you see a message about core quota limits. Console Copy Operation results in exceeding quota limits of Core. Maximum allowed: 350, Current in use: 350, Additional requested: 4.\n Cause Azure subscriptions have a CPU core quota limit which restricts the number of CPU cores you can use. This is a hard limit. If you try to start a cluster that would result in your account exceeding the CPU core quota the cluster launch will fail. Solution You can either free up resources or request a quota increase for your account. Stop inactive clusters to free up CPU cores for use. Open an Azure support case with a request to increase the CPU core quota limit for your subscription.",
          "title" : "CPU core limit prevents cluster creation",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/azure-core-limit"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/hive-udf",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Hive UDFs Article 03/11/2022 2 minutes to read 3 contributors In this article This article shows how to create a Hive UDF, register it in Spark, and use it in a Spark SQL query. Here is a Hive UDF that takes a long as an argument and returns its hexadecimal representation. Scala Copy import org.apache.hadoop.hive.ql.exec.UDF\nimport org.apache.hadoop.io.LongWritable\n\n// This UDF takes a long integer and converts it to a hexadecimal string.\n\nclass ToHex extends UDF {\n  def evaluate(n: LongWritable): String = {\n    Option(n)\n    .map { num =>\n        // Use Scala string interpolation. It's the easiest way, and it's\n        // type-safe, unlike String.format().\n        f\"0x${num.get}%x\"\n    }\n    .getOrElse(\"\")\n  }\n}\n Register the function: Scala Copy spark.sql(\"CREATE TEMPORARY FUNCTION to_hex AS 'com.ardentex.spark.hiveudf.ToHex'\")\n Use your function as any other registered function: Scala Copy spark.sql(\"SELECT first_name, to_hex(code) as hex_code FROM people\")\n You can find more examples and compilable code at Sample Hive UDF project.",
          "title" : "Hive UDFs",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/hive-udf"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/delta-cache-autoscaling",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How Delta cache behaves on an autoscaling cluster Article 03/11/2022 2 minutes to read 3 contributors In this article This article is about how Delta cache behaves on an auto-scaling cluster, which removes or adds nodes as needed. When a cluster downscales and terminates nodes: A Delta cache behaves in the same way as an RDD cache. Whenever a node goes down, all of the cached data in that particular node is lost. Delta cache data is not moved from the lost node. When a cluster upscales and adds new nodes: Whenever a cluster adds a new node, data is not moved between caches. Lost data is re-cached the next time an application accesses the data or tables again.",
          "title" : "How Delta cache behaves on an autoscaling cluster",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/delta-cache-autoscaling"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/spark-udf-performance",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Apache Spark Jobs hang due to non-deterministic custom UDF Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem Sometimes Apache Spark jobs hang indefinitely due to the non-deterministic behavior of a Spark User-Defined Function (UDF). Here is an example of such a function: Scala Copy val convertorUDF = (commentCol: String) =>\n    {\n              #UDF definition\n    }\nval translateColumn = udf(convertorUDF)\n If you call this UDF using the withColumn() API and then apply some filter transformation on the resulting DataFrame, the UDF could potentially execute multiple times for each record, affecting application performance. Scala Copy val translatedDF = df.withColumn(\"translatedColumn\", translateColumn( df(\"columnToTranslate\")))\nval filteredDF = translatedDF.filter(!translatedDF(\"translatedColumn\").contains(\"Invalid URL Provided\")) && !translatedDF(\"translatedColumn\").contains(\"Unable to connect to Microsoft API\"))\n Cause Sometimes a deterministic UDF can behave nondeterministically, performing duplicate invocations depending on the definition of the UDF. You often see this behavior when you use a UDF on a DataFrame to add an additional column using the withColumn() API, and then apply a transformation (filter) to the resulting DataFrame. Solution UDFs must be deterministic. Due to optimization, duplicate invocations might be eliminated or the function can be invoked more times than it is present in the query. The better option is to cache the DataFrame where you are using the UDF. If the DataFrame contains a large amount of data, then writing it to a Parquet format file is optimal. You can use the following code to cache the result: Scala Copy val translatedDF = df.withColumn(\"translatedColumn\", translateColumn( df(\"columnToTranslate\"))).cache()",
          "title" : "Apache Spark Jobs hang due to non-deterministic custom UDF",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/spark-udf-performance"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Scala with Apache Spark: tips and troubleshooting Article 03/11/2022 2 minutes to read 4 contributors In this article These articles can help you to use Scala with Apache Spark. Apache Spark job fails with Parquet column cannot be converted error Apache Spark read fails with Corrupted parquet page error Apache Spark UI is not in sync with job Best practice for cache(), count(), and take() Cannot import timestamp_millis or unix_millis Cannot modify the value of an Apache Spark config Convert flattened DataFrame to nested JSON Convert nested JSON to a flattened DataFrame Create a DataFrame from a JSON string or Python dictionary Decimal$DecimalIsFractional assertion error from_json returns null in Apache Spark 3.0 Intermittent NullPointerException when AQE is enabled Manage the size of Delta tables Trouble reading external JDBC tables after upgrading from Databricks Runtime 5.5 Running C++ code in Scala Select files using a pattern match Multiple Apache Spark JAR jobs fail when run concurrently",
          "title" : "Scala with Apache Spark: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/validate-environment-variable-behavior",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Validate environment variable behavior Article 03/11/2022 2 minutes to read 2 contributors In this article Behavior change examples Check the environment variable behavior In November 2021, the way environment variables are interpreted when creating, editing, or updating clusters was changed in some workspaces. This change will be reverted on December 3, 2021 from 01:00-03:00 UTC. After the change is reverted, environment variables will behave as they did before the change. This article explains how to validate the environment variable behavior on your cluster. Behavior change examples Use case “New” input Original input Expected value Escape special characters ($,`,”,) var=\" var=\\\" \" Use $ to access other vars Not supported otherVar=1 var=$otherVar2 12 Use ‘ or escape ‘ var=te'\\''st or var=te'\"'\"'st var=te'st te'st Note The “new” input behavior will no longer work after the change has been reverted. Check the environment variable behavior Use the following steps to determine if your workspace is using the original environment variable behavior, or if it is using the new environment variable behavior. The new behavior is scheduled to be reverted. Navigate to the Create Cluster page in your workspace. Expand Advanced options. Select Spark. Enter BEHAVIOR_CHECK=behavior_check in Environment variables. Click Create Cluster to launch the cluster. After the cluster is successfully launched, create a notebook, and attach it to the newly launched cluster. Copy the following command and run it in a cell: Bash Copy cat /databricks/spark/conf/spark-env.sh\n If BEHAVIOR_CHECK=\"behavior_check\" (double quotes) is included in the return value, your cluster is using the original behavior. NO ACTION IS REQUIRED If BEHAVIOR_CHECK='behavior_check' (single quotes) is included in the return value, your cluster is using the new behavior. If you have not made any changes to your configuration, you do not need to take any action. If you have made changes to your configuration to account for the new environment variable behavior, you must revert those changes when the platform change is reverted on December 3, 2021 from 01:00-03:00 UTC.",
          "title" : "Validate environment variable behavior",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/validate-environment-variable-behavior"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/common-errors-adf",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Common errors using Azure Data Factory Article 03/11/2022 3 minutes to read 3 contributors In this article Cluster could not be created Cluster ran into issues during data pipeline execution Azure Databricks service is experiencing high load Library installation timeout Azure Data Factory is a managed service that lets you author data pipelines using Azure Databricks notebooks, JARs, and Python scripts. This article describes common issues and solutions. Cluster could not be created When you create a data pipeline in Azure Data Factory that uses an Azure Databricks-related activity such as Notebook Activity, you can ask for a new cluster to be created. In Azure, cluster creation can fail for a variety of reasons: Your Azure subscription is limited in the number of virtual machines that can be provisioned. Failed to create cluster because of Azure quota indicates that the subscription you are using does not have enough quota to create the needed resources. For example, if you request 500 cores but your quota is 50 cores, the request will fail. Contact Azure Support to request a quota increase. Azure resource provider is currently under high load and requests are being throttled. This error indicates that your Azure subscription or perhaps even the region is being throttled. Simply retrying the data pipeline may not help. Learn more about this issue at Troubleshooting API throttling errors. Could not launch cluster due to cloud provider failures indicates a generic failure to provision one or more virtual machines for the cluster. Wait and try again later. Cluster ran into issues during data pipeline execution Azure Databricks includes a variety of mechanisms that increase the resilience of your Apache Spark cluster. That said, it cannot recover from every failure, leading to errors like this: Connection refused RPC timed out Exchange times out after X seconds Cluster became unreachable during run Too many execution contexts are open right now Driver was restarted during run Context ExecutionContextId is disconnected Could not reach driver of cluster for X seconds Most of the time, these errors do not indicate an issue with the underlying infrastructure of Azure. Instead, it is quite likely that the cluster has too many jobs running on it, which can overload the cluster and cause timeouts. As a general rule, you should move heavier data pipelines to run on their own Azure Databricks clusters. Integrating with Azure Monitor and observing execution metrics with Grafana can provide insight into clusters that are getting overloaded. Azure Databricks service is experiencing high load You may notice that certain data pipelines fail with errors like these: The service at {API} is temporarily unavailable Jobs is not fully initialized yet. Please retry later Failed or timeout processing HTTP request No webapps are available to handle your request These errors indicate that the Azure Databricks service is under heavy load. If this happens, try limiting the number of concurrent data pipelines that include a Azure Databricks activity. For example, if you are performing ETL with 1,000 tables from source to destination, instead of launching a data pipeline per table, either combine multiple tables in one data pipeline or stagger their execution so they don’t all trigger at once. Important Azure Databricks will not allow you to create more than 1,000 Jobs in a 3,600 second window. If you try to do so with Azure Data Factory, your data pipeline will fail. These errors can also show if you poll the Databricks Jobs API for job run status too frequently (e.g. every 5 seconds). The remedy is to reduce the frequency of polling. Library installation timeout Azure Databricks includes robust support for installing third-party libraries. Unfortunately, you may see issues like this: Failed or timed out installing libraries This happens because every time you start a cluster with a library attached, Azure Databricks downloads the library from the appropriate repository (such as PyPI). This operation can time out, causing your cluster to fail to start. There is no simple solution for this problem, other than limiting the number of libraries you attach to clusters.",
          "title" : "Common errors using Azure Data Factory",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/common-errors-adf"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/geospark-undefined-function-error-dbconnect",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents GeoSpark undefined function error with DBConnect Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to use the GeoSpark function st_geofromwkt with DBConnect and you get an Apache Spark error message. Console Copy Error: org.apache.spark.sql.AnalysisException: Undefined function: 'st_geomfromwkt'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.;\n This example code fails with the error when used with DBConnect. Scala Copy val sc = spark.sparkContext\nsc.setLogLevel(\"DEBUG\")\n\nval sqlContext = spark.sqlContext\nspark.sparkContext.addJar(\"~/jars/geospark-sql_2.3-1.2.0.jar\")\nspark.sparkContext.addJar(\"~/jars/geospark-1.2.0.jar\")\n\nGeoSparkSQLRegistrator.registerAll(sqlContext)\nprintln(spark.sessionState.functionRegistry.listFunction)\n\nspark.sql(\"select ST_GeomFromWKT(area) AS geometry from polygon\").show()\n Cause DBConnect does not support auto-sync of client side UDFs to the server. Solution You can use a custom utility jar with code that registers the UDF on the cluster using the SparkSessionExtensions class. Create a utility jar that registers GeoSpark functions using SparkSessionExtensions. This utility class definition can be built into a utility jar. Scala Copy package com.databricks.spark.utils\n\nimport org.apache.spark.sql.SparkSessionExtensions\nimport org.datasyslab.geosparksql.utils.GeoSparkSQLRegistrator\n\nclass GeoSparkUdfExtension extends (SparkSessionExtensions => Unit) {\n  def apply(e: SparkSessionExtensions): Unit = {\n    e.injectCheckRule(spark => {\n      println(\"INJECTING UDF\")\n      GeoSparkSQLRegistrator.registerAll(spark)\n      _ => Unit\n    })\n  }\n}\n Copy the GeoSpark jars and your utility jar to DBFS at dbfs:/databricks/geospark-extension-jars/. Create an init script (set_geospark_extension_jar.sh) that copies the jars from the DBFS location to the Spark class path and sets the spark.sql.extensions to the utility class. Scala Copy dbutils.fs.put(\n    \"dbfs:/databricks/<init-script-folder>/set_geospark_extension_jar.sh\",\n    \"\"\"#!/bin/sh\n      |sleep 10s\n      |# Copy the extension and GeoSpark dependency jars to /databricks/jars.\n      |cp -v /dbfs/databricks/geospark-extension-jars/{spark_geospark_extension_2_11_0_1.jar,geospark_sql_2_3_1_2_0.jar,geospark_1_2_0.jar} /databricks/jars/\n      |# Set the extension.\n      |cat << 'EOF' > /databricks/driver/conf/00-custom-spark.conf\n      |[driver] {\n      |    \"spark.sql.extensions\" = \"com.databricks.spark.utils.GeoSparkUdfExtension\"\n      |}\n      |EOF\n      |\"\"\".stripMargin,\n    overwrite = true\n)\n Install the init script as a cluster-scoped init script. You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/set_geospark_extension_jar.sh). Reboot your cluster. You can now use GeoSpark code with DBConnect.",
          "title" : "GeoSpark undefined function error with DBConnect",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/geospark-undefined-function-error-dbconnect"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/use-tcpdump-create-pcap-files",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Use tcpdump to create pcap files Article 03/11/2022 2 minutes to read 2 contributors In this article Create the tcpdump init script Configure the init script Locate pcap files Download pcap files If you want to analyze the network traffic between nodes on a specific cluster, you can install tcpdump on the cluster and use it to dump the network packet details to pcap files. The pcap files can then be downloaded to a local machine for analysis. Create the tcpdump init script Run this sample script in a notebook on the cluster to create the init script. Python Copy dbutils.fs.put(\"dbfs://databricks/<path-to-init-script>/tcp_dump.sh\",\"\"\"\n#!/bin/bash\nDB_CLUSTER_ID=$(echo $HOSTNAME | awk -F '-' '{print$1\"-\"$2\"-\"$3}')\n\nif [[ ! -d /dbfs/databricks/tcpdump/${DB_CLUSTER_ID} ]] ; then\nsudo mkdir -p /dbfs/databricks/tcpdump/${DB_CLUSTER_ID}\nfi\n\nBASEDIR=\"/dbfs/databricks/tcpdump/${DB_CLUSTER_ID}\"\n\nmkdir -p ${BASEDIR}\n\nMYIP=$(ip route get 10 | awk '{print $NF;exit}')\necho \"initiating tcpdump\"\nsudo tcpdump -w ${BASEDIR}/trace_%Y_%m_%d_%H_%M_%S_${MYIP}.pcap -W 1000 -G 1800 -C 200 &\necho \"initiated tcpdump\"\"\"\", True)\n Remember the path to the init script. You will need it when configuring your cluster. Configure the init script Follow the documentation to configure a cluster-scoped init script. Specify the path to the init script. Use the same path that you used in the sample script (dbfs://databricks/<path-to-init-script>/tcp_dump.sh) After configuring the init script, restart the cluster. Locate pcap files Once the cluster has started, it automatically starts creating pcap files which contain the recorded network information. The pcap files are located in the folder dbfs://databricks/tcpdump/${<cluster-id>}. Download pcap files Download the pcap files to your local host for analysis. There are multiple ways to download files to your local machine. One option is the Databricks CLI.",
          "title" : "Use tcpdump to create pcap files",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/use-tcpdump-create-pcap-files"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/execution/maximum-execution-context",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Maximum execution context or notebook attachment limit reached Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem Notebook or job execution stops and returns either of the following errors: Console Copy Run result unavailable: job failed with error message\nContext ExecutionContextId(1731742567765160237) is disconnected.\n Console Copy Can’t attach this notebook because the cluster has reached the attached notebook limit. Detach a notebook and retry.\n Cause When you attach a notebook to a cluster, Azure Databricks creates an execution context. If there are too many notebooks attached to a cluster or too many jobs are created, at some point the cluster reaches its maximum threshold limit of 145 execution contexts, and Azure Databricks returns an error. Solution Configure context auto-eviction, which allows Azure Databricks to remove (evict) idle execution contexts. Additionally, from the pipeline and ETL design perspective, you can avoid this issue by using: Fewer notebooks to reduce the number of execution contexts that are created. A job cluster instead of an interactive cluster. If the use case permits, submit notebooks or jars as jobs.",
          "title" : "Maximum execution context or notebook attachment limit reached",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/execution/maximum-execution-context"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/cmd-c-on-object-id-p0",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Error: Received command c on object id p0 Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have imported Python libraries, but when you try to execute Python code in a notebook you get a repeating message as output. Console Copy INFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\n Cause The default log level for py4j.java_gateway is ERROR. If any of the imported Python libraries set the log level to INFO you will see this message. Solution You can prevent the output of the INFO messages by setting the log level back to ERROR after importing the libraries. Python Copy import logging\nlogger = spark._jvm.org.apache.log4j\nlogging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR)",
          "title" : "Error: Received command c on object id p0",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/cmd-c-on-object-id-p0"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/display-not-show-microseconds",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents display() does not show microseconds correctly Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You want to display a timestamp value with microsecond precision, but when you use display() it does not show the value past milliseconds. For example, this Apache Spark SQL display() command: SQL Copy display(spark.sql(\"select cast('2021-08-10T09:08:56.740436' as timestamp) as test\"))\n Returns a truncated value: Console Copy 2021-08-10T09:08:56.740+0000\n Cause The DataFrame is converted to HTML internally before the output is rendered. This limits the displayed results to millisecond precision. It does not affect the stored value. Solution You should use show() instead of using display(). For example, this Apache Spark SQL show() command: SQL Copy spark.sql(\"select cast('2021-08-10T09:08:56.740436' as timestamp) as test\").show(truncate=False)\n Returns the correct value: Console Copy 2021-08-10 09:08:56.740436\n As an alternative, you can create a second column and copy the value to the column as a string. After conversion to a string, display() shows the full value.",
          "title" : "display() does not show microseconds correctly",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/display-not-show-microseconds"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/item-too-large-to-export",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Item was too large to export Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to export notebooks using the workspace UI and are getting an error message. Console Copy This item was too large to export. Try exporting smaller or fewer items.\n Cause The notebook files are larger than 10 MB in size. Solution The simplest solution is to limit the size of the notebook or folder that you are trying to download to 10 MB or less. If it is smaller than 10 MB in size, you can download it via the workspace UI. If the notebook or folder is larger than 10 MB in size, you should use the Databricks CLI to export the contents. Example code This example code exports all notebooks and folders in a workspace to a folder on your local machine. Make sure you create a workspace profile in the CLI before attempting to run this sample code. Copy the example code to your local machine as a Python file (ex. export-notebook.py). Python Copy import sys\nimport os\nimport subprocess\nfrom subprocess import call, check_output\n\nEXPORT_PROFILE = \"primary\"\n\n# Get a list of all users.\nuser_list_out = check_output([\"databricks\", \"workspace\", \"ls\", \"/Users\", \"--profile\", EXPORT_PROFILE])\nuser_list = (user_list_out.decode(encoding=\"utf-8\")).splitlines()\n\nprint (user_list)\n\n# Export folders and notebooks for each user.\n# Note: This does not include libraries.\n\nfor user in user_list:\n  print ((\"Trying to migrate workspace for user \") + user)\n\n  subprocess.call(str(\"mkdir -p \") + str(user), shell = True)\n  export_exit_status = call(\"databricks workspace export_dir /Users/\" + str(user) + \" ./\" + str(user) + \" --profile \" + EXPORT_PROFILE, shell = True)\n\n  if export_exit_status==0:\n    print (\"Export Success\")\n  else:\n    print (\"Export Failure\")\nprint (\"All done\")\n Run the sample code under Python 3. Copy python3 export-notebook.py\n The notebooks and folders from your workspace are downloaded to the local folder where you ran the example code.",
          "title" : "Item was too large to export",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/item-too-large-to-export"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/notebook-autosave",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Notebook autosave fails due to file size limits Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem Notebook autosaving fails with the following error message: Console Copy Failed to save revision: Notebook size exceeds limit. This is most commonly caused by cells with large results. Remove some cells or split the notebook.\n Cause The maximum notebook size allowed for autosaving is 8 MB. Solution First, check the size of your notebook file using your browser’s developer tools. In Chrome, for example, click View > Developer > Developer Tools. Click the Network tab and view the Size column for the notebook file. Then, there are two possible solutions: You can manually save notebooks up to 32 MB. You can reduce the size of your notebook by hiding large results. Graphing tools like plotly and matplotlib can generate large sets of results that display as large images. You can reduce the notebook size by hiding these large results and images.",
          "title" : "Notebook autosave fails due to file size limits",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/notebook-autosave"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/edited-policy-not-applied",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cannot apply updated cluster policy Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are attempting to update an existing cluster policy, however the update does not apply to the cluster associated with the policy. If you attempt to edit a cluster that is managed by a policy, the changes are not applied or saved. Cause This is a known issue that is being addressed. Solution You can use a workaround until a permanent fix is available. Edit the cluster policy. Re-attribute the policy to Free form. Add the edited policy back to the cluster. If you want to edit a cluster that is associated with a policy: Terminate the cluster. Associate a different policy to the cluster. Edit the cluster. Re-associate the original policy to the cluster.",
          "title" : "Cannot apply updated cluster policy",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/edited-policy-not-applied"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/set-core-site-xml",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Set Apache Hadoop core-site.xml properties Article 03/11/2022 2 minutes to read 2 contributors In this article Create the core-site.xml file in DBFS Create an init script that loads core-site.xml Attach the init script to your cluster You have a scenario that requires Apache Hadoop properties to be set. You would normally do this in the core-site.xml file. In this article, we explain how you can set core-site.xml in a cluster. Create the core-site.xml file in DBFS You need to create a core-site.xml file and save it to DBFS on your cluster. An easy way to create this file is via a bash script in a notebook. This example code creates a hadoop-configs folder on your cluster and then writes a single property core-site.xml file to that folder. Bash Copy mkdir -p /dbfs/hadoop-configs/\ncat << 'EOF' > /dbfs/hadoop-configs/core-site.xml\n <property>\n    <name><property-name-here></name>\n    <value><property-value-here></value>\n </property>\nEOF\n You can add multiple properties to the file by adding additional name/value pairs to the script. You can also create this file locally, and then upload it to your cluster. Create an init script that loads core-site.xml This example code creates an init script called set-core-site-configs.sh that uses the core-site.xml file you just created. If you manually uploaded a core-site.xml file and stored it elsewhere, you should update the config_xml value in the example code. Python Copy dbutils.fs.put(\"/databricks/scripts/set-core-site-configs.sh\", \"\"\"\n#!/bin/bash\n\necho \"Setting core-site.xml configs at `date`\"\n\nSTART_DRIVER_SCRIPT=/databricks/spark/scripts/start_driver.sh\nSTART_WORKER_SCRIPT=/databricks/spark/scripts/start_spark_slave.sh\n\nTMP_DRIVER_SCRIPT=/tmp/start_driver_temp.sh\nTMP_WORKER_SCRIPT=/tmp/start_spark_slave_temp.sh\n\nTMP_SCRIPT=/tmp/set_core-site_configs.sh\n\nconfig_xml=\"/dbfs/hadoop-configs/core-site.xml\"\n\ncat >\"$TMP_SCRIPT\" <<EOL\n#!/bin/bash\n## Setting core-site.xml configs\n\nsed -i '/<\\/configuration>/{\n    r $config_xml\n    a \\</configuration>\n    d\n}' /databricks/spark/dbconf/hadoop/core-site.xml\n\nEOL\ncat \"$TMP_SCRIPT\" > \"$TMP_DRIVER_SCRIPT\"\ncat \"$TMP_SCRIPT\" > \"$TMP_WORKER_SCRIPT\"\n\ncat \"$START_DRIVER_SCRIPT\" >> \"$TMP_DRIVER_SCRIPT\"\nmv \"$TMP_DRIVER_SCRIPT\" \"$START_DRIVER_SCRIPT\"\n\ncat \"$START_WORKER_SCRIPT\" >> \"$TMP_WORKER_SCRIPT\"\nmv \"$TMP_WORKER_SCRIPT\" \"$START_WORKER_SCRIPT\"\n\necho \"Completed core-site.xml config changes `date`\"\n\n\"\"\", True)\n Attach the init script to your cluster You need to configure the newly created init script as a cluster-scoped init script. If you used the example code, your Destination is DBFS and the Init Script Path is dbfs:/databricks/scripts/set-core-site-configs.sh. If you customized the example code, ensure that you enter the correct path and name of the init script when you attach it to the cluster.",
          "title" : "Set Apache Hadoop core-site.xml properties",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/set-core-site-xml"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/adf-notebook-activity-no-access",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cannot access workspace notebook in Azure Data Factory Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are editing a Databricks Notebook Activity in Azure Data Factory and cannot access your Azure Databricks workspace notebook. Cause You configured the Azure Databricks linked service in Azure Data Factory before Azure Databricks started using per-workspace URLs. Open the linked service and you see an Azure Databricks URL in the form of https://<region>.azuredatabricks.net. This type of URL is no longer used with Azure Databricks. Solution You must reconfigure the Azure Databricks linked service to use a per-workspace URL. Per-workspace URLs are in the form https://adb-<workspace-id>.<random-number>.azuredatabricks.net. You can find your per-workspace URL on your Azure portal workspace page. Review the create linked services Azure Data Factory documentation for more details on how to configure a linked service.",
          "title" : "Cannot access workspace notebook in Azure Data Factory",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/adf-notebook-activity-no-access"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/library-fail-dependency-exception",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Libraries fail with dependency exception Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You have a Python function that is defined in a custom egg or wheel file and also has dependencies that are satisfied by another customer package installed on the cluster. When you call this function, it returns an error that says the requirement cannot be satisfied. Console Copy org.apache.spark.SparkException: Process List(/local_disk0/pythonVirtualEnvDirs/virtualEnv-d82b31df-1da3-4ee9-864d-8d1fce09c09b/bin/python, /local_disk0/pythonVirtualEnvDirs/virtualEnv-d82b31df-1da3-4ee9-864d-8d1fce09c09b/bin/pip, install, fractal==0.1.0, --disable-pip-version-check) exited with code 1. Could not find a version that satisfies the requirement fractal==0.1.0 (from versions: 0.1.1, 0.1.2, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.2.7, 0.2.8, 0.2.9, 0.3.0)\n As an example, imagine that you have both wheel A and wheel B installed, either to the cluster via the UI or via notebook-scoped libraries. Assume that wheel A has a dependency on wheel B. dbutils.library.install(/path_to_wheel/A.whl) dbutils.library.install(/path_to_wheel/B.whl) When you try to make a call using one of these libraries, you get a requirement cannot be satisfied error. Cause Even though the requirements have been met by installing the required dependencies via the cluster UI or via a notebook-scoped library installation, Azure Databricks cannot guarantee the order in which specific libraries are installed on the cluster. If a library is being referenced and it has not been distributed to the executor nodes, it will fallback to PyPI and use it locally to satisfy the requirement. Solution You should use one egg or wheel file that contains all required code and dependencies. This ensures that your code has the correct libraries loaded and available at run time.",
          "title" : "Libraries fail with dependency exception",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/library-fail-dependency-exception"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/library-install-latency",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Library unavailability causing job failures Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution This article explains an Import Error you may encounter when launching jobs that import external libraries. Problem When a job causes a node to restart, the job fails with the following error message: Bash Copy ImportError: No module named XXX\n Cause The Cluster Manager is part of the Azure Databricks service that manages customer Apache Spark clusters. It sends commands to install Python and R libraries when it restarts each node. Sometimes, library installation or downloading of artifacts from the internet can take more time than expected. This occurs due to network latency, or it occurs if the library that is being attached to the cluster has many dependent libraries. The library installation mechanism guarantees that when a notebook attaches to a cluster, it can import installed libraries. When library installation through PyPI takes excessive time, the notebook attaches to the cluster before the library installation completes. In this case, the notebook is unable to import the library. Solution Method 1 Use notebook-scoped library installation commands in the notebook. You can enter the following commands in one cell, which ensures that all of the specified libraries are installed. Bash Copy dbutils.library.installPyPI(\"mlflow\")\ndbutils.library.restartPython()\n Method 2 To avoid delay in downloading the libraries from the internet repositories, you can cache the libraries in DBFS or Azure Blob Storage. For example, you can download the wheel or egg file for a Python library to a DBFS or Azure Blob Storage location. You can use the REST API or cluster-scoped init scripts to install libraries from DBFS or Azure Blob Storage. First, download the wheel or egg file from the internet to the DBFS or Azure Blob Storage location. This can be performed in a notebook as follows: Bash Copy %sh\ncd /dbfs/mnt/library\n wget <whl/egg file location from the pypi repository>\n After the wheel or egg file download completes, you can install the library to the cluster using the REST API, UI, or init script commands.",
          "title" : "Library unavailability causing job failures",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/library-install-latency"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/pypmml-fail-find-py4j-jar",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents PyPMML fails with Could not find py4j jar error Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem PyPMML is a Python PMML scoring library. After installing PyPMML in a Azure Databricks cluster, it fails with a Py4JError: Could not find py4j jar error. Python Copy from pypmml import Model\nmodelb = Model.fromFile('/dbfs/shyam/DecisionTreeIris.pmml')\n\nError : Py4JError: Could not find py4j jar at\n Cause This error occurs due to a dependency on the default Py4J library. Databricks Runtime 5.0-6.6 uses Py4J 0.10.7. Databricks Runtime 7.0 and above uses Py4J 0.10.9. The default Py4J library is installed to a different location than a standard Py4J package. As a result, when PyPMML attempts to invoke Py4J from the default path, it fails. Solution Setup a cluster-scoped init script that copies the required Py4J jar file into the expected location. Use pip to install the version of Py4J that corresponds to your Databricks Runtime version. For example, in Databricks Runtime 6.5 run pip install py4j==<0.10.7> in a notebook in install Py4J 0.10.7 on the cluster. Run find /databricks/ -name \"py4j*jar\" in a notebook to confirm the full path to the Py4J jar file. It is usually located in a path similar to /databricks/python3/share/py4j/. Manually copy the Py4J jar file from the install path to the DBFS path /dbfs/py4j/. Run the following code snippet in a Python notebook to create the install-py4j-jar.sh init script. Make sure the version number of Py4J listed in the snippet corresponds to your Databricks Runtime version. Python Copy dbutils.fs.put(\"/databricks/init-scripts/install-py4j-jar.sh\", \"\"\"\n\n#!/bin/bash\necho \"Copying at `date`\"\nmkdir -p /share/py4j/ /current-release/\ncp /dbfs/py4j/py4j<version number>.jar /share/py4j/\ncp /dbfs/py4j/py4j<version number>.jar /current-release/\necho \"Copying completed at `date`\"\n\n\"\"\", True)\n Attach the install-py4j-jar.sh init script to your cluster, following the instructions in configure a cluster-scoped init script. Restart the cluster. Verify that PyPMML works as expected.",
          "title" : "PyPMML fails with Could not find py4j jar error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/pypmml-fail-find-py4j-jar"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/xlsx-file-not-supported-xlrd",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Reading .xlsx files with xlrd fails Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are have xlrd installed on your cluster and are attempting to read files in the Excel .xlsx format when you get an error. Console Copy XLRDError: Excel xlsx file; not supported\n Cause xlrd 2.0.0 and above can only read .xls files. Support for .xlsx files was removed from xlrd due to a potential security vulnerability. Solution Use openpyxl to open .xlsx files instead of xlrd. Install the openpyxl library on your cluster. Confirm that you are using pandas version 1.0.1 or above. Python Copy import pandas as pd\nprint(pd.__version__)\n Specify openpyxl when reading .xlsx files with pandas. Python Copy import pandas\ndf = pandas.read_excel(`<name-of-file>.xlsx`, engine=`openpyxl`)\n Refer to the openpyxl documentation for more information.",
          "title" : "Reading .xlsx files with xlrd fails",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/xlsx-file-not-supported-xlrd"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/conda-fails-to-download-packages-from-anaconda",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Conda fails to download packages from Anaconda Article 03/11/2022 2 minutes to read 1 contributor In this article Problem Cause Solution Problem You are attempting to download packages from the Anaconda repository and get a PackagesNotFoundError error message. This error can occur when using %conda, %sh conda in notebooks, and when using Conda in an init script. Cause Anaconda Inc. updated the terms of service for repo.anaconda.com and anaconda.org/anaconda. Based on the Anaconda terms of service you may require a commercial license if you rely on Anaconda’s packaging and distribution. You should review the Anaconda Commercial Edition FAQ for more information. Note Your use of any Anaconda channels is governed by the Anaconda terms of service. As a result, the default channel configuration for the Conda package manager was removed in Databricks Runtime 7.3 LTS for Machine Learning and above. Solution You should review the Anaconda terms of service and determine if you require a commercial license. Once you have verified that you have a valid license, you must specify a channel to install or update packages with Conda. You can specify a Conda channel with -c <name-of-channel>. For example, %conda install matplotlib returns an error, while %conda install -c defaults matplotlib installs matplotlib.",
          "title" : "Conda fails to download packages from Anaconda",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/conda-fails-to-download-packages-from-anaconda"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/fit-spark-model-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Fitting an Apache SparkML model throws error Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem Azure Databricks throws an error when fitting a SparkML model or Pipeline: Console Copy org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 162.0 failed 4 times, most recent failure: Lost task 0.3 in stage 162.0 (TID 168, 10.205.250.130, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) =&gt; double)\n Cause Often, an error when fitting a SparkML model or Pipeline is a result of issues with the training data. Solution Check for the following issues: Identify and address NULL values in a dataset. Spark needs to know how to address missing values in the dataset. Discard rows with missing values with dropna(). Impute some value like zero or the average value of the column. This solution depends on what is meaningful for the data set. Ensure that all training data is appropriately transformed to a numeric format. Spark needs to know how to handle categorical and string variables. A variety of feature transformers are available to address data specific cases. Check for collinearity. Highly correlated or even duplicate features may cause issues with model fitting. This occurs on rare occasions, but you should make sure to rule it out.",
          "title" : "Fitting an Apache SparkML model throws error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/fit-spark-model-error"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/mlflow-artifacts-download",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Download artifacts from MLflow Article 03/11/2022 2 minutes to read 2 contributors In this article Example code Copy to an external filesystem Move to a mount point By default, the MLflow client saves artifacts to an artifact store URI during an experiment. The artifact store URI is similar to /dbfs/databricks/mlflow-tracking/<experiment-id>/<run-id>/artifacts/. This artifact store is a MLflow managed location, so you cannot download artifacts directly. You must use client.download_artifacts in the MLflow client to copy artifacts from the artifact store to another storage location. Example code This example code downloads the MLflow artifacts from a specific run and stores them in the location specified as local_dir. Replace <local-path-to-store-artifacts> with the local path where you want to store the artifacts. Replace <run-id> with the run_id of your specified MLflow run. Python Copy import mlflow\nimport os\nfrom mlflow.tracking import MlflowClient\nclient = MlflowClient()\nlocal_dir = \"<local-path-to-store-artifacts>\"\nif not os.path.exists(local_dir):\n  os.mkdir(local_dir)\n\n# Creating sample artifact \"features.txt\".\nfeatures = \"rooms, zipcode, median_price, school_rating, transport\"\nwith open(\"features.txt\", 'w') as f:\n    f.write(features)\n\n# Creating sample MLflow run & logging artifact \"features.txt\" to the MLflow run.\nwith mlflow.start_run() as run:\n    mlflow.log_artifact(\"features.txt\", artifact_path=\"features\")\n\n# Download the artifact to local storage.\nlocal_path = client.download_artifacts(<run-id>, \"features\", local_dir)\nprint(\"Artifacts downloaded in: {}\".format(local_dir))\nprint(\"Artifacts: {}\".format(local_dir))\n After the artifacts have been downloaded to local storage, you can copy (or move) them to an external filesystem or a mount point using standard tools. Copy to an external filesystem Scala Copy dbutils.fs.cp(local_dir, \"<filesystem://path-to-store-artifacts>\")\n Move to a mount point Python Copy shutil.move(local_dir, \"/dbfs/mnt/<path-to-store-artifacts>\")",
          "title" : "Download artifacts from MLflow",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/mlflow-artifacts-download"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/speed-up-cross-validation",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to speed up cross-validation Article 03/11/2022 2 minutes to read 3 contributors In this article Hyperparameter tuning of Apache SparkML models takes a very long time, depending on the size of the parameter grid. You can improve the performance of the cross-validation step in SparkML to speed things up: Cache the data before running any feature transformations or modeling steps, including cross-validation. Processes that refer to the data multiple times benefit from a cache. Remember to call an action on the DataFrame for the cache to take effect. Increase the parallelism parameter inside the CrossValidator, which sets the number of threads to use when running parallel algorithms. The default setting is 1. See the CrossValidator documentation for more information. Don’t use the pipeline as the estimator inside the CrossValidator specification. In some cases where the featurizers are being tuned along with the model, running the whole pipeline inside the CrossValidator makes sense. However, this executes the entire pipeline for every parameter combination and fold. Therefore, if only the model is being tuned, set the model specification as the estimator inside the CrossValidator. Note CrossValidator can be set as the final stage inside the pipeline after the featurizers. The best model identified by the CrossValidator is output.",
          "title" : "How to speed up cross-validation",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/speed-up-cross-validation"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/cant-uninstall-libraries",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cannot uninstall library from UI Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem Usually, libraries can be uninstalled in the Clusters UI. If the checkbox to select the library is disabled, then it’s not possible to uninstall the library from the UI. Cause If you create a library using REST API version 1.2 and if auto-attach is enabled, the library is installed on all clusters. In this scenario, the Clusters UI checkbox to select the library to uninstall is disabled. Solution Create a workspace library pointing to the DBFS location of the library that you are unable to uninstall. Example: You can’t uninstall a JAR library that is available at this DBFS location: Copy dbfs:/Filestore/jars/custom_elastic_spark.jar\n Create a new workspace library pointing to the same DBFS location. In the library UI, select the checkbox to uninstall the library from individual clusters.",
          "title" : "Cannot uninstall library from UI",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/cant-uninstall-libraries"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/install-turbodbc",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Install Turbodbc via init script Article 03/11/2022 2 minutes to read 2 contributors In this article Create the init script Configure the init script Turbodbc is a Python module that uses the ODBC interface to access relational databases. It has dependencies on libboost-all-dev, unixodbc-dev, and python-dev packages, which need to be installed in order. You can install these manually, or you can use an init script to automate the install. Create the init script Run this sample script in a notebook to create the init script on your cluster. Python Copy dbutils.fs.mkdirs(\"dbfs:/<path-to-init-script>\")\ndbutils.fs.put(\"dbfs:/<path-to-init-script>/turbodbc_install.sh\", \"\"\"\n#!/bin/bash\n#install dependent packages\nsudo apt-get -y install libboost-all-dev unixodbc-dev python-dev\npip install turbodbc==4.1.1\n\"\"\",True)\n Remember the path to the init script. You will need it when configuring your cluster. Configure the init script Follow the documentation to configure a cluster-scoped init script. Specify the path to the init script. Use the same path that you used in the sample script. After configuring the init script, restart the cluster.",
          "title" : "Install Turbodbc via init script",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/install-turbodbc"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/howto-jobsdeleterestapi",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to delete all jobs using the REST API Article 03/11/2022 2 minutes to read 3 contributors In this article Run the following commands to delete all jobs in an Azure Databricks workspace. Identify the jobs to delete and list them in a text file: Bash Copy curl -X GET -u \"Bearer: <token>\" https://<databricks-instance>/api/2.0/jobs/list | grep -o -P 'job_id.{0,6}' | awk -F':' '{print $2}' >> job_id.txt\n Run the curl command in a loop to delete the identified jobs: Bash Copy while read line\ndo\njob_id=$line\ncurl -X POST -u \"Bearer: <token>\" https://<databricks-instance>/api/2.0/jobs/delete -d '{\"job_id\": '\"$job_id\"'}'\ndone < job_id.txt",
          "title" : "How to delete all jobs using the REST API",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/howto-jobsdeleterestapi"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/driver-unavailable",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Spark job fails with Driver is temporarily unavailable Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem A Databricks notebook returns the following error: Console Copy Driver is temporarily unavailable\n This issue can be intermittent or not. A related error message is: Console Copy Lost connection to cluster. The notebook may have been detached.\n Cause One common cause for this error is that the driver is undergoing a memory bottleneck. When this happens, the driver crashes with an out of memory (OOM) condition and gets restarted or becomes unresponsive due to frequent full garbage collection. The reason for the memory bottleneck can be any of the following: The driver instance type is not optimal for the load executed on the driver. There are memory-intensive operations executed on the driver. There are many notebooks or jobs running in parallel on the same cluster. Solution The solution varies from case to case. The easiest way to resolve the issue in the absence of specific details is to increase the driver memory. You can increase driver memory simply by upgrading the driver node type on the cluster edit page in your Azure Databricks workspace. Other points to consider: Avoid memory intensive operations like: collect() operator, which brings a large amount of data to the driver. Conversion of a large DataFrame to Pandas If these operations are essential, ensure that enough driver memory is available. Avoid running batch jobs on a shared interactive cluster. Distribute the workloads into different clusters. No matter how big the cluster is, the functionalities of the Spark driver cannot be distributed within a cluster.",
          "title" : "Spark job fails with Driver is temporarily unavailable",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/driver-unavailable"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/cluster-restart-fails-admin-user",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Admin user cannot restart cluster to run job Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem When a user who has permission to start a cluster, such as a Azure Databricks Admin user, submits a job that is owned by a different user, the job fails with the following message: Console Copy Message: Run executed on existing cluster ID <cluster id> failed because of insufficient permissions. The error received from the cluster manager was: 'You are not authorized to restart this cluster. Please contact your administrator or the cluster creator.'\n Cause This error can occur when the job owner’s privilege to start the cluster is revoked. In this scenario, the job will fail even if it is submitted by an Admin user. Solution Re-grant the privilege to start the cluster (known as Can Manage) to the job owner. Change the job owner to a user or group that has the cluster start privilege. You can change it by navigating to your job page in Jobs, then to Advanced > Permissions > Edit.",
          "title" : "Admin user cannot restart cluster to run job",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/cluster-restart-fails-admin-user"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/date-int-only-spark-30",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Date functions only accept int values in Apache Spark 3.0 Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are attempting to use the date_add() or date_sub() functions in Spark 3.0, but they are returning an Error in SQL statement: AnalysisException error message. In Spark 2.4 and below, both functions work as normal. SQL Copy select date_add(cast('1964-05-23' as date), '12.34')\n Cause You are attempting to use a fractional or string value as the second argument. In Spark 2.4 and below, if the second argument is a fractional or string value, it is coerced to an int value before date_add() or date_sub() is evaluated. Using the example code listed above, the value 12.34 is converted to 12 before date_add() is evaluated. In Spark 3.0, if the second argument is a fractional or string value, it returns an error. Solution Use int, smallint, or tinyint values as the second argument for the date_add() or date_sub() functions in Spark 3.0. SQL Copy select date_add(cast('1964-05-23' as date), '12')\n SQL Copy select date_add(cast('1964-05-23' as date), 12)\n Both of these examples work properly in Spark 3.0. Note If you are importing this data from another source, you should create a routine to sanitize the values and ensure the data is in integer form before passing it to one of the date functions.",
          "title" : "Date functions only accept int values in Apache Spark 3.0",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/date-int-only-spark-30"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/error-download-full-results",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Error when downloading full results after join Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are working with two tables in a notebook. You perform a join. You can preview the output, but when you try to Download full results you get an error. Console Copy Error in SQL statement: AnalysisException: Found duplicate column(s) when inserting into dbfs:/databricks-results/\n Reproduce error Create two tables. Python Copy from pyspark.sql.functions import *\n\ndf = spark.range(12000)\ndf = df.withColumn(\"col2\",lit(\"test\"))\ndf.createOrReplaceTempView(\"table1\")\n\ndf1 = spark.range(5)\ndf1.createOrReplaceTempView(\"table2\")\n Perform left outer join on the tables. SQL Copy select * from table1 t1 left join table2 t2 on t1.id = t2.id\n Click Download preview. A CSV file downloads. Click Download full results. An error is generated. Cause Download preview works because this is a frontend only operation that runs in the browser. No constraints are checked and only 1000 rows are included in the CSV file. Download full results re-executes the query in Apache Spark and writes the CSV file internally. The error occurs when duplicate columns are found after a join operation. Solution Option 1 If you select all the required columns, and avoid duplicate columns after the join operation, you will not get the error and can download the full result. SQL Copy select t1.id, t1.col2 from table1 t1 left join table2 t2 on t1.id = t2.id\n Option 2 You can use DataFrames to prevent duplicated columns. If there are no duplicated columns after the join operation, you will not get the error and can download the full result. Python Copy result_df = df.join(df1, [\"id\"],\"left\")\ndisplay(result_df)",
          "title" : "Error when downloading full results after join",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/error-download-full-results"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/find-size-of-table",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Find the size of a table Article 03/11/2022 2 minutes to read 2 contributors In this article Size of a delta table Size of a non-delta table This article explains how to find the size of a table. The command used depends on if you are trying to find the size of a delta table or a non-delta table. Size of a delta table To find the size of a delta table, you can use a Apache Spark SQL command. Scala Copy import com.databricks.sql.transaction.tahoe._\nval deltaLog = DeltaLog.forTable(spark, \"dbfs:/<path-to-delta-table>\")\nval snapshot = deltaLog.snapshot               // the current delta table snapshot\nprintln(s\"Total file size (bytes): ${deltaLog.snapshot.sizeInBytes}\")\n Size of a non-delta table You can determine the size of a non-delta table by calculating the total sum of the individual files within the underlying directory. You can also use queryExecution.analyzed.stats to return the size. Scala Copy spark.read.table(\"<non-delta-table-name>\").queryExecution.analyzed.stats",
          "title" : "Find the size of a table",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/find-size-of-table"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/gen-unique-increasing-values",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Generate unique increasing numeric values Article 03/11/2022 2 minutes to read 2 contributors In this article Use zipWithIndex() in a Resilient Distributed Dataset (RDD) Use monotonically_increasing_id() for unique, but not consecutive numbers Combine monotonically_increasing_id() with row_number() for two columns This article shows you how to use Apache Spark functions to generate unique increasing numeric values in a column. We review three different methods to use. You should select the method that works best with your use case. Use zipWithIndex() in a Resilient Distributed Dataset (RDD) The zipWithIndex() function is only available within RDDs. You cannot use it directly on a DataFrame. Convert your DataFrame to a RDD, apply zipWithIndex() to your data, and then convert the RDD back to a DataFrame. We are going to use the following example code to add unique id numbers to a basic table with two entries. Python Copy df = spark.createDataFrame(\n    [\n        ('Alice','10'),('Susan','12')\n    ],\n    ['Name','Age']\n)\n\ndf1=df.rdd.zipWithIndex().toDF()\ndf2=df1.select(col(\"_1.*\"),col(\"_2\").alias('increasing_id'))\ndf2.show()\n Run the example code and we get the following results: Console Copy +-----+---+-------------+\n| Name|Age|increasing_id|\n+-----+---+-------------+\n|Alice| 10|            0|\n|Susan| 12|            1|\n+-----+---+-------------+\n Use monotonically_increasing_id() for unique, but not consecutive numbers The monotonically_increasing_id() function generates monotonically increasing 64-bit integers. The generated id numbers are guaranteed to be increasing and unique, but they are not guaranteed to be consecutive. We are going to use the following example code to add monotonically increasing id numbers to a basic table with two entries. Python Copy from pyspark.sql.functions import *\n\ndf_with_increasing_id = df.withColumn(\"monotonically_increasing_id\", monotonically_increasing_id())\ndf_with_increasing_id.show()\n Run the example code and we get the following results: Console Copy +-----+---+---------------------------+\n| Name|Age|monotonically_increasing_id|\n+-----+---+---------------------------+\n|Alice| 10|                 8589934592|\n|Susan| 12|                25769803776|\n+-----+---+---------------------------+\n Combine monotonically_increasing_id() with row_number() for two columns The row_number() function generates numbers that are consecutive. Combine this with monotonically_increasing_id() to generate two columns of numbers that can be used to identify data entries. We are going to use the following example code to add monotonically increasing id numbers and row numbers to a basic table with two entries. Python Copy from pyspark.sql.functions import *\nfrom pyspark.sql.window import *\n\nwindow = Window.orderBy(col('monotonically_increasing_id'))\ndf_with_consecutive_increasing_id = df_with_increasing_id.withColumn('increasing_id', row_number().over(window))\ndf_with_consecutive_increasing_id.show()\n Run the example code and we get the following results: Console Copy +-----+---+---------------------------+-------------+\n| Name|Age|monotonically_increasing_id|increasing_id|\n+-----+---+---------------------------+-------------+\n|Alice| 10|                 8589934592|            1|\n|Susan| 12|                25769803776|            2|\n+-----+---+---------------------------+-------------+\n If you need to increment based on the last updated maximum value, you can define a previous maximum value and then start counting from there. We’re going to build on the example code that we just ran. First, we need to define the value of previous_max_value. You would normally do this by fetching the value from your existing output table. For this example, we are going to define it as 1000. Python Copy previous_max_value = 1000\ndf_with_consecutive_increasing_id.withColumn(\"cnsecutiv_increase\", col(\"increasing_id\") + lit(previous_max_value)).show()\n When this is combined with the previous example code and run, we get the following results: Console Copy +-----+---+---------------------------+-------------+------------------+\n| Name|Age|monotonically_increasing_id|increasing_id|cnsecutiv_increase|\n+-----+---+---------------------------+-------------+------------------+\n|Alice| 10|                 8589934592|            1|              1001|\n|Susan| 12|                25769803776|            2|              1002|\n+-----+---+---------------------------+-------------+------------------+",
          "title" : "Generate unique increasing numeric values",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/gen-unique-increasing-values"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/global-temp-view-not-found",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Error in SQL statement: AnalysisException: Table or view not found Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem When you try to query a table or view, you get this error: Console Copy AnalysisException:Table or view not found when trying to query a global temp view\n Cause You typically create global temp views so they can be accessed from different sessions and kept alive until the application ends. You can create a global temp view with the following statement: Scala Copy df.createOrReplaceGlobalTempView(\"<global-view-name>\")\n Here, df is the DataFrame. Another way to create the view is with: SQL Copy CREATE GLOBAL TEMP VIEW <global-view-name>\n All global temporary views are tied to a system temporary database named global_temp. If you query the global table or view without explicitly mentioning the global_temp database, then the error occurs. Solution Always use the qualified table name with the global_temp database, so that you can query the global view data successfully. For example: SQL Copy %sql\nselect * from global_temp.<global-view-name>;",
          "title" : "Error in SQL statement: AnalysisException: Table or view not found",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/global-temp-view-not-found"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/show-databases-unexpected-name",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents SHOW DATABASES command returns unexpected column name Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are using the SHOW DATABASES command and it returns an unexpected column name. Cause The column name returned by the SHOW DATABASES command changed in Databricks Runtime 7.0. Databricks Runtime 6.4 Extended Support and below: SHOW DATABASES returns namespace as the column name. Databricks Runtime 7.0 and above: SHOW DATABASES returns databaseName as the column name. Solution You can enable legacy column naming by setting the property spark.sql.legacy.keepCommandOutputSchema to false in the cluster’s Spark Config.",
          "title" : "SHOW DATABASES command returns unexpected column name",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/show-databases-unexpected-name"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/readstream-is-not-whitelisted",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents readStream() is not whitelisted error when running a query Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have table access control enabled on your cluster. You are trying to run a structured streaming query and get and error message. Console Copy py4j.security.Py4JSecurityException: Method public org.apache.spark.sql.streaming.DataStreamReader org.apache.spark.sql.SQLContext.readStream() is not whitelisted on class class org.apache.spark.sql.SQLContext\n Cause Streaming is not supported on clusters that have table access control enabled. Access control allows you to set permissions for data objects on a cluster. It requires user interaction to validate and refresh credentials. Because streaming queries run continuously, it is not supported on clusters with table access control. Solution You should use a cluster that does not have table access control enabled for streaming queries.",
          "title" : "readStream() is not whitelisted error when running a query",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/readstream-is-not-whitelisted"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/stream-xml-auto-loader",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Stream XML files using an auto-loader Article 03/11/2022 2 minutes to read 2 contributors In this article Install Spark-XML library Create the XML file Define imports Define a UDF to convert binary to string Extract XML schema Implement the stream reader View output Example notebook Apache Spark does not include a streaming API for XML files. However, you can combine the auto-loader features of the Spark batch API with the OSS library, Spark-XML, to stream XML files. In this article, we present a Scala based solution that parses XML data using an auto-loader. Install Spark-XML library You must install the Spark-XML OSS library on your Azure Databricks cluster. Review the install a library on a cluster documentation for more details. Note You must ensure that the version of Spark-XML you are installing matches the version of Spark on your cluster. Create the XML file Create the XML file and use DBUtils to save it to your cluster. Scala Copy val xml2=\"\"\"<people>\n  <person>\n    <age born=\"1990-02-24\">25</age>\n  </person>\n  <person>\n    <age born=\"1985-01-01\">30</age>\n  </person>\n  <person>\n    <age born=\"1980-01-01\">30</age>\n  </person>\n</people>\"\"\"\n\ndbutils.fs.put(\"/<path-to-save-xml-file>/<name-of-file>.xml\",xml2)\n Define imports Import the required functions. Scala Copy import com.databricks.spark.xml.functions.from_xml\nimport com.databricks.spark.xml.schema_of_xml\nimport spark.implicits._\nimport com.databricks.spark.xml._\nimport org.apache.spark.sql.functions.{<input_file_name>}\n Define a UDF to convert binary to string The streaming DataFrame requires data to be in string format. You should define a user defined function to convert binary data to string data. Scala Copy val toStrUDF = udf((bytes: Array[Byte]) => new String(bytes, \"UTF-8\"))\n Extract XML schema You must extract the XML schema before you can implement the streaming DataFrame. This can be inferred from the file using the schema_of_xml method from Spark-XML. The XML string is passed as input, from the binary Spark data. Scala Copy val df_schema = spark.read.format(\"binaryFile\").load(\"/FileStore/tables/test/xml/data/age/\").select(toStrUDF($\"content\").alias(\"text\"))\n\nval payloadSchema = schema_of_xml(df_schema.select(\"text\").as[String])\n Implement the stream reader At this point, all of the required dependencies have been met, so you can implement the stream reader. Use readStream with binary and autoLoader listing mode options enabled. Note Listing mode is used when working with small amounts of data. You can leverage fileNotificationMode if you need to scale up your application. toStrUDF is used to convert binary data to string format (text). from_xml is used to convert the string to a complex struct type, with the user-defined schema. Scala Copy val df = spark.readStream.format(\"cloudFiles\")\n  .option(\"cloudFiles.useNotifications\", \"false\") // Using listing mode, hence false is used\n  .option(\"cloudFiles.format\", \"binaryFile\")\n  .load(\"/FileStore/tables/test/xml/data/age/\")\n  .select(toStrUDF($\"content\").alias(\"text\")) // UDF to convert the binary to string\n  .select(from_xml($\"text\", payloadSchema).alias(\"parsed\")) // Function to convert string to complex types\n  .withColumn(\"path\",input_file_name) // input_file_name is used to extract the paths of input files\n View output Once everything is setup, view the output of display(df) in a notebook. Example notebook This example notebook combines all of the steps into a single, functioning example. Import it into your cluster to run the examples. Streaming XML example notebook Get notebook",
          "title" : "Stream XML files using an auto-loader",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/stream-xml-auto-loader"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Welcome to the Knowledge Base for Azure Databricks Article 03/11/2022 2 minutes to read 3 contributors In this article This Knowledge Base provides a wide variety of troubleshooting, how-to, and best practices articles to help you succeed with Azure Databricks, Delta Lake, and Apache Spark. These articles were written mostly by support and field engineers, in response to typical customer questions and issues. Azure Databricks administration: tips and troubleshooting Azure infrastructure: tips and troubleshooting Business intelligence tools: tips and troubleshooting Clusters: tips and troubleshooting Data management: tips and troubleshooting Data sources: tips and troubleshooting Databricks File System (DBFS): tips and troubleshooting Databricks SQL: tips and troubleshooting Developer tools: tips and troubleshooting Delta Lake: tips and troubleshooting Jobs: tips and troubleshooting Job execution: tips and troubleshooting Libraries: tips and troubleshooting Machine learning: tips and troubleshooting Metastore: tips and troubleshooting Metrics: tips and troubleshooting Notebooks: tips and troubleshooting Security and permissions: tips and troubleshooting Streaming: tips and troubleshooting Visualizations: tips and troubleshooting Python with Apache Spark: tips and troubleshooting R with Apache Spark: tips and troubleshooting Scala with Apache Spark: tips and troubleshooting SQL with Apache Spark: tips and troubleshooting",
          "title" : "Welcome to the Knowledge Base for Azure Databricks",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/get-and-set-spark-config",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Get and set Apache Spark configuration properties in a notebook Article 03/11/2022 2 minutes to read 3 contributors In this article Get Spark configuration properties Set Spark configuration properties Examples In most cases, you set the Spark configuration at the cluster level. However, there may be instances when you need to check (or set) the values of specific Spark configuration properties in a notebook. This article shows you how to display the current value of a Spark configuration property in a notebook. It also shows you how to set a new value for a Spark configuration property in a notebook. Get Spark configuration properties To get the current value of a Spark configuration property, evaluate the property without including a value. Python Python Copy spark.conf.get(\"spark.<name-of-property>\")\n R R Copy library(SparkR)\nsparkR.conf(\"spark.<name-of-property>\")\n Scala Scala Copy spark.conf.get(\"spark.<name-of-property>\")\n SQL SQL Copy GET spark.<name-of-property>;\n Set Spark configuration properties To set the value of a Spark configuration property, evaluate the property and assign a value. Note You can only set Spark configuration properties that start with the spark.sql prefix. Python Python Copy spark.conf.set(\"spark.sql.<name-of-property>\", <value>)\n R R Copy library(SparkR)\nsparkR.session()\nsparkR.session(sparkConfig = list(spark.sql.<name-of-property> = \"<value>\"))\n Scala Scala Copy spark.conf.set(\"spark.sql.<name-of-property>\", <value>)\n SQL SQL Copy SET spark.sql.<name-of-property> = <value>;\n Examples Get the current value of spark.rpc.message.maxSize. SQL Copy SET spark.rpc.message.maxSize;\n Set the value of spark.sql.autoBroadcastJoinThreshold to -1. Python Copy spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)",
          "title" : "Get and set Apache Spark configuration properties in a notebook",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/get-and-set-spark-config"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/list-delete-files-faster",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to list and delete files faster in Azure Databricks Article 03/11/2022 3 minutes to read 3 contributors In this article Scenario List files Delete files Summary Scenario Suppose you need to delete a table that is partitioned by year, month, date, region, and service. However, the table is huge, and there will be around 1000 part files per partition. You can list all the files in each partition and then delete them using an Apache Spark job. For example, suppose you have a table that is partitioned by a, b, and c: Scala Copy Seq((1,2,3,4,5),\n  (2,3,4,5,6),\n  (3,4,5,6,7),\n  (4,5,6,7,8))\n  .toDF(\"a\", \"b\", \"c\", \"d\", \"e\")\n  .write.mode(\"overwrite\")\n  .partitionBy(\"a\", \"b\", \"c\")\n  .parquet(\"/mnt/path/table\")\n List files You can list all the part files using this function: Scala Copy import org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{Path, FileSystem}\nimport org.apache.spark.deploy.SparkHadoopUtil\nimport org.apache.spark.sql.execution.datasources.InMemoryFileIndex\nimport java.net.URI\n\ndef listFiles(basep: String, globp: String): Seq[String] = {\n  val conf = new Configuration(sc.hadoopConfiguration)\n  val fs = FileSystem.get(new URI(basep), conf)\n\n  def validated(path: String): Path = {\n    if(path startsWith \"/\") new Path(path)\n    else new Path(\"/\" + path)\n  }\n\n  val fileCatalog = InMemoryFileIndex.bulkListLeafFiles(\n    paths = SparkHadoopUtil.get.globPath(fs, Path.mergePaths(validated(basep), validated(globp))),\n    hadoopConf = conf,\n    filter = null,\n    sparkSession = spark, areRootPaths=true)\n\n // If you are using Databricks Runtime 6.x and below,\n // remove <areRootPaths=true> from the bulkListLeafFiles function parameter.\n\n  fileCatalog.flatMap(_._2.map(_.path))\n}\n\nval root = \"/mnt/path/table\"\nval globp = \"[^_]*\" // glob pattern, e.g. \"service=webapp/date=2019-03-31/*log4j*\"\n\nval files = listFiles(root, globp)\nfiles.toDF(\"path\").show()\n Copy +------------------------------------------------------------------------------------------------------------------------------+\n|path                                                                                                                          |\n+------------------------------------------------------------------------------------------------------------------------------+\n|dbfs:/mnt/path/table/a=1/b=2/c=3/part-00000-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-5.c000.snappy.parquet|\n|dbfs:/mnt/path/table/a=2/b=3/c=4/part-00001-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-6.c000.snappy.parquet|\n|dbfs:/mnt/path/table/a=3/b=4/c=5/part-00002-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-7.c000.snappy.parquet|\n|dbfs:/mnt/path/table/a=4/b=5/c=6/part-00003-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-8.c000.snappy.parquet|\n+------------------------------------------------------------------------------------------------------------------------------+\n The listFiles function takes a base path and a glob path as arguments, scans the files and matches with the glob pattern, and then returns all the leaf files that were matched as a sequence of strings. The function also uses the utility function globPath from the SparkHadoopUtil package. This function lists all the paths in a directory with the specified prefix, and does not further list leaf children (files). The list of paths is passed into InMemoryFileIndex.bulkListLeafFiles method, which is a Spark internal API for distributed file listing. Neither of these listing utility functions work well alone. By combining them you can get a list of top-level directories that you want to list using globPath function, which will run on the driver, and you can distribute the listing for all child leaves of the top-level directories into Spark workers using bulkListLeafFiles. The speed-up can be around 20-50x faster according to Amdahl’s law. The reason is that, you can easily control the glob path according to the real file physical layout and control the parallelism through spark.sql.sources.parallelPartitionDiscovery.parallelism for InMemoryFileIndex. Delete files When you delete files or partitions from an unmanaged table, you can use the Azure Databricks utility function dbutils.fs.rm. This function leverages the native cloud storage file system API, which is optimized for all file operations. However, you can’t delete a gigantic table directly using dbutils.fs.rm(\"path/to/the/table\"). You can list files efficiently using the script above. For smaller tables, the collected paths of the files to delete fit into the driver memory, so you can use a Spark job to distribute the file deletion task. For gigantic tables, even for a single top-level partition, the string representations of the file paths cannot fit into the driver memory. The easiest way to solve this problem is to collect the paths of the inner partitions recursively, list the paths, and delete them in parallel. Scala Copy import scala.util.{Try, Success, Failure}\n\ndef delete(p: String): Unit = {\n  dbutils.fs.ls(p).map(_.path).toDF.foreach { file =>\n    dbutils.fs.rm(file(0).toString, true)\n    println(s\"deleted file: $file\")\n  }\n}\n\nfinal def walkDelete(root: String)(level: Int): Unit = {\n  dbutils.fs.ls(root).map(_.path).foreach { p =>\n    println(s\"Deleting: $p, on level: ${level}\")\n    val deleting = Try {\n      if(level == 0) delete(p)\n      else if(p endsWith \"/\") walkDelete(p)(level-1)\n      //\n      // Set only n levels of recursion, so it won't be a problem\n      //\n      else delete(p)\n    }\n    deleting match {\n      case Success(v) => {\n        println(s\"Successfully deleted $p\")\n        dbutils.fs.rm(p, true)\n      }\n      case Failure(e) => println(e.getMessage)\n    }\n  }\n}\n The code deletes inner partitions while ensuring that the partition that is being deleted is small enough. It does this by searching through the partitions recursively by each level, and only starts deleting when it hits the level you set. For instance, if you want to start with deleting the top-level partitions, use walkDelete(root)(0). Spark will delete all the files under dbfs:/mnt/path/table/a=1/, then delete .../a=2/, following the pattern until it is exhausted. The Spark job distributes the deletion task using the delete function shown above, listing the files with dbutils.fs.ls with the assumption that the number of child partitions at this level is small. You can also be more efficient by replacing the dbutils.fs.ls function with the listFiles function shown above, with only slight modification. Summary These two approaches highlight methods for listing and deleting gigantic tables. They use some Spark utility functions and functions specific to the Azure Databricks environment. Even if you cannot use them directly, you can create your own utility functions to solve the problem in an analogous way.",
          "title" : "How to list and delete files faster in Azure Databricks",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/list-delete-files-faster"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/wrong-schema-in-files",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Incompatible schema in some files Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem The Spark job fails with an exception like the following while reading Parquet files: Console Copy Error in SQL statement: SparkException: Job aborted due to stage failure:\nTask 20 in stage 11227.0 failed 4 times, most recent failure: Lost task 20.3 in stage 11227.0\n(TID 868031, 10.111.245.219, executor 31):\njava.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\n    at org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:52)\n Cause The java.lang.UnsupportedOperationException in this instance is caused by one or more Parquet files written to a Parquet folder with an incompatible schema. Solution Find the Parquet files and rewrite them with the correct schema. Try to read the Parquet dataset with schema merging enabled: Scala Copy spark.read.option(\"mergeSchema\", \"true\").parquet(path)\n or Scala Copy spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"true\")\nspark.read.parquet(path)\n If you do have Parquet files with incompatible schemas, the snippets above will output an error with the name of the file that has the wrong schema. You can also check if two schemas are compatible by using the merge method. For example, let’s say you have these two schemas: Scala Copy import org.apache.spark.sql.types._\n\nval struct1 = (new StructType)\n  .add(\"a\", \"int\", true)\n  .add(\"b\", \"long\", false)\n\nval struct2 = (new StructType)\n  .add(\"a\", \"int\", true)\n  .add(\"b\", \"long\", false)\n  .add(\"c\", \"timestamp\", true)\n Then you can test if they are compatible: Scala Copy struct1.merge(struct2).treeString\n This will give you: Scala Copy res0: String =\n\"root\n|-- a: integer (nullable = true)\n|-- b: long (nullable = false)\n|-- c: timestamp (nullable = true)\n\"\n However, if struct2 has the following incompatible schema: Scala Copy val struct2 = (new StructType)\n  .add(\"a\", \"int\", true)\n  .add(\"b\", \"string\", false)\n Then the test will give you the following SparkException: Console Copy org.apache.spark.SparkException: Failed to merge fields 'b' and 'b'. Failed to merge incompatible data types LongType and StringType",
          "title" : "Incompatible schema in some files",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/wrong-schema-in-files"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/hive-metastore-troubleshooting",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to troubleshoot several Apache Hive metastore problems Article 03/11/2022 3 minutes to read 3 contributors In this article Problem 1: External metastore tables not available Problem 2: Hive metastore verification failed Problem 3: Metastore connection limit exceeded Problem 4: Table actions fail because column has too much metadata Problem 1: External metastore tables not available When you inspect the driver logs, you see a stack trace that includes the error Required table missing: Console Copy WARN Query: Query for candidates of org.apache.hadoop.hive.metastore.model.MDatabase and subclasses resulted in no possible candidates\n\nRequired table missing: \"DBS\" in Catalog \"\" Schema \"\". DataNucleus requires this table to perform its\npersistence operations. Either your MetaData is incorrect, or you need to enable\n\"datanucleus.schema.autoCreateTables\"\n\norg.datanucleus.store.rdbms.exceptions.MissingTableException: Required table missing : \"DBS\" in Catalog \"\"  Schema \"\". DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable\n\"datanucleus.schema.autoCreateTables\"\n\n   at\n\norg.datanucleus.store.rdbms.table.AbstractTable.exists(AbstractTable.java:606)\n\n   at\n\norg.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.performTablesValidation(RDBMSStoreManager.java:33\n85)\n Cause The database is present, but there are no metastore tables. Solution If the external metastore version is Hive 2.0 or above, use the Hive Schema Tool to create the metastore tables. For versions below Hive 2.0, add the metastore tables with the following configurations in your existing init script: ini Copy spark.hadoop.datanucleus.autoCreateSchema=true\nspark.hadoop.datanucleus.fixedDatastore=false\n You can also set these configurations in the Apache Spark configuration directly: ini Copy datanucleus.autoCreateSchema true\ndatanucleus.fixedDatastore false\n Problem 2: Hive metastore verification failed When you inspect the driver logs, you see a stack trace that includes an error like the following: Console Copy 18/09/24 14:51:07 ERROR RetryingHMSHandler: HMSHandler Fatal error:\nMetaException(message:Version information not found in metastore. )\n\n   at\norg.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore\n.java:7564)\n\n   at\norg.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.\njava:7542)\n\n   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n Cause The VERSION table in the metastore is empty. Solution Do one of the following: Populate the VERSION table with the correct version values using an INSERT query. Set the following configurations to turn off the metastore verification in the Spark configuration of the cluster: ini Copy hive.metastore.schema.verification false\nhive.metastore.schema.verification.record.version false\n Problem 3: Metastore connection limit exceeded Commands run on the cluster fail with the following stack trace in the driver logs: Console Copy Unable to open a test connection to the given\ndatabase. JDBC url =\njdbc:<jdbcURL>?trustServerCertificate=true&useSS\nL=true, username = <REDACTED>. Terminating\nconnection pool (set lazyInit to true if you\nexpect to start your database after your app).\nOriginal Exception: ------\n\njava.sql.SQLSyntaxErrorException: User\n'<userId>' has exceeded the\n'max_user_connections' resource (current value:\n100)\nat\norg.mariadb.jdbc.internal.util.exceptions.Except\nionMapper.get(ExceptionMapper.java:163)\nat\norg.mariadb.jdbc.internal.util.exceptions.Except\nionMapper.getException(ExceptionMapper.java:106)\nat\norg.mariadb.jdbc.internal.protocol.AbstractConne\nctProtocol.connectWithoutProxy(AbstractConnectPr\notocol.java:1036)\n Cause The metastore configuration allows only 100 connections. When the connection limit is reached, new connections are not allowed, and commands fail with this error. Each cluster in the Azure Databricks workspace establishes a connection with the metastore. If you have a large number of clusters running, then this issue can occur. Additionally, incorrect configurations can cause a connection leak, causing the number of connections to keep increasing until the limit is reached. Solution Correct the problem with one of the following actions: If you are using an external metastore and you have a large number of clusters running, then increase the connection limit on your external metastore. If you are not using an external metastore, ensure that you do not have any custom Hive metastore configurations on your cluster. When using the metastore provided by Azure Databricks, you should use the default configurations on the cluster for the Hive metastore. If you are using the default configuration and still encounter this issue, contact Azure Databricks Support. Depending on the configuration of your Azure Databricks workspace, it might be possible to increase the number of connections allowed to the internal metastore. Problem 4: Table actions fail because column has too much metadata When the quantity of metadata for a single column exceeds 4000 characters, table actions fail with an error like this: Console Copy Error in SQL statement: IllegalArgumentException:\nError: type expected at the position 3998 of 'struct<num_ad_accounts:bigint,num_benchmarks:bigint,num_days_range:string,num_days_in_history:string,num_fb_pages:bigint,num_g_profiles:bigint,num_ga_views:bigint,num_groups:bigint,num_ig_profiles:bigint,num_li_pages:bigint,num_labels:string,num_labels_added:bigint,num_labels_\n Cause This is a bug that was fixed in Hive Metastore version 2.3.0 (HIVE-12274). Azure Databricks uses an earlier version of Hive Metastore (version 0.13), so this bug occurs when there is too much metadata for a column, such as an imported JSON schema. Solution As a workaround, set up an external Hive metastore that uses version 2.3.0 or above. Then delete the existing table with the following command: Scala Copy spark.sessionState\n  .catalog\n  .externalCatalog\n  .dropTable(\"default\", \"test_table_tabledrop_1\", ignoreIfNotExists = false, purge = false)",
          "title" : "How to troubleshoot several Apache Hive metastore problems",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/hive-metastore-troubleshooting"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/convert-datetime-to-string",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Convert Python datetime object to string Article 03/11/2022 2 minutes to read 2 contributors In this article Display timestamp as a column value Assign timestamp to datetime object Convert to string There are multiple ways to display date and time values with Python, however not all of them are easy to read. For example, when you collect a timestamp column from a DataFrame and save it as a Python variable, the value is stored as a datetime object. If you are not familiar with the datetime object format, it is not as easy to read as the common YYYY-MM-DD HH:MM:SS format. If you wanted to print the date and time, or maybe use it for timestamp validation, you can convert the datetime object to a string. This automatically converts the datetime object into a common time format. In this article, we show you how to display the timestamp as a column value, before converting it to a datetime object, and finally, a string value. Display timestamp as a column value To display the current timestamp as a column value, you should call current_timestamp(). This provides the date and time as of the moment it is called. Python Copy from pyspark.sql.functions import *\ndisplay(spark.range(1).withColumn(\"date\",current_timestamp()).select(\"date\"))\n Sample output: Assign timestamp to datetime object Instead of displaying the date and time in a column, you can assign it to a variable. Python Copy mydate = spark.range(1).withColumn(\"date\",current_timestamp()).select(\"date\").collect()[0][0]\n Once this assignment is made, you can call the variable to display the stored date and time value as a datetime object. Python Copy mydate\n Sample output: Console Copy datetime.datetime(2021, 6, 25, 11, 0, 56, 813000)\n Note The date and time is current as of the moment it is assigned to the variable as a datetime object, but the datetime object value is static unless a new value is assigned. Convert to string You can convert the datetime object to a string by calling str() on the variable. Calling str() just converts the datetime object to a string. It does not update the value with the current date and time. Python Copy str(mydate)\n Sample output: Console Copy '2021-06-25 11:00:56.813000'",
          "title" : "Convert Python datetime object to string",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/convert-datetime-to-string"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/bi/configure-simba-azure-ad-creds",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Configure Simba JDBC driver using Azure AD Article 03/11/2022 2 minutes to read 2 contributors In this article Create a service principal Configure service principal permissions Update service principal manifest Download and configure the JDBC driver Obtain the Azure AD token Pass the Azure AD token to the JDBC driver This article describes how to access Azure Databricks with a Simba JDBC driver using Azure AD authentication. This can be useful if you want to use an Azure AD user account to connect to Azure Databricks. Note Power BI has native support for Azure AD authentication with Azure Databricks. Review the Power BI documentation for more information. Create a service principal Create a service principal in Azure AD. The service principal obtains an access token for the user. Open the Azure Portal. Open the Azure Active Directory service. Click App registrations in the left menu. Click New registration. Complete the form and click Register. Your service principal has been successfully created. Configure service principal permissions Open the service principal you created. Click API permissions in the left menu. Click Add a permission. Click Azure Rights Management Services. Click Delegated permissions. Select user_impersonation. Click Add permissions. The user_impersonation permission is now assigned to your service principal. Note If Grant admin consent is not enabled, you may encounter an error later on in the process. Update service principal manifest Click Manifest in the left menu. Look for the line containing the \"allowPublicClient\" property. Set the value to true. Click Save. Download and configure the JDBC driver Download the Databricks JDBC Driver. Configure the JDBC driver as detailed in the documentation. Obtain the Azure AD token Use the sample code to obtain the Azure AD token for the user. Replace the variables with values that are appropriate for your account. Python Copy from adal import AuthenticationContext\n\nauthority_host_url = \"https://login.microsoftonline.com/\"\"\n# Application ID of Azure Databricks\nazure_databricks_resource_id = \"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d\"\n\n# Required user input\nuser_parameters = {\n   \"tenant\" : \"<tenantId>\",\n   \"client_id\" : \"<clientId>\",\n   \"username\" : \"<user@domain.com>\",\n   \"password\" : <password>\n}\n\n# configure AuthenticationContext\n# authority URL and tenant ID are used\nauthority_url = authority_host_url + user_parameters['tenant']\ncontext = AuthenticationContext(authority_url)\n\n# API call to get the token\ntoken_response = context.acquire_token_with_username_password(\n  azure_databricks_resource_id,\n  user_parameters['username'],\n  user_parameters['password'],\n  user_parameters['client_id']\n)\n\naccess_token = token_response['accessToken']\nrefresh_token = token_response['refreshToken']\n Pass the Azure AD token to the JDBC driver Now that you have the user’s Azure AD token, you can pass it to the JDBC driver using Auth_AccessToken in the JDBC URL as detailed in the Azure Active Directory token authentication documentation. This sample code demonstrates how to pass the Azure AD token. Python Copy # Install jaydebeapi pypi module (used for demo)\n\nimport jaydebeapi\nimport pandas as pd\n\nimport os os.environ[\"CLASSPATH\"] = \"<path to downloaded Simba Spark JDBC/ODBC driver>\"\n\n# JDBC connection string\nurl=\"jdbc:spark://adb-111111111111xxxxx.xx.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/<workspaceId>/<clusterId>;AuthMech=11;Auth_Flow=0;Auth_AccessToken={0}\".format(access_token)\n\ntry:\n  conn=jaydebeapi.connect(\"com.simba.spark.jdbc.Driver\", url)\n  cursor = conn.cursor()\n\n  # Execute SQL query\n  sql=\"select * from <tablename>\"\n  cursor.execute(sql)\n  results = cursor.fetchall()\n  column_names = [x[0] for x in cursor.description]\n  pdf = pd.DataFrame(results, columns=column_names)\n  print(pdf.head())\n\n # Uncomment the following two lines if this code is running in the Databricks Connect IDE or within a workspace notebook.\n # df = spark.createDataFrame(pdf)\n # df.show()\n\nfinally:\n        if cursor is not None:\n            cursor.close()",
          "title" : "Configure Simba JDBC driver using Azure AD",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/bi/configure-simba-azure-ad-creds"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/append-slow-with-spark-2.0.0",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Spark 2.0.0 cluster takes a long time to append data Article 03/11/2022 2 minutes to read 3 contributors In this article If you find that a cluster using Spark 2.0.0 version takes a longer time to append data to an existing dataset and in particular, all of Spark jobs have finished, but your command has not finished, it is because driver node is moving the output files of tasks from the job temporary directory to the final destination one-by-one, which is slow with cloud storage. To resolve this issue, set mapreduce.fileoutputcommitter.algorithm.version to 2. This issue does not affect overwriting a dataset or writing data to a new location. Note Starting with Spark 2.0.1-db1, the default value of mapreduce.fileoutputcommitter.algorithm.version is 2. If you are using Spark 2.0.0, manually set this config if you experience this slowness issue. How to confirm if I am experiencing this issue? You can confirm if you are experiencing this issue by checking the following things: All of your Spark jobs have finished and your cell has not finished. The progress bar should look like The thread dump of the driver (you can find it on the executor page of the Spark UI) shows that there is a thread spending a long time inside the commitJob method of FileOutputCommitter class. How do I set spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version to 2? You can set this config by using any of the following methods: When you launch your cluster, you can put spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2 in the Spark config. In your notebook, you can run %sql set mapreduce.fileoutputcommitter.algorithm.version=2 or spark.conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\") (spark is a SparkSession object provided with Databricks notebooks). When you write data using Dataset API, you can set it in the option, i.e. dataset.write.option(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\"). What is the cause? When Spark appends data to an existing dataset, Spark uses FileOutputCommitter to manage staging output files and final output files. The behavior of FileOutputCommitter has direct impact on the performance of jobs that write data. A FileOutputCommitter has two methods, commitTask and commitJob. Apache Spark 2.0 and higher versions use Apache Hadoop 2, which uses the value of mapreduce.fileoutputcommitter.algorithm.version to control how commitTask and commitJob work. In Hadoop 2, the default value of mapreduce.fileoutputcommitter.algorithm.version is 1. For this version, commitTask moves data generated by a task from the task temporary directory to job temporary directory and when all tasks complete, commitJob moves data to from job temporary directory to the final destination [1]. Because the driver is doing the work of commitJob, for cloud storage, this operation can take a long time. You may often think that your cell is “hanging”. However, when the value of mapreduce.fileoutputcommitter.algorithm.version is 2, commitTask moves data generated by a task directly to the final destination and commitJob is basically a no-op.",
          "title" : "Spark 2.0.0 cluster takes a long time to append data",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/append-slow-with-spark-2.0.0"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/chained-transformations",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Simplify chained transformations Article 03/11/2022 2 minutes to read 3 contributors In this article DataFrame transform API Function.chain API implicit class Sometimes you may need to perform multiple transformations on your DataFrame: Scala Copy import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.DataFrame\n\nval testDf = (1 to 10).toDF(\"col\")\n\ndef func0(x: Int => Int, y: Int)(in: DataFrame): DataFrame = {\n  in.filter('col > x(y))\n}\ndef func1(x: Int)(in: DataFrame): DataFrame = {\n  in.selectExpr(\"col\", s\"col + $x as col1\")\n}\ndef func2(add: Int)(in: DataFrame): DataFrame = {\n  in.withColumn(\"col2\", expr(s\"col1 + $add\"))\n}\n When you apply these transformations, you may end up with spaghetti code like this: Scala Copy def inc(i: Int) = i + 1\n\nval tmp0 = func0(inc, 3)(testDf)\nval tmp1 = func1(1)(tmp0)\nval tmp2 = func2(2)(tmp1)\nval res = tmp2.withColumn(\"col3\", expr(\"col2 + 3\"))\n This article describes several methods to simplify chained transformations. DataFrame transform API To benefit from the functional programming style in Spark, you can leverage the DataFrame transform API, for example: Scala Copy val res = testDf.transform(func0(inc, 4))\n                .transform(func1(1))\n                .transform(func2(2))\n                .withColumn(\"col3\", expr(\"col2 + 3\"))\n Function.chain API To go even further, you can leverage the Scala Function library, to chain the transformations, for example: Scala Copy val chained = Function.chain(List(func0(inc, 4)(_), func1(1)(_), func2(2)(_)))\nval res = testDf.transform(chained)\n                .withColumn(\"col3\", expr(\"col2 + 3\"))\n implicit class Another alternative is to define a Scala implicit class, which allows you to eliminate the DataFrame transform API: Scala Copy implicit class MyTransforms(df: DataFrame) {\n    def func0(x: Int => Int, y: Int): DataFrame = {\n        df.filter('col > x(y))\n    }\n    def func1(x: Int): DataFrame = {\n        df.selectExpr(\"col\", s\"col + $x as col1\")\n    }\n    def func2(add: Int): DataFrame = {\n        df.withColumn(\"col2\", expr(s\"col1 + $add\"))\n    }\n}\n Then you can call the functions directly: Scala Copy val res = testDf.func0(inc, 1)\n            .func1(2)\n            .func2(3)\n            .withColumn(\"col3\", expr(\"col2 + 3\"))",
          "title" : "Simplify chained transformations",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/chained-transformations"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/revoke-all-user-privileges",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Revoke all user privileges Article 03/11/2022 2 minutes to read 2 contributors In this article Example code When user permissions are explicitly granted for individual tables and views, the selected user can access those tables and views even if they don’t have permission to access the underlying database. If you want to revoke a user’s access, you can do so with the REVOKE command. However, the REVOKE command is explicit, and is strictly scoped to the object specified in the command. For example: SQL Copy REVOKE ALL PRIVILEGES ON DATABASE <database-name> FROM `<user>@<domain-name>`\nREVOKE SELECT ON <table-name> FROM `<user>@<domain-name>`\n If you want to revoke all privileges for a single user you can do it with a series of multiple commands, or you can use a regular expression and a series of for loops to automate the process. Example code This example code matches the <search-string> pattern to the database name and the table name and then revokes the user’s privileges. The search is recursive. Python Copy from re import search\ndatabaseQuery = sqlContext.sql(\"show databases\")\ndatabaseList = databaseQuery.collect()\n# This loop revokes at the database level.\nfor db in databaseList:\n  listTables = sqlContext.sql(\"show tables from \"+db['databaseName'])\n  tableRows = listTables.collect()\n  if search(<search-string>, db['databaseName']):\n    revokeDatabase=sqlContext.sql(\"REVOKE ALL PRIVILAGES ON DATABASE \"+db['databaseName']+\" to `<username>`\")\n    display(revokeDatabase)\n    print(\"Ran the REVOKE query on \"+db['databaseName']+\" for <username>\")\n  # This loop revokes at the table level.\n  for table in tableRows:\n    if search(<search-string>,table['tableName']):\n      revokeCommand=sqlContext.sql(\"REVOKE SELECT ON \"+table['database']+\".\"+table['tableName']+\" FROM `<username>`\")\n      display(revokeCommand)\n      print(\"Revoked the SELECT permissions on \"+table['database']+\".\"+table['tableName']+\" for <username>\")\n Note These commands only work if you have enabled table access control for the cluster.",
          "title" : "Revoke all user privileges",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/revoke-all-user-privileges"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/schema-from-case-class",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Generate schema from case class Article 03/11/2022 2 minutes to read 3 contributors In this article Spark provides an easy way to generate a schema from a Scala case class. For case class A, use the method ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]. For example: Scala Copy %scala\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.catalyst.ScalaReflection\n\ncase class A(key: String, time: java.sql.Timestamp, date: java.sql.Date, decimal: java.math.BigDecimal, map: Map[String, Int], nested: Seq[Map[String, Seq[Int]]])\nval schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]\nschema.printTreeString",
          "title" : "Generate schema from case class",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/schema-from-case-class"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/data-too-long-for-column",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Data too long for column error Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to insert a struct into a table, but you get a java.sql.SQLException: Data too long for column error. Console Copy Caused by: java.sql.SQLException: Data too long for column 'TYPE_NAME' at row 1\nQuery is: INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,`COLUMN_NAME`,TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) , parameters [103182,<null>,'address','struct<street_address1:struct<street_number:int,street_name:string,street_type:string,country:string,postal_code:string>,street_address2:struct<street_number:int,street_name:string,street_type:string,country:string,postal_code:string>,street_address3:struct<street_number:int,street_name:string,street_type:string,country:string,postal_code:string>,street_address4:struct<street_number:int,street_name:string,street_type:string,country:string,postal_code:string>,street_address5:struct<street_number:int,street_name:string,street_type:string,country:string,postal_code:string>,street_address6:struct<street_number:int,street_name:string,street_type:string,country:string,postal_code:string>,street_address7:struct<street_number:int,street_name:string,street_type:string,country:string,postal_code:string>,street_address8:struct<street_number:int,street_name:string,street_type:string,coun...\n        at org.mariadb.jdbc.internal.util.LogQueryTool.exceptionWithQuery(LogQueryTool.java:153)\n        at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.executeQuery(AbstractQueryProtocol.java:255)\n        at org.mariadb.jdbc.MariaDbPreparedStatementClient.executeInternalBatch(MariaDbPreparedStatementClient.java:368)\n        at org.mariadb.jdbc.MariaDbPreparedStatementClient.executeBatch(MariaDbPreparedStatementClient.java:280)\n Cause The root cause of this issue is a default datatype of varchar(4000) for the column in your table. If you have a deeply nested struct that exceeds more than 4000 characters in length, it exceeds the size of the default datatype and results in an error message. You can validate this by describing the column you are trying to insert the data into. It will return a datatype of varchar(4000). Solution You should use an external metastore if you are going to exceed 4000 characters within a column. The default datatype for the Azure Databricks Hive metastore is varchar(4000) and cannot be changed. When you use an external metastore, you have full control over the length of column and database names. You also have control over the collation of column names, database names, and table names. Review the external Apache Hive metastore documentation to learn how to setup an external metastore. Once the metastore tables have been created, you can directly modify the column data types by using the ALTER TABLE command. Note When using an external metastore, you are responsible for maintaining, patching, and upgrading the metastore. It does not happen automatically.",
          "title" : "Data too long for column error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/data-too-long-for-column"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/jpn-char-external-metastore",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Japanese character support in external metastore Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to use Japanese characters in your tables, but keep getting errors. Create a table with the OPTIONS keyword OPTIONS provides extra metadata to the table. You try creating a table with OPTIONS and specify the charset as utf8mb4. SQL Copy CREATE TABLE default.JPN_COLUMN_NAMES('作成年月' string\n,'計上年月' string\n,'所属コード' string\n,'生保代理店コード＿８桁' string\n,'所属名' string\n)\nusing csv  OPTIONS (path \"/mnt/tabledata/testdata/\", header \"true\", delimiter \",\", inferSchema \"false\", ignoreLeadingWhiteSpace \"false\", ignoreTrailingWhiteSpace \"false\", multiLine \"true\", escape \"\\\"\" , charset \"utf8mb4\");\n The result is an error. Console Copy Error in SQL statement: AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Put request failed : INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (?,?,?)\n Create a table without the OPTIONS keyword You try to create a table without using OPTIONS. SQL Copy CREATE TABLE test.JPN_COLUMN_NAMES (`作成年月` string ,`計上年月` string) USING csv\ndescribe extended test.JPN_COLUMN_NAMES;\n The table appears to be created, but the column names are shown as ???? instead of using the specified Japanese characters. Create a table with Hive table expression You try creating a Hive format table and specify the charset as utf8mb4. SQL Copy CREATE TABLE test.JPN_COLUMN_NAMES (`作成年月` string ,`計上年月` string)\n   ROW FORMAT SERDE \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\"\n   WITH SERDEPROPERTIES ( \"separatorChar\" = \",\",\n    \"quoteChar\" = \"\\\"\",\n    \"escapeChar\" = \"\\\\\",\n    \"serialization.encoding\"='utf8mb4')\n    TBLPROPERTIES ( 'store.charset'='utf8mb4',\n    'retrieve.charset'='utf8mb4');\n The result is an error. Console Copy Caused by: java.sql.SQLException: Incorrect string value: '\\xE4\\xBD\\x9C\\xE6\\x88\\x90...' for column 'COLUMN_NAME' at row 1\nQuery is: INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,`COLUMN_NAME`,TYPE_NAME,INTEGER_IDX) VALUES (6544,<null>,'作成年月','string',0)\n Cause When a table is created, an entry is updated in the Hive metastore. The Hive metastore is typically a MySQL database. When a new table is created, the names of the columns are inserted into the TABLE_PARAMS of the metastore. The charset collation of PARAM_VALUE from TABLE_PARAMS is latin1_bin as collation and the charset is latin1. Scala Copy executeQuery(\"\"\"SELECT TABLE_SCHEMA , TABLE_NAME , COLUMN_NAME , COLLATION_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'TABLE_PARAMS' \"\"\")\n Solution latin1 does not have support for Japanese characters, but UTF-8 does. You need to use an external metastore with UTF-8_bin as collation and the charset as UTF-8. Any MySQL database 5.6 or above can be used as a Hive metastore. For this example, we are using MySQL 8.0.13-4. Create an external Apache Hive metastore. Create a database to instantiate the new metastore with default tables. SQL Copy create database <database_name>\n The newly created tables can be explored in the external database objects browser or by using the show tables command. SQL Copy -- Run in the metastore database.\nshow tables in <database_name>\n Check the collation information in MySQL at the table level. SQL Copy SELECT TABLE_COLLATION,TABLE_NAME,TABLE_TYPE,TABLE_COLLATION FROM INFORMATION_SCHEMA.TABLES where TABLE_TYPE like 'BASE%'\n Check the collation information in MySQL at the column level. SQL Copy SELECT TABLE_SCHEMA , TABLE_NAME , COLUMN_NAME , COLLATION_NAME FROM INFORMATION_SCHEMA.COLUMNS\n Change the charset from latin1 to UTF-8. SQL Copy -- Run in the metastore database. All queries are compatible with MySQL.\n-- Change collation and charset across the database.\nALTER DATABASE <database_name> CHARACTER SET utf8 COLLATE utf8_bin;\n-- Change collation and charset per table.\nALTER TABLE <table_name> CONVERT TO CHARACTER SET utf8 COLLATE utf8_bin;\n-- Change collation and charset at the column level.\nALTER TABLE <table_name> MODIFY <column_name> <datatype> CHARACTER SET utf8 COLLATE utf8_bin;\n You can now correctly view Japanese characters when you display the table.",
          "title" : "Japanese character support in external metastore",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/jpn-char-external-metastore"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/list-tables",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Listing table names Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution This article explains why spark.catalog.listTables() and %sql show tables have different performance characteristics. Problem To fetch all the table names from metastore you can use either spark.catalog.listTables() or %sql show tables. If you observe the duration to fetch the details you can see spark.catalog.listTables() usually takes longer than %sql show tables. Cause spark.catalog.listTables() tries to fetch every table’s metadata first and then show the requested table names. This process is slow when dealing with complex schemas and larger numbers of tables. Solution To get only the table names, use %sql show tables which internally invokes SessionCatalog.listTables which fetches only the table names.",
          "title" : "Listing table names",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/list-tables"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/slow-autoscaling-external-metastore",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Autoscaling is slow with an external metastore Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have an external metastore configured on your cluster and autoscaling is enabled, but the cluster is not autoscaling effectively. Cause You are copying the metastore jars to every executor, when they are only needed in the driver. It takes time to initialize and run the jars every time a new executor spins up. As a result, adding more executors takes longer than it should. Solution You should configure your cluster so the metastore jars are only copied to the driver. Option 1: Use an init script to copy the metastore jars. Create a cluster with spark.sql.hive.metastore.jars set to maven and spark.sql.hive.metastore.version to match the version of your metastore. Start the cluster and search the driver logs for a line that includes Downloaded metastore jars to. Console Copy 17/11/18 22:41:19 INFO IsolatedClientLoader: Downloaded metastore jars to <path>\n <path> is the location of the downloaded jars in the driver node of the cluster. Copy the jars to a DBFS location. Bash Copy cp -r <path> /dbfs/ExternalMetaStore_jar_location\n Create the init script. Python Copy dbutils.fs.put(\"dbfs:/databricks/<init-script-folder>/external-metastore-jars-to-driver.sh\",\n\"\"\"\n#!/bin/bash\nif [[ $DB_IS_DRIVER = \"TRUE\" ]]; then\nmkdir -p /databricks/metastorejars/\ncp -r /dbfs/ExternalMetaStore_jar_location/* /databricks/metastorejars/\nfi\"\"\", True)\n Install the init script that you just created as a cluster-scoped init script. You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/external-metastore-jars-to-driver.sh). Restart the cluster. Option 2: Use the Apache Spark configuration settings to copy the metastore jars to the driver. Enter the following settings into your Spark Config: text Copy spark.hadoop.javax.jdo.option.ConnectionURL jdbc:mysql://<mysql-host>:<mysql-port>/<metastore-db>\nspark.hadoop.javax.jdo.option.ConnectionDriverName <driver>\nspark.hadoop.javax.jdo.option.ConnectionUserName <mysql-username>\nspark.hadoop.javax.jdo.option.ConnectionPassword <mysql-password>\nspark.sql.hive.metastore.version <hive-version>\nspark.sql.hive.metastore.jars /dbfs/metastore/jars/*\n The source path can be external mounted storage or DBFS. The metastore configuration can be applied globally within the workspace by using cluster policies. Option 3: Build a custom Azure Databricks container with preloaded jars. Review the documentation on customizing containers with Databricks Container Services.",
          "title" : "Autoscaling is slow with an external metastore",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/slow-autoscaling-external-metastore"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/from-json-null-spark3",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents from_json returns null in Apache Spark 3.0 Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem The from_json function is used to parse a JSON string and return a struct of values. For example, if you have the JSON string [{\"id\":\"001\",\"name\":\"peter\"}], you can pass it to from_json with a schema and get parsed struct values in return. Python Copy from pyspark.sql.functions import col, from_json\ndisplay(\n  df.select(col('value'), from_json(col('value'), json_df_schema, {\"mode\" : \"PERMISSIVE\"}))\n)\n In this example, the dataframe contains a column “value”, with the contents [{“id”:”001”,”name”:”peter”}] and the schema is StructType(List(StructField(id,StringType,true),StructField(name,StringType,true))). This works correctly on Spark 2.4 and below (Databricks Runtime 6.4 ES and below). Console Copy * id:\n  \"001\"\n* name:\n  \"peter\"\n This returns null values on Spark 3.0 and above (Databricks Runtime 7.3 LTS and above). Console Copy * id:\n  null\n* name:\n  null\n Cause This occurs because Spark 3.0 and above cannot parse JSON arrays as structs. You can confirm this by running from_json in FAILFAST mode. Python Copy from pyspark.sql.functions import col, from_json\ndisplay(\n  df.select(col('value'), from_json(col('value'), json_df_schema, {\"mode\" : \"FAILFAST\"}))\n)\n This returns an error message that defines the root cause. Console Copy Caused by: RuntimeException: Parsing JSON arrays as structs is forbidden\n Solution You must pass the schema as ArrayType instead of StructType in Databricks Runtime 7.3 LTS and above. Python Copy from pyspark.sql.types import StringType, ArrayType, StructType, StructField\nschema_spark_3 = ArrayType(StructType([StructField(\"id\",StringType(),True),StructField(\"name\",StringType(),True)]))\n\nfrom pyspark.sql.functions import col, from_json\ndisplay(\n  df.select(col('value'), from_json(col('value'), schema_spark_3, {\"mode\" : \"PERMISSIVE\"}))\n)\n In this example code, the previous StructType schema is enclosed in ArrayType and the new schema is used with from_json. This parses the JSON string correctly and returns the expected values.",
          "title" : "from_json returns null in Apache Spark 3.0",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/from-json-null-spark3"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/spark-read-fail-corrupted-parquet-page",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Apache Spark read fails with Corrupted parquet page error Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to read data in Parquet or Delta format and you get a Corrupted parquet page error. Console Copy java.lang.RuntimeException: Corrupted parquet page\n    at com.databricks.sql.io.parquet.NativeColumnReader.readBatchNative(Native Method)\n    at com.databricks.sql.io.parquet.NativeColumnReader.readBatch(NativeColumnReader.java:477)\n    at com.databricks.sql.io.parquet.DatabricksVectorizedParquetRecordReader.nextBatch(DatabricksVectorizedParquetRecordReader.java:346)\n    at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:159)\n    at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:40)\n    at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:236)\n    at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n    at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:204)\n    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n    at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)\n    at org.apache.spark.scheduler.Task.run(Task.scala:112)\n    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1526)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n Cause This error can occur when the Parquet fast reader, native reader, and vectorized reader are enabled. Solution If you encounter the Corrupted parquet page error, you should disable the fast reader, native reader, and vectorized reader in your cluster or notebook, and then try the read operation again. If the error persists, even with these options disabled, open a case with Azure Databricks support. Note If you apply these changes at the notebook level, they only apply to the Spark context. If you apply the changes at the cluster level, they apply to all notebooks attached to the cluster. Disable fast reader Set spark.databricks.io.parquet.fastreader.enabled to false in the cluster’s Spark configuration to disable the fast Parquet reader at the cluster level. You can also disable the fast reader at the notebook level by running: Scala Copy spark.conf.set(\"spark.databricks.io.parquet.fastreader.enabled\",\"false\")\n Disable native reader Set spark.databricks.io.parquet.nativeReader.enabled to false in the cluster’s Spark configuration to disable the native Parquet reader at the cluster level. You can also disable the native reader at the notebook level by running: Scala Copy spark.conf.set(\"spark.databricks.io.parquet.nativeReader.enabled\",\"false\")\n Disable vectorized reader Set spark.sql.parquet.enableVectorizedReader to false in the cluster’s Spark configuration to disable the vectorized Parquet reader at the cluster level. You can also disable the vectorized reader at the notebook level by running: Scala Copy spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n Note The vectorized Parquet reader enables native record-level filtering using push-down filters, improving memory locality and cache utilization. If you disable the vectorized Parquet reader, there may be a minor performance impact.",
          "title" : "Apache Spark read fails with Corrupted parquet page error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/spark-read-fail-corrupted-parquet-page"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/security/troubleshoot-key-vault-access",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Troubleshoot key vault access issues Article 03/11/2022 2 minutes to read 2 contributors In this article Confirm permissions are correctly set on the key vault Inspect the firewall configuration on the key vault List all secrets in the secret scope Try to access individual secrets Enable individual secrets Verify that individual secrets are working You are trying to access secrets, when you get an error message. Console Copy com.databricks.common.client.DatabricksServiceException: INVALID_STATE: Databricks could not access keyvault: https://xxxxxxx.vault.azure.net/.\n There is not a single root cause for this error message, so you will have to do some troubleshooting. Confirm permissions are correctly set on the key vault Load the Azure Portal. Open Key vaults. Click the key vault. Click Access policies. Verify the Get and List permissions are applied. Inspect the firewall configuration on the key vault Load the Azure Portal. Open Key vaults. Click the key vault. Click Networking. Click Firewalls and virtual networks. Select Private endpoint and selected networks. Verify that Allow trusted Microsoft services to bypass this firewall? is set to Yes. Attempt to access the secrets. If you can view the secrets, the issue is resolved. If you are still getting the INVALID_STATE: Databricks could not access keyvault error, continue troubleshooting. List all secrets in the secret scope Open a notebook. List all secrets in scope. Python Copy dbutils.secrets.list(\"<scopename>\")\n Try to access individual secrets Try to access a few different, random secrets. Python Copy dbutils.secrets.get(\"<KeyvaultSecretScope>\", \"<SecretName>\")\n If some secrets can be fetched, while others fail, the failed secrets are either disabled or inactive. Enable individual secrets Load the Azure Portal. Open Key vaults. Click the key vault. Click Secrets. Click the secret and verify that the status is set to Enabled. If the secret is disabled, enable it, or create a new version. Verify that individual secrets are working Try to access the previously failed secrets. Python Copy dbutils.secrets.get(\"<KeyvaultSecretScope>\", \"<SecretName>\")\n You can fetch all of them.",
          "title" : "Troubleshoot key vault access issues",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/security/troubleshoot-key-vault-access"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/cannot-grow-bufferholder-exceeds-size",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cannot grow BufferHolder; exceeds size limitation Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow BufferHolder error. Console Copy java.lang.IllegalArgumentException: Cannot grow BufferHolder by size XXXXXXXXX because the size after growing exceeds size limitation 2147483632\n Cause BufferHolder has a maximum size of 2147483632 bytes (approximately 2 GB). If a column value exceeds this size, Spark returns the exception. This can happen when using aggregates like collect_list. This example code generates duplicates in the column values which exceed the maximum size of BufferHolder. As a result, it returns an IllegalArgumentException: Cannot grow BufferHolder error when run in a notebook. SQL Copy import org.apache.spark.sql.functions._\nspark.range(10000000).withColumn(\"id1\",lit(\"jkdhdbjasdshdjkqgdkdkasldksashjckabacbaskcbakshckjasbc$%^^&&&&&*jxcfdkwbfkjwdqndlkjqslkndskbndkjqbdjkbqwjkdbxnsa xckqjwbdxsabvnxbaskxqbhwdhqjskdjxbqsjdhqkjsdbkqsjdkjqdhkjqsabcxns ckqjdkqsbcxnsab ckjqwbdjckqscx ns csjhdjkqsdhjkqshdjsdhqksjdhxqkjshjkshdjkqsdhkjqsdhjqskxb kqscbxkjqsc\")).groupBy(\"id1\").\nagg(collect_list(\"id1\").alias(\"days\")).\nshow()\n Solution You must ensure that column values do not exceed 2147483632 bytes. This may require you to adjust how you process data in your notebook. Looking at our example code, using collect_set instead of collect_list, resolves the issue and allows the example to run to completion. This single change works because the example data set contains a large number of duplicate entries. SQL Copy import org.apache.spark.sql.functions._\nspark.range(10000000).withColumn(\"id1\",lit(\"jkdhdbjasdshdjkqgdkdkasldksashjckabacbaskcbakshckjasbc$%^^&&&&&*jxcfdkwbfkjwdqndlkjqslkndskbndkjqbdjkbqwjkdbxnsa xckqjwbdxsabvnxbaskxqbhwdhqjskdjxbqsjdhqkjsdbkqsjdkjqdhkjqsabcxns ckqjdkqsbcxnsab ckjqwbdjckqscx ns csjhdjkqsdhjkqshdjsdhqksjdhxqkjshjkshdjkqsdhkjqsdhjqskxb kqscbxkjqsc\")).groupBy(\"id1\").\nagg(collect_set(\"id1\").alias(\"days\")).\nshow()\n If using collect_set does not keep the size of the column below the BufferHolder limit of 2147483632 bytes, the IllegalArgumentException: Cannot grow BufferHolder error still occurs. In this case, we would have to split the list into multiple DataFrames and write it out as separate files.",
          "title" : "Cannot grow BufferHolder; exceeds size limitation",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/cannot-grow-bufferholder-exceeds-size"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/hdfs-to-read-files",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Use the HDFS API to read files in Python Article 03/11/2022 2 minutes to read 3 contributors In this article There may be times when you want to read files directly without using third party libraries. This can be useful for reading small files when your regular storage blobs are not available as local DBFS mounts. Use the following example code for Azure Blob storage. Python Copy URI = sc._gateway.jvm.java.net.URI\nPath = sc._gateway.jvm.org.apache.hadoop.fs.Path\nFileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\nconf = sc._jsc.hadoopConfiguration()\n\nconf.set(\n  \"fs.azure.account.key.<account-name>.blob.core.windows.net,\n  \"<account-access-key>\")\n\nfs = Path('wasbs://<container-name>@<account-name>.blob.core.windows.net/<file-path>/').getFileSystem(sc._jsc.hadoopConfiguration())\nistream = fs.open(Path('wasbs://<container-name>@<account-name>.blob.core.windows.net/<file-path>/'))\n\nreader = sc._gateway.jvm.java.io.BufferedReader(sc._jvm.java.io.InputStreamReader(istream))\n\nwhile True:\n  thisLine = reader.readLine()\n  if thisLine is not None:\n    print(thisLine)\n  else:\n    break\n\nistream.close()\n where <account-name> is your Azure account name. <container-name> is the container name. <file-path> is the full path to the file. <account-access-key> is the account access key.",
          "title" : "Use the HDFS API to read files in Python",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/hdfs-to-read-files"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/list-all-workspace-objects",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents List all workspace objects Article 03/11/2022 2 minutes to read 2 contributors In this article Define function Run function You can use the Azure Databricks Workspace API to recursively list all workspace objects under a given path. Common use cases for this include: Indexing all notebook names and types for all users in your workspace. Use the output, in conjunction with other API calls, to delete unused workspaces or to manage notebooks. Dynamically get the absolute path of a notebook under a given user, and submit that to the Azure Databricks Jobs API to trigger notebook-based jobs. Define function This example code defines the function and the logic needed to run it. You should place this code at the beginning of your notebook. You need to replace <token> with your personal access token. Python Copy import requests\nimport json\nfrom ast import literal_eval\n\n# Authorization\nheaders = {\n  'Authorization': 'Bearer <token>',\n}\n\n# Define rec_req as a function.\n# Note: Default path is \"/\" which scans all users and folders.\n\ndef rec_req(instanceName,loc=\"/\"):\n data_path = '{{\"path\": \"{0}\"}}'.format(loc)\n instance = instanceName\n url = '{}/api/2.0/workspace/list'.format(instance)\n response = requests.get(url, headers=headers, data=data_path)\n # Raise exception if a directory or URL does not exist.\n response.raise_for_status()\n jsonResponse = response.json()\n for i,result in jsonResponse.items():\n   for value in result:\n    dump = json.dumps(value)\n    data = literal_eval(dump)\n    if data['object_type'] == 'DIRECTORY':\n     # Iterate through all folders.\n     rec_req(instanceName,data['path'])\n    elif data['object_type'] == 'NOTEBOOK':\n     # Return the notebook path.\n     print(data)\n    else:\n     # Skip imported libraries.\n     pass\n Run function Once you have defined the function in your notebook, you can call it at any time. You need to replace <instance-name> with the instance name of your Azure Databricks deployment. This is typically the URL, without any workspace ID. You need to replace <path> with the full path you want to search. This is typically /. Python Copy rec_req(\"https://<instance-name>\", \"<path>\")\n Note You should NOT include a trailing / as the last character of the instance name. The function generates an error if a trailing / is included.",
          "title" : "List all workspace objects",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/list-all-workspace-objects"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/python-repl-fails-dcs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Python REPL fails to start in Docker Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Version Cause Solution Problem When you use a Docker container that includes prebuilt Python libraries, Python commands fail and the virtual environment is not created. The following error message is visible in the driver logs. Console Copy 20/02/29 16:38:35 WARN PythonDriverWrapper: Failed to start repl ReplId-5b591-0ce42-78ef3-7\njava.io.IOException: Cannot run program \"/local_disk0/pythonVirtualEnvDirs/virtualEnv-56a5be60-3e71-486f-ac04-08e8f2491032/bin/python\" (in directory \".\"): error=2, No such file or directory\n        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n        at org.apache.spark.util.Utils$.executeCommand(Utils.scala:1367)\n        at org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:1393)\n        at org.apache.spark.util.Utils$.executePythonAndGetOutput(Utils.scala:\n…\n        at java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n        at java.lang.UNIXProcess.forkAndExec(Native Method)\n        at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n        at java.lang.ProcessImpl.start(ProcessImpl.java:134)\n        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n        ... 17 more\n You can confirm the issue by running the following command in a notebook: Bash Copy %sh virtualenv --no-site-packages\n The result is an error message similar to the following: Console Copy usage: virtualenv [--version] [--with-traceback] [-v | -q] [--discovery {builtin}] [-p py] [--creator {builtin,cpython3-posix,venv}] [--seeder {app-data,pip}] [--no-seed] [--activators comma_separated_list] [--clear]\n                  [--system-site-packages] [--symlinks | --copies] [--download | --no-download] [--extra-search-dir d [d ...]] [--pip version] [--setuptools version] [--wheel version] [--no-pip] [--no-setuptools] [--no-wheel]\n                  [--clear-app-data] [--symlink-app-data] [--prompt prompt] [-h]\n                  dest\nvirtualenv: error: the following arguments are required: dest\n The virtualenv command does not recognize the --no-site-packages option. Version The problem affects all current Databricks Runtime versions, except for Databricks Runtime versions that include Conda. It affects virtualenv library version 20.0.0 and above. Cause This issue is caused by using a Python virtualenv library version in the Docker container that does not support the --no-site-packages option. Databricks Runtime requires a virtualenv library that supports the --no-site-packages option. This option was removed in virtualenv library version 20.0.0 and above. You can verify your virtualenv library version by running the following command in a notebook: Bash Copy %sh virtualenv --version\n Solution You can resolve the issue by specifying a compatible version when you install the virtualenv library. For example, setting virtualenv==16.0.0 in the Dockerfile installs virtualenv library version 16.0.0. This version of the library supports the required option.",
          "title" : "Python REPL fails to start in Docker",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/python-repl-fails-dcs"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/install-rjava-rjdbc-libraries",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Install rJava and RJDBC libraries Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution This article explains how to install rJava and RJBDC libraries. Problem When you install rJava and RJDBC libraries with the following command in a notebook cell: R Copy install.packages(c(\"rJava\", \"RJDBC\"))\n You observe the following error: Console Copy ERROR: configuration failed for package 'rJava'\n Cause The rJava and RJDBC packages check for Java dependencies and file paths that are not present in the Azure Databricks R directory. Solution Follow the steps below to install these libraries on running clusters. Run following commands in a %sh cell. Bash Copy %sh\nls -l /usr/bin/java\nls -l /etc/alternatives/java\nln -s /usr/lib/jvm/java-8-openjdk-amd64 /usr/lib/jvm/default-java\nR CMD javareconf\n Install the rJava and RJDBC packages. R Copy install.packages(c(\"rJava\", \"RJDBC\"))\n Verify that the rJava package is installed. R Copy dyn.load('/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so')\nlibrary(rJava)",
          "title" : "Install rJava and RJDBC libraries",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/install-rjava-rjdbc-libraries"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/access-adls1-from-sparklyr",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Error when reading data from ADLS Gen1 with Sparklyr Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem When using a cluster with Azure AD Credential Passthrough enabled, commands that you run on that cluster are able to read and write your data in Azure Data Lake Storage Gen1 without requiring you to configure service principal credentials for access to storage. For example, you can directly access data using Python Copy spark.read.csv(\"adl://myadlsfolder.azuredatalakestore.net/MyData.csv\").collect()\n However, when you try to access data directly using Sparklyr: R Copy spark_read_csv(sc, name = \"air\", path = \"adl://myadlsfolder.azuredatalakestore.net/MyData.csv\")\n It fails with the error: Console Copy com.databricks.backend.daemon.data.client.adl.AzureCredentialNotFoundException: Could not find ADLS Gen1 Token\n Cause The spark_read_csv function in Sparklyr is not able to extract the ADLS token to enable authentication and read data. Solution A workaround is to use an Azure application id, application key, and directory id to mount the ADLS location in DBFS: Python Copy # Get credentials and ADLS URI from Azure\napplicationId= <application-id>\napplicationKey= <application-key>\ndirectoryId= <directory-id>\nadlURI=<adl-uri>\nassert adlURI.startswith(\"adl:\"), \"Verify the adlURI variable is set and starts with adl:\"\n\n# Mount ADLS location to DBFS\ndbfsMountPoint=<mount-point-location>\ndbutils.fs.mount(\n  mount_point = dbfsMountPoint,\n  source = adlURI,\n  extra_configs = {\n    \"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\n    \"dfs.adls.oauth2.client.id\": applicationId,\n    \"dfs.adls.oauth2.credential\": applicationKey,\n    \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com/{}/oauth2/token\".format(directoryId)\n  })\n Then, in your R code, read data using the mount point: R Copy # Install Sparklyr\n%r\ninstall.packages(\"sparklyr\")\nlibrary(sparklyr)\n# Create a sparklyr connection\nsc <- spark_connect(method = \"databricks\")\n\n# Read Data\n%r\nmyData = spark_read_csv(sc, name = \"air\", path = \"dbfs:/<mount-point-location>/myData.csv\")",
          "title" : "Error when reading data from ADLS Gen1 with Sparklyr",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/access-adls1-from-sparklyr"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/cosmosdb-connector-lib-conf",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents CosmosDB-Spark connector library conflict Article 03/11/2022 2 minutes to read 3 contributors In this article Affected versions Problem Cause Solution This article explains how to resolve an issue running applications that use the CosmosDB-Spark connector in the Azure Databricks environment. Affected versions Databricks Runtime 4.0 and above (runtimes that include Spark 2.3). Problem Normally if you add a Maven dependency to your Spark cluster, your app should be able to use the required connector libraries. But currently, if you simply specify the CosmosDB-Spark connector’s Maven co-ordinates as a dependency for the cluster, you will get the following exception: Console Copy java.lang.NoClassDefFoundError: Could not initialize class com.microsoft.azure.cosmosdb.Document\n Cause This occurs because Spark 2.3 uses jackson-databind-2.6.7.1, whereas the CosmosDB-Spark connector uses jackson-databind-2.9.5. This creates a library conflict, and at the executor level you observe the following exception: Console Copy java.lang.NoSuchFieldError: ALLOW_TRAILING_COMMA\nat com.microsoft.azure.cosmosdb.internal.Utils.<clinit>(Utils.java:69)\n Solution To avoid this problem: Directly download the CosmosDB-Spark connector Uber JAR: azure-cosmosdb-spark_2.3.0_2.11-1.2.2-uber.jar. Upload the downloaded JAR to Azure Databricks following the instructions in Install the uploaded libraries. Install the uploaded libraries to your Azure Databricks cluster. For more information, see Azure Cosmos DB.",
          "title" : "CosmosDB-Spark connector library conflict",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/cosmosdb-connector-lib-conf"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/kafka-client-term-offsetoutofrange",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Kafka client terminated with OffsetOutOfRangeException Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have an Apache Spark application that is trying to fetch messages from an Apache Kafka source when it is terminated with a kafkashaded.org.apache.kafka.clients.consumer.OffsetOutOfRangeException error message. Cause Your Spark application is trying to fetch expired data offsets from Kafka. We generally see this in these two scenarios: Scenario 1 The Spark application is terminated while processing data. When the Spark application is restarted, it tries to fetch data based on previously calculated data offsets. If any of the data offsets have expired during the time the Spark application was terminated, this issue can occur. Scenario 2 Your retention policy is set to a shorter time than the time require to process the batch. By the time the batch is done processing, some of the Kafka partition offsets have expired. The offsets are calculated for the next batch, and if there is a mismatch in the checkpoint metadata due to the expired offsets, this issue can occur. Solution Scenario 1 - Option 1 Delete the existing checkpoint before restarting the Spark application. A new checkpoint offset is created with the details of the newly fetched offset. The downside to this approach is that some of the data may be missed, because the offsets have expired in Kafka. Scenario 1 - Option 2 Increase the Kafka retention policy of the topic so that it is longer than the time the Spark application is offline. No data is missed with this solution, because no offsets have expired before the Spark application is restarted. There are two types of retention policies: Time based retention - This type of policy defines the amount of time to keep a log segment before it is automatically deleted. The default time based data retention window for all topics is seven days. You can review the Kafka documentation for log.retention.hours, log.retention.minutes, and log.retention.ms for more information. Size based retention - This type of policy defines the amount of data to retain in the log for each topic-partition. This limit is per-partition. This value is unlimited by default. You can review the Kafka documentation for log.retention.bytes for more information. Note If multiple retention policies are set, the more restrictive one controls. This can be overridden on a per topic basis. Review Kafka’s Topic-level configuration for more information on how to set a per topic override. Scenario 2 - Option 1 Increase the retention policy of the partition. This is accomplished in the same way as the solution for Scenario 1 - Option 2. Scenario 2 - Option 2 Increase the number of parallel workers by configuring .option(\"minPartitions\",<X>) for readStream. The option minPartitions defines the minimum number of partitions to read from Kafka. By default, Spark uses a one-to-one mapping of Kafka topic partitions to Spark partitions when consuming data from Kafka. If you set the option minPartitions to a value greater than the number of your Kafka topic partitions, Spark separates the Kafka topic partitions into smaller pieces. This option is recommended at times of data skew, peak loads, and if your stream is falling behind. Setting this value greater than the default results in the initialization of Kafka consumers at each trigger. This can impact performance if you use SSL when connecting to Kafka.",
          "title" : "Kafka client terminated with OffsetOutOfRangeException",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/kafka-client-term-offsetoutofrange"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/delta-write-fails",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Delta Lake write job fails with java.lang.UnsupportedOperationException Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem Delta Lake write jobs sometimes fail with the following exception: Console Copy java.lang.UnsupportedOperationException: com.databricks.backend.daemon.data.client.DBFSV1.putIfAbsent(path: Path, content: InputStream).\nDBFS v1 doesn't support transactional writes from multiple clusters. Please upgrade to DBFS v2.\nOr you can disable multi-cluster writes by setting 'spark.databricks.delta.multiClusterWrites.enabled' to 'false'.\nIf this is disabled, writes to a single table must originate from a single cluster.\n Cause Delta Lake multi-cluster writes are only supported with DBFS v2. Azure Databricks clusters use DBFS v2 by default. All sparkSession objects use DBFS v2. However, if the application uses the FileSystem API and calls FileSystem.close(), the file system client falls back to the default value, which is v1. In this case, Delta Lake multi-cluster write operations fail. The following log trace shows that the file system object fell back to the default v1 version. Console Copy <date> <time> INFO DBFS: Initialized DBFS with DBFSV1 as the delegate.\n Solution There are two approaches to prevent this: Never call FileSystem.close() inside the application code. If it is necessary to call the close() API, then first instantiate a new FileSystem client object with a configuration object from the current Apache Spark session, instead of an empty configuration object: Scala Copy val fileSystem = FileSystem.get(new java.net.URI(path), sparkSession.sessionState.newHadoopConf())\n Alternatively, this code sample achieves the same goal: Scala Copy val fileSystem = FileSystem.get(new java.net.URI(path), sc.hadoopConfiguration())",
          "title" : "Delta Lake write job fails with java.lang.UnsupportedOperationException",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/delta-write-fails"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-fails-no-library",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Azure Databricks job fails because library is not installed Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem An Azure Databricks job fails because the job requires a library that is not yet installed, causing Import errors. Cause The error occurs because the job starts running before required libraries install. If you run a job on a cluster in either of the following situations, the cluster can experience a delay in installing libraries: When you start an existing cluster with libraries in terminated state. When you start a new cluster that uses a shared library (a library installed on all clusters). Solution If a job requires certain libraries, make sure to attach the libraries as dependent libraries within job itself. Refer to the following article and steps on how to set up dependent libraries when you create a job. Add libraries as dependent libraries when you create the job. Open Add Dependent Library dialog: Choose library: Verify library:",
          "title" : "Azure Databricks job fails because library is not installed",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-fails-no-library"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/custom-dns-routing",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Configure custom DNS settings using dnsmasq Article 03/11/2022 2 minutes to read 2 contributors In this article dnsmasq is a tool for installing and configuring DNS routing rules for cluster nodes. You can use it to set up routing between your Azure Databricks environment and your on-premise network. Warning If you use your own DNS server and it goes down, you will experience an outage and will not be able to create clusters. Use the following cluster-scoped init script to configure dnsmasq for a cluster node. Use netcat (nc) to test connectivity from the notebook environment to your on-premise network. Bash Copy nc -vz <on-premise-ip> 53\n Create the base directory you want to store the init script in if it does not already exist. Scala Copy dbutils.fs.mkdirs(\"dbfs:/databricks/<init-script-folder>/\")\n Create the script. Scala Copy dbutils.fs.put(\"/databricks/<init-script-folder>/dns-masq-az.sh\";,\"\"\"\n#!/bin/bash\nsudo apt-get update -y\nsudo apt-get install dnsmasq -y --force-yes\n\n## Add dns entries for internal nameservers\necho server=/databricks.net/<dns-server-ip> | sudo tee --append /etc/dnsmasq.conf\n\n## Find the default DNS settings for the instance and use them as the default DNS route\nazvm_dns=cat /etc/resolv.conf | grep \"nameserver\"; | cut -d' ' -f 2\necho \"Old dns in resolv.conf $azvm_dns\"\necho \"server=$azvm_dns\" | sudo tee --append /etc/dnsmasq.conf\n\n## configure resolv.conf to point to dnsmasq service instead of static resolv.conf file\nmv /etc/resolv.conf /etc/resolv.conf.orig\necho nameserver 127.0.0.1 | sudo tee --append /etc/resolv.conf\nsudo systemctl disable --now systemd-resolved\nsudo systemctl enable --now dnsmasq\n\"\"\", true)\n Check that the script exists. Scala Copy display(dbutils.fs.ls(\"dbfs:/databricks/<init-script-folder>/dns-masq-az.sh\"))\n Install the init script that you just created as a cluster-scoped init script. You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/dns-masq-az.sh). Launch a zero-node cluster to confirm that you can create clusters.",
          "title" : "Configure custom DNS settings using dnsmasq",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/custom-dns-routing"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/autotermination-disabled-error-creating-job",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Auto termination is disabled when starting a job cluster Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to start a job cluster, but the job creation fails with an error message. Console Copy Error creating job\nCluster autotermination is currently disabled.\n Cause Job clusters auto terminate once the job is completed. As a result, they do not support explicit autotermination policies. If you include autotermination_minutes in your cluster policy JSON, you get the error on job creation. JSON Copy {\n \"autotermination_minutes\": {\n  \"type\": \"fixed\",\n   \"value\": 30,\n   \"hidden\": true\n  }\n}\n Solution Do not define autotermination_minutes in the cluster policy for job clusters. Auto termination should only be used for all-purpose clusters.",
          "title" : "Auto termination is disabled when starting a job cluster",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/autotermination-disabled-error-creating-job"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/cluster-spark-config-not-applied",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cluster Apache Spark configuration not applied Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem Your cluster’s Spark configuration values are not applied. Cause This happens when the Spark config values are declared in the cluster configuration as well as in an init script. When Spark config values are located in more than one place, the configuration in the init script takes precedence and the cluster ignores the configuration settings in the UI. Solution You should define your Spark configuration values in one place. Choose to define the Spark configuration in the cluster configuration or include the Spark configuration in an init script. Do not do both.",
          "title" : "Cluster Apache Spark configuration not applied",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/cluster-spark-config-not-applied"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/check-spark-property-modifiable",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to check if a spark property is modifiable in a notebook Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Solution Problem You can tune applications by setting various configurations. Some configurations must be set at the cluster level, whereas some are set inside notebooks or applications. Solution To check if a particular Spark configuration can be set in a notebook, run the following command in a notebook cell: Scala Copy spark.conf.isModifiable(\"spark.databricks.preemption.enabled\")\n If true is returned, then the property can be set in the notebook. Otherwise, it must be set at the cluster level.",
          "title" : "How to check if a spark property is modifiable in a notebook",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/check-spark-property-modifiable"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/streaming-notebook-stuck",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cannot run notebook commands after canceling streaming cell Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Version Cause Solution Problem After you cancel a running streaming cell in a notebook attached to a Databricks Runtime 5.0 cluster, you cannot run any subsequent commands in the notebook. The commands are left in the “waiting to run” state, and you must clear the notebook’s state or detach and reattach the cluster before you can successfully run commands on the notebook. Note that this issue occurs only when you cancel a single cell; it does not apply when you run all and cancel all cells. Version This problem affects Databricks Runtime 5.0 clusters. It also affects Databricks Runtime 4.3 clusters whose Spark Configuration spark.databricks.chauffeur.enableIdleContextTracking has been set to true. Cause Databricks Runtime 4.3 introduced an optional idle execution context feature, which is enabled by default in Databricks Runtime 5.0, that allows the execution context to track streaming execution sequences to determine if they are idle. Unfortunately, this introduced an issue that causes the underlying execution context to be left in an invalid state when you cancel a streaming cell. This prevents additional commands from being run until the notebook state is reset. This behavior is specific to interactive notebooks and does not affect jobs. For more information about idle execution contexts, see Execution contexts. Solution Azure Databricks is working to resolve this issue and release a maintenance update for Databricks Runtime 5.0. In the meantime, you can do either of the following: To remediate an affected notebook without restarting the cluster, go to the notebook’s Clear menu and select Clear State: If restarting the cluster is acceptable, you can solve the issue by turning off idle context tracking. Set the following Spark configuration value on the cluster: Bash Copy spark.databricks.chauffeur.enableIdleContextTracking false\n Then restart the cluster.",
          "title" : "Cannot run notebook commands after canceling streaming cell",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/streaming-notebook-stuck"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/enable-gcm-cipher",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Enable GCM cipher suites Article 03/11/2022 2 minutes to read 2 contributors In this article Verify required cipher suites Create an init script to enable GCM cipher suites Configure cluster with init script Verify that GCM cipher suites are enabled Connect to the external server Azure Databricks clusters do not have GCM (Galois/Counter Mode) cipher suites enabled by default. You must enable GCM cipher suites on your cluster to connect to an external server that requires GCM cipher suites. Verify required cipher suites Use the nmap utility to verify which cipher suites are required by the external server. Bash Copy nmap --script ssl-enum-ciphers -p <port> <hostname>\n Note If nmap is not installed, run sudo apt-get install -y nmap to install it on your cluster. Create an init script to enable GCM cipher suites Use the example code to create an init script that enables GCM cipher suites on your cluster. Python Copy dbutils.fs.put(\"/<path-to-init-script>/enable-gcm.sh\", \"\"\"#!/bin/bash\nsed -i 's/, GCM//g' /databricks/spark/dbconf/java/extra.security\n\"\"\",True)\n Scala Copy dbutils.fs.put(\"/<path-to-init-script>/enable-gcm.sh\", \"\"\"#!/bin/bash\nsed -i 's/, GCM//g' /databricks/spark/dbconf/java/extra.security\n\"\"\",true)\n Remember the path to the init script. You will need it when configuring your cluster. Configure cluster with init script Follow the documentation to configure a cluster-scoped init script. You must specify the path to the init script. After configuring the init script, restart the cluster. Verify that GCM cipher suites are enabled This example code queries the cluster for all supported cipher suites and then prints the output. Scala Copy import java.util.Map;\nimport java.util.TreeMap;\nimport javax.net.ssl.SSLServerSocketFactory\nimport javax.net.ssl._\nSSLContext.getDefault.getDefaultSSLParameters.getProtocols.foreach(println)\nSSLContext.getDefault.getDefaultSSLParameters.getCipherSuites.foreach(println)\n If the GCM cipher suites are enabled, you will see the following AES-GCM ciphers listed in the output. Console Copy TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\nTLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\nTLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\nTLS_RSA_WITH_AES_256_GCM_SHA384\nTLS_ECDH_ECDSA_WITH_AES_256_GCM_SHA384\nTLS_ECDH_RSA_WITH_AES_256_GCM_SHA384\nTLS_DHE_RSA_WITH_AES_256_GCM_SHA384\nTLS_DHE_DSS_WITH_AES_256_GCM_SHA384\nTLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\nTLS_RSA_WITH_AES_128_GCM_SHA256\nTLS_ECDH_ECDSA_WITH_AES_128_GCM_SHA256\nTLS_ECDH_RSA_WITH_AES_128_GCM_SHA256\nTLS_DHE_RSA_WITH_AES_128_GCM_SHA256\nTLS_DHE_DSS_WITH_AES_128_GCM_SHA256\n Connect to the external server Once you have verified that GCM cipher suites are installed on your cluster, make a connection to the external server.",
          "title" : "Enable GCM cipher suites",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/enable-gcm-cipher"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/overwrite-log4j-logs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to overwrite log4j configurations on Azure Databricks clusters Article 03/11/2022 2 minutes to read 4 contributors In this article Important This article describes steps related to customer use of Log4j 1.x within an Azure Databricks cluster. Log4j 1.x is no longer maintained and has three known CVEs (CVE-2021-4104, CVE-2020-9488, and CVE-2019-17571). If your code uses one of the affected classes (JMSAppender or SocketServer), your use may potentially be impacted by these vulnerabilities. You should not enable either of these classes in your cluster. There is no standard way to overwrite log4j configurations on clusters with custom configurations. You must overwrite the configuration files using init scripts. The current configurations are stored in two log4j.properties files: On the driver: Bash Copy %sh\ncat /home/ubuntu/databricks/spark/dbconf/log4j/driver/log4j.properties\n On the worker: Bash Copy %sh\ncat /home/ubuntu/databricks/spark/dbconf/log4j/executor/log4j.properties\n To set class-specific logging on the driver or on workers, use the following script: Bash Copy #!/bin/bash\necho \"Executing on Driver: $DB_IS_DRIVER\"\nif [[ $DB_IS_DRIVER = \"TRUE\" ]]; then\nLOG4J_PATH=\"/home/ubuntu/databricks/spark/dbconf/log4j/driver/log4j.properties\"\nelse\nLOG4J_PATH=\"/home/ubuntu/databricks/spark/dbconf/log4j/executor/log4j.properties\"\nfi\necho \"Adjusting log4j.properties here: ${LOG4J_PATH}\"\necho \"log4j.<custom-prop>=<value>\" >> ${LOG4J_PATH}\n Replace <custom-prop> with the property name, and <value> with the property value. Upload the script to DBFS and select a cluster using the cluster configuration UI. You can also set log4j.properties for the driver in the same way. See Cluster Node Initialization Scripts for more information.",
          "title" : "How to overwrite log4j configurations on Azure Databricks clusters",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/overwrite-log4j-logs"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/replay-cluster-spark-events",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Replay Apache Spark events in a cluster Article 03/11/2022 2 minutes to read 2 contributors In this article Enable cluster log delivery Confirm cluster logs exist Launch a single node cluster Run the Event Log Replay notebook Prevent items getting dropped from the UI The Spark UI is commonly used as a debugging tool for Spark jobs. If the Spark UI is inaccessible, you can load the event logs in another cluster and use the Event Log Replay notebook to replay the Spark events. Important Cluster log delivery is not enabled by default. You must enable cluster log delivery before starting your cluster, otherwise there will be no logs to replay. Enable cluster log delivery Follow the documentation to configure Cluster log delivery on your cluster. The location of the cluster logs depends on the Cluster Log Path that you set during cluster configuration. For example, if the log path is dbfs:/cluster-logs, the log files for a specific cluster will be stored in dbfs:/cluster-logs/<cluster-name> and the individual event logs will be stored in dbfs:/cluster-logs/<cluster-name>/eventlog/<cluster-name-cluster-ip>/<log-id>/. Confirm cluster logs exist Review the cluster log path and verify that logs are being written for your chosen cluster. Log files are written every five minutes. Launch a single node cluster Launch a single node cluster. You will replay the logs on this cluster. Select the instance type based on the size of the event logs that you want to replay. Run the Event Log Replay notebook Attach the Event Log Replay notebook to the single node cluster. Enter the path to your chosen cluster event logs in the event_log_path field in the notebook. Run the notebook. Event Log Replay notebook Get notebook Prevent items getting dropped from the UI If you have a long-running cluster, it is possible for some jobs and/or stages to get dropped from the Spark UI. This happens due to default UI limits that are intended to prevent the UI from using up too much memory and causing an out-of-memory error on the cluster. If you are using a single node cluster to replay the event logs, you can increase the default UI limits and devote more memory to the Spark UI. This prevents items from getting dropped. You can adjust these values during cluster creation by editing the Spark Config. This example contains the default values for these properties. text Copy spark.ui.retainedJobs 1000\nspark.ui.retainedStages 1000\nspark.ui.retainedTasks 100000\nspark.sql.ui.retainedExecutions 1000",
          "title" : "Replay Apache Spark events in a cluster",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/replay-cluster-spark-events"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/library-fail-transient-maven",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Libraries failing due to transient Maven issue Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem Job fails because libraries cannot be installed. Console Copy Library resolution failed. Cause: java.lang.RuntimeException: Cannot download some libraries due to transient Maven issue. Please try again later\n Cause After an Azure Databricks upgrade, your cluster attempts to download any required libraries from Maven. After downloading, the libraries are stored as Workspace libraries. The next time you run a cluster that requires the libraries, they are loaded as Workspace libraries. This behavior is due to legacy code. Solution There is no workaround for this issue. This only happens after a maintenance window, and only if Maven is unavailable at the time. It is a rare corner case.",
          "title" : "Libraries failing due to transient Maven issue",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/library-fail-transient-maven"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/tensorflow-fails-to-import",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents TensorFlow fails to import Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have TensorFlow installed on your cluster. When you try to import TensorFlow, it fails with an Invalid Syntax or import error. Cause The version of protobuf installed on your cluster is not compatible with your version of TensorFlow. Solution Use a cluster-scoped init script to install TensorFlow with matching versions of NumPy and protobuf. Create the init script. Python Copy dbutils.fs.put(\"/databricks/<init-script-folder>/install-tensorflow.sh\",\"\"\"\n#!/bin/bash\nset -e\n/databricks/python/bin/python -V\n/databricks/python/bin/pip install tensorflow protobuf==3.17.3 numpy==1.15.0\n\"\"\", True)\n Install the init script that you just created as a cluster-scoped init script. You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/install-tensorflow.sh). Restart the cluster after you have installed the init script. Note Uninstall all existing versions of NumPy before installing the init script on your cluster.",
          "title" : "TensorFlow fails to import",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/tensorflow-fails-to-import"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/h2o-cluster-not-reachable-exception",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents H2O.ai Sparkling Water cluster not reachable Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to initialize H2O.ai’s Sparkling Water on Databricks Runtime 7.0 and above when you get a H2OClusterNotReachableException error message. Python Copy import ai.h2o.sparkling._\nval h2oContext = H2OContext.getOrCreate()\n Console Copy ai.h2o.sparkling.backend.exceptions.H2OClusterNotReachableException: H2O cluster X.X.X.X:54321 - sparkling-water-root_app-20210720231748-0000 is not reachable.\n Cause This error occurs when you are trying to use a version of the Sparkling Water package which is not compatible with the version of Apache Spark used on your Azure Databricks cluster. Solution Make sure you are downloading the correct version of Sparkling Water from the Sparkling Water download page. By default, the download page provides the latest version of Sparkling Water. If you are still having trouble, you may want to try rolling back to a prior version of Sparkling Water that is compatible with your Spark version. If you are still having trouble configuring Sparkling Water, open a case with H20.ai support.",
          "title" : "H2O.ai Sparkling Water cluster not reachable",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/h2o-cluster-not-reachable-exception"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/hyperopt-fail-maxnumconcurrenttasks",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Hyperopt fails with maxNumConcurrentTasks error Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are tuning machine learning parameters using Hyperopt when your job fails with a py4j.Py4JException: Method maxNumConcurrentTasks([]) does not exist error. You are using a Databricks Runtime for Machine Learning (Databricks Runtime ML) cluster. Cause Databricks Runtime ML has a compatible version of Hyperopt pre-installed. If you manually install a second version of Hyperopt, it causes a conflict. Solution Do not install Hyperopt on Databricks Runtime ML clusters.",
          "title" : "Hyperopt fails with maxNumConcurrentTasks error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/hyperopt-fail-maxnumconcurrenttasks"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/mlflow-artifacts-outdated-client",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents OSError when accessing MLflow experiment artifacts Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You get an OSError: No such file or directory error message when trying to download or log artifacts using one of the following: MlflowClient.download_artifacts() mlflow.[flavor].log_model() mlflow.[flavor].load_model() mlflow.log_artifacts() Console Copy OSError: No such file or directory: '/dbfs/databricks/mlflow-tracking/<experiment-id>/<run-id>/artifacts/...'\n Cause Your MLflow client is out of date. Older versions of MLflow do not provide support for artifacts stored in dbfs:/databricks/mlflow-tracking/. Solution Upgrade to MLflow version 1.9.1 or higher and try again. Bash Copy %sh\npip install --upgrade mlflow",
          "title" : "OSError when accessing MLflow experiment artifacts",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/mlflow-artifacts-outdated-client"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/onehotencoderestimator-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Error when importing OneHotEncoderEstimator Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have migrated a notebook from Databricks Runtime 6.4 for Machine Learning or below to Databricks Runtime 7.3 for Machine Learning or above. You are attempting to import OneHotEncoderEstimator and you get an import error. Console Copy ImportError: cannot import name 'OneHotEncoderEstimator' from 'pyspark.ml.feature' (/databricks/spark/python/pyspark/ml/feature.py)\n Cause OneHotEncoderEstimator was renamed to OneHotEncoder in Apache Spark 3.0. Solution You must replace OneHotEncoderEstimator references in your notebook with OneHotEncoder. For example, the following sample code returns an import error in Databricks Runtime 7.3 for Machine Learning or above: Python Copy from pyspark.ml.feature import OneHotEncoderEstimator\n The following sample code functions correctly in Databricks Runtime 7.3 for Machine Learning or above: Python Copy from pyspark.ml.feature import OneHotEncoder",
          "title" : "Error when importing OneHotEncoderEstimator",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/onehotencoderestimator-error"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/runs-not-nested-sparktrials-hyperopt",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Runs are not nested when SparkTrials is enabled in Hyperopt Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem SparkTrials is an extension of Hyperopt, which allows runs to be distributed to Spark workers. When you start an MLflow run with nested=True in the worker function, the results are supposed to be nested under the parent run. Sometimes the results are not correctly nested under the parent run, even though you ran SparkTrials with nested=True in the worker function. For example: Python Copy from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n\ndef train(params):\n  \"\"\"\n  An example train method that computes the square of the input.\n  This method will be passed to `hyperopt.fmin()`.\n\n  :param params: hyperparameters. Its structure is consistent with how search space is defined. See below.\n  :return: dict with fields 'loss' (scalar loss) and 'status' (success/failure status of run)\n  \"\"\"\n  with mlflow.start_run(run_name='inner_run', nested=True) as run:\n\n    x, = params\n  return {'loss': x ** 2, 'status': STATUS_OK}\n\nwith mlflow.start_run(run_name='outer_run_with_sparktrials'):\n  spark_trials_run_id = mlflow.active_run().info.run_id\n  argmin = fmin(\n    fn=train,\n    space=search_space,\n    algo=algo,\n    max_evals=16,\n    trials=spark_trials\n  )\n Expected results: Actual results: Cause The open source version of Hyperopt does not support the required features necessary to properly nest SparkTrials MLflow runs on Azure Databricks. Solution Databricks Runtime for Machine Learning includes an internal fork of Hyperopt with additional features. If you want to use SparkTrials, you should use Databricks Runtime for Machine Learning instead of installing Hyperopt manually from open-source repositories.",
          "title" : "Runs are not nested when SparkTrials is enabled in Hyperopt",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/runs-not-nested-sparktrials-hyperopt"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/cannot-import-egg-module",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cannot import module in egg library Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You try to install an egg library to your cluster and it fails with a message that the a module in the library cannot be imported. Even a simple import fails. Copy import sys\negg_path='/dbfs/<path-to-egg-file>/<egg-file>.egg'\nsys.path.append(egg_path)\nimport shap_master\n Cause This error message occurs due to the way the library is packed. Solution If the standard library import options do not work, you should use easy_install to import the library. Python Copy dbutils.fs.put(\"/<path>/<library-name>.sh\",\"\"\"\n#!/bin/bash\neasy_install-3.7 /dbfs/<path-to-egg-file>/<egg-file>.egg\"\"\")\n\"\"\")\n Important The version of easy_install must match the version of Python on the cluster. You can determine the version of Python on your cluster by reviewing the release notes.",
          "title" : "Cannot import module in egg library",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/cannot-import-egg-module"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/install-pyodbc-on-cluster",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Error when installing pyodbc on a cluster Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem One of the following errors occurs when you use pip to install the pyodbc library. Console Copy java.lang.RuntimeException: Installation failed with message: Collecting pyodbc\n Console Copy \"Library installation is failing due to missing dependencies. sasl and thrift_sasl are optional dependencies for SASL or Kerberos support\"\n Cause Although sasl and thrift_sasl are optional dependencies for SASL or Kerberos support, they need to be present for pyodbc installation to succeed. Solution Set up solution in a single notebook In the notebook, check the version of thrift and upgrade to the latest version. Bash Copy %sh\npip list | egrep 'thrift-sasl|sasl'\npip install --upgrade thrift\n Ensure that dependent packages are installed. Bash Copy %sh dpkg -l | egrep 'thrift_sasl|libsasl2-dev|gcc|python-dev'\n Install nnixodbc before installing pyodbc. Bash Copy %sh sudo apt-get -y install unixodbc-dev libsasl2-dev gcc python-dev\n Set up solution as a cluster-scoped init script You can put these commands into a single init script and attach it to the cluster. This ensures that the dependent libraries for pyodbc are installed before the cluster starts. Create the base directory to store the init script in, if the base directory does not exist. Here, use dbfs:/databricks/<directory> as an example. Bash Copy dbutils.fs.mkdirs(\"dbfs:/databricks/<directory>/\")\n Create the script and save it to a file. Bash Copy dbutils.fs.put(\"dbfs:/databricks/<directory>/tornado.sh\",\"\"\"\n#!/bin/bash\npip list | egrep 'thrift-sasl|sasl'\npip install --upgrade thrift\ndpkg -l | egrep 'thrift_sasl|libsasl2-dev|gcc|python-dev'\nsudo apt-get -y install unixodbc-dev libsasl2-dev gcc python-dev\n\"\"\",True)\n Check that the script exists. Python Copy display(dbutils.fs.ls(\"dbfs:/databricks/<directory>/tornado.sh\"))\n On the cluster configuration page, click the Advanced Options toggle. At the bottom of the page, click the Init Scripts tab. In the Destination drop-down, select DBFS, provide the file path to the script, and click Add. Restart the cluster For more details about cluster-scoped init scripts, see Cluster-scoped init scripts.",
          "title" : "Error when installing pyodbc on a cluster",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/install-pyodbc-on-cluster"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/dbcli-win-fail-create-process",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Failed to create process error with Databricks CLI in Windows Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem While trying to access the Databricks CLI in Windows, you get a failed to create process error message. Cause This can happen: If multiple instances of the Databricks CLI are installed on the system. If the Python path on your Windows system includes a space. Note There is a known issue in pip which causes pip installed software to fail if there is a space in your Python path. Solution Ensure that you do not have multiple instances of the Databricks CLI installed by running where databricks. If you do have multiple instances installed, delete all instances except the one in the user profile path. Ensure that Python is installed to a path without spaces, or ensure that you have enclosed the path in quotes when it is referenced on the first line of any script in the \\Scripts directory. If the first line of your script looks like this, it will fail: text Copy #!c:\\program files\\python\\python38\\python.exe\n If the first line of your script looks like this, it will work correctly: text Copy #!\"c:\\program files\\python\\python38\\python.exe\"",
          "title" : "Failed to create process error with Databricks CLI in Windows",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/dbcli-win-fail-create-process"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-fails-invalid-access-token",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Job fails with invalid access token Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem Long running jobs, such as streaming jobs, fail after 48 hours when using dbutils.secrets.get(). For example: Python Copy streamingInputDF1 = (\n     spark\n    .readStream\n    .format(\"delta\")\n    .table(\"default.delta_sorce\")\n  )\n\ndef writeIntodelta(batchDF, batchId):\n  table_name = dbutils.secrets.get(\"secret1\",\"table_name\")\n  batchDF = batchDF.drop_duplicates()\n  batchDF.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n\nstreamingInputDF1 \\\n  .writeStream \\\n  .format(\"delta\") \\\n  .option(\"checkpointLocation\", \"dbfs:/tmp/delta_to_delta\") \\\n  .foreachBatch(writeIntodelta) \\\n  .outputMode(\"append\") \\\n  .start()\n This example code returns an error after 48 hours. HTML Copy <head>\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\"/>\n<title>Error 403 Invalid access token.</title>\n</head>\n<body><h2>HTTP ERROR 403</h2>\n<p>Problem accessing /api/2.0/secrets/get. Reason:\n<pre>    Invalid access token.</pre></p>\n</body>\n Cause Databricks Utilities (dbutils) tokens expire after 48 hours. This is by design. Solution You cannot extend the life of a token. Jobs that take more than 48 hours to complete should not use dbutils.secrets.get().",
          "title" : "Job fails with invalid access token",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-fails-invalid-access-token"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/convert-flat-df-to-nested-json",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Convert flattened DataFrame to nested JSON Article 03/11/2022 2 minutes to read 3 contributors In this article Define nested schema Convert flattened DataFrame to a nested structure Write out nested DataFrame as a JSON file Example notebook This article explains how to convert a flattened DataFrame to a nested structure, by nesting a case class within another case class. You can use this technique to build a JSON file, that can then be sent to an external API. Define nested schema We’ll start with a flattened DataFrame. Using this example DataFrame, we define a custom nested schema using case classes. Scala Copy case class empId(id:String)\ncase class depId(dep_id:String)\ncase class details(id:empId,name:String,position:String,depId:depId)\ncase class code(manager_id:String)\ncase class reporting(reporting:Array[code])\ncase class hireDate(hire_date:String)\ncase class emp_record(emp_details:details,incrementDate:String,commission:String,country:String,hireDate:hireDate,reports_to:reporting)\n You can see that the case classes nest different data types within one another. Convert flattened DataFrame to a nested structure Use DF.map to pass every row object to the corresponding case class. Scala Copy import spark.implicits._\nval nestedDF= DF.map(r=>{\nval empID_1= empId(r.getString(0))\nval depId_1 = depId(r.getString(7))\nval details_1=details(empID_1,r.getString(1),r.getString(2),depId_1)\nval code_1=code(r.getString(3))\nval reporting_1 = reporting(Array(code_1))\nval hireDate_1 = hireDate(r.getString(4))\nemp_record(details_1,r.getString(8),r.getString(6),r.getString(9),hireDate_1,reporting_1)\n\n}\n)\n This creates a nested DataFrame. Write out nested DataFrame as a JSON file Use the repartition().write.option function to write the nested DataFrame to a JSON file. Scala Copy nestedDF.repartition(1).write.option(\"multiLine\",\"true\").json(\"dbfs:/tmp/test/json1/\")\n Example notebook Run the example notebook to see each of these steps performed. DataFrame to nested JSON example Get notebook",
          "title" : "Convert flattened DataFrame to nested JSON",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/convert-flat-df-to-nested-json"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/intermittent-nullpointerexception-aqe-enabled",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Intermittent NullPointerException when AQE is enabled Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You get an intermittent NullPointerException error when saving your data. Console Copy Py4JJavaError: An error occurred while calling o2892.save.\n: java.lang.NullPointerException\n    at org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin.$anonfun$getMapSizesForReduceId$1(OptimizeSkewedJoin.scala:167)\n    at org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin.$anonfun$getMapSizesForReduceId$1$adapted(OptimizeSkewedJoin.scala:167)\n    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n    at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n    ....\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n    at py4j.Gateway.invoke(Gateway.java:295)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.GatewayConnection.run(GatewayConnection.java:251)\n    at java.lang.Thread.run(Thread.java:748)\n Cause This error can occur if adaptive query execution (AQE) is enabled and you are joining data. If AQE is enabled, skew join is also enabled. If any of the shuffle data fails due to a cluster scaling down event it generates a NullPointerException error. Solution Set spark.sql.adaptive.skewJoin.enabled to false in your Spark configuration.",
          "title" : "Intermittent NullPointerException when AQE is enabled",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/intermittent-nullpointerexception-aqe-enabled"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/read-fail-jdbc-dbr6x",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Trouble reading external JDBC tables after upgrading from Databricks Runtime 5.5 Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem Attempting to read external tables via JDBC works fine on Databricks Runtime 5.5, but the same table reads fail on Databricks Runtime 6.0 and above. You see an error similar to the following: Console Copy com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: java.util.concurrent.ExecutionException: org.apache.spark.sql.AnalysisException: org.apache.spark.sql.jdbc does not allow user-specified schemas.\nat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)\nat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)\nat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\nat java.lang.Thread.run(Thread.java:748)\n.\nCaused by: org.apache.spark.sql.AnalysisException: org.apache.spark.sql.jdbc does not allow user-specified schemas.;\n\nat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n Cause Databricks Runtime 5.5 and below infers the session_id attribute as a smallint. Databricks Runtime 6.0 and above infers the session_id attribute as an int. This change to the session_id attribute causes queries to fail with a schema issue. Solution If you are using external tables that were created in Databricks Runtime 5.5 and below in Databricks Runtime 6.0 and above, you must set the Apache Spark configuration spark.sql.legacy.mssqlserver.numericMapping.enabled to true. This ensures that Databricks Runtime 6.0 and above infers the session_id attribute as a smallint. Open the Clusters page. Select a cluster. Click Edit. Click Advanced Options. Click Spark. In the Spark Config field, enter spark.sql.legacy.mssqlserver.numericMapping.enabled true. Save the change and start, or restart, the cluster.",
          "title" : "Trouble reading external JDBC tables after upgrading from Databricks Runtime 5.5",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/read-fail-jdbc-dbr6x"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/running-c-plus-plus-code-scala",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Running C++ code in Scala Article 03/11/2022 2 minutes to read 3 contributors In this article Run C++ from Scala notebook Run C++ from Scala notebook Get notebook",
          "title" : "Running C++ code in Scala",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/running-c-plus-plus-code-scala"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/security/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Security and permissions: tips and troubleshooting Article 03/11/2022 2 minutes to read 3 contributors In this article These articles can help you with access control lists (ACLs), secrets, and other security- and permissions-related functionality. Table creation fails with security exception Troubleshoot key vault access issues",
          "title" : "Security and permissions: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/security/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/security/table-create-security-exception",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Table creation fails with security exception Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You attempt to create a table using a cluster that has Table ACLs enabled, but the following error occurs: Console Copy Error in SQL statement: SecurityException: User does not have permission SELECT on any file.\n Cause This error occurs on a Table ACL-enabled cluster if you are not an administrator and you do not have sufficient privileges to create a table. For example, in your notebook you attempt to create a table using a Parquet data source located on Azure Blob Storage: SQL Copy CREATE TABLE mytable\n  USING PARQUET\n  OPTIONS (PATH='wasbs://my-container@my-storage-account.blob.core.windows.net/my-table')\n Solution You should ask your administrator to grant you access to the blob storage filesystem, using either of the following options. If an administrator cannot grant you access to the data object, you’ll have to ask an administrator to make the table for you. If you want to use a CTAS (CREATE TABLE AS SELECT) statement to create the table, the administrator should grant you SELECT privileges on the filesystem: SQL Copy GRANT SELECT ON ANY FILE TO `user1`\n Example CTAS statement: SQL Copy CREATE TABLE mytable\n      AS SELECT * FROM parquet.`wasbs://my-container@my-storage-account.blob.core.windows.net/my-table`\n If you want to use a CTOP (CREATE TABLE OPTIONS PATH) statement to make the table, the administrator must elevate your privileges by granting MODIFY in addition to SELECT. SQL Copy GRANT SELECT, MODIFY ON ANY FILE TO `user1`\n Example CTOP statement: SQL Copy CREATE TABLE mytable\n   USING PARQUET\n   OPTIONS (PATH='wasbs://my-container@my-storage-account.blob.core.windows.net/my-table')\n Important It is important to understand the security implications of granting ANY FILE permissions on a filesystem. You should only grant ANY FILE to privileged users. Users with lower privileges on the cluster should never access data by referencing an actual storage location. Instead, they should access data from tables that are created by privileged users, thus ensuring that Table ACLS are enforced. In addition, if files in the Azure Databricks root and data buckets are accessible by the cluster and users have MODIFY privileges, the admin should lock down the DBFS root. Granting the data access privileges described above does not supersede any underlying user permissions or Blob Storage container access control. For example, if a grant statement like GRANT SELECT, MODIFY ON ANY FILE TO user1``` is executed but a user permission attached to the cluster explicitly denies reads to the target container, then the ``GRANT` statement will not make the container or the objects within the container suddenly readable.",
          "title" : "Table creation fails with security exception",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/security/table-create-security-exception"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/jdbc-write-fails-primarykeyviolation",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents JDBC write fails with a PrimaryKeyViolation error Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You are using JDBC to write to a SQL table that has primary key constraints, and the job fails with a PrimaryKeyViolation error. Alternatively, you are using JDBC to write to a SQL table that does not have primary key constraints, and you see duplicate entries in recently written tables. Cause When Apache Spark performs a JDBC write, one partition of the DataFrame is written to a SQL table. This is generally done as a single JDBC transaction, in order to avoid repeatedly inserting data. However, if the transaction fails after the commit occurs, but before the final stage completes, it is possible for duplicate data to be copied into the SQL table. The PrimaryKeyViolation error occurs when a write operation is attempting to insert a duplicate entry for the primary key. Solution You should use a temporary table to buffer the write, and ensure there is no duplicate data. Verify that speculative execution is disabled in your Spark configuration: spark.speculation false. This is disabled by default. Create a temporary table on your SQL database. Modify your Spark code to write to the temporary table. After the Spark writes have completed, check the temporary table to ensure there is no duplicate data. Merge the temporary table with the target table on your SQL database. Delete the temporary table. Note This workaround should only be used if you encounter the listed data duplication issue, as there is a small performance penalty when compared to Spark jobs that write directly to the target table.",
          "title" : "JDBC write fails with a PrimaryKeyViolation error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/jdbc-write-fails-primarykeyviolation"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/query-not-skip-header-ext-table",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Query does not skip header row on external table Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are attempting to query an external Hive table, but it keeps failing to skip the header row, even though TBLPROPERTIES ('skip.header.line.count'='1') is set in the HiveContext. You can reproduce the issue by creating a table with this sample code. SQL Copy CREATE EXTERNAL TABLE school_test_score (\n  `school` varchar(254),\n  `student_id` varchar(254),\n  `gender` varchar(254),\n  `pretest` varchar(254),\n  `posttest` varchar(254))\nROW FORMAT DELIMITED\n  FIELDS TERMINATED BY ','\n  LINES TERMINATED BY '\\n'\nSTORED AS INPUTFORMAT\n  'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n  'dbfs:/FileStore/table_header/'\nTBLPROPERTIES (\n   'skip.header.line.count'='1'\n)\n If you try to select the first five rows from the table, the first row is the header row. SQL Copy SELECT * FROM school_test_score LIMIT 5\n Cause If you query directly from Hive, the header row is correctly skipped. Apache Spark does not recognize the skip.header.line.count property in HiveContext, so it does not skip the header row. Spark is behaving as designed. Solution You need to use Spark options to create the table with a header option. SQL Copy CREATE TABLE student_test_score (school String, student_id String, gender String, pretest String, posttest String) USING CSV\nOPTIONS (path \"dbfs:/FileStore/table_header/\",\n        delimiter \",\",\n        header \"true\")\n        ;\n Select the first five rows from the table and the header row is not included. SQL Copy SELECT * FROM school_test_score LIMIT 5",
          "title" : "Query does not skip header row on external table",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/query-not-skip-header-ext-table"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/append-output-not-supported-no-watermark",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Append output is not supported without a watermark Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are performing an aggregation using append mode and an exception error message is returned. Console Copy Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark\n Cause You cannot use append mode on an aggregated DataFrame without a watermark. This is by design. Solution You must apply a watermark to the DataFrame if you want to use append mode on an aggregated DataFrame. The aggregation must have an event-time column, or a window on the event-time column. Group the data by window and word and compute the count of each group. .withWatermark() must be called on the same column as the timestamp column used in the aggregation. The example code shows how this can be done. Replace the value <type> with the type of element you are processing. For example, you would use Row if you are processing by row. Replace the value <words> with the streaming DataFrame of schema { timestamp: Timestamp, word: String }. Java Copy Dataset<type> windowedCounts = <words>\n    .withWatermark(\"timestamp\", \"10 minutes\")\n    .groupBy(\n        functions.window(words.col(\"timestamp\"), \"10 minutes\", \"5 minutes\"),\n        words.col(\"word\"))\n    .count();\n Python Copy windowedCounts = <words> \\\n    .withWatermark(\"timestamp\", \"10 minutes\") \\\n    .groupBy(\n        window(words.timestamp, \"10 minutes\", \"5 minutes\"),\n        words.word) \\\n    .count()\n Scala Copy import spark.implicits._\n\nval windowedCounts = <words>\n    .withWatermark(\"timestamp\", \"10 minutes\")\n    .groupBy(\n        window($\"timestamp\", \"10 minutes\", \"5 minutes\"),\n        $\"word\")\n    .count()\n You must call .withWatermark() before you perform the aggregation. Attempting otherwise fails with an error message. For example, df.groupBy(\"time\").count().withWatermark(\"time\", \"1 min\") returns an exception. Please refer to the Apache Spark documentation on conditions for watermarking to clean the aggregation slate for more information.",
          "title" : "Append output is not supported without a watermark",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/append-output-not-supported-no-watermark"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/checkpoint-no-cleanup-display",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Checkpoint files not being deleted when using display() Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You have a streaming job using display() to display DataFrames. Scala Copy val streamingDF = spark.readStream.schema(schema).parquet(<input_path>)\ndisplay(streamingDF)\n Checkpoint files are being created, but are not being deleted. You can verify the problem by navigating to the root directory and looking in the /local_disk0/tmp/ folder. Checkpoint files remain in the folder. Cause The command display(streamingDF) is a memory sink implementation that can display the data from the streaming DataFrame for every micro-batch. A checkpoint directory is required to track the streaming updates. If you have not specified a custom checkpoint location, a default checkpoint directory is created at /local_disk0/tmp/. Azure Databricks uses the checkpoint directory to ensure correct and consistent progress information. When a stream is shut down, either purposely or accidentally, the checkpoint directory allows Azure Databricks to restart and pick up exactly where it left off. If a stream is shut down by cancelling the stream from the notebook, the Azure Databricks job attempts to clean up the checkpoint directory on a best-effort basis. If the stream is terminated in any other way, or if the job is terminated, the checkpoint directory is not cleaned up. This is as designed. Solution You can prevent unwanted checkpoint files with the following guidelines. You should not use display(streamingDF) in production jobs. If display(streamingDF) is mandatory for your use case, you should manually specify the checkpoint directory by using the Apache Spark config option spark.sql.streaming.checkpointLocation. If you manually specify the checkpoint directory, you should periodically delete any remaining files in this directory. This can be done on a weekly basis.",
          "title" : "Checkpoint files not being deleted when using display()",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/checkpoint-no-cleanup-display"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/get-file-path-auto-loader",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Get the path of files consumed by Auto Loader Article 03/11/2022 2 minutes to read 2 contributors In this article When you process streaming files with Auto Loader, events are logged based on the files created in the underlying storage. This article shows you how to add the file path for every filename to a new column in the output DataFrame. One use case for this is auditing. When files are ingested to a partitioned folder structure there is often useful metadata, such as the timestamp, which can be extracted from the path for auditing purposes. For example, assume a file path and filename of 2020/2021-01-01/file1_T191634.csv. From this path you can apply custom UDFs and use regular expressions to extract details like the date (2021-01-01) and the timestamp (T191634). The following example code uses input_file_name() get the path and filename for every row and write it to a new column named filePath. Scala Copy val df = spark.readStream.format(\"cloudFiles\")\n  .schema(schema)\n  .option(\"cloudFiles.format\", \"csv\")\n  .option(\"cloudFiles.region\",\"ap-south-1\")\n  .load(\"path\")\n  .withColumn(\"filePath\",input_file_name())",
          "title" : "Get the path of files consumed by Auto Loader",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/get-file-path-auto-loader"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/function-object-no-attribute",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents AttributeError: ‘function’ object has no attribute Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are selecting columns from a DataFrame and you get an error message. Console Copy ERROR: AttributeError: 'function' object has no attribute '_get_object_id' in job\n Cause The DataFrame API contains a small number of protected keywords. If a column in your DataFrame uses a protected keyword as the column name, you will get an error message. For example, summary is a protected keyword. If you use summary as a column name, you will see the error message. This sample code uses summary as a column name and generates the error message when run. Python Copy df=spark.createDataFrame([1,2], \"int\").toDF(\"id\")\ndf.show()\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\n\ndf1 = spark.createDataFrame(\n  [(10,), (11,), (13,)],\n  StructType([StructField(\"summary\", IntegerType(), True)]))\n\ndf1.show()\n\nResultDf = df1.join(df, df1.summary == df.id, \"inner\").select(df.id,df1.summary)\nResultDf.show()\n Solution You should not use DataFrame API protected keywords as column names. If you must use protected keywords, you should use bracket based column access when selecting columns from a DataFrame. Do not use dot notation when selecting columns that use protected keywords. Python Copy ResultDf = df1.join(df, df1[\"summary\"] == df.id, \"inner\").select(df.id,df1[\"summary\"])",
          "title" : "AttributeError: ‘function’ object has no attribute",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/function-object-no-attribute"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/special-characters-in-xml",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Load special characters with Spark-XML Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have special characters in your source files and are using the OSS library Spark-XML. The special characters do not render correctly. For example, “CLU®” is rendered as “CLU�”. Cause Spark-XML supports the UTF-8 character set by default. You are using a different character set in your XML files. Solution You must specify the character set you are using in your XML files when reading the data. Use the charset option to define the character set when reading an XML file with Spark-XML. For example, if your source file is using ISO-8859-1: Python Copy dfResult = spark.read.format('xml').schema(customSchema) \\\n.options(rowTag='Entity') \\\n.options(charset='ISO-8859-1')\\\n.load('/<path-to-xml>/<sample-file>.xml')\n Review the Spark-XML README file for more information on supported options.",
          "title" : "Load special characters with Spark-XML",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/special-characters-in-xml"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/sql-in-python",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to run SQL queries from Python scripts Article 03/11/2022 2 minutes to read 4 contributors In this article Install PyHive and Thrift Run SQL script You may want to access your tables outside of Azure Databricks notebooks. Besides connecting BI tools via JDBC, you can also access tables by using Python scripts. You can connect to a Spark cluster via JDBC using PyHive and then run a script. You should have PyHive installed on the machine where you are running the Python script. Note Python 2 is considered end-of-life. You should use Python 3 to run the script provided in this article. If you have both Python 2 and Python 3 running on your system, you should make sure your version of pip is linked to Python 3 before you proceed. You can check your version of pip by running pip -V at the command prompt. This command returns the version of pip and the version of Python it is using. Install PyHive and Thrift Use pip to install PyHive and Thrift. Bash Copy    pip install pyhive thrift\n Run SQL script This sample Python script sends the SQL query show tables to your cluster and then displays the result of the query. Do the following before you run the script: Replace <token> with your Azure Databricks API token. Replace <databricks-instance> with the domain name of your Databricks deployment. Replace <workspace-id> with the Workspace ID. Replace <cluster-id> with a cluster ID. To get the API token, see Generate a token. To determine the other values, see How to get Workspace, Cluster, Notebook, and Job Details. Python Copy #!/usr/bin/python\n\nimport os\nimport sys\nfrom pyhive import hive\nfrom thrift.transport import THttpClient\nimport base64\n\nTOKEN = \"<token>\"\nWORKSPACE_URL = \"<databricks-instance>\"\nWORKSPACE_ID = \"<workspace-id>\"\nCLUSTER_ID = \"<cluster-id>\"\n\nconn = 'https://%s/sql/protocolv1/o/%s/%s' % (WORKSPACE_URL, WORKSPACE_ID, CLUSTER_ID)\nprint(conn)\n\ntransport = THttpClient.THttpClient(conn)\n\nauth = \"token:%s\" % TOKEN\nPY_MAJOR = sys.version_info[0]\n\nif PY_MAJOR < 3:\n  auth = base64.standard_b64encode(auth)\nelse:\n  auth = base64.standard_b64encode(auth.encode()).decode()\n\ntransport.setCustomHeaders({\"Authorization\": \"Basic %s\" % auth})\n\ncursor = hive.connect(thrift_transport=transport).cursor()\n\ncursor.execute('show tables',async_=True)\n\npending_states = (\n        hive.ttypes.TOperationState.INITIALIZED_STATE,\n        hive.ttypes.TOperationState.PENDING_STATE,\n        hive.ttypes.TOperationState.RUNNING_STATE)\n\nwhile cursor.poll().operationState in pending_states:\n    print(\"Pending...\")\n\nprint(\"Done. Results:\")\n\nfor table in cursor.fetchall():\n    print(table)",
          "title" : "How to run SQL queries from Python scripts",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/sql-in-python"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/change-r-version",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Change version of R (r-base) Article 03/11/2022 2 minutes to read 3 contributors In this article List available r-base-core versions Install a specific R version These instructions describe how to install a different version of R (r-base) on a cluster. You can check the default r-base version that each Databricks Runtime version is installed with in the System environment section of each Databricks Runtime release note. List available r-base-core versions To list the versions of r-base-core that can be installed and the version format: Paste the following shell command in a notebook cell: Bash Copy %sh\nadd-apt-repository 'deb [arch=amd64,i386] https://cran.rstudio.com/bin/linux/ubuntu xenial//'\napt-get -y update\napt-cache madison r-base-core\n Run the cell. For example, you can install version 3.3.3 by specifying 3.3.3-1xenial0. Install a specific R version Paste the following shell command into a notebook cell. Set <r-version> to the R version to be installed. Set <init-script-path> to a file path under /dbfs where this init script will be saved. Bash Copy %sh\n\nR_VERSION='<r-version>'\nINIT_SCRIPT_PATH='<init-script-path>'\n\nmkdir -p $(dirname $INIT_SCRIPT_PATH)\n\necho \"set -e\n\n# Add the repository containing another version of R\nadd-apt-repository 'deb [arch=amd64,i386] https://cran.rstudio.com/bin/linux/ubuntu xenial//'\napt-get -y update\n\n# Uninstall current R version\napt-get remove -y r-base-core\n\n# Install another version of R\napt-get install -y r-base-core=$R_VERSION\n\n# Must install Rserve to use Databricks notebook\nR -e \\\"install.packages('Rserve', repos='https://rforge.net/', type = 'source')\\\"\nR -e \\\"install.packages('hwriterPlus', repos='https://mran.revolutionanalytics.com/snapshot/2017-02-26')\\\"\" > $INIT_SCRIPT_PATH\n Run the notebook cell to save the init script to a file on DBFS. Configure a cluster with the cluster-scoped init script. When specifying the init script path in the cluster creation UI, modify the format of the init script path to change /dbfs to dbfs:/. For example, if <init-script-path> is set to /dbfs/examplepath/change-r-base.sh, then in the cluster creation UI specify the init script path dbfs:/examplepath/change-r-base.sh. After the cluster starts up, verify that the desired R version is installed by running %r R.version in a notebook cell.",
          "title" : "Change version of R (r-base)",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/change-r-version"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/persist-share-code-rstudio",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to persist and share code in RStudio Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Solution Problem Unlike a Azure Databricks notebook that has version control built in, code developed in RStudio is lost when the high concurrency cluster hosting Rstudio is shut down. Solution To persist and share code in RStudio, do one of the following: From RStudio, save the code to a folder on DBFS which is accessible from both Azure Databricks notebooks and RStudio. Use the integrated support for version control like Git in RStudio. Save the R notebook to your local file system by exporting it as Rmarkdown, then import the file into the RStudio instance. The blog Sharing R Notebooks using RMarkdown describes the steps in more detail. This process allows you to persist code developed in RStudio and share notebooks between the Azure Databricks notebook environment and RStudio.",
          "title" : "How to persist and share code in RStudio",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/persist-share-code-rstudio"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Clusters: tips and troubleshooting Article 03/15/2022 2 minutes to read 5 contributors In this article These articles can help you manage your Apache Spark clusters. Enable OpenJSSE and TLS 1.3 How to calculate the number of cores in a cluster Install a private PyPI repo IP access list update returns INVALID_STATE CPU core limit prevents cluster creation IP address limit prevents cluster creation Slow cluster launch and missing nodes Cannot apply updated cluster policy Cluster Apache Spark configuration not applied Cluster failed to launch Custom Docker image requires root Custom garbage collection prevents cluster launch Job fails due to cluster manager core instance request limit Admin user cannot restart cluster to run job Auto termination is disabled when starting a job cluster Cluster fails to start with dummy does not exist error Cluster slowdown due to Ganglia metrics filling root partition Persist Apache Spark CSV metrics to a DBFS location Replay Apache Spark events in a cluster Set Apache Hadoop core-site.xml properties Set executor log level Unexpected cluster termination How to overwrite log4j configurations on Azure Databricks clusters Adding a configuration setting overwrites all default spark.executor.extraJavaOptions settings Apache Spark executor memory allocation Apache Spark UI shows less than total node memory SSH to the cluster driver node Configure a cluster to use a custom NTP server Enable GCM cipher suites Enable retries in init script Failed to create cluster with invalid tag value UnknownHostException on cluster launch Validate environment variable behavior",
          "title" : "Clusters: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/spark-executor-memory",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Apache Spark executor memory allocation Article 03/11/2022 2 minutes to read 3 contributors In this article By default, the amount of memory available for each executor is allocated within the Java Virtual Machine (JVM) memory heap. This is controlled by the spark.executor.memory property. However, some unexpected behaviors were observed on instances with a large amount of memory allocated. As JVMs scale up in memory size, issues with the garbage collector become apparent. These issues can be resolved by limiting the amount of memory under garbage collector management. Selected Azure Databricks cluster types enable the off-heap mode, which limits the amount of memory under garbage collector management. This is why certain Spark clusters have the spark.executor.memory value set to a fraction of the overall cluster memory. The off-heap mode is controlled by the properties spark.memory.offHeap.enabled and spark.memory.offHeap.size which are available in Spark 1.6.0 and above. The following Azure Databricks cluster types enable the off-heap memory policy: Standard_L8s_v2 Standard_L16s_v2 Standard_L32s_v2 Standard_L32s_v2 Standard_L80s_v2",
          "title" : "Apache Spark executor memory allocation",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/spark-executor-memory"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/adls-gen1-firewall-access",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Unable to access Azure Data Lake Storage (ADLS) Gen1 when firewall is enabled Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem When you have a firewall enabled on your Azure virtual network (VNet) and you try to access ADLS using the ADLS Gen1 connector, it fails with the error: Console Copy 328 format(target_id, \".\", name), value) 329 else: 330 raise Py4JError(Py4JJavaError:\nAn error occurred while calling o196.parquet.: java.lang.RuntimeException:\nCould not find ADLS Token at com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider$$anonfun$get Token$1.apply(AdlCredentialContextTokenProvider.scala:18)\nat com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider$$anonfun$get\nToken$1.apply(AdlCredentialContextTokenProvider.scala:18)\nat scala.Option.getOrElse(Option.scala:121)\nat com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider.getToken(AdlCredentialContextTokenProvider.scala:18)\nat com.microsoft.azure.datalake.store.ADLStoreClient.getAccessToken(ADLStoreClient.java:1036)\nat com.microsoft.azure.datalake.store.HttpTransport.makeSingleCall(HttpTransport.java:177)\nat com.microsoft.azure.datalake.store.HttpTransport.makeCall(HttpTransport.java:91)\nat com.microsoft.azure.datalake.store.Core.getFileStatus(Core.java:655)\nat com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:735)\nat com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:718)\nat com.databricks.adl.AdlFileSystem.getFileStatus(AdlFileSystem.java:423)\nat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)\nat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:94)\n Cause This is a known issue with the ADLS Gen1 connector. Connecting to ADLS Gen1 when a firewall is enabled is unsupported. Solution Use ADLS Gen2 instead.",
          "title" : "Unable to access Azure Data Lake Storage (ADLS) Gen1 when firewall is enabled",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/adls-gen1-firewall-access"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/inconsistent-timestamp-results-jdbc",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Inconsistent timestamp results with JDBC applications Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem When using JDBC applications with Azure Databricks clusters you see inconsistent java.sql.Timestamp results when switching between standard time and daylight saving time. Cause Azure Databricks clusters use UTC by default. java.sql.Timestamp uses the JVM’s local time zone. If a Azure Databricks cluster returns 2021-07-12 21:43:08 as a string, the JVM parses it as 2021-07-12 21:43:08 and assumes the time zone is local. This works normally for most of the year, but when the local time zone has a DST adjustment, it causes an issue as UTC does not change. For example, on March 14, 2021, the US switched from standard time to daylight saving time. This means that local time went from 1:59 am to 3:00 am. If a Azure Databricks cluster returns 2021-03-14 02:10:55, the JVM automatically converts it to 2021-03-14 03:10:55 because 02:10:55 does not exist in local time on that date. Solution Option 1: Configure the JVM time zone to UTC. Set the user.timezone property to GMT. Review the Java time zone settings documentation for more information. Option 2: Use ODBC instead of JDBC. ODBC interprets timestamps as UTC. Install the Databricks ODBC Driver. Connect pyodbc to Azure Databricks. You can also use turbodbc. Option 3: Set the local time zone to UTC in your JDBC application. Review the documentation for your JDBC application to learn how to configure the local time zone settings.",
          "title" : "Inconsistent timestamp results with JDBC applications",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/inconsistent-timestamp-results-jdbc"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/wasb-check-blob-types",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Unable to read files and list directories in a WASB filesystem Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem When you try reading a file on WASB with Spark, you get the following exception: Console Copy org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 19, 10.139.64.5, executor 0): shaded.databricks.org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: Incorrect Blob type, please use the correct Blob type to access a blob on the server. Expected BLOCK_BLOB, actual APPEND_BLOB.\n When you try listing files in WASB using dbutils.fs.ls or the Hadoop API, you get the following exception: Console Copy java.io.FileNotFoundException: File/<some-directory> does not exist.\n Cause The WASB filesystem supports three types of blobs: block, page, and append. Block blobs are optimized for upload of large blocks of data (the default in Hadoop). Page blobs are optimized for random read and write operations. Append blobs are optimized for append operations. See Understanding block blobs, append blobs, and page blobs for details. The errors described above occur if you try to read an append blob or list a directory that contains only append blobs. The Azure Databricks and Hadoop Azure WASB implementations do not support reading append blobs. Similarly when listing a directory, append blobs are ignored. There is no workaround to enable reading append blobs or listing a directory that contains only append blobs. However, you can use either Azure CLI or Azure Storage SDK for Python to identify if a directory contains append blobs or a file is an append blob. You can verify whether a directory contains append blobs by running the following Azure CLI command: PowerShell Copy az storage blob list \\\n  --auth-mode key \\\n  --account-name <account-name> \\\n  --container-name <container-name> \\\n  --prefix <path>\n The result is returned as a JSON document, in which you can easily find the blob type for each file. If directory is large, you can limit number of results with the flag --num-results <num>. You can also use Azure Storage SDK for Python to list and explore files in a WASB filesystem: Python Copy iter = service.list_blobs(\"container\")\nfor blob in iter:\n  if blob.properties.blob_type == \"AppendBlob\":\n    print(\"\\t Blob name: %s, %s\" % (blob.name, blob.properties.blob_type))\n Azure Databricks does support accessing append blobs using the Hadoop API, but only when appending to a file. Solution There is no workaround for this issue. Use Azure CLI or Azure Storage SDK for Python to identify if the directory contains append blobs or the object is an append blob. You can implement either a Spark SQL UDF or custom function using RDD API to load, read, or convert blobs using Azure Storage SDK for Python.",
          "title" : "Unable to read files and list directories in a WASB filesystem",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/wasb-check-blob-types"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/bucketing",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to improve performance with bucketing Article 03/11/2022 2 minutes to read 3 contributors In this article Bucketing example notebook Bucketing is an optimization technique in Apache Spark SQL. Data is allocated among a specified number of buckets, according to values derived from one or more bucketing columns. Bucketing improves performance by shuffling and sorting data prior to downstream operations such as table joins. The tradeoff is the initial overhead due to shuffling and sorting, but for certain data transformations, this technique can improve performance by avoiding later shuffling and sorting. This technique is useful for dimension tables, which are frequently used tables containing primary keys. It is also useful when there are frequent join operations involving large and small tables. The example notebook below shows the differences in physical plans when performing joins of bucketed and unbucketed tables. Bucketing example notebook Get notebook",
          "title" : "How to improve performance with bucketing",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/bucketing"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/match-parquet-schema",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to handle corrupted Parquet files with different schema Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Solution Problem Let’s say you have a large list of essentially independent Parquet files, with a variety of different schemas. You want to read only those files that match a specific schema and skip the files that don’t match. One solution could be to read the files in sequence, identify the schema, and union the DataFrames together. However, this approach is impractical when there are hundreds of thousands of files. Solution Set the Apache Spark property spark.sql.files.ignoreCorruptFiles to true and then read the files with the desired schema. Files that don’t match the specified schema are ignored. The resultant dataset contains only data from those files that match the specified schema. Set the Spark property using spark.conf.set: Bash Copy spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n Alternatively, you can set this property in your Spark configuration.",
          "title" : "How to handle corrupted Parquet files with different schema",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/match-parquet-schema"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/spark-default-perms-adls-gen1",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Access files written by Apache Spark on ADLS Gen1 Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are using Azure Databricks and have a Spark job that is writing to ADLS Gen1 storage. When you try to manually read, write, or delete data in the folders you get an error message. Console Copy Forbidden. ACL verification failed. Either the resource does not exist or the user is not authorized to perform the requested operation\n Cause When writing data to ADLS Gen1 storage, Apache Spark uses the service principal as the owner of the files it creates. The service principal is defined in dfs.adls.oauth2.client.id. When files are created, they inherit the default permissions from the Hadoop filesystem. The Hadoop filesystem has a default permission of 666 (-rw-rw-rw-) and a default umask of 022, which results in the 644 permission setting as the default for files. When folders are created, they inherit the parent folder permissions, which are 770 by default. Because the owner is the service principal and not the user, you don’t have permission to access the folder due to the 0 bit in the folder permissions. Solution Option 1 Make the service principal user part of the same group as the default user. This will allow access when accessing storage through the portal. Please reach out to Microsoft support for assistance. Option 2 Create a base folder in ADLS Gen1 and set the permissions to 777. Write Spark output under this folder. Because folders created by Spark inherit the parent folder permissions, all folders created by Spark will have 777 permissions. This allows any user to access the folders. Option 3 Change the default umask from 022 to 000 on your Azure Databricks clusters. Set spark.hadoop.fs.permissions.umask-mode 000 in the Spark configuration for your cluster. With a umask of 000, the default Hadoop filesystem permission of 666 becomes the default permission used when Azure Databricks creates objects.",
          "title" : "Access files written by Apache Spark on ADLS Gen1",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/spark-default-perms-adls-gen1"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/update-nested-column",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to update nested columns Article 03/11/2022 2 minutes to read 3 contributors In this article Spark doesn’t support adding new columns or dropping existing columns in nested structures. In particular, the withColumn and drop methods of the Dataset class don’t allow you to specify a column name different from any top level columns. For example, suppose you have a dataset with the following schema: Scala Copy val schema = (new StructType)\n      .add(\"metadata\",(new StructType)\n             .add(\"eventid\", \"string\", true)\n             .add(\"hostname\", \"string\", true)\n             .add(\"timestamp\", \"string\", true)\n           , true)\n      .add(\"items\", (new StructType)\n             .add(\"books\", (new StructType).add(\"fees\", \"double\", true), true)\n             .add(\"paper\", (new StructType).add(\"pages\", \"int\", true), true)\n           ,true)\nschema.treeString\n The schema looks like: Copy root\n |-- metadata: struct (nullable = true)\n |    |-- eventid: string (nullable = true)\n |    |-- hostname: string (nullable = true)\n |    |-- timestamp: string (nullable = true)\n |-- items: struct (nullable = true)\n |    |-- books: struct (nullable = true)\n |    |    |-- fees: double (nullable = true)\n |    |-- paper: struct (nullable = true)\n |    |    |-- pages: integer (nullable = true)\n Suppose you have the DataFrame: Scala Copy val rdd: RDD[Row] = sc.parallelize(Seq(Row(\n  Row(\"eventid1\", \"hostname1\", \"timestamp1\"),\n  Row(Row(100.0), Row(10)))))\nval df = spark.createDataFrame(rdd, schema)\ndisplay(df)\n You want to increase the fees column, which is nested under books, by 1%. To update the fees column, you can reconstruct the dataset from existing columns and the updated column as follows: Scala Copy val updated = df.selectExpr(\"\"\"\n    named_struct(\n        'metadata', metadata,\n        'items', named_struct(\n          'books', named_struct('fees', items.books.fees * 1.01),\n          'paper', items.paper\n        )\n    ) as named_struct\n\"\"\").select($\"named_struct.metadata\", $\"named_struct.items\")\nupdated.show(false)\n Then you will get the result: Copy +-----------------------------------+-----------------+\n| metadata                          | items           |\n+===================================+=================+\n| [eventid1, hostname1, timestamp1] | [[101.0], [10]] |\n+-----------------------------------+-----------------+",
          "title" : "How to update nested columns",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/update-nested-column"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/delete-checkpoint-restart",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Delete your streaming query checkpoint and restart Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem Your job fails with a Delta table <value> doesn't exist. Please delete your streaming query checkpoint and restart. error message. Cause Two different streaming sources are configured to use the same checkpoint directory. This is not supported. For example, assume streaming query A streams data from Delta table A, and uses the directory /checkpoint/A as a checkpoint. If streaming query B streams data from Delta table B, but attempts to use the directory /checkpoint/A as a checkpoint, the reservoirId of the Delta tables doesn’t match and the query fails with an exception. Note A similar issue can occur with ABS-AQS if you attempt to share the checkpoint directory. This is because ABS-AQS uses an internal Delta table to maintain the event messages. Solution You should not share checkpoint directories between different streaming queries. Use a new checkpoint directory for every new streaming query.",
          "title" : "Delete your streaming query checkpoint and restart",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/delete-checkpoint-restart"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/adls-gen1-mount-problem",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Unable to mount Azure Data Lake Storage Gen1 account Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem When you try to mount an Azure Data Lake Storage (ADLS) Gen1 account on Azure Databricks, it fails with the error: Console Copy com.microsoft.azure.datalake.store.ADLException: Error creating directory /\nError fetching access token\nOperation null failed with exception java.io.IOException : Server returned HTTP response code: 401 for URL: https://login.windows.net/18b0b5d6-b6eb-4f5d-964b-c03a6dfdeb22/oauth2/token\nLast encountered exception thrown after 5 tries. [java.io.IOException,java.io.IOException,java.io.IOException,java.io.IOException,java.io.IOException]\n [ServerRequestId:null]\nat com.microsoft.azure.datalake.store.ADLStoreClient.getExceptionFromResponse(ADLStoreClient.java:1169)\nat com.microsoft.azure.datalake.store.ADLStoreClient.createDirectory(ADLStoreClient.java:589)\nat com.databricks.adl.AdlFileSystem.mkdirs(AdlFileSystem.java:533)\nAt com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$mkdirs$1$$anonfun$apply$mcZ$sp$7$$anonfun$apply$mcZ$sp$8.apply$mcZ$sp(DatabricksFileSystemV2.scala:638)\n Cause This error can occur if the ADLS Gen1 account was previously mounted in the workspace, but not unmounted, and the credential used for that mount subsequently expired. When you try to mount the same account with a new credential, there is a conflict between the expired and new credentials. Solution You need to unmount all existing mounts, and then create a new mount with a new, unexpired credential. For more information, see Mount Azure Data Lake Storage Gen1 with DBFS.",
          "title" : "Unable to mount Azure Data Lake Storage Gen1 account",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/adls-gen1-mount-problem"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/azure-vnet-gen1-issue",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Network configuration of Azure Data Lake Storage Gen1 causes ADLException: Error getting info for file Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem Access to Azure Data Lake Storage Gen1 (ADLS Gen1) fails with ADLException: Error getting info for file <filename> when the following network configuration is in place: Azure Databricks workspace is deployed in your own virtual network (uses VNet injection). Traffic is allowed via Azure Data Lake Storage credential passthrough. ADLS Gen1 storage firewall is enabled. Azure Active Directory (Azure AD) service endpoint is enabled for the Azure Databricks workspace’s virtual network. Cause Azure Databricks uses a control plane located in its own virtual network, and the control plane is responsible for obtaining a token from Azure AD. ADLS credential passthrough uses the control plane to obtain Azure AD tokens to authenticate the interactive user with ADLS Gen1. When you deploy your Databricks workspace in your own virtual network (using VNet injection), Azure Databricks clusters are created in your own virtual network. For increased security, you can restrict access to the ADLS Gen 1 account by configuring the ADLS Gen1 firewall to allow only requests from your own virtual network, by implementing service endpoints to Azure AD. However, ADLS credential passthrough fails in this case. The reason is that when ADLS Gen1 checks for the virtual network where the token was created, it finds the network to be the Azure Databricks control plane and not the customer-provided virtual network where the original passthrough call was made. Solution To use ADLS credential passthrough with a service endpoint, storage firewall, and ADLS Gen1, enable Allow access to Azure services in the firewall settings. If you have security concerns about enabling this setting in the firewall, you can upgrade to ADLS Gen2. ADLS Gen2 works with the network configuration described above. For more information, see: Deploying Azure Databricks in your Azure Virtual Network Accessing Azure Data Lake Storage Automatically with your Azure Active Directory Credentials",
          "title" : "Network configuration of Azure Data Lake Storage Gen1 causes ADLException: Error getting info for file",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/azure-vnet-gen1-issue"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/azure-nodes-not-acquired",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Slow cluster launch and missing nodes Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem A cluster takes a long time to launch and displays an error message similar to the following: Console Copy Cluster is running but X nodes could not be acquired\n Cause Provisioning an Azure VM typically takes 2-4 minutes, but if all the VMs in a cluster cannot be provisioned at the same time, cluster creation can be delayed. This is due to Azure Databricks having to reissue VM creation requests over a period of time. Solution If a cluster launches without all of the nodes, Azure Databricks automatically tries to acquire the additional nodes and will update the cluster once available. To workaround this, you should configure a cluster with a bigger instance type and a smaller number of nodes.",
          "title" : "Slow cluster launch and missing nodes",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/azure-nodes-not-acquired"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/parquet-to-delta-fails",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Converting from Parquet to Delta Lake fails Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are attempting to convert a Parquet file to a Delta Lake file. The directory containing the Parquet file contains one or more subdirectories. The conversion fails with the error message: Expecting 0 partition column(s): [], but found 1 partition column(s): [<column_name>] from parsing the file name: <path_to_the_file_location>;. Cause The conversion process is attempting to process the subdirectory as a partition. This causes the error message. Solution If you are using Databricks Runtime 7.5 or below, ensure that directories containing Parquet files do not have subdirectories. This issue is resolved in Databricks Runtime 8.0 and above.",
          "title" : "Converting from Parquet to Delta Lake fails",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/parquet-to-delta-fails"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/zordering-ineffective-column-stats",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Z-Ordering will be ineffective, not collecting stats Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to optimize a Delta table by Z-Ordering and receive an error about not collecting stats for the columns. Console Copy AnalysisException: Z-Ordering on [col1, col2] will be ineffective, because we currently do not collect stats for these columns.\n Note Please review Z-Ordering (multi-dimensional clustering) for more information on data skipping and z-ordering. Cause Delta Lake collects statistics on the first 32 columns defined in your table schema. If the columns you are attempting to Z-Order are not in the first 32 columns, no statistics are collected for those columns. Solution Reorder the columns in your table, so the columns you are attempting to Z-Order are in the first 32 columns in your table. You can use an ALTER TABLE statement to reorder the columns. SQL Copy ALTER TABLE table_name CHANGE [COLUMN] col_name col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name]\n For example, this statement brings the column with <column-name> to the first column in the table. SQL Copy ALTER TABLE <delta-table-name> CHANGE COLUMN <column-name> <column-name> <data-type> FIRST\n Recompute the statistics after you have reordered the columns in the table. Scala Copy import com.databricks.sql.transaction.tahoe._\nimport org.apache.spark.sql.catalyst.TableIdentifier\nimport com.databricks.sql.transaction.tahoe.stats.StatisticsCollection\n\nval tableName = \"<name of table>\"\nval deltaLog = DeltaLog.forTable(spark, TableIdentifier(tableName))\n\nStatisticsCollection.recompute(spark, deltaLog)\n Rerun the Z-Order on the table and it should complete successfully.",
          "title" : "Z-Ordering will be ineffective, not collecting stats",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/zordering-ineffective-column-stats"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-fails-sqlalchemy",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Jobs failing on Databricks Runtime 5.5 LTS with an SQLAlchemy package error Article 03/11/2022 2 minutes to read 4 contributors In this article Problem Version Cause Solution Problem Azure Databricks jobs that require the third-party library SQLAlchemy are failing. This issue started occurring on or about March 10, 2020. The error message location differs for job clusters and all-purpose clusters, but it is similar to the following example error message: Console Copy Library installation failed for library pypi {\n package: \"sqlalchemy\"\n}\n. Error messages:\njava.lang.RuntimeException: ManagedLibraryInstallFailed: org.apache.spark.SparkException: Process List(/databricks/python/bin/pip, install, sqlalchemy, --disable-pip-version-check) exited with code 2. ERROR: Exception:\n Failure on job clusters On a job cluster the error manifests as a failure to start. You can confirm the issue by viewing the job run results and looking for text similar to the example error message. Failure on all-purpose clusters If you have all-purpose clusters using Databricks Runtime 5.5 LTS, you can view the error message within the workspace UI. Click Clusters. Click the name of your cluster. Click Libraries. Click sqlalchemy. Read the error messages under the Messages heading. Look for text similar to the example error message. Version The problem affects clusters on Databricks Runtime 5.5 LTS using SQLAlchemy 1.3.15. Cause On March 10, 2020, PyPI updated the release version of SQLAlchemy to 1.3.15. If you are using PyPI to automatically download the most current version of SQLAlchemy and you are using Databricks Runtime 5.5 LTS clusters, the update to SQLAlchemy may result in job failures. Solution There are two workarounds available. Restrict the version of SQLAlchemy to 1.3.13. Prevent Python from using the pep517 build system. Restrict SQLAlchemy to version 1.3.13 Install SQLAlchemy using the PyPI package installation instructions and set the version value to 1.3.13. Prevent Python from using the pep517 build system You can use an init script to prevent Python from using the pep517 build system. Use the following code block to generate the init script install-sqlalchemy.sh on your cluster: Python Copy dbutils.fs.put(\"/databricks/init-scripts/install-sqlalchemy.sh\", \"\"\"\n#!/bin/bash\n/databricks/python/bin/pip install sqlalchemy --disable-pip-version-check --no-use-pep517\n\"\"\", True)\n Follow the existing documentation to install the script as a cluster-scoped init script. Restart the cluster after you have installed the script. Best practice recommendation Whenever you use third-party libraries, you should always configure your clusters to use specific versions of each library that are known to be working. New versions of libraries can offer new features, but they can also introduce problems if they are deployed without testing and validation.",
          "title" : "Jobs failing on Databricks Runtime 5.5 LTS with an SQLAlchemy package error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-fails-sqlalchemy"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/join-two-dataframes-duplicated-columns",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Prevent duplicated columns when joining two DataFrames Article 03/11/2022 2 minutes to read 3 contributors In this article Join on columns Solution If you perform a join in Spark and don’t specify your join correctly you’ll end up with duplicate column names. This makes it harder to select those columns. This article and notebook demonstrate how to perform a join so that you don’t have duplicated columns. Join on columns If you join on columns, you get duplicated columns. Scala Scala Copy %scala\n\nval llist = Seq((\"bob\", \"2015-01-13\", 4), (\"alice\", \"2015-04-23\",10))\nval left = llist.toDF(\"name\",\"date\",\"duration\")\nval right = Seq((\"alice\", 100),(\"bob\", 23)).toDF(\"name\",\"upload\")\n\nval df = left.join(right, left.col(\"name\") === right.col(\"name\"))\n Python Python Copy %python\n\nllist = [('bob', '2015-01-13', 4), ('alice', '2015-04-23',10)]\nleft = spark.createDataFrame(llist, ['name','date','duration'])\nright = spark.createDataFrame([('alice', 100),('bob', 23)],['name','upload'])\n\ndf = left.join(right, left.name == right.name)\n Solution Specify the join column as an array type or string. Scala Scala Copy %scala\n\nval df = left.join(right, Seq(\"name\"))\n Scala Copy %scala\n\nval df = left.join(right, \"name\")\n Python Python Copy %python\ndf = left.join(right, [\"name\"])\n Python Copy %python\ndf = left.join(right, \"name\")\n R First register the DataFrames as tables. Python Copy %python\n\nleft.createOrReplaceTempView(\"left_test_table\")\nright.createOrReplaceTempView(\"right_test_table\")\n R Copy %r\nlibrary(SparkR)\nsparkR.session()\nleft <- sql(\"SELECT * FROM left_test_table\")\nright <- sql(\"SELECT * FROM right_test_table\")\n The above code results in duplicate columns. The following code does not. R Copy %r\nhead(drop(join(left, right, left$name == right$name), left$name))\n Join DataFrames with duplicated columns notebook Get notebook",
          "title" : "Prevent duplicated columns when joining two DataFrames",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/join-two-dataframes-duplicated-columns"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/backfill-delta-table-cols",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to populate or update columns in an existing Delta table Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Solution Problem You have an existing Delta table, with a few empty columns. You need to populate or update those columns with data from a raw Parquet file. Solution In this example, there is a customers table, which is an existing Delta table. It has an address column with missing values. The updated data exists in Parquet format. Create a DataFrame from the Parquet file using an Apache Spark API statement: Python Copy updatesDf = spark.read.parquet(\"/path/to/raw-file\")\n View the contents of the updatesDF DataFrame: Python Copy display(updatesDf)\n Create a table from the updatesDf DataFrame. In this example, it is named updates. Python Copy updatesDf.createOrReplaceTempView(\"updates\")\n Check the contents of the updates table, and compare it to the contents of customers: Python Copy display(customers)\n Use the MERGE INTO statement to merge the data from the updates table into the original customers table. SQL Copy MERGE INTO customers\nUSING updates\nON customers.customerId = source.customerId\nWHEN MATCHED THEN\n  UPDATE SET address = updates.address\nWHEN NOT MATCHED\n  THEN INSERT (customerId, address) VALUES (updates.customerId, updates.address)\n Here, customers is the original Delta table that has an address column with missing values. updates is the table created from the DataFrame updatesDf, which is created by reading data from the raw file. The address column of the original Delta table is populated with the values from updates, overwriting any existing values in the address column. If updates contains customers that are not already in the customers table, then the command adds these new customer records. For more examples of using MERGE INTO, see Merge Into (Delta Lake).",
          "title" : "How to populate or update columns in an existing Delta table",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/backfill-delta-table-cols"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/anaconda-environment",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Create a cluster with Conda Article 03/11/2022 2 minutes to read 3 contributors In this article Conda is a popular open source package management system for the Anaconda repo. Databricks Runtime for Machine Learning (Databricks Runtime ML) uses Conda to manage Python library dependencies. If you want to use Conda, you should use Databricks Runtime ML. Attempting to install Anaconda or Conda for use with Databricks Runtime is not supported. Follow the Create a cluster using Databricks Runtime ML instructions to create a cluster with Conda. Once the cluster has been created, you can use Conda to manage Python packages on the cluster.",
          "title" : "Create a cluster with Conda",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/anaconda-environment"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/create-table-ddl-for-metastore",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to create table DDLs to import into an external metastore Article 03/11/2022 2 minutes to read 3 contributors In this article Azure Databricks supports using external metastores instead of the default Hive metastore. You can export all table metadata from Hive to the external metastore. Use the Apache Spark Catalog API to list the tables in the databases contained in the metastore. Use the SHOW CREATE TABLE statement to generate the DDLs and store them in a file. Use the file to import the table DDLs into the external metastore. The following code accomplishes the first two steps. Python Copy dbs = spark.catalog.listDatabases()\nfor db in dbs:\n  f = open(\"your_file_name_{}.ddl\".format(db.name), \"w\")\n  tables = spark.catalog.listTables(db.name)\n  for t in tables:\n    DDL = spark.sql(\"SHOW CREATE TABLE {}.{}\".format(db.name, t.name))\n    f.write(DDL.first()[0])\n    f.write(\"\\n\")\nf.close()\n You can use the resulting file to import the table DDLs into the external metastore.",
          "title" : "How to create table DDLs to import into an external metastore",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/create-table-ddl-for-metastore"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/parquet-timestamp-requires-msver12",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Parquet timestamp requires Hive metastore 1.2 or above Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to create a Parquet table using TIMESTAMP, but you get an error message. Console Copy Error in SQL statement: QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.UnsupportedOperationException: Parquet does not support timestamp. See HIVE-6384\n Example code SQL Copy CREATE EXTERNAL TABLE IF NOT EXISTS testTable (\n  emp_name STRING,\n  joing_datetime TIMESTAMP,\n)\nPARTITIONED BY\n  (date DATE)\nSTORED AS\n  PARQUET\nLOCATION\n  \"/mnt/<path-to-data>/emp.testTable\"\n Cause Parquet requires a Hive metastore version of 1.2 or above in order to use TIMESTAMP. Note The default Hive metastore client version used in Databricks Runtime is 0.13.0. Solution You must upgrade the Hive metastore client on the cluster. You can do this by adding the following settings to the cluster’s Spark configuration. Databricks Runtime 6.6 and below text Copy spark.sql.hive.metastore.version 1.2.1\nspark.sql.hive.metastore.jars builtin\n Databricks Runtime 7.0 and above text Copy spark.sql.hive.metastore.jars /dbfs <path-to-downloaded-jars>\nspark.sql.hive.metastore.version 1.2.1\n Note For Databricks Runtime 7.0 and above you must download the metastore jars and point to them as detailed in the Azure Databricks documentation.",
          "title" : "Parquet timestamp requires Hive metastore 1.2 or above",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/parquet-timestamp-requires-msver12"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/troubleshoot-cancel-command",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Troubleshooting unresponsive Python notebooks or canceled commands Article 03/11/2022 2 minutes to read 3 contributors In this article Check metastore connectivity Check for conflicting libraries This article provides an overview of troubleshooting steps you can take if a notebook is unresponsive or cancels commands. Check metastore connectivity Problem Simple commands in newly-attached notebooks fail, but succeed in notebooks that were attached to the same cluster earlier. Troubleshooting steps Check metastore connectivity. The inability to connect to the Hive metastore can cause REPL initialization to hang, making the cluster appear unresponsive. Are you are using the Azure Databricks metastore or your own external metastore? If you are using an external metastore, have you changed anything recently? Did you upgrade your metastore version? Rotate passwords or configurations? Change security group rules? See Metastore: tips and troubleshooting for more troubleshooting tips and solutions. Check for conflicting libraries Problem Python library conflicts can result in cancelled commands. The Azure Databricks support organization sees conflicts most often with versions of ipython, numpy, scipy, and pandas. Troubleshooting steps See Cluster cancels Python command execution due to library conflict. For more notebook troubleshooting information, see Notebooks: tips and troubleshooting.",
          "title" : "Troubleshooting unresponsive Python notebooks or canceled commands",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/troubleshoot-cancel-command"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/spark-submit-fail-parse-byte-string",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Apache Spark job fails with Failed to parse byte string Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem Spark-submit jobs fail with a Failed to parse byte string: -1 error message. Console Copy java.util.concurrent.ExecutionException: java.lang.NumberFormatException: Size must be specified as bytes (b), kibibytes (k), mebibytes (m), gibibytes (g), tebibytes (t), or pebibytes(p). E.g. 50b, 100k, or 250m.\nFailed to parse byte string: -1\nat java.util.concurrent.FutureTask.report(FutureTask.java:122)\nat java.util.concurrent.FutureTask.get(FutureTask.java:206)\nat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:182)\n... 108 more\nCaused by: java.lang.NumberFormatException: Size must be specified as bytes (b), kibibytes (k), mebibytes (m), gibibytes (g), tebibytes (t), or pebibytes(p). E.g. 50b, 100k, or 250m.\nFailed to parse byte string: -1\n Cause The value of the spark.driver.maxResultSize application property is negative. Solution The value assigned to spark.driver.maxResultSize defines the maximum size (in bytes) of the serialized results for each Spark action. You can assign a positive value to the spark.driver.maxResultSize property to define a specific size. You can also assign a value of 0 to define an unlimited maximum size. You cannot assign a negative value to this property. If the total size of a job is above the spark.driver.maxResultSize value, the job is aborted. You should be careful when setting an excessively high (or unlimited) value for spark.driver.maxResultSize. A high limit can cause out-of-memory errors in the driver if the spark.driver.memory property is not set high enough. See Spark Configuration Application Properties for more details.",
          "title" : "Apache Spark job fails with Failed to parse byte string",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/spark-submit-fail-parse-byte-string"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/get-notebooks-deleted-user",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Access notebooks owned by a deleted user Article 03/11/2022 2 minutes to read 3 contributors In this article When you remove a user from Azure Databricks, a special backup folder is created in the workspace. This backup folder contains all of the deleted user’s content. Backup folders appear in the workspace as -backup-#. Note Only an admin user can access a backup folder. To access a backup folder: Log into Azure Databricks as an admin user. Select Workspace from the sidebar. Select Users. Select the backup folder. You can delete the backup folder once it is no longer required.",
          "title" : "Access notebooks owned by a deleted user",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/get-notebooks-deleted-user"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/enable-openjsse-tls13",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Enable OpenJSSE and TLS 1.3 Article 03/11/2022 2 minutes to read 2 contributors In this article Queries and transformations are encrypted before being send to your clusters. By default, the data exchanged between worker nodes in a cluster is not encrypted. If you require that data is encrypted at all times, you can encrypt traffic between cluster worker nodes using AES 128 over a TLS 1.2 connection. In some cases, you may want to use TLS 1.3 instead of TLS 1.2 because it allows for stronger ciphers. To use TLS 1.3 on your clusters, you must enable OpenJSSE in the cluster’s Apache Spark configuration. Add spark.driver.extraJavaOptions -XX:+UseOpenJSSE to your Spark Config. Restart your cluster. OpenJSSE and TLS 1.3 are now enabled on your cluster and can be used in notebooks.",
          "title" : "Enable OpenJSSE and TLS 1.3",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/enable-openjsse-tls13"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/fail-create-cluster-tag-limit",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Failed to create cluster with invalid tag value Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to create a cluster, but it is failing with an invalid tag value error message. Console Copy System.Exception: Content={\"error_code\":\"INVALID_PARAMETER_VALUE\",\"message\":\"\\nInvalid tag value (<<<<TAG-VALUE>>>>) - the length cannot exceed 256\\nUnicode characters in UTF-8.\\n \"}\n Cause Limitations on tag Key and Value are set by Azure. Azure tag keys must: Contain 1-512 characters Contain letters, numbers, spaces (except < > * % & : \\ ? / + ) Not start with azure, microsoft, or windows Not duplicate an existing key Azure tag values must: Contain 1-256 characters Contain letters, numbers, spaces (except < > * % & : \\ ? / + ) Not start with azure, microsoft, or windows For more information, please refer to the Azure tag resource limitations documentation. Solution Requests to update any limits on tagging must be made directly with the Azure support team.",
          "title" : "Failed to create cluster with invalid tag value",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/fail-create-cluster-tag-limit"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/install-private-pypi-repo",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Install a private PyPI repo Article 03/11/2022 2 minutes to read 2 contributors In this article Create init script Install as a cluster-scoped init script Restart the cluster Use the init script with a job cluster Certain use cases may require you to install libraries from private PyPI repositories. If you are installing from a public repository, you should review the library documentation. This article shows you how to configure an example init script that authenticates and downloads a PyPI library from a private repository. Create init script Create (or verify) a directory to store the init script. <init-script-folder> is the name of the folder where you store your init scripts. Scala Copy dbutils.fs.mkdirs(\"dbfs:/databricks/<init-script-folder>/\")\n Create the init script. Scala Copy dbutils.fs.put(\"/databricks/<init-script-folder>/private-pypi-install.sh\",\"\"\"\n#!/bin/bash\n/databricks/python/bin/pip install --index-url=https://${<repo-username>}:${<repo-password>}@<private-pypi-repo-domain-name> private-package==<version>\n\"\"\", True)\n Verify that your init script exists. Scala Copy display(dbutils.fs.ls(\"dbfs:/databricks/<init-script-folder>/private-pypi-install.sh\"))\n Install as a cluster-scoped init script Install the init script that you just created as a cluster-scoped init script. You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/private-pypi-install.sh). Restart the cluster Restart your cluster after you have installed the init script. Once the cluster starts up, verify that it successfully installed the custom library from the private PyPI repository. If the custom library is not installed, double check the username and password that you set for the private PyPI repository in the init script. Use the init script with a job cluster Once you have the init script created, and verified working, you can include it in a create-job.json file when using the Jobs API to start a job cluster. JSON Copy {\n  \"cluster_id\": \"1202-211320-brick1\",\n  \"num_workers\": 1,\n  \"spark_version\": \"<spark-version>\",\n  \"node_type_id\": \"<node-type>\",\n  \"cluster_log_conf\": {\n    \"dbfs\" : {\n      \"destination\": \"dbfs:/cluster-logs\"\n    }\n  },\n  \"init_scripts\": [ {\n    \"dbfs\": {\n      \"destination\": \"dbfs:/databricks/<init-script-folder>/private-pypi-install.sh\"\n    }\n  } ]\n}",
          "title" : "Install a private PyPI repo",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/install-private-pypi-repo"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/set-executor-log-level",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Set executor log level Article 03/11/2022 2 minutes to read 4 contributors In this article Important This article describes steps related to customer use of Log4j 1.x within an Azure Databricks cluster. Log4j 1.x is no longer maintained and has three known CVEs (CVE-2021-4104, CVE-2020-9488, and CVE-2019-17571). If your code uses one of the affected classes (JMSAppender or SocketServer), your use may potentially be impacted by these vulnerabilities. To set the log level on all executors, you must set it inside the JVM on each worker. For example: Scala Copy sc.parallelize(Seq(\"\")).foreachPartition(x => {\n  import org.apache.log4j.{LogManager, Level}\n  import org.apache.commons.logging.LogFactory\n\n  LogManager.getRootLogger().setLevel(Level.DEBUG)\n  val log = LogFactory.getLog(\"EXECUTOR-LOG:\")\n  log.debug(\"START EXECUTOR DEBUG LOG LEVEL\")\n})\n To verify that the level is set, navigate to the Spark UI, select the Executors tab, and open the stderr log for any executor:",
          "title" : "Set executor log level",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/set-executor-log-level"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/termination-reasons",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Unexpected cluster termination Article 03/11/2022 2 minutes to read 3 contributors In this article Azure Databricks initiated request limit exceeded Cloud provider initiated terminations Sometimes a cluster is terminated unexpectedly, not as a result of a manual termination or a configured automatic termination. A cluster can be terminated for many reasons. Some terminations are initiated by Azure Databricks and others are initiated by the cloud provider. This article describes termination reasons and steps for remediation. Azure Databricks initiated request limit exceeded To defend against API abuses, ensure quality of service, and prevent you from accidentally creating too many large clusters, Azure Databricks throttles all cluster up-sizing requests, including cluster creation, starting, and resizing. The throttling uses the token bucket algorithm to limit the total number of nodes that anyone can launch over a defined interval across your Databricks deployment, while allowing burst requests of certain sizes. Requests coming from both the web UI and the APIs are subject to rate limiting. When cluster requests exceed rate limits, the limit-exceeding request fails with a REQUEST_LIMIT_EXCEEDED error. Solution If you hit the limit for your legitimate workflow, Databricks recommends that you do the following: Retry your request a few minutes later. Spread out your recurring workflow evenly in the planned time frame. For example, instead of scheduling all of your jobs to run at an hourly boundary, try distributing them at different intervals within the hour. Consider using clusters with a larger node type and smaller number of nodes. Use autoscaling clusters. If these options don’t work for you, contact Azure Databricks Support to request a limit increase for the core instance. For other Azure Databricks initiated termination reasons, see Termination Code. Cloud provider initiated terminations This article lists common cloud provider related termination reasons and remediation steps. Launch failure This termination reason occurs when Azure Databricks fails to acquire virtual machines. The error code and message from the API are propagated to help you troubleshoot the issue. OperationNotAllowed You have reached a quota limit, usually number of cores, that your subscription can launch. Request a limit increase in Azure portal. See Azure subscription and service limits, quotas, and constraints. PublicIPCountLimitReached You have reached the limit of the public IPs that you can have running. Request a limit increase in Azure Portal. SkuNotAvailable The resource SKU you have selected (such as VM size) is not available for the location you have selected. To resolve, see Resolve errors for SKU not available. ReadOnlyDisabledSubscription Your subscription was disabled. Follow the steps in Why is my Azure subscription disabled and how do I reactivate it? to reactivate your subscription. ResourceGroupBeingDeleted Can occur if someone cancels your Azure Databricks workspace in the Azure portal and you try to create a cluster at the same time. The cluster fails because the resource group is being deleted. SubscriptionRequestsThrottled Your subscription is hitting the Azure Resource Manager request limit (see Throttling Resource Manager requests). Typical cause is that another system outside Azure Databricks) making a lot of API calls to Azure. Contact Azure support to identify this system and then reduce the number of API calls. Communication lost Azure Databricks was able to launch the cluster, but lost the connection to the instance hosting the Spark driver. Caused by the driver virtual machine going down or a networking issue.",
          "title" : "Unexpected cluster termination",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/termination-reasons"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/unknown-host-exception-on-launch",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents UnknownHostException on cluster launch Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem When you launch an Azure Databricks cluster, you get an UnknownHostException error. You may also get one of the following error messages: Error: There was an error in the network configuration. databricks_error_message: Could not access worker artifacts. Error: Temporary failure in name resolution. Internal error message: Failed to launch spark container on instance XXX. Exception: Could not add container for XXX with address X.X.X.X.mysql.database.azure.com: Temporary failure in name resolution. Cause These errors indicate an issue with DNS settings. Primary DNS could be down or unresponsive. Artifacts are not being resolved, which results in the cluster launch failure. You may have a host record listing the artifact public IP as static, but it has changed. Solution Identify a working DNS server and update the DNS entry on the cluster. Start a standalone Azure VM and verify that the artifacts blob storage account is reachable from the instance. Bash Copy `telnet dbartifactsprodeastus.blob.core.windows.net 443`.\n Verify that you can reach your primary DNS server from a notebook by running a ping command. If your DNS server is not responding, try to reach your secondary DNS server from a notebook by running a ping command. Launch a Web Terminal from the cluster workspace. Edit the /etc/resolv.conf file on the cluster. Update the nameserver value with your working DNS server. Save the changes to the file. Restart systemd-resolved. Bash Copy $ sudo systemctl restart systemd-resolved.service\n Note This is a temporary change to the DNS and will be lost on cluster restart. After verifying that the custom DNS settings are correct, you can configure custom DNS settings using dnsmasq to make the change permanent. Further troubleshooting If you are still having DNS issues, you should try the following steps: Verify that port 43 (used for whois) and port 53 (used for DNS) are open in your firewall. Add the Azure recursive resolver (168,.63.129.16) to the default DNS forwarder. Review the VMs and role instances documentation for more information. Verify that nslookup results are identical between your laptop and the default DNS. If there is a mistmatch, your DNS server may have an incorrect host record. Verify that everything works with a default Azure DNS server. If it works with Azure DNS, but fails with your custom DNS, your DNS admin should review your DNS server settings.",
          "title" : "UnknownHostException on cluster launch",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/unknown-host-exception-on-launch"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/init-script-fail-download-maven",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Init script fails to download Maven JAR Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have an init script that is attempting to install a library via Maven, but it fails when trying to download a JAR. Console Copy https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark_2.12/0.4.1/rapids-4-spark_2.12-0.4.1.jar%0D\nResolving repo1.maven.org (repo1.maven.org)... 151.101.248.209\nConnecting to repo1.maven.org (repo1.maven.org)|151.101.248.209|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2021-07-30 01:31:11 ERROR 404: Not Found.\n Cause There is a carriage return (%0D) character at the end of one or more of the lines in the init script. This is usually caused by editing a file in Windows and then uploading it to your Azure Databricks workspace without removing the excess carriage returns. Solution Remove the Windows carriage returns by running dos2unix on the file after you have uploaded it to the workspace. Bash Copy sudo apt-get install dos2unix -y\ndos2unix file <initscript.sh>\n Once you have removed the Windows carriage returns from the file, you can configure the init script as normal.",
          "title" : "Init script fails to download Maven JAR",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/init-script-fail-download-maven"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/remove-log4j1x-jmsappender-socketserver-classes",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Remove Log4j 1.x JMSAppender and SocketServer classes from classpath Article 03/11/2022 2 minutes to read 2 contributors In this article Configure the global init script Verify the affected classes are not available Caveats Databricks recently published a blog on Log4j 2 Vulnerability (CVE-2021-44228) Research and Assessment. Databricks does not directly use a version of Log4j known to be affected by this vulnerability within the Azure Databricks platform in a way we understand may be vulnerable. Azure Databricks also does not use the affected classes from Log4j 1.x with known vulnerabilities (CVE-2021-4104, CVE-2020-9488, and CVE-2019-17571). However, if your code uses one of these classes (JMSAppender or SocketServer), your use may potentially be impacted by these vulnerabilities. If your code uses Log4j, you should upgrade to Log4j 2.17 or above. If you cannot upgrade for technical reasons, you can use a global init script to strip the affected classes from Log4j on cluster start. Important Because we do not control the code you run, we cannot guarantee that this solution will prevent Log4j from loading the affected classes in all cases. Configure the global init script Note Running this script is a breaking change for any code that relies on the affected classes. Go to the Admin Console and click the Global Init Scripts tab. Click the + Add button. Enter the name of the script. Copy the following script into the Script field. Bash Copy  #!/bin/bash\n\n echo 'Init script to remove certain Log4J 1.x classes, version 1.0 (2021-12-17)'\n\n FILES_TO_DELETE=(\n   org/apache/log4j/net/JMSAppender.class\n   org/apache/log4j/net/SocketServer.class\n )\n\n find \"/databricks\" \\\n     -name '*log4j*.jar' \\\n     -exec echo -e \"\\nProcessing {}\" \\; -exec zip -d {} \"${FILES_TO_DELETE[@]}\" \\;\n\n exit 0\n If you have more than one global init script configured for your workspace, you should configure this script to run after your other scripts. Ensure the Enabled switch is toggled on. Click Add. Restart ALL running clusters. Verify the affected classes are not available You should run a test on each cluster to ensure the affected classes are not available. Test 1 You can run an assert check on the affected classes in a notebook. Scala Copy assert(this.getClass.getClassLoader().getResource(\"org/apache/log4j/net/JMSAppender.class\") == null)\nassert(this.getClass.getClassLoader().getResource(\"org/apache/log4j/net/SocketServer.class\") == null)\n This sample code runs successfully if you have disabled the affected classes. This sample code should return an error if you have NOT disabled the affected classes. Test 2 You can attempt to import the affected classes into a notebook. Scala Copy import org.apache.log4j.net.JMSAppender\nimport org.apache.log4j.net.SocketServer\n This sample code runs successfully if you have NOT disabled the affected classes. This sample code should return an error if you have disabled the affected classes. Caveats There are some corner cases where you can re-introduce the Log4j 1.x versions of JMSAppender or SocketServer. Problem If you install a Maven library with a transitive dependency on Log4j 1.x, all of its classes are re-added to the classpath. Solution You can work around this issue by adding Log4j to the Exclusions field when installing Maven libraries. Problem If you configure an external Apache Hive metastore, Apache Spark uses Ivy to resolve and download the correct metastore client library, and all of its transitive dependencies, possibly including Log4j 1.x. To speed up cluster launch, you can cache the downloaded jars on DBFS and use an init script to install from the cache. If you cache jars like this, it is possible that Log4j 1.x may be included. Solution You can configure the init script for your external metastore to delete the affected classes.",
          "title" : "Remove Log4j 1.x JMSAppender and SocketServer classes from classpath",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/remove-log4j1x-jmsappender-socketserver-classes"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/replace-default-jar-new-jar",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Replace a default library jar Article 03/11/2022 2 minutes to read 3 contributors In this article Identify the artifact id Use the artifact id to find the jar filename Upload the replacement jar file Create the init script Install the init script and restart Azure Databricks includes a number of default Java and Scala libraries. You can replace any of these libraries with another version by using a cluster-scoped init script to remove the default library jar and then install the version you require. Important Removing default libraries and installing new versions may cause instability or completely break your Azure Databricks cluster. You should thoroughly test any new library version in your environment before running production jobs. Identify the artifact id To identify the name of the jar file you want to remove: Click the Databricks Runtime version you are using from the list of supported releases. Navigate to the Java and Scala libraries section. Identify the Artifact ID for the library you want to remove. Use the artifact id to find the jar filename Use the ls -l command in a notebook to find the jar that contains the artifact id. For example, to find the jar filename for the spark-snowflake_2.12 artifact id in Databricks Runtime 7.0 you can use the following code: Scala Copy %sh\nls -l /databricks/jars/*spark-snowflake_2.12*\n This returns the jar filename Console Copy `----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.5.9-spark_2.4.jar`.\n Upload the replacement jar file Upload your replacement jar file to a DBFS path. Create the init script Use the following template to create a cluster-scoped init script. Scala Copy #!/bin/bash\nrm -rf /databricks/jars/<jar_filename_to_remove>.jar\ncp /dbfs/<path_to_replacement_jar>/<replacement_jar_filename>.jar /databricks/jars/\n Using the spark-snowflake_2.12 example from the prior step would result in an init script similar to the following: Scala Copy #!/bin/bash\nrm -rf /databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.5.9-spark_2.4.jar\ncp /dbfs/FileStore/jars/e43fe9db_c48d_412b_b142_cdde10250800-spark_snowflake_2_11_2_7_1_spark_2_4-b2adc.jar /databricks/jars/\n Install the init script and restart Install the cluster-scoped init script on the cluster, following the instructions in Configure a cluster-scoped init script. Restart the cluster.",
          "title" : "Replace a default library jar",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/replace-default-jar-new-jar"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/verify-log4j-version",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Verify the version of Log4j on your cluster Article 03/11/2022 3 minutes to read 3 contributors In this article Check to see if Log4j 2 is installed Upgrade your Log4j 2 version Restart your cluster after upgrading Databricks recently published a blog on Log4j 2 Vulnerability (CVE-2021-44228) Research and Assessment. Databricks does not directly use a version of Log4j known to be affected by this vulnerability within the Azure Databricks platform in a way we understand may be vulnerable. If you are using Log4j within your cluster (for example, if you are processing user-controlled strings through Log4j), your use may be potentially vulnerable to the exploit if you have installed, and are using, an affected version or have installed services that transitively depend on an affected version. This article explains how to check your cluster for installed versions of Log4j 2 and how to upgrade those instances. Important DISCLAIMER: The suggestions provided in this article reflect Databricks’s best understanding of the ways to make these determinations at this time. Because we do not control your code, we cannot guarantee that if you fail to find Log4j by following these directions or using the suggested scanners, that affected Log4j code is not present in your code. Check to see if Log4j 2 is installed Check for a manual install Manually review the libraries installed on your cluster. If you have explicitly installed a version of Log4j 2 via Maven, it is listed under Libraries in the cluster UI. Scan the classpath Scan your classpath to check for a version of Log4j 2. Start your cluster. Attach a notebook to your cluster. Run this code to scan your classpath: Scala Copy {\n  import scala.util.{Try, Success, Failure}\n  import java.lang.ClassNotFoundException\n  Try(Class.forName(\"org.apache.logging.log4j.core.Logger\", false, this.getClass.getClassLoader)) match {\n    case Success(loggerCls) =>\n      Option(loggerCls.getPackage) match {\n          case Some(pkg) =>\n            println(s\"Version: ${pkg.getSpecificationTitle} ${pkg.getSpecificationVersion}\")\n          case None =>\n            println(\"Could not determine Log4J 2 version\")\n      }\n    case Failure(e: ClassNotFoundException) =>\n      println(\"Could not load Log4J 2 class\")\n    case Failure(e) =>\n      println(s\"Unexpected Error: $e\")\n      throw e\n  }\n}\n If Log4j 2 is NOT PRESENT on your classpath, you see a result like this: Console Copy Could not load Log4J 2 class\n If Log4j 2 is PRESENT on your classpath, you should see a result like this, which includes the Log4j 2 version: Console Copy Version: Apache Log4j Core 2.15.0\n Note This method does not identify cases where Log4j classes are shaded or included transitively. Scan all user installed jars Locate all of the user installed jar files on your cluster and run a scanner to check for vulnerable Log4j 2 versions. Start your cluster. Attach a notebook to your cluster. Run this code to identify the location of the jar files: Scala Copy import org.apache.spark._\n\nval sparkEnv = SparkEnv.get\nval field = SparkEnv.get.getClass.getDeclaredField(\"driverTmpDir\")\nfield.setAccessible(true)\nprintln(s\"Your jars are installed under ${field.get(sparkEnv).asInstanceOf[Option[String]].get}\\n\")\n The code displays the location of your jar files. Console Copy Your jars are installed under /local_disk0/spark-1a6be695-9318-463c-b966-256c32e3771c/userFiles-582ca64b-93c9-444c-85b8-7779bd2c5e52\n Download the jar files to your local machine. Run a scanner like Logpresso to check for vulnerable Log4j 2 versions. Important DISCLAIMER: The Logpresso scanner is open source software provided by a third party. Databricks makes no representations of any kind regarding the function or quality of Logpresso. Upgrade your Log4j 2 version Upgrade via cluster UI If you manually installed Log4j 2 via the cluster UI, ensure that it is version 2.17 or above. In this case, no action is required. If you manually installed Log4j 2 via the cluster UI, and it is 2.16 or below, you should uninstall the library from the cluster and install version 2.17 or above. Note If Log4j 2 is a transitive dependency for another library, upgrade the library that uses Log4j 2 to a secure version. You can also exclude the Log4j 2 package when pulling in an outdated library, and explicitly include a secure version of Log4j 2. This is not guaranteed to work. Upgrade via command line If you have installed Log4j 2 via command line (or via SSH), use the same method to upgrade Log4j 2 to a secure version. Upgrade custom built jar If you include Log4j 2 in a custom built jar, upgrade Log4j 2 to a secure version and rebuild your jar. Re-attach the updated jar to your cluster. Restart your cluster after upgrading Restart your cluster after upgrading Log4j 2.",
          "title" : "Verify the version of Log4j on your cluster",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/verify-log4j-version"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/install-package-cran-snapshot",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Install package using previous CRAN snapshot Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to install a library package via CRAN, and are getting a Library installation failed for library due to infra fault error message. Console Copy Library installation failed for library due to infra fault for Some(cran {\npackage: \"<name-of-package>\"\n}\n). Error messages:\njava.lang.RuntimeException: Installation failed with message:\n\nError installing R package: Could not install package with error: installation of package <U+2018><name-of-package><U+2019> had non-zero exit status\n Cause CRAN maintains daily snapshots. If a snapshot is invalid for some reason, you get an error when trying to install a package. Solution Specify a previous snapshot when installing your library. Check the date of the most recent snapshot at https://cran.microsoft.com/snapshot/. Pick a date that is at least one day older than the most recent snapshot. For example, if the most recent snapshot is dated July 9, 2021, you should use July 8, 2021. Enter the full URL to your chosen snapshot in the Repository field when you install the package on your cluster. To use the July 8, 2021 snapshot enter https://cran.microsoft.com/snapshot/2021-07-08/ as the full URL for the repository.",
          "title" : "Install package using previous CRAN snapshot",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/install-package-cran-snapshot"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/bchashjoin-exceeds-bcjointhreshold-oom",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Broadcast join exceeds threshold, returns out of memory error Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You are attempting to join two large tables, projecting selected columns from the first table and all columns from the second table. Despite the total size exceeding the limit set by spark.sql.autoBroadcastJoinThreshold, BroadcastHashJoin is used and Apache Spark returns an OutOfMemorySparkException error. Console Copy org.apache.spark.sql.execution.OutOfMemorySparkException: Size of broadcasted table far exceeds estimates and exceeds limit of spark.driver.maxResultSize=1073741824. You can disable broadcasts for this query using set spark.sql.autoBroadcastJoinThreshold=-1\n Cause This is due to a limitation with Spark’s size estimator. If the estimated size of one of the DataFrames is less than the autoBroadcastJoinThreshold, Spark may use BroadcastHashJoin to perform the join. If the available nodes do not have enough resources to accommodate the broadcast DataFrame, your job fails due to an out of memory error. Solution There are three different ways to mitigate this issue. Use ANALYZE TABLE to collect details and compute statistics about the DataFrames before attempting a join. Cache the table you are broadcasting. Run explain on your join command to return the physical plan. SQL Copy explain(<join command>)\n Review the physical plan. If the broadcast join returns BuildLeft, cache the left side table. If the broadcast join returns BuildRight, cache the right side table. In Databricks Runtime 7.0 and above, set the join type to SortMergeJoin with join hints enabled.",
          "title" : "Broadcast join exceeds threshold, returns out of memory error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/bchashjoin-exceeds-bcjointhreshold-oom"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/azure-throttling",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Job failure due to Azure Data Lake Storage (ADLS) CREATE limits Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem When you run a job that involves creating files in Azure Data Lake Storage (ADLS), either Gen1 or Gen2, the following exception occurs: Console Copy Caused by: java.io.IOException: CREATE failed with error 0x83090c25 (Files and folders are being created at too high a rate). [745c5836-264e-470c-9c90-c605f1c100f5] failed with error 0x83090c25 (Files and folders are being created at too high a rate). [2019-04-12T10:06:43.1117197-07:00] [ServerRequestId:745c5836-264e-470c-9c90-c605f1c100f5]\nat com.microsoft.azure.datalake.store.ADLStoreClient.getRemoteException(ADLStoreClient.java:1191)\nat com.microsoft.azure.datalake.store.ADLStoreClient.getExceptionFromResponse(ADLStoreClient.java:1154)\nat com.microsoft.azure.datalake.store.ADLStoreClient.createFile(ADLStoreClient.java:281)\nat com.databricks.adl.AdlFileSystem.create(AdlFileSystem.java:348)\nat com.databricks.spark.metrics.FileSystemWithMetrics.create(FileSystemWithMetrics.scala:280)\nat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$create$1$$anonfun$apply$10$$anonfun$apply$11.apply(DatabricksFileSystemV2.scala:483)\n Cause Each ADLS subscription level has a limit on the number of files that can be created per unit of time, although the limits may differ depending on whether you are using ADLS Gen1 or Gen2. When the limit is exceeded, file creation is throttled, and the job fails. Potential causes for this error include: Your application creates a large number of small files. External applications create a large number of files. The current limit for the subscription is too low. Solution If your application or an external application is generating a large number of files, then you need to optimize the application. If the limit on your current subscription is not appropriate for your use case, then contact the Microsoft Azure Team for assistance.",
          "title" : "Job failure due to Azure Data Lake Storage (ADLS) CREATE limits",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/azure-throttling"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/cluster-failed-launch",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cluster failed to launch Article 03/11/2022 3 minutes to read 3 contributors In this article Cluster timeout Global or cluster-specific init scripts Too many libraries installed in cluster UI Cloud provider limit Cloud provider shutdown Instances unreachable This article describes several scenarios in which a cluster fails to launch, and provides troubleshooting steps for each scenario based on error messages found in logs. Cluster timeout Error messages: Console Copy Driver failed to start in time\n\nINTERNAL_ERROR: The Spark driver failed to start within 300 seconds\n\nCluster failed to be healthy within 200 seconds\n Cause The cluster can fail to launch if it has a connection to an external Hive metastore and it tries to download all the Hive metastore libraries from a maven repo. A cluster downloads almost 200 JAR files, including dependencies. If the Azure Databricks cluster manager cannot confirm that the driver is ready within 5 minutes, then cluster launch fails. This can occur because JAR downloading is taking too much time. Solution Store the Hive libraries in DBFS and access them locally from the DBFS location. See Spark Options. Global or cluster-specific init scripts Error message: Console Copy The cluster could not be started in 50 minutes. Cause: Timed out with exception after <xxx> attempts\n Cause Init scripts that run during the cluster spin-up stage send an RPC (remote procedure call) to each worker machine to run the scripts locally. All RPCs must return their status before the process continues. If any RPC hits an issue and doesn’t respond back (due to a transient networking issue, for example), then the 1-hour timeout can be hit, causing the cluster setup job to fail. Solution Use a cluster-scoped init script instead of global or cluster-named init scripts. With cluster-scoped init scripts, Azure Databricks does not use synchronous blocking of RPCs to fetch init script execution status. Too many libraries installed in cluster UI Error message: Console Copy Library installation timed out after 1800 seconds. Libraries that are not yet installed:\n Cause This is usually an intermittent problem due to network problems. Solution Usually you can fix this problem by re-running the job or restarting the cluster. The library installer is configured to time out after 3 minutes. While fetching and installing jars, a timeout can occur due to network problems. To mitigate this issue, you can download the libraries from maven to a DBFS location and install it from there. Cloud provider limit Error message: Console Copy Cluster terminated. Reason: Cloud Provider Limit\n Cause This error is usually returned by the cloud provider. Solution See the cloud provider error information in cluster unexpected termination. Cloud provider shutdown Error message: Console Copy Cluster terminated. Reason: Cloud Provider Shutdown\n Cause This error is usually returned by the cloud provider. Solution See the cloud provider error information in cluster unexpected termination. Instances unreachable Error message: Console Copy Cluster terminated. Reason: Instances Unreachable\n\nAn unexpected error was encountered while setting up the cluster. Please retry and contact Azure Databricks if the problem persists. Internal error message: Timeout while placing node\n Cause This error is usually returned by the cloud provider. Typically, it occurs when you have an Azure Databricks workspace deployed to your own virtual network (VNet) (as opposed to the default VNet created when you launch a new Azure Databricks workspace). If the virtual network where the workspace is deployed is already peered or has an ExpressRoute connection to on-premises resources, the virtual network cannot make an ssh connection to the cluster node when Azure Databricks is attempting to create a cluster. Solution Add a user-defined route (UDR) to give the Azure Databricks control plane ssh access to the cluster instances, Blob Storage instances, and artifact resources. This custom UDR allows outbound connections and does not interfere with cluster creation. For detailed UDR instructions, see Step 3: Create user-defined routes and associate them with your Azure Databricks virtual network subnets. For more VNet-related troubleshooting information, see Troubleshooting.",
          "title" : "Cluster failed to launch",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/cluster-failed-launch"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/dbfs-file-size-limit",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Reading large DBFS-mounted files using Python APIs Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution This article explains how to resolve an error that occurs when you read large DBFS-mounted files using local Python APIs. Problem If you mount a folder onto dbfs:// and read a file larger than 2GB in a Python API like pandas, you will see following error: Console Copy /databricks/python/local/lib/python2.7/site-packages/pandas/parser.so in pandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)()\n/databricks/python/local/lib/python2.7/site-packages/pandas/parser.so in pandas.parser.TextReader._setup_parser_source (pandas/parser.c:6883)()\nIOError: Initializing from file failed\n Cause The error occurs because one argument in the Python method to read a file is a signed int, the length of the file is an int, and if the object is a file larger than 2GB, the length can be larger than maximum signed int. Solution Move the file from dbfs:// to local file system (file://). Then read using the Python API. For example: Copy the file from dbfs:// to file://: Bash Copy %fs cp dbfs:/mnt/large_file.csv file:/tmp/large_file.csv\n Read the file in the pandas API: Python Copy import pandas as pd\npd.read_csv('file:/tmp/large_file.csv',).head()",
          "title" : "Reading large DBFS-mounted files using Python APIs",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/dbfs-file-size-limit"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/python-2-eol",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Python 2 sunset status Article 03/11/2022 2 minutes to read 3 contributors In this article Databricks Runtime 6.0 and above Databricks Runtime 5.5 LTS Should I upgrade to Python 3? Support Python.org officially moved Python 2 into EoL (end-of-life) status on January 1, 2020. What does this mean for you? Databricks Runtime 6.0 and above Databricks Runtime 6.0 and above support only Python 3. You cannot create a cluster with Python 2 using these runtimes. Any clusters created with these runtimes use Python 3 by definition. Databricks Runtime 5.5 LTS When you create a Databricks Runtime 5.5 LTS cluster by using the workspace UI, the default is Python 3. You have the option to specify Python 2. If you use the Databricks REST API to create a cluster using Databricks Runtime 5.5 LTS, the default is Python 2. If you have a Databricks Runtime 5.5 LTS cluster running Python 2, you are not required to upgrade to Python 3. You can use the following call to specify Python 3 when you create a cluster using the Databricks REST API. Copy \"spark_env_vars\": {\n  \"PYSPARK_PYTHON\": \"/databricks/python3/bin/python3\"\n},\n Should I upgrade to Python 3? The decision to upgrade depends on your specific circumstances, including reliance on other systems and dependencies. This is a decision that should be made in conjunction with your engineering organization. The official Python.org statement is as follows: As of January 1st, 2020 no new bug reports, fixes, or changes will be made to Python 2, and Python 2 is no longer supported. We have not yet released the few changes made between when we released Python 2.7.17 (on October 19th, 2019) and January 1st. As a service to the community, we will bundle those fixes (and only those fixes) and release a 2.7.18. We plan on doing that in April 2020, because that’s convenient for the release managers, not because it implies anything about when support ends. Support Azure Databricks does not offer official support for discontinued third-party software. Support requests related to Python 2 are not eligible for engineering support.",
          "title" : "Python 2 sunset status",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/python-2-eol"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/python-cmd-fail-high-con-cluster",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Python commands fail on high concurrency clusters Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are attempting to run Python commands on a high concurrency cluster. All Python commands fail with a WARN error message. Console Copy WARN PythonDriverWrapper: Failed to start repl ReplId-61bef-9fc33-1f8f6-2\nExitCodeException exitCode=1: chown: invalid user: ‘spark-9fcdf4d2-045d-4f3b-9293-0f’\n Cause Both spark.databricks.pyspark.enableProcessIsolation true and spark.databricks.session.share true are set in the Apache Spark configuration on the cluster. These two Spark properties conflict with each other and prevent the cluster from running Python commands. Solution You can only have one of these two Spark properties enabled on your cluster at a time. You must choose process isolation or a Spark shared session based on your needs. Disable the other option.",
          "title" : "Python commands fail on high concurrency clusters",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/python-cmd-fail-high-con-cluster"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/python-cmd-fails-tornado-version",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cluster cancels Python command execution after installing Bokeh Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem The cluster returns Cancelled in a Python notebook. Inspect the driver log (std.err) in the Cluster Configuration page for a stack trace and error message similar to the following: Console Copy log4j:WARN No appenders could be found for logger (com.databricks.conf.trusted.ProjectConf$).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See https://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nOpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0\nTraceback (most recent call last):\n  File \"/local_disk0/tmp/1551693540856-0/PythonShell.py\", line 30, in <module>\n    from IPython.nbconvert.filters.ansi import ansi2html\n  File \"/databricks/python/lib/python3.5/site-packages/IPython/nbconvert/__init__.py\", line 6, in <module>\n    from . import postprocessors\n  File \"/databricks/python/lib/python3.5/site-packages/IPython/nbconvert/postprocessors/__init__.py\", line 6, in <module>\n    from .serve import ServePostProcessor\n  File \"/databricks/python/lib/python3.5/site-packages/IPython/nbconvert/postprocessors/serve.py\", line 29, in <module>\n    class ProxyHandler(web.RequestHandler):\n  File \"/databricks/python/lib/python3.5/site-packages/IPython/nbconvert/postprocessors/serve.py\", line 31, in ProxyHandler\n    @web.asynchronous\nAttributeError: module 'tornado.web' has no attribute 'asynchronous'\n Cause When you install the bokeh library, by default tornado version 6.0a1 is installed, which is an alpha release. The alpha release causes this error, so the solution is to revert to the stable version of tornado. Solution Follow the steps below to create a cluster-scoped init script. The init script removes the newer version of tornado and installs the stable version. If the init script does not already exist, create a base directory to store it: Bash Copy dbutils.fs.mkdirs(\"dbfs:/databricks/<directory>/\")\n Create the following script: Bash Copy dbutils.fs.put(\"dbfs:/databricks/<directory>/tornado.sh\",\"\"\"\n#!/bin/bash\npip uninstall --yes tornado\nrm -rf /home/ubuntu/databricks/python/lib/python3.5/site-packages/tornado*\nrm -rf /databricks/python/lib/python3.5/site-packages/tornado*\n/usr/bin/yes | /home/ubuntu/databricks/python/bin/pip install tornado==5.1.1\n\"\"\",True)\n Confirm that the script exists: Bash Copy display(dbutils.fs.ls(\"dbfs:/databricks/<directory>/tornado.sh\"))\n Go to the cluster configuration page and click the Advanced Options toggle. At the bottom of the page, click the Init Scripts tab: In the Destination drop-down, select DBFS, provide the file path to the script, and click Add. Restart the cluster. For more information, see: CI failures with tornado 6.0a1 Convert proxy handler from callback to coroutine Cluster-scoped init scripts",
          "title" : "Cluster cancels Python command execution after installing Bokeh",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/python-cmd-fails-tornado-version"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/python-exec-display-cancelled",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Python command execution fails with AttributeError Article 03/11/2022 3 minutes to read 3 contributors In this article Problem: 'tuple' object has no attribute 'type' Problem: module 'lib' has no attribute 'SSL_ST_INIT' This article can help you resolve scenarios in which Python command execution fails with an AttributeError. Problem: 'tuple' object has no attribute 'type' When you run a notebook, Python command execution fails with the following error and stack trace: Console Copy AttributeError: 'tuple' object has no attribute 'type'\n Console Copy Traceback (most recent call last):\nFile \"/local_disk0/tmp/1547561952809-0/PythonShell.py\", line 23, in <module>\n  import matplotlib as mpl\nFile \"/databricks/python/local/lib/python2.7/site-packages/matplotlib/__init__.py\", line 122, in <module>\n  from matplotlib.cbook import is_string_like, mplDeprecation, dedent, get_label\nFile \"/databricks/python/local/lib/python2.7/site-packages/matplotlib/cbook.py\", line 33, in <module>\n  import numpy as np\nFile \"/databricks/python/local/lib/python2.7/site-packages/numpy/__init__.py\", line 142, in <module>\n  from . import core\nFile \"/databricks/python/local/lib/python2.7/site-packages/numpy/core/__init__.py\", line 57, in <module>\n  from . import numerictypes as nt\nFile \"/databricks/python/local/lib/python2.7/site-packages/numpy/core/numerictypes.py\", line 111, in <module>\n  from ._type_aliases import (\nFile \"/databricks/python/local/lib/python2.7/site-packages/numpy/core/_type_aliases.py\", line 63, in <module>\n  _concrete_types = {v.type for k, v in _concrete_typeinfo.items()}\nFile \"/databricks/python/local/lib/python2.7/site-packages/numpy/core/_type_aliases.py\", line 63, in <setcomp>\n  _concrete_types = {v.type for k, v in _concrete_typeinfo.items()}\nAttributeError: 'tuple' object has no attribute 'type'\n\n19/01/15 11:29:26 WARN PythonDriverWrapper: setupRepl:ReplId-7d8d1-8cc01-2d329-9: at the end, the status is\nError(ReplId-7d8d1-8cc01-2d329-,com.databricks.backend.daemon.driver.PythonDriverLocal$PythonException: Python shell failed to start in 30 seconds)\n Cause A newer version of numpy (1.16.1), which is installed by default by some PyPI clients, is incompatible with other libraries. Solution Follow the steps below to create a cluster-scoped init script that removes the current version and installs version 1.15.0 of numpy. If the init script does not already exist, create a base directory to store it: Python Copy dbutils.fs.mkdirs(\"dbfs:/databricks/<directory>/\")\n Create the following script: If the cluster is running Python 2, use this init script: Python Copy dbutils.fs.put(\"dbfs:/databricks/<directory>/numpy.sh\",\"\"\"\n#!/bin/bash\npip uninstall --yes numpy\nrm -rf /home/ubuntu/databricks/python/lib/python2.7/site-packages/numpy*\nrm -rf /databricks/python/lib/python2.7/site-packages/numpy*\n/usr/bin/yes | /home/ubuntu/databricks/python/bin/pip install numpy==1.15.0\n\"\"\",True)\n If the cluster is running Python 3, use this init script: Python Copy dbutils.fs.put(\"dbfs:/databricks/<directory>/numpy.sh\",\"\"\"\n#!/bin/bash\npip uninstall --yes numpy\nrm -rf /home/ubuntu/databricks/python/lib/python3.5/site-packages/numpy*\nrm -rf /databricks/python/lib/python3.5/site-packages/numpy*\n/usr/bin/yes | /home/ubuntu/databricks/python/bin/pip install numpy==1.15.0\n\"\"\",True)\n Confirm that the script exists: Python Copy display(dbutils.fs.ls(\"dbfs:/databricks/<directory>/numpy.sh\"))\n Go to the cluster configuration page and click the Advanced Options toggle. At the bottom of the page, click the Init Scripts tab: In the Destination drop-down, select DBFS, provide the file path to the script, and click Add. Restart the cluster. In your PyPI client, pin the numpy installation to version 1.15.1, the latest working version. Problem: module 'lib' has no attribute 'SSL_ST_INIT' When you run a notebook, library installation fails and all Python commands executed on the notebook are cancelled with the following error and stack trace: Console Copy AttributeError: module 'lib' has no attribute 'SSL_ST_INIT'\n Console Copy Traceback (most recent call last): File \"/databricks/python3/bin/pip\", line 7, in <module>\n from pip._internal import main\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_internal/__init__.py\", line 40, in <module>\n from pip._internal.cli.autocompletion import autocomplete\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_internal/cli/autocompletion.py\", line 8, in <module>\n from pip._internal.cli.main_parser import create_main_parser\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_internal/cli/main_parser.py\", line 12, in <module>\n from pip._internal.commands import (\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_internal/commands/__init__.py\", line 6, in <module>\n from pip._internal.commands.completion import CompletionCommand\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_internal/commands/completion.py\", line 6, in <module>\n from pip._internal.cli.base_command import Command\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_internal/cli/base_command.py\", line 20, in <module>\n from pip._internal.download import PipSession\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_internal/download.py\", line 15, in <module>\n from pip._vendor import requests, six, urllib3\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_vendor/requests/__init__.py\", line 97, in <module>\n from pip._vendor.urllib3.contrib import pyopenssl\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 46, in <module>\n import OpenSSL.SSL\nFile \"/databricks/python3/lib/python3.5/site-packages/OpenSSL/__init__.py\", line 8, in <module>\n from OpenSSL import rand, crypto, SSL\nFile \"/databricks/python3/lib/python3.5/site-packages/OpenSSL/SSL.py\", line 124, in <module>\n SSL_ST_INIT = _lib.SSL_ST_INIT AttributeError: module 'lib' has no attribute 'SSL_ST_INIT'\n Cause A newer version of the cryptography package (in this case, 2.7) was installed by default along with another PyPI library, and this cryptography version is incompatible with the version of pyOpenSSL included in Databricks Runtimes. Solution To resolve and prevent this issue, upgrade pyOpenSSL to the most recent version before you install any library. Use a cluster-scoped init script to install the most recent version of pyOpenSSL: Create a base directory to store the init script: Python Copy dbutils.fs.mkdirs(\"dbfs:/databricks/<directory>/\")\n Create the following script: Python Copy dbutils.fs.put(\"dbfs:/databricks/<directory>/openssl_fix.sh\",\"\"\"\n#!/bin/bash\necho \"Removing pyOpenSSL package\"\nrm -rf /databricks/python2/lib/python2.7/site-packages/OpenSSL\nrm -rf /databricks/python2/lib/python2.7/site-packages/pyOpenSSL-16.0.0-*.egg-info\nrm -rf /databricks/python3/lib/python3.5/site-packages/OpenSSL\nrm -rf /databricks/python3/lib/python3.5/site-packages/pyOpenSSL-16.0.0*.egg-info\n/databricks/python2/bin/pip install pyOpenSSL==19.0.0\n/databricks/python3/bin/pip3 install pyOpenSSL==19.0.0\n\"\"\", True)\n Confirm that the script exists: Python Copy display(dbutils.fs.ls(\"dbfs:/databricks/<directory>/openssl_fix.sh\"))\n Go to the cluster configuration page and click the Advanced Options toggle. At the bottom of the page, click the Init Scripts tab: In the Destination drop-down, select DBFS, provide the file path to the script, and click Add. Restart the cluster.",
          "title" : "Python command execution fails with AttributeError",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/python-exec-display-cancelled"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents R with Apache Spark: tips and troubleshooting Article 03/11/2022 2 minutes to read 4 contributors In this article These articles can help you to use R with Apache Spark. Change version of R (r-base) Fix the version of R packages How to parallelize R code with gapply How to parallelize R code with spark.lapply How to persist and share code in RStudio Install rJava and RJDBC libraries Rendering an R markdown file containing sparklyr code fails Resolving package or namespace loading error RStudio server backend connection error Verify R packages installed via init script",
          "title" : "R with Apache Spark: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/namespace-onload",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Resolving package or namespace loading error Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution This article explains how to resolve the package or namespace loading error. Problem When you install and load some libraries in a notebook cell, like: R Copy library(BreakoutDetection)\n You may observe the following error: Console Copy Loading required package: BreakoutDetection:\n\nError : package or namespace load failed for ‘BreakoutDetection’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\nnamespace ‘rlang’ 0.3.1 is already loaded, but >= 0.3.4 is required\n Cause While a notebook is attached to a cluster, the R namespace cannot be refreshed. When an R package depends on a newer package version, the required package is downloaded but not loaded. When you load the package, you can observe this error. Solution To resolve this error, install the required package as a cluster-installed library.",
          "title" : "Resolving package or namespace loading error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/namespace-onload"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/rmarkdown-sparklyr-code",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Rendering an R markdown file containing sparklyr code fails Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem After you install and configure RStudio in the Azure Databricks environment, when you launch RStudio and click the Knit button to knit a Markdown file that contains code to initialize a sparklyr context, rendering fails with the following error: Console Copy failed to start sparklyr backend:object 'DATABRICKS_GUID' not found Calls: <Anonymous>… tryCatch -> tryCatchList-> tryCatchOne -> <Anonymous> Execution halted\n Cause If you try to initialize a sparklyr context in a Markdown notebook with code similar to the following, the Markdown page fails to render because the knitr process spawns a new namespace that is missing the 'DATABRICKS_GUID' global variable. R Copy case library(sparklyr)\nsc <- spark_connect(method = \"databricks\")\n Solution Instead of rendering the Markdown page by clicking the Knit button in the R Markdown console, use the following script: R Copy rmarkdown::render(\"your_doc.Rmd\")\n Render the Markdown file using the R console, and then you can access the file in the RStudio Files tab.",
          "title" : "Rendering an R markdown file containing sparklyr code fails",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/rmarkdown-sparklyr-code"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/rstudio-server-backend-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents RStudio server backend connection error Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You get a backend connection error when using RStudio server. Console Copy Error in Sys.setenv(EXISTING_SPARKR_BACKEND_PORT = system(paste0(\"wget -qO - 'http://localhost:6061/?type=\\\"com.databricks.backend.common.rpc.DriverMessages$StartRStudioSparkRBackend\\\"' --post-data='{\\\"@class\\\":\\\"com.databricks.backend.common.rpc.DriverMessages$StartRStudioSparkRBackend\\\", \\\"guid\\\": \\\"\", :\nwrong length for argument\n If you view the cluster driver and worker logs, you see a message about exceeding the maximum number of RBackends. Console Copy 21/08/09 15:02:26 INFO RDriverLocal: 312. RDriverLocal.3f6d80d6-70c4-4101-b50f-2530df112ea2: Exceeded maximum number of RBackends limit: 200\n21/08/09 15:03:55 INFO RDriverLocal: 313. RDriverLocal.3f6d80d6-70c4-4101-b50f-2530df112ea2: Exceeded maximum number of RBackends limit: 200\n21/08/09 15:04:06 INFO RDriverLocal: 314. RDriverLocal.3f6d80d6-70c4-4101-b50f-2530df112ea2: Exceeded maximum number of RBackends limit: 200\n21/08/09 15:13:42 INFO RDriverLocal: 315. RDriverLocal.3f6d80d6-70c4-4101-b50f-2530df112ea2: Exceeded maximum number of RBackends limit: 200\n Cause Azure Databricks clusters are configured for 200 RBackends by default. If you exceed this limit, you get an error. Solution You can use an init script to increase the soft limit of RBackends available for use. This sample code creates an init script that sets a limit of 400 RBackends on the cluster. Scala Copy %scala\nval initScriptContent = s\"\"\"\n |#!/bin/bash\n |cat > /databricks/common/conf/rbackend_limit.conf << EOL\n |{\n | databricks.daemon.driver.maxNumRBackendsPerDriver = 400\n |}\n |EOL\n\"\"\".stripMargin\n\ndbutils.fs.put(\"dbfs:/databricks/<init-script-folder>/set_rbackend.sh\",initScriptContent, true)\n Note The sample code sets the RBackends limit to 400. You can adjust this number as needed. You should not exceed 500 RBackends. Install the newly created init script as a cluster-scoped init script. You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/set_rbackend.sh). Restart the cluster after you have installed the init script. Validate solution You can confirm that the changes were successful by running this sample code in a notebook. R Copy library(magrittr)\nSparkR:::callJStatic(\n  \"com.databricks.backend.daemon.driver.RDriverLocal\",\n  \"getDriver\",\n  get(DB_GUID_, envir = .GlobalEnv)) %>% SparkR:::callJMethod(\"conf\") %>% SparkR:::callJMethod(\"maxNumRBackendsPerDriver\")\n When run, this code returns the current RBackends limit on the cluster. Best practices Ensure that you log out of RStudio when you are finished using it. This terminates the R session and cleans the RBackend. If the RStudio server is killed, or the RSession terminates unexpectedly, the cleanup step may not happen. Databricks Runtime 9.0 and above automatically cleans up idle RBackend sessions.",
          "title" : "RStudio server backend connection error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/rstudio-server-backend-error"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/verify-r-packages-installed-init",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Verify R packages installed via init script Article 03/11/2022 2 minutes to read 2 contributors In this article List installed packages List packages that did not install When you configure R packages to install via an init script, it is possible for a package install to fail if dependencies are not installed. You can use the R commands in a notebook to check that all of the packages correctly installed. Note This article does require you to provide a list of packages to check against. List installed packages Make a list of all R package names that you have listed in your init script or scripts. Enter the list of packages in this sample code. R Copy my_packages <- list(\"<package-1>\", \"<package-2>\", \"<package-3>\" )\nfind.package(my_packages, quiet=TRUE)\n The output is a list of all installed packages. Verify the output against the input list to ensure that all packages were successfully installed. List packages that did not install Make a list of all R package names that you have listed in your init script or scripts. Enter the list of packages in this sample code. R Copy my_packages <- c(\"<package-1>\", \"<package-2>\", \"<package-3>\" )\nnot_installed <- my_packages[!(my_packages %in% installed.packages()[ , \"Package\"])]\nprint(not_installed)\n The output is a list of all packages that failed to install. If you have packages that are consistently failing to install, you should enable cluster log delivery and review the cluster logs for failures.",
          "title" : "Verify R packages installed via init script",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/r/verify-r-packages-installed-init"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/administration/who-deleted-cluster",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to discover who deleted a cluster in Azure portal Article 03/11/2022 2 minutes to read 3 contributors In this article If a cluster in your workspace has disappeared or been deleted, you can identify which user deleted it by running a query in the Log Analytics workspaces service in the Azure portal. Note If you do not have an analytics workspace set up, you must configure Diagnostic Logging in Azure Databricks before you continue. Load the Log Analytics workspaces service in the Azure portal. Click the name of your workspace. Click Logs. Look for the following text: Type your query here or click one of the example queries to start. Enter the following query: Copy DatabricksClusters\n| where ActionName == \"permanentDelete\"\n     and Response contains \"\\\"statusCode\\\":200\"\n     and RequestParams contains \"\\\"cluster_id\\\":\\\"0210-024915-bore731\\\"\"  // Add cluster_id filter if cluster id is known\n     and TimeGenerated between(datetime(\"2020-01-25 00:00:00\") .. datetime(\"2020-01-28 00:00:00\"))  // Add timestamp (in UTC) filter to narrow down the result.\n| extend id = parse_json(Identity)\n| extend requestParams = parse_json(RequestParams)\n| project UserEmail=id.email,clusterId = requestParams.cluster_id, SourceIPAddress, EventTime=TimeGenerated\n Edit the cluster_id as required. Edit the datetime values to filter on a specific time range. Click Run to execute the query. The results (if any) display below the query box. If you are still unable to find who deleted the cluster, create a support case with Microsoft Support. Provide details such as the workspace id and the time range of the event (including your time zone). Microsoft Support will review the corresponding backend activity logs.",
          "title" : "How to discover who deleted a cluster in Azure portal",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/administration/who-deleted-cluster"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/azure-vnet-single-ip",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to assign a single public IP for VNet-injected workspaces using Azure Firewall Article 03/11/2022 2 minutes to read 3 contributors In this article You can use an Azure Firewall to create a VNet-injected workspace in which all clusters have a single IP outbound address. The single IP address can be used as an additional security layer with other Azure services and applications that allow access based on specific IP addresses. Set up an Azure Databricks Workspace in your own virtual network. Set up a firewall within the virtual network. See Create an NVA. When you create the firewall, you should: Note both the private and public IP addresses for the firewall for later use. Create a network rule for the public subnet to forward all traffic to the internet: Name: any arbitrary name Priority: 100 Protocol: Any Source Addresses: IP range for the public subnet in the virtual network that you created Destination Addresses: 0.0.0.0/1 Destination Ports: * Create a Custom Route Table and associate it with the public subnet. Add custom routes, also known as user-defined routes (UDR) for the following services. Specify the Azure Databricks region addresses for your region. For Next hop type, enter Internet, as shown in creating a route table. Control Plane NAT VIP Webapp Metastore Artifact Blob Storage Logs Blob Storage Add a custom route for the firewall with the following values: Address prefix: 0.0.0.0./0 Next hop type: Virtual appliance Next hop address: The private IP address for the firewall. Associate the route table with the public subnet. Validate the setup Create a cluster in the Azure Databricks workspace. Next, query blob storage to your own paths or run %fs ls in a cell. If it fails, confirm that the route table has all required UDRs (including Service Endpoint instead of the UDR for Blob Storage) For more information, see Route Azure Databricks traffic using a virtual appliance or firewall.",
          "title" : "How to assign a single public IP for VNet-injected workspaces using Azure Firewall",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/azure-vnet-single-ip"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/har-log-analysis",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to analyze user interface performance issues Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Troubleshooting procedure Problem The Azure Databricks user interface seems to be running slowly. Cause User interface performance issues typically occur due to network latency or a database query taking more time than expected. In order to troubleshoot this type of problem, you need to collect network logs and analyze them to see which network traffic is affected. In most cases, you will need the assistance of Databricks Support to identify and resolve issues with Databricks user interface performance, but you can also analyze the logs yourself with a tool such as G Suite Toolbox HAR Analyzer. This tool helps you analyze the logs and identify the exact API and the time taken for each request. Troubleshooting procedure This is the procedure for Google Chrome. For other browsers, see G Suite Toolbox HAR Analyzer. Open Google Chrome and go to the page where the issue occurs. In the Chrome menu bar, select View > Developer > Developer Tools. In the panel at the bottom of your screen, select the Network tab. Look for a round Record button in the upper left corner of the Network tab, and make sure it is red. If it is grey, click it once to start recording. Check the box next to Preserve log. Click Clear to clear out any existing logs from the Network tab. Reproduce the issue while the network requests are being recorded. After you reproduce and record the issue, right-click anywhere on the grid of network requests to open a context menu, select Save all as HAR with Content, and save the file to your computer. Analyze the file using the HAR Analyzer tool. If this analysis does not resolve the problem, open a support ticket and upload the HAR file or attach it to your email so that Databricks can analyze it. Example output from HAR Analyzer",
          "title" : "How to analyze user interface performance issues",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/har-log-analysis"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/sas-requires-current-abfs-client",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents SAS requires current ABFS client Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem While using SAS token authentication, you encounter an IllegalArgumentException error. Console Copy IllegalArgumentException: No enum constant shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AuthType.SAS\n Cause SAS requires the current ABFS client. Previous ABFS clients do not support SAS. Solution You must use the current ABFS client (shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem) to use SAS. This ABFS client is available by default in Databricks Runtime 7.3 LTS and above. If you are using an old ABFS client, you should update your code so it references the current ABFS client.",
          "title" : "SAS requires current ABFS client",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/cloud/sas-requires-current-abfs-client"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/azure-ssh-cluster-driver-node",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents SSH to the cluster driver node Article 03/11/2022 2 minutes to read 2 contributors In this article Configure an Azure network security group Generate SSH key pair Configure a new cluster with your public key Configure an existing cluster with your public key SSH into the Spark driver This article explains how to use SSH to connect to an Apache Spark driver node for advanced troubleshooting and installing custom software. Important You can only use SSH if your workspace is deployed in an Azure Virtual Network (VNet) under your control. If your workspace is NOT VNet injected, the SSH option will not appear. Configure an Azure network security group The network security group associated with your VNet must allow SSH traffic. The default port for SSH is 2200. If you are using a custom port, you should make note of it before proceeding. You also have to identify a traffic source. This can be a single IP address, or it can be an IP range that represents your entire office. In the Azure portal, find the network security group. The network security group name can be found in the public subnet. Edit the inbound security rules to allow connections to the SSH port. In this example, we are using the default port. Note Make sure that your computer and office firewall rules allow you to send TCP traffic on the port you are using for SSH. If the SSH port is blocked at your computer or office firewall, you cannot connect to the Azure VNet via SSH. Generate SSH key pair Open a local terminal. Create an SSH key pair by running this command: ssh-keygen -t rsa -b 4096 -C Note You must provide the path to the directory where you want to save the public and private key. The public key is saved with the extension .pub. Configure a new cluster with your public key Copy the ENTIRE contents of the public key file. Open the cluster configuration page. Click Advanced Options. Click the SSH tab. Paste the ENTIRE contents of the public key into the Public key field. Continue with cluster configuration as normal. Configure an existing cluster with your public key If you have an existing cluster and did not provide the public key during cluster creation, you can inject the public key from a notebook. Open any notebook that is attached to the cluster. Copy the following code into the notebook, updating it with your public key as noted: Scala Copy val publicKey = \"<put your public key here>\"\n\ndef addAuthorizedPublicKey(key: String): Unit = {\n  val fw = new java.io.FileWriter(\"/home/ubuntu/.ssh/authorized_keys\", /* append */ true)\n  fw.write(\"\\n\" + key)\n  fw.close()\n}\naddAuthorizedPublicKey(publicKey)\n Run the code block to inject the public key. SSH into the Spark driver Open the cluster configuration page. Click Advanced Options. Click the SSH tab. Note the Driver Hostname. Open a local terminal. Run the following command, replacing the hostname and private key file path: ssh ubuntu@<hostname> -p 2200 -i <private-key-file-path>",
          "title" : "SSH to the cluster driver node",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/azure-ssh-cluster-driver-node"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/calculate-number-of-cores",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to calculate the number of cores in a cluster Article 03/11/2022 2 minutes to read 3 contributors In this article If your organization has installed a metrics service on your cluster nodes, you can view the number of cores in an Azure Databricks cluster in the Workspace UI using the Metrics tab on the cluster details page. If the driver and executors are of the same node type, you can also determine the number of cores available in a cluster programmatically, using Scala utility code: Use sc.statusTracker.getExecutorInfos.length to get the total number of nodes. The result includes the driver node, so subtract 1. Use java.lang.Runtime.getRuntime.availableProcessors to get the number of cores per node. Multiply both results (subtracting 1 from the total number of nodes) to get the total number of cores available: Scala Copy java.lang.Runtime.getRuntime.availableProcessors * (sc.statusTracker.getExecutorInfos.length -1)",
          "title" : "How to calculate the number of cores in a cluster",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/calculate-number-of-cores"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/cluster-manager-limit",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Job fails due to cluster manager core instance request limit Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem An Azure Databricks Notebook or Job API returns the following error: Console Copy Unexpected failure while creating the cluster for the job. Cause REQUEST_LIMIT_EXCEEDED: Your request was rejected due to API rate limit. Please retry your request later, or choose a larger node type instead.\n Cause The error indicates the Cluster Manager Service core instance request limit was exceeded. A Cluster Manager core instance can support a maximum of 1000 requests. Solution Contact Azure Databricks Support to increase the limit set in the core instance. Azure Databricks can increase the job limit maxBurstyUpsizePerOrg up to 2000, and upsizeTokenRefillRatePerMin up to 120. Current running jobs are affected when the limit is increased. Increasing these values can stop the throttling issue, but can also cause high CPU utilization. The best solution for this issue is to replace the Cluster Manager core instance with a larger instance that can support maximum data transmission rates. Azure Databricks Support can change the current Cluster Manager instance type to a larger one.",
          "title" : "Job fails due to cluster manager core instance request limit",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/cluster-manager-limit"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/cluster-terminated-driver-down",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cluster fails to start with dummy does not exist error Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You try to start a cluster, but it fails to start. You get an Apache Spark error message. Console Copy Internal error message: Spark error: Driver down\n You review the cluster driver and worker logs and see an error message containing java.io.FileNotFoundException: File file:/databricks/driver/dummy does not exist. Console Copy 21/07/14 21:44:06 ERROR DriverDaemon$: XXX Fatal uncaught exception. Terminating driver.\njava.io.FileNotFoundException: File file:/databricks/driver/dummy does not exist\n   at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\n   at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\n   at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\n   at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\n   at org.apache.spark.SparkContext.addFile(SparkContext.scala:1668)\n   at org.apache.spark.SparkContext.addFile(SparkContext.scala:1632)\n   at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:511)\n   at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:511)\n   at scala.collection.immutable.List.foreach(List.scala:392)\n Cause You have spark.files dummy set in your Spark Config, but no such file exists. Spark interprets the dummy configuration value as a valid file path and tries to find it on the local file system. If the file does not exist, it generates the error message. Console Copy java.io.FileNotFoundException: File file:/databricks/driver/dummy does not exist\n Solution Option 1: Delete spark.files dummy from your Spark Config if you are not passing actual files to Spark. Option 2: Create a dummy file and place it on the cluster. You can do this with an init script. Create the init script. Python Copy dbutils.fs.put(\"dbfs:/databricks/<init-script-folder>/create_dummy_file.sh\",\n\"\"\"\n#!/bin/bash\ntouch /databricks/driver/dummy\"\"\", True)\n Install the init script that you just created as a cluster-scoped init script. You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/create_dummy_file.sh). Restart the cluster Restart your cluster after you have installed the init script.",
          "title" : "Cluster fails to start with dummy does not exist error",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/cluster-terminated-driver-down"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/conf-overwrites-default-settings",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Adding a configuration setting overwrites all default spark.executor.extraJavaOptions settings Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Version Cause Solution Problem When you add a configuration setting by entering it in the Apache Spark Config text area, the new setting replaces existing settings instead of being appended. Version Databricks Runtime 5.1 and below. Cause When the cluster restarts, the cluster reads settings from a configuration file that is created in the Clusters UI, and overwrites the default settings. For example, when you add the following extraJavaOptions to the Spark Config text area: Java Copy spark.executor.extraJavaOptions -\njavaagent:/opt/prometheus_jmx_exporter/jmx_prometheus_javaagent.jar=9404:/opt/prometheus\n_jmx_exporter/jmx_prometheus_javaagent.yml\n Then, in Spark UI > Environment > Spark Properties under spark.executor.extraJavaOptions, only the newly added configuration setting shows: Java Copy -javaagent:/opt/prometheus_jmx_exporter/jmx_prometheus_javaagent.jar=9404:/opt/prometheus\n_jmx_exporter/jmx_prometheus_javaagent.yml\n Any existing settings are removed. For reference, the default settings are: Java Copy -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -\nXX:+UseCodeCacheFlushing -Ddatabricks.serviceName=spark-executor-1 -\nDjava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -\nXX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -\nDjavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.Dataty\npeFactoryImpl -\nDjavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.Documen\ntBuilderFactoryImpl -\nDjavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFact\noryImpl -\nDjavax.xml.validation.SchemaFactory=https://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -\nDorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -\nDorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMX\nSImplementationSourceImpl\n Solution To add a new configuration setting to spark.executor.extraJavaOptions without losing the default settings: In Spark UI > Environment > Spark Properties, select and copy all of the properties set by default for spark.executor.extraJavaOptions. Click Edit. In the Spark Config text area (Clusters > cluster-name > Advanced Options > Spark), paste the default settings. Append the new configuration setting below the default settings. Click outside the text area, then click Confirm. Restart the cluster. For example, let’s say you paste the following settings into the Spark Config text area. The new configuration setting is appended to the default settings. Java Copy spark.executor.extraJavaOptions = -Djava.io.tmpdir=/local_disk0/tmp -\nXX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Ddatabricks.serviceName=spark-\nexecutor-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -\nXX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -\nDjavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.Dataty\npeFactoryImpl -\nDjavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentB\nuilderFactoryImpl -\nDjavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactor\nyImpl -\nDjavax.xml.validation.SchemaFactory:https://www.w3.org/2001/XMLSchema=com.sun.org.apache.xer\nces.internal.jaxp.validation.XMLSchemaFactory -\nDorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -\nDorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplem\nentationSourceImpl -\njavaagent:/opt/prometheus_jmx_exporter/jmx_prometheus_javaagent.jar=9404:/opt/prometheus_jm\nx_exporter/jmx_prometheus_javaagent.yml\n After you restart the cluster, the default settings and newly added configuration setting appear in Spark UI > Environment > Spark Properties.",
          "title" : "Adding a configuration setting overwrites all default spark.executor.extraJavaOptions settings",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/conf-overwrites-default-settings"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/custom-docker-requires-root",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Custom Docker image requires root Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to launch an Azure Databricks cluster with a custom Docker container, but cluster creation fails with an error. JSON Copy {\n\"reason\": {\n\"code\": \"CONTAINER_LAUNCH_FAILURE\",\n\"type\": \"SERVICE_FAULT\",\n\"parameters\": {\n\"instance_id\": \"i-xxxxxxx\",\n\"databricks_error_message\": \"Failed to launch spark container on instance i-xxxx. Exception: Could not add container for xxxx with address xxxx. Could not mkdir in container\"\n              }\n          }\n}\n Cause Azure Databricks clusters require a root user and sudo. Custom container images that are configured to start as a non-root user are not supported. For more information, review the custom container documentation. Solution You must configure your Docker container to start as the root user. Example This container configuration starts as the standard user ubuntu. It fails to launch. text Copy FROM databricksruntime/standard:8.x\nRUN apt-get update -y && apt-get install -y git && \\\nln -s /databricks/conda/envs/dcs-minimal/bin/pip /usr/local/bin/pip && \\\nln -s /databricks/conda/envs/dcs-minimal/bin/python /usr/local/bin/python\nCOPY . /app\nWORKDIR /app\nRUN pip install -r requirements.txt .\nRUN chown -R ubuntu /app\nUSER ubuntu\n This container configuration starts as the root user. It launches successfully. text Copy FROM databricksruntime/standard:8.x\nRUN apt-get update -y && apt-get install -y git && \\\nln -s /databricks/conda/envs/dcs-minimal/bin/pip /usr/local/bin/pip && \\\nln -s /databricks/conda/envs/dcs-minimal/bin/python /usr/local/bin/python\nCOPY . /app\nWORKDIR /app\nRUN pip install -r requirements.txt .",
          "title" : "Custom Docker image requires root",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/custom-docker-requires-root"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/append-a-row-to-rdd-or-dataframe",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Append to a DataFrame Article 03/11/2022 2 minutes to read 3 contributors In this article To append to a DataFrame, use the union method. Scala Copy %scala\n\nval firstDF = spark.range(3).toDF(\"myCol\")\nval newRow = Seq(20)\nval appended = firstDF.union(newRow.toDF())\ndisplay(appended)\n Python Copy %python\n\nfirstDF = spark.range(3).toDF(\"myCol\")\nnewRow = spark.createDataFrame([[20]])\nappended = firstDF.union(newRow)\ndisplay(appended)",
          "title" : "Append to a DataFrame",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/append-a-row-to-rdd-or-dataframe"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/no-use-perms-db",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents No USAGE permission on database Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are using a cluster running Databricks Runtime 7.3 LTS and above. You have enabled table access control for your workspace as the admin user, and granted the SELECT privilege to a standard user-group that needs to access the tables. A user tries to access an object in the database and gets a SecurityException error message. Console Copy Error in SQL statement: SecurityException: User does not have permission USAGE on database <databasename>\n Cause A new USAGE privilege was added to the available data access privileges. This privilege is enforced on clusters running Databricks Runtime 7.3 LTS and above. Solution Grant the USAGE privilege to the user-group. Login to the workspace as an admin user. Open a notebook. Run the following command: SQL Copy GRANT USAGE ON DATABASE <databasename> TO <user-group>;\n Review the USAGE privilege documentation for more information.",
          "title" : "No USAGE permission on database",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/no-use-perms-db"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/null-empty-strings",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Nulls and empty strings in a partitioned column save as nulls Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem If you save data containing both empty strings and null values in a column on which the table is partitioned, both values become null after writing and reading the table. To illustrate this, create a simple DataFrame: Scala Copy import org.apache.spark.sql.types._\nimport org.apache.spark.sql.catalyst.encoders.RowEncoder\nval data = Seq(Row(1, \"\"), Row(2, \"\"), Row(3, \"\"), Row(4, \"hello\"), Row(5, null))\nval schema = new StructType().add(\"a\", IntegerType).add(\"b\", StringType)\nval df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)\n At this point, if you display the contents of df, it appears unchanged: Write df, read it again, and display it. The empty strings are replaced by null values: Cause This is the expected behavior. It is inherited from Apache Hive. Solution In general, you shouldn’t use both null and empty strings as values in a partitioned column.",
          "title" : "Nulls and empty strings in a partitioned column save as nulls",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/null-empty-strings"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/skew-hints-in-join",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to specify skew hints in dataset and DataFrame-based join commands Article 03/11/2022 2 minutes to read 3 contributors In this article Example When you perform a join command with DataFrame or Dataset objects, if you find that the query is stuck on finishing a small number of tasks due to data skew, you can specify the skew hint with the hint(\"skew\") method: df.hint(\"skew\"). The skew join optimization is performed on the DataFrame for which you specify the skew hint. In addition to the basic hint, you can specify the hint method with the following combinations of parameters: column name, list of column names, and column name and skew value. DataFrame and column name. The skew join optimization is performed on the specified column of the DataFrame. Python Copy df.hint(\"skew\", \"col1\")\n DataFrame and multiple columns. The skew join optimization is performed for multiple columns in the DataFrame. Python Copy df.hint(\"skew\", [\"col1\",\"col2\"])\n DataFrame, column name, and skew value. The skew join optimization is performed on the data in the column with the skew value. Python Copy df.hint(\"skew\", \"col1\", \"value\")\n Example This example shows how to specify the skew hint for multiple DataFrame objects involved in a join operation: Scala Copy val joinResults = ds1.hint(\"skew\").as(\"L\").join(ds2.hint(\"skew\").as(\"R\"), $\"L.col1\" === $\"R.col1\")",
          "title" : "How to specify skew hints in dataset and DataFrame-based join commands",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data/skew-hints-in-join"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbfs/dbfs-root-permissions",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cannot read Azure Databricks objects stored in the DBFS root directory Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem An Access Denied error returns when you attempt to read Azure Databricks objects stored in the DBFS root directory in blob storage from outside an Azure Databricks cluster. Cause This is normal behavior for the DBFS root directory. Azure Databricks stores objects like libraries and other temporary system files in the DBFS root directory. Azure Databricks is the only user that can read these objects. Solution Azure Databricks does not recommend using the root directory for storing any user files or objects. Instead, create a different blob storage directory and mount it to DBFS.",
          "title" : "Cannot read Azure Databricks objects stored in the DBFS root directory",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbfs/dbfs-root-permissions"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbfs/parallelize-fs-operations",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Parallelize filesystem operations Article 03/11/2022 2 minutes to read 2 contributors In this article Import required libraries Broadcast information from the driver to executors Copy paths to a sequence Parallelize the sequence and divide the workload When you need to speed up copy and move operations, parallelizing them is usually a good option. You can use Apache Spark to parallelize operations on executors. On Azure Databricks you can use DBUtils APIs, however these API calls are meant for use on driver nodes, and shouldn’t be used on Spark jobs running on executors. In this article, we are going to show you how to use the Apache Hadoop FileUtil function along with DBUtils to parallelize a Spark copy operation. You can use this example as a basis for other filesystem operations. Note The example copy operation may look familiar as we are using DBUtils and Hadoop FileUtil to emulate the functions of the Hadoop DistCp tool. Import required libraries Import the Hadoop functions and define your source and destination locations. Scala Copy import org.apache.hadoop.fs._\n\nval source = \"<source dir>\"\nval dest = \"<destination dir>\"\n\ndbutils.fs.mkdirs(dest)\n Broadcast information from the driver to executors Scala Copy val conf = new org.apache.spark.util.SerializableConfiguration(sc.hadoopConfiguration)\nval broadcastConf = sc.broadcast(conf)\nval broadcastDest = sc.broadcast(dest)\n Copy paths to a sequence Scala Copy val filesToCopy = dbutils.fs.ls(source).map(_.path)\n Parallelize the sequence and divide the workload Here we first get the Hadoop configuration and destination path. Then we create the path objects, before finally executing the FileUtil.copy command. Scala Copy spark.sparkContext.parallelize(filesToCopy).foreachPartition { rows =>\n  rows.foreach { file =>\n\n    val conf = broadcastConf.value.value\n    val destPathBroadcasted = broadcastDest.value\n\n    val fromPath = new Path(file)\n    val toPath = new Path(destPathBroadcasted)\n    val fromFs = fromPath.getFileSystem(conf)\n    val toFs = toPath.getFileSystem(conf)\n\n    FileUtil.copy(fromFs, fromPath, toFs, toPath, false, conf)\n  }\n}",
          "title" : "Parallelize filesystem operations",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbfs/parallelize-fs-operations"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/compare-versions-delta-table",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Compare two versions of a Delta table Article 03/11/2022 2 minutes to read 2 contributors In this article Identify all differences Identify files added to a specific version Delta Lake supports time travel, which allows you to query an older snapshot of a Delta table. One common use case is to compare two versions of a Delta table in order to identify what changed. For more details on time travel, please review the Delta Lake time travel documentation. Identify all differences You can use a SQL SELECT query to identify all differences between two versions of a Delta table. You need to know the name of the table and the version numbers of the snapshots you want to compare. SQL Copy select * from <table-name>@v<version-number>\nexcept all\nselect * from\n<table-name>@v<version-number>\n For example, if you had a table named “schedule” and you wanted to compare version 2 with the original version, your query would look like this: SQL Copy select * from schedule@v2\nexcept all\nselect * from\nschedule@v0\n Identify files added to a specific version You can use a scala query to retrieve a list of files that were added to a specific version of the Delta table. Scala Copy display(spark.read.json(\"dbfs:/<path-to-delta-table>/_delta_log/00000000000000000002.json\").where(\"add is not null\").select(\"add.path\"))\n In this example, we are getting a list of all files that were added to version 2 of the Delta table. 00000000000000000002.json contains the list of all files in version 2. After reading in the full list, we are excluding files that already existed, so the displayed list only includes files added to version 2.",
          "title" : "Compare two versions of a Delta table",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/compare-versions-delta-table"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/data-missing-vacuum-parallel-write",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Vaccuming with zero retention results in data loss Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You add data to a Delta table, but the data disappears without warning. There is no obvious error message. Cause This can happen when spark.databricks.delta.retentionDurationCheck.enabled is set to false and VACUUM is configured to retain 0 hours. SQL Copy VACUUM <name-of-delta-table> RETAIN 0 HOURS\n When VACUUM is configured to retain 0 hours it can delete any file that is not part of the version that is being vacuumed. This includes committed files, uncommitted files, and temporary files for concurrent transactions. Consider the following example timeline: VACUUM starts running at 01:17 UTC on version 100. A data file named sample-data-part-0-1-2.parquet is added to version 101 at 01:18 UTC. Version 101 is committed at 01:19 UTC. VACUUM is still running on version 101 and deletes sample-data-part-0-1-2.parquet at 01:20 UTC. VACUUM completes at 01:22 UTC. In this example, VACUMM executed on version 100 and deleted everything that was added to version 101. Solution Databricks recommends that you set a VACUUM retention interval to be at least 7 days, because old snapshots and uncommitted files can still be in use by concurrent readers or writers to the table. Do not set spark.databricks.delta.retentionDurationCheck.enabled to false in your Spark config. If you do set spark.databricks.delta.retentionDurationCheck.enabled to false in your Spark config, you must choose an interval that is longer than the longest running concurrent transaction and the longest period that any stream can lag behind the most recent update to the table. Review the Azure Databricks VACUMM documentation for more information.",
          "title" : "Vaccuming with zero retention results in data loss",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/data-missing-vacuum-parallel-write"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Delta Lake: tips and troubleshooting Article 04/14/2022 2 minutes to read 4 contributors In this article These articles can help you with Delta Lake. A file referenced in the transaction log cannot be found Compare two versions of a Delta table Converting from Parquet to Delta Lake fails Delta Merge cannot resolve nested field Delete your streaming query checkpoint and restart FileReadException when reading a Delta table How Delta cache behaves on an autoscaling cluster How to improve performance of Delta Lake MERGE INTO queries using partition pruning Best practices for dropping a managed Delta Lake table Delta Lake write job fails with java.lang.UnsupportedOperationException How to populate or update columns in an existing Delta table Identify duplicate data on append operations Optimize a Delta sink in a structured streaming application Delta Lake UPDATE query fails with IllegalState exception Unable to cast string to varchar Vaccuming with zero retention results in data loss Z-Ordering will be ineffective, not collecting stats",
          "title" : "Delta Lake: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Libraries: tips and troubleshooting Article 03/11/2022 2 minutes to read 4 contributors In this article These articles can help you manage libraries in Azure Databricks. Cannot import module in egg library Cannot import TabularPrediction from AutoGluon Latest PyStan fails to install on Databricks Runtime 6.4 Library unavailability causing job failures How to correctly update a Maven library in Azure Databricks Init script fails to download Maven JAR Install package using previous CRAN snapshot Install PyGraphViz Install Turbodbc via init script Cannot uninstall library from UI Error when installing Cartopy on a cluster Error when installing pyodbc on a cluster Install TensorFlow 2.1 on Databricks Runtime 6.5 ML GPU clusters Libraries fail with dependency exception Libraries failing due to transient Maven issue Reading .xlsx files with xlrd fails Remove Log4j 1.x JMSAppender and SocketServer classes from classpath Replace a default library jar Python command fails with AssertionError: wrong color format PyPMML fails with Could not find py4j jar error TensorFlow fails to import Verify the version of Log4j on your cluster",
          "title" : "Libraries: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Metastore: tips and troubleshooting Article 03/11/2022 2 minutes to read 4 contributors In this article These articles can help you manage your Apache Hive Metastore for Azure Databricks. Autoscaling is slow with an external metastore Data too long for column error Drop database without deletion How to create table DDLs to import into an external metastore Drop tables with corrupted metadata from the metastore Error in CREATE TABLE with external Hive metastore AnalysisException when dropping table on Azure-backed metastore How to troubleshoot several Apache Hive metastore problems Listing table names How to set up Hive metastore on SQL Server with Hive 2.0-2.2 How to set up an embedded Apache Hive metastore Japanese character support in external metastore Parquet timestamp requires Hive metastore 1.2 or above",
          "title" : "Metastore: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/drop-table-corruptedmetadata",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Drop tables with corrupted metadata from the metastore Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution This article explains how to drop tables with corrupted metadata. Problem Sometimes you cannot drop a table from the Azure Databricks UI. Using %sql or spark.sql to drop table doesn’t work either. Cause The metadata (table schema) stored in the metastore is corrupted. When you run Drop table command, Spark checks whether table exists or not before dropping the table. Since the metadata is corrupted for the table Spark can’t drop the table and fails with following exception. Scala Copy com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: The metadata is corrupted\n Solution Use a Hive client to drop the table since the Hive client doesn’t check for the table existence as Spark does. To drop a table: Create a function inside Hive package. Scala Copy package org.apache.spark.sql.hive {\nimport org.apache.spark.sql.hive.HiveUtils\nimport org.apache.spark.SparkContext\n\nobject utils {\n    def dropTable(sc: SparkContext, dbName: String, tableName: String, ignoreIfNotExists: Boolean, purge: Boolean): Unit = {\n      HiveUtils\n          .newClientForMetadata(sc.getConf, sc.hadoopConfiguration)\n          .dropTable(dbName, tableName, ignoreIfNotExists, false)\n    }\n  }\n}\n Drop corrupted tables. Scala Copy import org.apache.spark.sql.hive.utils\nutils.dropTable(sc, \"default\", \"my_table\", true, true)",
          "title" : "Drop tables with corrupted metadata from the metastore",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/drop-table-corruptedmetadata"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/set-up-embedded-metastore",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to set up an embedded Apache Hive metastore Article 03/11/2022 2 minutes to read 3 contributors In this article Set up an embedded Hive metastore notebook You can set up an Azure Databricks cluster to use an embedded metastore. You can use an embedded metastore when you only need to retain table metadata during the life of the cluster. If the cluster is restarted, the metadata is lost. If you need to persist the table metadata or other data after a cluster restart, then you should use the default metastore or set up an external metastore. This example uses the Apache Derby embedded metastore, which is an in-memory lightweight database. Follow the instructions in the notebook to install the metastore. You should always perform this procedure on a test cluster before applying it to other clusters. Set up an embedded Hive metastore notebook Get notebook",
          "title" : "How to set up an embedded Apache Hive metastore",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/set-up-embedded-metastore"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metrics/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Metrics: tips and troubleshooting Article 03/11/2022 2 minutes to read 3 contributors In this article These articles can help you configure Apache Spark and Azure Databricks metrics. How to explore Apache Spark metrics with Spark listeners How to use Apache Spark metrics",
          "title" : "Metrics: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metrics/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/bi/configure-simba-proxy-windows",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Configure Simba ODBC driver with a proxy in Windows Article 03/11/2022 2 minutes to read 2 contributors In this article Download the Simba driver for Windows Add proxy settings to the Windows registry Configure settings in ODBC Data Source Administrator In this article you learn how to configure the Databricks ODBC Driver when your local Windows machine is behind a proxy server. Download the Simba driver for Windows Download and install the latest version of the Databricks ODBC Driver for Windows. Add proxy settings to the Windows registry Open the Windows registry and add the proxy settings to the Simba Spark ODBC Driver key. Open the Windows Registry Editor. Navigate to the HKEY_LOCAL_MACHINE\\SOFTWARE\\Simba\\Simba Spark ODBC Driver\\Driver key. Click Edit. Select New. Click String Value. Enter UseProxy as the Name and 1 as the Data value. Repeat this until you have added the following string value pairs: Name ProxyHost Data <proxy-host-address> Name ProxyPort Data <proxy-port-number> Name ProxyUID Data <proxy-username> Name ProxyPWD Data <proxy-password Close the registry editor. Configure settings in ODBC Data Source Administrator Open the ODBC Data Sources application. Click the System DSN tab. Select the Simba Spark ODBC Driver and click Configure. Enter the connection information of your Apache Spark server. Click Advanced Options. Enable the Driver Config Take Precedence check box. Click OK. Click OK. Click OK.",
          "title" : "Configure Simba ODBC driver with a proxy in Windows",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/bi/configure-simba-proxy-windows"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/bi/powerbi-proxy-ssl-configuration",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Power BI proxy and SSL configuration Article 03/11/2022 3 minutes to read 2 contributors In this article Driver configurations Troubleshooting Driver configurations You can set driver configurations using the microsoft.sparkodbc.ini file which can be found in the ODBC Drivers\\Simba Spark ODBC Driver directory. The absolute path of the microsoft.sparkodbc.ini directory depends on whether you are using Power BI Desktop or on-premises Power BI Gateway: Power BI Desktop: C:\\Program Files\\Microsoft Power BI Desktop\\bin\\ODBC Drivers\\Simba Spark ODBC Driver\\microsoft.sparkodbc.ini Power BI Gateway: m\\ODBC Drivers\\Simba Spark ODBC Driver\\microsoft.sparkodbc.ini, where m is placed inside the gateway installation directory. Set driver configurations Check if the microsoft.sparkodbc.ini file was already created. If it is then jump to step 3. Open Notepad or File Explorer as Run As Administrator and create a file at ODBC DriversSimba Spark ODBC Drivermicrosoft.sparkodbc.ini. Add the new driver configurations to the file below the header [Driver] by using the syntax =. Configuration keys can be found in the manual provided with the installation of the Databricks ODBC Driver. The manual is located at C:\\Program Files\\Simba Spark ODBC Driver\\Simba Apache Spark ODBC Connector Install and Configuration Guide.html. Configuring a proxy To configure a proxy, add the following configurations to the driver configuration in the microsoft.sparkodbc.ini file: Console Copy [Driver]\nUseProxy=1\nProxyHost=<proxy.example.com>\nProxyPort=<port>\nProxyUID=<username>\nProxyPWD=<password>\n Depending on the firewall configuration it might also be necessary to add: Console Copy [Driver]\nCheckCertRevocation=0\n Troubleshooting Error: SSL_connect: certificate verify failed When SSL issues occur, the ODBC driver returns a generic error SSL_connect: certificate verify failed. You can get more detailed SSL debugging logs by setting in the ODBC DriversSimba Spark ODBC Drivermicrosoft.sparkodbc.inimicrosoft.sparkodbc.ini file the following two configurations: Console Copy [Driver]\nAllowDetailedSSLErrorMessages=1\nEnableCurlDebugLogging=1\n Diagnose issues by analyzing CryptoAPI logs Most issues can be diagnosed by using Windows CryptoAPI logs, which can be found in the Event Viewer. The following steps describe how to capture these logs. Open Event Viewer and go to Applications and Services Logs > Microsoft > Windows > CAPI2 > Operational. In Filter Current Log, check the boxes Critical, Error and Warning and click OK. In the Event Viewer, go to Actions > Enable Log to start collecting logs. Connect Power BI to Azure Databricks to reproduce the issue. In the Event Viewer, go to Actions > Disable Log to stop collecting logs. Click Refresh to retrieve the list of collected events. Export logs by clicking Actions > Save Filtered Log File As. Diagnose Build Chain or Verify Chain Policy event errors If the collected logs contain an error on the Build Chain or Verify Chain Policy events, this likely points to the issue. More details can be found by selecting the event and reading the Details section. Two fields of interest are Result, and RevocationResult. The revocation status of the certificate or one of the certificates in the certificate chain is unknown. CAPI2 error: RevocationResult: [80092013] The revocation function was unable to check revocation because the revocation server was offline. Cause: The revocation check failed due to an unavailable certificate revocation server. Resolution: Disable certificate revocation checking. The certificate chain is not complete. CAPI2 error: Result: [800B010A] A certificate chain could not be built to a trusted root authority. Cause: The certificate advertised by the VPN or proxy server is incomplete and does not contain a full chain to the trusted root authority. Resolution: The preferred solution is to configure the VPN or proxy server to advertise the full chain. If this is not possible, a workaround is to obtain the intermediate certificates for the Databricks workspace, and install these in the Intermediate Certification Authorities store, to enable Windows to find the unadvertised certificates. See Install intermediate certificates. If possible, it is recommended to install these certificates for all Power BI users using a group policy in Windows. This has to be set up by the system administrator. Certificate configurations Disable certificate revocation checking If the ODBC driver is unable to reach the certificate revocation list server, for example because of a firewall configuration, it will fail to validate the certificate. This can be resolved by disabling this check. To disable certificate revocation checking, set the configuration CheckCertRevocation=0 to the microsoft.sparkodbc.ini file. Install intermediate certificates Open your Azure Databricks workspace URL in Chrome and go to View site information by clicking the padlock icon in the address bar. Click Certificate > Certificate Path and repeat steps 3 to 6 for every intermediate certificate in the chain. Choose an intermediate certificate and go to Details > Copy to File > Next to export the certificate. Select the location of the certificate and click Finish. Open the exported certificate and click Install Certificate > Next. From the Certificate Import Wizard click Place all certificates in the following store > Browse and choose Intermediate Certification Authorities.",
          "title" : "Power BI proxy and SSL configuration",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/bi/powerbi-proxy-ssl-configuration"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/access-blobstore-odbc",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2 Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem Note In general, you should use Databricks Runtime 5.2 and above, which include a built-in Azure Blob File System (ABFS) driver, when you want to access Azure Data Lake Storage Gen2 (ADLS Gen2). This article applies to users who are accessing ADLS Gen2 storage using JDBC/ODBC instead. When you run a SQL query from a JDBC or ODBC client to access ADLS Gen2, the following error occurs: Console Copy com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalArgumentException: No value for dfs.adls.oauth2.access.token.provider found in conf file.\n\n18/10/23 21:03:28 ERROR SparkExecuteStatementOperation: Error executing query, currentState RUNNING,\njava.util.concurrent.ExecutionException: java.io.IOException: There is no primary group for UGI (Basic token)chris.stevens+dbadmin (auth:SIMPLE)\n  at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)\n  at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)\n  at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n  at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n  at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2344)\n  at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2316)\n  at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n  at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n  at com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n  at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:158)\n  at org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:257)\n  at org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:313)\n  at\n  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n  at scala.collection.immutable.List.foldLeft(List.scala:84)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:87)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:79)\n When you run the query from the SQL client, you get the following error: Console Copy An error occurred when executing the SQL command:\nselect * from test_databricks limit 50\n\n[Simba][SparkJDBCDriver](500051) ERROR processing query/statement. Error Code: 0, SQL state: com.google.common.util.concurrent.UncheckedExecutionException: com.databricks.backend.daemon.data.common.InvalidMountException: Error while using path /mnt/crm_gen2/phonecalls for resolving path '/phonecalls' within mount at '/mnt/crm_gen2'., Query: SELECT * FROM `default`.`test_databricks` `default_test_databricks` LIMIT 50. [SQL State=HY000, DB Errorcode=500051]\n\nWarnings:\n[Simba][SparkJDBCDriver](500100) Error getting table information from database.\n Cause The root cause is incorrect configuration settings to create a JDBC or ODBC connection to ABFS via ADLS Gen2, which cause queries to fail. Solution Set spark.hadoop.hive.server2.enable.doAs to false in the cluster configuration settings.",
          "title" : "Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/access-blobstore-odbc"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/create-table-json-serde",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Create tables on JSON datasets Article 03/11/2022 2 minutes to read 2 contributors In this article Download the JSON SerDe JAR Install the JSON SerDe JAR on your cluster Configure SerDe properties in the create table statement Run a repair table statement after the table is created In this article we cover how to create a table on JSON datasets using SerDe. Download the JSON SerDe JAR Open the hive-json-serde 1.3.8 download page. Click on json-serde-1.3.8-jar-with-dependencies.jar to download the file json-serde-1.3.8-jar-with-dependencies.jar. Note You can review the Hive-JSON-Serde GitHub repo for more information on the JAR, including source code. Install the JSON SerDe JAR on your cluster Select your cluster in the workspace. Click the Libraries tab. Click Install new. In the Library Source button list, select Upload. In the Library Type button list, select JAR. Click Drop JAR here. Select the json-serde-1.3.8-jar-with-dependencies.jar file. Click Install. Configure SerDe properties in the create table statement SQL Copy ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\nSTORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION '<path-to-json-files>'\n For example: SQL Copy create table <name-of-table> (timestamp_unix string, comments string, start_date string, end_date string)\npartitioned by (yyyy string, mm string, dd string)\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\nSTORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION '<path-to-json-files>'\n This example creates a table that is partitioned by the columns yyyy, mm, and dd. Run a repair table statement after the table is created For example: SQL Copy msck repair table <name-of-table>",
          "title" : "Create tables on JSON datasets",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/create-table-json-serde"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/use-cx-oracle-connect-server",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Use cx_Oracle to connect to an Oracle server Article 03/11/2022 2 minutes to read 2 contributors In this article Install overview Create an init script Configure the init script Install cx_Oracle library Restart the cluster cx_Oracle is a module that enables access to Oracle Database and conforms to the Python database API specification. Using cx_Oracle requires Oracle Client libraries to be installed. These provide the necessary network connectivity allowing cx_Oracle to access an Oracle Database instance. Oracle Instant Client enables the development and deployment of applications that connect to Oracle Database, either on-premise or in the Cloud. The Instant Client libraries provide the necessary network connectivity and advanced data features to make full use of Oracle Database. The libraries are used by the Oracle APIs of popular languages and environments including Python, Node.js, Go, PHP and Ruby, as well as providing access for Oracle Call Interface (OCI), Oracle C++ Call Interface (OCCI), JDBC-OCI, ODBC and Pro*C applications. Since Oracle Instant Client is not included in the Databricks clusters, it has to be installed and environment variables need to be configured in order to connect to an on-prem or cloud oracle server. If Oracle Instant Client is not installed or the environment variables are not configured correctly, you will get a database error. Console Copy DatabaseError: DPI-1047: Cannot locate a 64-bit Oracle Client library: 'libclntsh.so: cannot open shared object file: No such file or directory'.\n Install overview The following steps need to be performed in order to properly install cx_Oracle and the client libraries. Instead of performing these steps manually, you should use an init script. Download the latest version of the Oracle Instant Client Basic Light Package (ZIP) from the Oracle Instant Client Downloads for Linux x86-64 (64-bit) page. Unzip the contents to a folder. Upload the instant client folder to a cluster. Copy the instant client folder to a system directory. Set the environment variables LD_LIBRARY_PATH and ORACLE_HOME. Install cx_Oracle from PyPI. Restart the cluster. Create an init script Use this template to create a cluster-scoped init script that automatically downloads and installs the Oracle Instant Client. Python Copy ​​​​​dbutils.fs.put(\"dbfs:/databricks/<init-script-folder>/oracle_ctl.sh\",\"\"\"\n#!/bin/bash\nwget --quiet -O /tmp/instantclient-basiclite-linuxx64.zip https://download.oracle.com/otn_software/linux/instantclient/instantclient-basiclite-linuxx64.zip\nunzip /tmp/instantclient-basiclite-linuxx64.zip -d /databricks/driver/oracle_ctl/\nsudo echo 'export LD_LIBRARY_PATH=\"/databricks/driver/oracle_ctl/\"' >> /databricks/spark/conf/spark-env.sh\nsudo echo 'export ORACLE_HOME=\"/databricks/driver/oracle_ctl/\"' >> /databricks/spark/conf/spark-env.sh\n\"\"\", True)\n Configure the init script Install the newly created init script as a cluster-scoped init script. You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/oracle_ctl.sh). Install cx_Oracle library Install cx_Oracle as a cluster-installed library. Restart the cluster Restart your cluster after cx_Oracle and the client libraries have been installed. You can now access your Oracle server.",
          "title" : "Use cx_Oracle to connect to an Oracle server",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/use-cx-oracle-connect-server"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/error-run-msck-repair-table-parallel",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Error when running MSCK REPAIR TABLE in parallel Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to run MSCK REPAIR TABLE <table-name> commands for the same table in parallel and are getting java.net.SocketTimeoutException: Read timed out or out of memory error messages. Cause When you try to add a large number of new partitions to a table with MSCK REPAIR in parallel, the Hive metastore becomes a limiting factor, as it can only add a few partitions per second. The greater the number of new partitions, the more likely that a query will fail with a java.net.SocketTimeoutException: Read timed out error or an out of memory error message. Solution You should not attempt to run multiple MSCK REPAIR TABLE <table-name> commands in parallel. Azure Databricks uses multiple threads for a single MSCK REPAIR by default, which splits createPartitions() into batches. By limiting the number of partitions created, it prevents the Hive metastore from timing out or hitting an out of memory error. It also gathers the fast stats (number of files and the total size of files) in parallel, which avoids the bottleneck of listing the metastore files sequentially. This is controlled by spark.sql.gatherFastStats, which is enabled by default.",
          "title" : "Error when running MSCK REPAIR TABLE in parallel",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/sql/error-run-msck-repair-table-parallel"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/checkpoint-no-cleanup-foreachbatch",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Checkpoint files not being deleted when using foreachBatch() Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You have a streaming job using foreachBatch() to process DataFrames. Scala Copy streamingDF.writeStream.outputMode(\"append\").foreachBatch { (batchDF: DataFrame, batchId: Long) =>\n  batchDF.write.format(\"parquet\").mode(\"overwrite\").save(output_directory)\n}.start()\n Checkpoint files are being created, but are not being deleted. You can verify the problem by navigating to the root directory and looking in the /local_disk0/tmp/ folder. Checkpoint files remain in the folder. Cause The command foreachBatch() is used to support DataFrame operations that are not normally supported on streaming DataFrames. By using foreachBatch() you can apply these operations to every micro-batch. This requires a checkpoint directory to track the streaming updates. If you have not specified a custom checkpoint location, a default checkpoint directory is created at /local_disk0/tmp/ . Azure Databricks uses the checkpoint directory to ensure correct and consistent progress information. When a stream is shut down, either purposely or accidentally, the checkpoint directory allows Azure Databricks to restart and pick up exactly where it left off. If a stream is shut down by cancelling the stream from the notebook, the Azure Databricks job attempts to clean up the checkpoint directory on a best-effort basis. If the stream is terminated in any other way, or if the job is terminated, the checkpoint directory is not cleaned up. This is as designed. Solution You should manually specify the checkpoint directory with the checkpointLocation option. Scala Copy streamingDF.writeStream.option(\"checkpointLocation\",\"<checkpoint-path>\").outputMode(\"append\").foreachBatch { (batchDF: DataFrame, batchId: Long) =>\nbatchDF.write.format(\"parquet\").mode(\"overwrite\").save(output_directory)\n}.start()",
          "title" : "Checkpoint files not being deleted when using foreachBatch()",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/checkpoint-no-cleanup-foreachbatch"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/dstream-not-supported",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Apache Spark DStream is not supported Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You are attempting to use a Spark Discretized Stream (DStream) in a Azure Databricks streaming job, but the job is failing. Cause DStreams and the DStream API are not supported by Azure Databricks. Solution Instead of using Spark DStream, you should migrate to Structured Streaming. Review the Azure Databricks Structured Streaming in production documentation for more information.",
          "title" : "Apache Spark DStream is not supported",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/dstream-not-supported"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/file-sink-streaming",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Streaming with File Sink: Problems with recovery if you change checkpoint or output directories Article 03/11/2022 2 minutes to read 3 contributors In this article When you stream data into a file sink, you should always change both checkpoint and output directories together. Otherwise, you can get failures or unexpected outputs. Apache Spark creates a folder inside the output directory named _spark_metadata. This folder contains write-ahead logs for every batch run. This is how Spark gets exactly-once guarantees when writing to a file system. This folder contains save files for each batch (named 0,1,2,3 etc + 19.compact, n.compact etc). These files include JSON that gives details about the output for the particular batch. With the help of this data, once a batch has succeeded, any duplicate batch output is discarded. If you change the checkpoint directory but not the output directory: When you change the checkpoint directory, the stream job will start batches again from 0. Since 0 is already present in the _spark_metadata folder, the output file will be discarded even if it has new data. That is, if you stop the previous run on the 500th batch, the next run with same output directory and different checkpoint directory will give output only on the 501st batch. All of the previous batches will be silently discarded. If you change the output directory but not the checkpoint directory: When you change only the output directory, it loses all of the batch data from the _spark_metadata folder. But Spark starts writing from the next batch according to the checkpoint directory. For example, if the previous run was stopped at 500, the first write of the new stream job will be at file 501 on _spark_metadata and you lose all of the old batches. When you read the files back, you get the error metadata for batch 0(or first compact file (19.compact)) is not found.",
          "title" : "Streaming with File Sink: Problems with recovery if you change checkpoint or output directories",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/file-sink-streaming"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/streaming-job-stuck-writing-checkpoint",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Streaming job gets stuck writing to checkpoint Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are monitoring a streaming job, and notice that it appears to get stuck when processing data. When you review the logs, you discover the job gets stuck when writing data to a checkpoint. Console Copy INFO HDFSBackedStateStoreProvider: Deleted files older than 381160 for HDFSStateStoreProvider[id = (op=0,part=89),dir = dbfs:/FileStore/R_CHECKPOINT5/state/0/89]:\nINFO StateStore: Retrieved reference to StateStoreCoordinator: org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef@56a4cb80\nINFO HDFSBackedStateStoreProvider: Deleted files older than 381160 for HDFSStateStoreProvider[id = (op=0,part=37),dir = dbfs:/FileStore/R_CHECKPOINT5/state/0/37]:\nINFO StateStore: Retrieved reference to StateStoreCoordinator: org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef@56a4cb80\nINFO HDFSBackedStateStoreProvider: Deleted files older than 313920 for HDFSStateStoreProvider[id = (op=0,part=25),dir = dbfs:/FileStore/PYTHON_CHECKPOINT5/state/0/25]:\n Cause You are trying to use a checkpoint location in your local DBFS path. Scala Copy query = streamingInput.writeStream.option(\"checkpointLocation\", \"/FileStore/checkpoint\").start()\n Solution You should use persistent storage for streaming checkpoints. You should not use DBFS for streaming checkpoint storage.",
          "title" : "Streaming job gets stuck writing to checkpoint",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/streaming-job-stuck-writing-checkpoint"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/access-blob-fails-wasb",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Failure when mounting or accessing Azure Blob storage Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem When you try to access an already created mount point or create a new mount point, it fails with the error: Console Copy WASB: Fails with java.lang.NullPointerException\n Cause This error can occur when the root mount path (such as /mnt/) is also mounted to blob storage. Run the following command to check if the root path is also mounted: Python Copy dbutils.fs.mounts()\n Check if /mnt appears in the list. Solution Unmount the /mnt/ mount point using the command: Python Copy dbutils.fs.unmount(\"/mnt\")\n Now you should be able to access your existing mount points and create new ones.",
          "title" : "Failure when mounting or accessing Azure Blob storage",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/access-blob-fails-wasb"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbfs/errno95-operation-not-supported",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Operation not supported during append Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are attempting to append data to a file saved on an external storage mount point and are getting an error message: OSError: [Errno 95] Operation not supported. The error occurs when trying to append to a file from both Python and R. Cause Direct appends and random writes are not supported in FUSE v2, which is available in Databricks Runtime 6.0 and above. This is by design. The underlying storage that is mounted to DBFS does not support append. This means that Azure Databricks would have to download the data, run the append, and reupload the data in order to support the command. This works for small files, but quickly becomes an issue as file size increases. Because the DBFS mount is shared between driver and worker nodes, appending to a file from multiple nodes can cause data corruption. Solution As a workaround, you should run your append on a local disk, such as /tmp, and move the entire file at the end of the operation. If you need to perform cross-session appends, please contact your account team to discuss enabling an NFS mount on your clusters.",
          "title" : "Operation not supported during append",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbfs/errno95-operation-not-supported"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/delta-merge-into",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to improve performance of Delta Lake MERGE INTO queries using partition pruning Article 03/11/2022 3 minutes to read 3 contributors In this article Discussion Demonstration: no partition pruning Solution This article explains how to trigger partition pruning in Delta Lake MERGE INTO queries from Azure Databricks. Partition pruning is an optimization technique to limit the number of partitions that are inspected by a query. Discussion MERGE INTO is an expensive operation when used with Delta tables. If you don’t partition the underlying data and use it appropriately, query performance can be severely impacted. The main lesson is this: if you know which partitions a MERGE INTO query needs to inspect, you should specify them in the query so that partition pruning is performed. Demonstration: no partition pruning Here is an example of a poorly performing MERGE INTO query without partition pruning. Start by creating the following Delta table, called delta_merge_into: Scala Copy val df = spark.range(30000000)\n    .withColumn(\"par\", ($\"id\" % 1000).cast(IntegerType))\n    .withColumn(\"ts\", current_timestamp())\n    .write\n      .format(\"delta\")\n      .mode(\"overwrite\")\n      .partitionBy(\"par\")\n      .saveAsTable(\"delta_merge_into\")\n Then merge a DataFrame into the Delta table to create a table called update: Scala Copy val updatesTableName = \"update\"\nval targetTableName = \"delta_merge_into\"\nval updates = spark.range(100).withColumn(\"id\", (rand() * 30000000 * 2).cast(IntegerType))\n    .withColumn(\"par\", ($\"id\" % 2).cast(IntegerType))\n    .withColumn(\"ts\", current_timestamp())\n    .dropDuplicates(\"id\")\nupdates.createOrReplaceTempView(updatesTableName)\n The update table has 100 rows with three columns, id, par, and ts. The value of par is always either 1 or 0. Let’s say you run the following simple MERGE INTO query: Scala Copy spark.sql(s\"\"\"\n    |MERGE INTO $targetTableName\n    |USING $updatesTableName\n     |ON $targetTableName.id = $updatesTableName.id\n     |WHEN MATCHED THEN\n     |  UPDATE SET $targetTableName.ts = $updatesTableName.ts\n    |WHEN NOT MATCHED THEN\n    |  INSERT (id, par, ts) VALUES ($updatesTableName.id, $updatesTableName.par, $updatesTableName.ts)\n \"\"\".stripMargin)\n The query takes 13.16 minutes to complete: The physical plan for this query contains PartitionCount: 1000, as shown below. This means Apache Spark is scanning all 1000 partitions in order to execute the query. This is not an efficient query, because the update data only has partition values of 1 and 0: Copy == Physical Plan ==\n*(5) HashAggregate(keys=[], functions=[finalmerge_count(merge count#8452L) AS count(1)#8448L], output=[count#8449L])\n+- Exchange SinglePartition\n   +- *(4) HashAggregate(keys=[], functions=[partial_count(1) AS count#8452L], output=[count#8452L])\n    +- *(4) Project\n       +- *(4) Filter (isnotnull(count#8440L) && (count#8440L > 1))\n          +- *(4) HashAggregate(keys=[_row_id_#8399L], functions=[finalmerge_sum(merge sum#8454L) AS sum(cast(one#8434 as bigint))#8439L], output=[count#8440L])\n             +- Exchange hashpartitioning(_row_id_#8399L, 200)\n                +- *(3) HashAggregate(keys=[_row_id_#8399L], functions=[partial_sum(cast(one#8434 as bigint)) AS sum#8454L], output=[_row_id_#8399L, sum#8454L])\n                   +- *(3) Project [_row_id_#8399L, UDF(_file_name_#8404) AS one#8434]\n                      +- *(3) BroadcastHashJoin [cast(id#7514 as bigint)], [id#8390L], Inner, BuildLeft, false\n                         :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n                         :  +- *(2) HashAggregate(keys=[id#7514], functions=[], output=[id#7514])\n                         :     +- Exchange hashpartitioning(id#7514, 200)\n                         :        +- *(1) HashAggregate(keys=[id#7514], functions=[], output=[id#7514])\n                         :           +- *(1) Filter isnotnull(id#7514)\n                         :              +- *(1) Project [cast(((rand(8188829649009385616) * 3.0E7) * 2.0) as int) AS id#7514]\n                         :                 +- *(1) Range (0, 100, step=1, splits=36)\n                         +- *(3) Filter isnotnull(id#8390L)\n                            +- *(3) Project [id#8390L, _row_id_#8399L, input_file_name() AS _file_name_#8404]\n                               +- *(3) Project [id#8390L, monotonically_increasing_id() AS _row_id_#8399L]\n                                  +- *(3) Project [id#8390L, par#8391, ts#8392]\n                                     +- *(3) FileScan parquet [id#8390L,ts#8392,par#8391] Batched: true, DataFilters: [], Format: Parquet, Location: TahoeBatchFileIndex[dbfs:/user/hive/warehouse/delta_merge_into], PartitionCount: 1000, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,ts:timestamp>\n Solution Rewrite the query to specify the partitions. This MERGE INTO query specifies the partitions directly: Scala Copy spark.sql(s\"\"\"\n     |MERGE INTO $targetTableName\n     |USING $updatesTableName\n     |ON $targetTableName.par IN (1,0) AND $targetTableName.id = $updatesTableName.id\n     |WHEN MATCHED THEN\n     |  UPDATE SET $targetTableName.ts = $updatesTableName.ts\n     |WHEN NOT MATCHED THEN\n     |  INSERT (id, par, ts) VALUES ($updatesTableName.id, $updatesTableName.par, $updatesTableName.ts)\n \"\"\".stripMargin)\n Now the query takes just 20.54 seconds to complete on the same cluster. The physical plan for this query contains PartitionCount: 2, as shown below. With only minor changes, the query is now more than 40X faster: Copy == Physical Plan ==\n*(5) HashAggregate(keys=[], functions=[finalmerge_count(merge count#7892L) AS count(1)#7888L], output=[count#7889L])\n+- Exchange SinglePartition\n   +- *(4) HashAggregate(keys=[], functions=[partial_count(1) AS count#7892L], output=[count#7892L])\n    +- *(4) Project\n       +- *(4) Filter (isnotnull(count#7880L) && (count#7880L > 1))\n          +- *(4) HashAggregate(keys=[_row_id_#7839L], functions=[finalmerge_sum(merge sum#7894L) AS sum(cast(one#7874 as bigint))#7879L], output=[count#7880L])\n             +- Exchange hashpartitioning(_row_id_#7839L, 200)\n                +- *(3) HashAggregate(keys=[_row_id_#7839L], functions=[partial_sum(cast(one#7874 as bigint)) AS sum#7894L], output=[_row_id_#7839L, sum#7894L])\n                   +- *(3) Project [_row_id_#7839L, UDF(_file_name_#7844) AS one#7874]\n                      +- *(3) BroadcastHashJoin [cast(id#7514 as bigint)], [id#7830L], Inner, BuildLeft, false\n                         :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n                         :  +- *(2) HashAggregate(keys=[id#7514], functions=[], output=[id#7514])\n                         :     +- Exchange hashpartitioning(id#7514, 200)\n                         :        +- *(1) HashAggregate(keys=[id#7514], functions=[], output=[id#7514])\n                         :           +- *(1) Filter isnotnull(id#7514)\n                         :              +- *(1) Project [cast(((rand(8188829649009385616) * 3.0E7) * 2.0) as int) AS id#7514]\n                         :                 +- *(1) Range (0, 100, step=1, splits=36)\n                         +- *(3) Project [id#7830L, _row_id_#7839L, _file_name_#7844]\n                            +- *(3) Filter (par#7831 IN (1,0) && isnotnull(id#7830L))\n                               +- *(3) Project [id#7830L, par#7831, _row_id_#7839L, input_file_name() AS _file_name_#7844]\n                                  +- *(3) Project [id#7830L, par#7831, monotonically_increasing_id() AS _row_id_#7839L]\n                                     +- *(3) Project [id#7830L, par#7831, ts#7832]\n                                        +- *(3) FileScan parquet [id#7830L,ts#7832,par#7831] Batched: true, DataFilters: [], Format: Parquet, Location: TahoeBatchFileIndex[dbfs:/user/hive/warehouse/delta_merge_into], PartitionCount: 2, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,ts:timestamp>",
          "title" : "How to improve performance of Delta Lake MERGE INTO queries using partition pruning",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/delta-merge-into"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/spark-overwrite-cancel",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Create table in overwrite mode fails when interrupted Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Version Cause Solution Problem When you attempt to rerun an Apache Spark write operation by cancelling the currently running job, the following error occurs: Console Copy Error: org.apache.spark.sql.AnalysisException: Cannot create the managed table('`testdb`.` testtable`').\nThe associated location ('dbfs:/user/hive/warehouse/testdb.db/metastore_cache_ testtable) already exists.;\n Version This problem can occur in Databricks Runtime 5.0 and above. Cause This problem is due to a change in the default behavior of Spark in version 2.4. This problem can occur if: The cluster is terminated while a write operation is in progress. A temporary network issue occurs. The job is interrupted. Once the metastore data for a particular table is corrupted, it is hard to recover except by dropping the files in that location manually. Basically, the problem is that a metadata directory called _STARTED isn’t deleted automatically when Azure Databricks tries to overwrite it. You can reproduce the problem by following these steps: Create a DataFrame: val df = spark.range(1000) Write the DataFrame to a location in overwrite mode: df.write.mode(SaveMode.Overwrite).saveAsTable(\"testdb.testtable\") Cancel the command while it is executing. Re-run the write command. Solution Set the flag spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation to true. This flag deletes the _STARTED directory and returns the process to the original state. For example, you can set it in the notebook: Python Copy spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n Or you can set it as a cluster level Spark configuration: ini Copy spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation true\n Another option is to manually clean up the data directory specified in the error message. You can do this with dbutils.fs.rm. Scala Copy dbutils.fs.rm(\"<path-to-directory>\", true)",
          "title" : "Create table in overwrite mode fails when interrupted",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/spark-overwrite-cancel"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/delta-merge-cannot-resolve-field",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Delta Merge cannot resolve nested field Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are attempting a Delta Merge with automatic schema evolution, but it fails with a Delta Merge: cannot resolve 'field' due to data type mismatch error message. Cause This can happen if you have made changes to the nested column fields. For example, assume we have a column called Address with the fields streetName, houseNumber, and city nested inside. Attempting to add an additional field, or remove a field, causes any upcoming insert or update transaction on the table to fail, even if mergeSchema is true for the transaction. Solution This behavior is by design. The Delta automatic schema evolution feature only supports top level columns. Nested fields are not supported. Please review the Delta Lake Automatic schema evolution documentation for more information.",
          "title" : "Delta Merge cannot resolve nested field",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/delta-merge-cannot-resolve-field"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/filereadexception-when-reading-delta-table",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents FileReadException when reading a Delta table Article 04/14/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You attempt to read a Delta table from mounted storage and get a FileReadException error. Console Copy FileReadException: Error while reading file abfss:REDACTED@REDACTED.dfs.core.windows.net/REDACTED/REDACTED/REDACTED/REDACTED/PARTITION=REDACTED/part-00042-0725ec45-5c32-412a-ab27-5bc88c058773.c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\n Caused by: FileNotFoundException: Operation failed: 'The specified path does not exist.', 404, HEAD, https:// REDACTED.dfs.core.windows.net/ REDACTED/ REDACTED/REDACTED/REDACTED/PARTITION=REDACTED/part-00042-0725ec45-5c32-412a-ab27-5bc88c058773.c000.snappy.parquet?upn=false&action=getStatus&timeout=90\n Cause FileReadException errors occur when the underlying data does not exist. The most common cause is manual deletion. If the underlying data was not manually deleted, the mount point for the storage blob was removed and recreated while the cluster was writing to the Delta table. Delta Lake does not fail a table write if the location is removed while the data write is ongoing. Instead, a new folder is created in the default storage account of the workspace, with the same path as the removed mount. Data continues to be written in that location. If the mount is recreated before the write operation is finished, and the Delta transaction logs are made available again, Delta updates the transaction logs and the write is considered successful. When this happens, data files written to the default storage account while the mount was deleted are not accessible, as the path currently references the mounted storage account location. Note You can use diagnostic logging to verify that a mount was removed. Query the dbfs table for mount and unmount events. For example: Copy DatabricksDBFS\n| where ActionName == \"unmount\" or ActionName == \"mount\"\n Solution You can restore the missing data in one of two ways. Repair the Delta table and add the missing data back with a custom job. Use FSCK to repair the table. SQL Copy FSCK REPAIR TABLE <table-name>\n Rewrite the missing data with a custom job. This option is a good choice if you can re-run the last job without risking duplicate data. Manually recover the missing files. Verify that there are no active jobs reading or writing to the mounted storage account that contains the Delta table. Unmount the mount path. This allows you to access the /mnt/<path-to-table> directory in the default storage account. Python Copy dbutils.fs.unmount(\"/mnt/<mount-containing-table>\")\n Use dbutils.fs.mv to move the files located in the table path to a temporary location. Python Copy dbutils.fs.mv(\"/mnt/<path-to-table>\", \"/tmp/tempLocation/\", True))\n Recreate the mount point. Python Copy dbutils.fs.mount(source = \"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/\", mount_point = \"/mnt/<mount-containing-table>\", extra_configs = configs)\n Review the Mount ADLS Gen2 storage documentation for more information. Move the files from the temporary location to the updated Delta table path. Python Copy dbutils.fs.mv(\"/tmp/tempLocation\", \"/mnt/<path-to-table>\", True))\n If any jobs are reading or writing to the mount point when you attempt a manual recovery you may cause the issue to reoccur. Verify that the mount is not in use before attempting a manual repair. Best practices Instruct users to get approval before unmounting a storage location. If you must unmount a storage location, verify there are no jobs running on the cluster. Use dbutils.fs.updateMount to update information about the mount. Do not use unmount and mount to update the mount. Use diagnostic logging to identify any possible unmount issues. Run production jobs only on job clusters which are not affected by temporary unmount commands while running, unless they run the dbutils.fs.refreshMounts command. When running jobs on interactive clusters, add a verification step at the end of a job (such as a count) to check for missing data files. If any are missing an error is triggered immediately.",
          "title" : "FileReadException when reading a Delta table",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/delta/filereadexception-when-reading-delta-table"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/active-vs-dead-jobs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Distinguish active and dead jobs Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution This article describes how to distinguish between active and dead jobs. Problem On clusters where there are too many concurrent jobs, you often see some jobs stuck in the Spark UI without any progress. This complicates identifying which are the active jobs/stages versus the dead jobs/stages. Cause Whenever there are too many concurrent jobs running on a cluster, there is a chance that the Spark internal eventListenerBus drops events. These events are used to track job progress in the Spark UI. Whenever the event listener drops events you start seeing dead jobs/stages in Spark UI, which never finish. The jobs are actually finished but not shown as completed in the Spark UI. You observe the following traces in driver logs: Console Copy 18/01/25 06:37:32 WARN LiveListenerBus: Dropped 5044 SparkListenerEvents since Thu Jan 25 06:36:32 UTC 2018\n Solution There is no way to remove dead jobs from the Spark UI without restarting the cluster. However, you can identify the active jobs and stages by running the following commands: Scala Copy sc.statusTracker.getActiveJobIds()  // Returns an array containing the IDs of all active jobs.\nsc.statusTracker.getActiveStageIds() // Returns an array containing the IDs of all active stages.",
          "title" : "Distinguish active and dead jobs",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/active-vs-dead-jobs"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/jobs-idempotency",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to ensure idempotency for jobs Article 03/11/2022 2 minutes to read 3 contributors In this article When you submit jobs through the Azure Databricks Jobs REST API, idempotency is not guaranteed. If the client request is timed out and the client resubmits the same request, you may end up with duplicate jobs running. To ensure job idempotency when you submit jobs through the Jobs API, you can use an idempotency token to define a unique value for a specific job run. If the same job has to be retried because the client did not receive a response due to a network error, the client can retry the job using the same idempotency token, ensuring that a duplicate job run is not triggered. Here is an example of a REST API payload for the Runs Submit API using an idempotency_token with a value of 123: JSON Copy {\n  \"run_name\":\"my spark task\",\n  \"new_cluster\": {\n    \"spark_version\":\"5.5.x-scala2.11\",\n    \"node_type_id\":\"Standard_D3_v2\",\n    \"num_workers\":10\n  },\n  \"libraries\": [\n    {\n      \"jar\":\"dbfs:/my-jar.jar\"\n    },\n    {\n      \"maven\": {\n        \"coordinates\":\"org.jsoup:jsoup:1.7.2\"\n      }\n    }\n  ],\n  \"spark_jar_task\": {\n    \"main_class_name\":\"com.databricks.ComputeModels\"\n  },\n  \"idempotency_token\":\"123\"\n}\n All requests with the same idempotency token should return 200 with the same run ID.",
          "title" : "How to ensure idempotency for jobs",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/jobs-idempotency"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/performance-degradation",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Streaming job has degraded performance Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have a streaming job which has its performance degrade over time. You start a new streaming job with the same configuration and same source, and it performs better than the existing job. Cause Issues with old checkpoints can result in performance degradation in long running streaming jobs. This can happen if the job was intermittently halted and restarted from the same checkpoint. You can validate the issue by reviewing the latest micro batch offset sequence number. Solution Change the checkpoint directory. Avoid restarting old streaming jobs with the same checkpoint directories. If you cannot change the checkpoint directory, increase the cluster capacity.",
          "title" : "Streaming job has degraded performance",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/performance-degradation"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/spark-ui-not-in-sync-with-job",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Apache Spark UI is not in sync with job Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem The status of your Spark jobs is not correctly shown in the Spark UI. Some of the jobs that are confirmed to be in the Completed state are shown as Active/Running in the Spark UI. In some cases the Spark UI may appear blank. When you review the driver logs, you see an AsyncEventQueue warning. Console Copy Logs\n=====\n20/12/23 21:20:26 WARN AsyncEventQueue: Dropped 93909 events from shared since Wed Dec 23 21:19:26 UTC 2020.\n20/12/23 21:21:26 WARN AsyncEventQueue: Dropped 52354 events from shared since Wed Dec 23 21:20:26 UTC 2020.\n20/12/23 21:22:26 WARN AsyncEventQueue: Dropped 94137 events from shared since Wed Dec 23 21:21:26 UTC 2020.\n20/12/23 21:23:26 WARN AsyncEventQueue: Dropped 44245 events from shared since Wed Dec 23 21:22:26 UTC 2020.\n20/12/23 21:24:26 WARN AsyncEventQueue: Dropped 126763 events from shared since Wed Dec 23 21:23:26 UTC 2020.\n20/12/23 21:25:26 WARN AsyncEventQueue: Dropped 94156 events from shared since Wed Dec 23 21:24:26 UTC 2020.\n Note This is related to Apache Spark UI shows wrong number of jobs. Cause All Spark jobs, stages, and tasks are pushed to the event queue. The backend listener reads the Spark UI events from this queue and renders the Spark UI. The default capacity of the event queue (spark.scheduler.listenerbus.eventqueue.capacity) is 20000. If more events are pushed to the event queue than the backend listener can consume, the oldest events get dropped from the queue and the listener never consumes them. These events are lost and do not get rendered in the Spark UI. Solution Set the value of spark.scheduler.listenerbus.eventqueue.capacity in your cluster’s Spark Config at cluster level to a value greater than 20000. This value sets the capacity for the app status event queue, which holds events for internal application status listeners. Increasing this value allows the event queue to hold a larger number of events, but may result in the driver using more memory.",
          "title" : "Apache Spark UI is not in sync with job",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/scala/spark-ui-not-in-sync-with-job"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/custom-garbage-collection-prevents-launch-dbr10",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Custom garbage collection prevents cluster launch Article 03/15/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to use a custom Apache Spark garbage collection algorithm (other than the default one (parallel garbage collection) on clusters running Databricks Runtime 10.0 and above. When you try to start a cluster, it fails to start. If the configuration is set on an executor, the executor is immediately terminated. For example, if you set either of the following custom garbage collection algorithms in your Spark config, the cluster creation fails. Spark driver Copy spark.driver.extraJavaOptions  -XX:+UseG1GC\n Spark executor Copy spark.executor.extraJavaOptions -XX:+UseG1GC\n Cause A new Java virtual machine (JVM) flag was introduced to set the garbage collection algorithm to parallel garbage collection. If you do not change the default, the change has no impact. If you change the garbage collection algorithm by setting spark.executor.extraJavaOptions or spark.driver.extraJavaOptions in your Spark config, the value conflicts with the new flag. As a result, the JVM crashed and prevents the cluster from starting. Solution To work around this issue, you must explicitly remove the parallel garbage collection flag in your Spark config. This must be done at the cluster level. Copy spark.driver.extraJavaOptions -XX:-UseParallelGC -XX:+UseG1GC\n\nspark.executor.extraJavaOptions -XX:-UseParallelGC -XX:+UseG1GC",
          "title" : "Custom garbage collection prevents cluster launch",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/custom-garbage-collection-prevents-launch-dbr10"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbsql/display-null-as-nan",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Null column values display as NaN Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have a table with null values in some columns. When you query the table using a select statement in Azure Databricks, the null values appear as null. When you query the table using the same select statement in Databricks SQL, the null values appear as NaN. SQL Copy select * from default.<table-name> where <column-name> is null\n Azure Databricks Databricks SQL Cause NaN is short for not a number. This is how null values are displayed in Databricks SQL. Solution This is not a problem. Databricks SQL is working as designed. The representation of null values in Databricks SQL is different from the representation of null values in Azure Databricks, but the data itself is not changed.",
          "title" : "Null column values display as NaN",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbsql/display-null-as-nan"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbsql/retrieve-queries-disabled-user",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Retrieve queries owned by a disabled user Article 03/11/2022 2 minutes to read 2 contributors In this article Clone a query Delete a query When a Databricks SQL user is removed from an organization, the queries owned by the user remain, but they are only visible to those who already have permission to access them. A Databricks SQL admin can transfer ownership to other users, as well as delete alerts, dashboards, and queries owned by the disabled user account. Clone a query A Databricks SQL admin has view access to all queries by default. As an admin, you can view and delete any query. You cannot edit a query, if it is not shared with you. This includes admin users. The solution is to clone a query, and then edit the permissions. Open Databricks SQL Click Queries. Click Admin View. Select the query you want to clone. Click the vertical ellipsis and select Clone. You can now edit the copy of the original query as needed. Delete a query Open Databricks SQL Click Queries. Click Admin View. Select the query you want to delete. Click the vertical ellipsis and select Move to Trash. Click Move to Trash to confirm. Note When you delete a query, all alerts and dashboard widgets created with its visualizations are also deleted.",
          "title" : "Retrieve queries owned by a disabled user",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dbsql/retrieve-queries-disabled-user"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/get-spark-config-in-dbconnect",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Get Apache Spark config in DBConnect Article 03/11/2022 2 minutes to read 2 contributors In this article You can always view the Spark configuration for your cluster by reviewing the cluster details in the workspace. If you are using DBConnect you may want to quickly review the current Spark configuration details without switching over to the workspace UI. This example code shows you how to get the current Spark configuration for your cluster by making a REST API call in DBConnect. Python Copy import json\nimport requests\nimport base64\n\nwith open(\"/<path-to-dbconnect-config>/.databricks-connect\") as readconfig:\n    conf = json.load(readconfig)\n\nCLUSTER_ID = conf[\"cluster_id\"]\nTOKEN = conf[\"token\"]\nAPI_URL = conf[\"host\"]\n\nheaders = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + TOKEN}\npayload =  {'cluster_id': '' + CLUSTER_ID}\nresponse = requests.get(API_URL + \"/api/2.0/clusters/get/?cluster_id=\"+CLUSTER_ID, headers=headers, json = payload)\nsparkconf = response.json()[\"spark_conf\"]\n\nfor config_key, config_value in sparkconf.items():\n    print(config_key, config_value)\n Important DBConnect only works with supported Databricks Runtime versions. Ensure that you are using a supported runtime on your cluster before using DBConnect.",
          "title" : "Get Apache Spark config in DBConnect",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/get-spark-config-in-dbconnect"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/invalid-access-token-airflow",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Invalid Access Token error when running jobs with Airflow Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem When you run scheduled Airflow Azure Databricks jobs, you get this error: Console Copy Invalid Access Token : 403 Forbidden Error\n Cause To run or schedule Azure Databricks jobs through Airflow, you need to configure the Azure Databricks connection using the Airflow web UI. Any of the following incorrect settings can cause the error: Set the host field to the Azure Databricks workspace hostname. Set the login field to token. Set the password field to the Azure Databricks-generated personal access token. Set the Extra field to a JSON string, where the key is token and the value is your personal access token. The Azure Databricks-generated personal access token is normally valid for 90 days. If the token expires, then this 403 Forbidden Error occurs. Solution Verify that the Extra field is correctly configured with the JSON string: JSON Copy {\"token\": \"<your personal access token>\"}\n Verify that the token is mentioned in both the password field and the Extra field. Verify that the host, login, and password fields are configured correctly. Verify that the personal access token has not expired. If necessary, generate a new token.",
          "title" : "Invalid Access Token error when running jobs with Airflow",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/invalid-access-token-airflow"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/execution/spark-serialized-task-is-too-large",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Serialized task is too large Article 03/11/2022 2 minutes to read 3 contributors In this article If you see the follow error message, you may be able to fix this error by changing the Spark configuration when you start the cluster. To change the Spark configuration, set the property: While tuning the configuration is one option, typically this error message means that you send some large objects from the driver to executors, e.g., call parallelize with a large list, or convert a large R DataFrame to a Spark DataFrame. If so, we recommend first auditing your code to remove large objects that you use, or leverage broadcast variables instead. If that does not resolve this error, you can increase the partition number to split the large list to multiple small ones to reduce the Spark RPC message size. Here are examples for Scala and Python: Scala Copy val largeList = Seq(...) // This is a large list\nval partitionNum = 100 // Increase this number if necessary\nval rdd = sc.parallelize(largeList, partitionNum)\nval ds = rdd.toDS()\n Python Copy largeList = [...] # This is a large list\npartitionNum = 100 # Increase this number if necessary\nrdd = sc.parallelize(largeList, partitionNum)\nds = rdd.toDS()\n R users need to increase the Spark configuration spark.default.parallelism to increase the partition number at cluster initialization. You cannot set this configuration after cluster creation.",
          "title" : "Serialized task is too large",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/execution/spark-serialized-task-is-too-large"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metrics/explore-spark-metrics",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to explore Apache Spark metrics with Spark listeners Article 03/11/2022 3 minutes to read 3 contributors In this article Build the Spark Metrics package Gather metrics Apache Spark provides several useful internal listeners that track metrics about tasks and jobs. During the development cycle, for example, these metrics can help you to understand when and why a task takes a long time to finish. Of course, you can leverage the Spark UI or History UI to see information for each task and stage, but there are some downsides. For instance, you can’t compare the statistics for two Spark jobs side by side, and the Spark History UI can take a long time to load for large Spark jobs. You can extract the metrics generated by Spark internal classes and persist them to disk as a table or a DataFrame. Then you can query the DataFrame just like any other data science table. You can use this SparkTaskMetrics package to explore how to use Spark listeners to extract metrics from tasks and jobs. Build the Spark Metrics package Use the following command to build the package. Bash Copy sbt package\n Gather metrics Import TaskMetricsExplorer. Create the query sql(\"\"\"SELECT * FROM nested_data\"\"\").show(false) and pass it into runAndMeasure. The query should include at least one Spark action in order to trigger a Spark job. Spark does not generate any metrics until a Spark job is executed. For example: Scala Copy import com.databricks.TaskMetricsExplorer\n\nval t = new TaskMetricsExplorer(spark)\nsql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW nested_data AS\n       SELECT id AS key,\n       ARRAY(CAST(RAND(1) * 100 AS INT), CAST(RAND(2) * 100 AS INT), CAST(RAND(3) * 100 AS INT), CAST(RAND(4) * 100 AS INT), CAST(RAND(5) * 100 AS INT)) AS values,\n       ARRAY(ARRAY(CAST(RAND(1) * 100 AS INT), CAST(RAND(2) * 100 AS INT)), ARRAY(CAST(RAND(3) * 100 AS INT), CAST(RAND(4) * 100 AS INT), CAST(RAND(5) * 100 AS INT))) AS nested_values\n       FROM range(5)\"\"\")\nval query = sql(\"\"\"SELECT * FROM nested_data\"\"\").show(false)\nval res = t.runAndMeasure(query)\n The runAndMeasure method runs the command and gets the task’s internal metrics using a Spark listener. It then runs the query and returns the result: Copy +---+-------------------+-----------------------+\n|key|values             |nested_values          |\n+---+-------------------+-----------------------+\n|0  |[26, 11, 66, 8, 47]|[[26, 11], [66, 8, 47]]|\n|1  |[66, 8, 47, 91, 6] |[[66, 8], [47, 91, 6]] |\n|2  |[8, 47, 91, 6, 70] |[[8, 47], [91, 6, 70]] |\n|3  |[91, 6, 70, 41, 19]|[[91, 6], [70, 41, 19]]|\n|4  |[6, 70, 41, 19, 12]|[[6, 70], [41, 19, 12]]|\n+---+-------------------+-----------------------+\n The task metrics information is saved in a DataFrame. You can display it with this command: Scala Copy res.select($\"stageId\", $\"taskType\", $\"taskLocality\", $\"executorRunTime\", $\"duration\", $\"executorId\", $\"host\", $\"jvmGCTime\").show(false)\n Then you will get: Copy +-------+----------+-------------+---------------+--------+----------+---------+---------+\n|stageId|taskType  |taskLocality |executorRunTime|duration|executorId| host    |jvmGCTime|\n+-------+----------+-------------+---------------+--------+----------+---------+---------+\n|3      |ResultTask|PROCESS_LOCAL|2              |9       |driver    |localhost|0        |\n|4      |ResultTask|PROCESS_LOCAL|3              |11      |driver    |localhost|0        |\n|4      |ResultTask|PROCESS_LOCAL|3              |16      |driver    |localhost|0        |\n|4      |ResultTask|PROCESS_LOCAL|2              |20      |driver    |localhost|0        |\n|4      |ResultTask|PROCESS_LOCAL|4              |22      |driver    |localhost|0        |\n|5      |ResultTask|PROCESS_LOCAL|2              |12      |driver    |localhost|0        |\n|5      |ResultTask|PROCESS_LOCAL|3              |17      |driver    |localhost|0        |\n|5      |ResultTask|PROCESS_LOCAL|7              |21      |driver    |localhost|0        |\n+-------+----------+-------------+---------------+--------+----------+---------+---------+\n To view all available metrics names and data types, display the schema of the res DataFrame: Scala Copy res.schema.treeString\n Copy root\n |-- stageId: integer (nullable = false)\n |-- stageAttemptId: integer (nullable = false)\n |-- taskType: string (nullable = true)\n |-- index: long (nullable = false)\n |-- taskId: long (nullable = false)\n |-- attemptNumber: integer (nullable = false)\n |-- launchTime: long (nullable = false)\n |-- finishTime: long (nullable = false)\n |-- duration: long (nullable = false)\n |-- schedulerDelay: long (nullable = false)\n |-- executorId: string (nullable = true)\n |-- host: string (nullable = true)\n |-- taskLocality: string (nullable = true)\n |-- speculative: boolean (nullable = false)\n |-- gettingResultTime: long (nullable = false)\n |-- successful: boolean (nullable = false)\n |-- executorRunTime: long (nullable = false)\n |-- executorCpuTime: long (nullable = false)\n |-- executorDeserializeTime: long (nullable = false)\n |-- executorDeserializeCpuTime: long (nullable = false)\n |-- resultSerializationTime: long (nullable = false)\n |-- jvmGCTime: long (nullable = false)\n |-- resultSize: long (nullable = false)\n |-- numUpdatedBlockStatuses: integer (nullable = false)\n |-- diskBytesSpilled: long (nullable = false)\n |-- memoryBytesSpilled: long (nullable = false)\n |-- peakExecutionMemory: long (nullable = false)\n |-- recordsRead: long (nullable = false)\n |-- bytesRead: long (nullable = false)\n |-- recordsWritten: long (nullable = false)\n |-- bytesWritten: long (nullable = false)\n |-- shuffleFetchWaitTime: long (nullable = false)\n |-- shuffleTotalBytesRead: long (nullable = false)\n |-- shuffleTotalBlocksFetched: long (nullable = false)\n |-- shuffleLocalBlocksFetched: long (nullable = false)\n |-- shuffleRemoteBlocksFetched: long (nullable = false)\n |-- shuffleWriteTime: long (nullable = false)\n |-- shuffleBytesWritten: long (nullable = false)\n |-- shuffleRecordsWritten: long (nullable = false)\n |-- errorMessage: string (nullable = true)",
          "title" : "How to explore Apache Spark metrics with Spark listeners",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metrics/explore-spark-metrics"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metrics/spark-metrics",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to use Apache Spark metrics Article 03/11/2022 2 minutes to read 3 contributors In this article This article gives an example of how to monitor Apache Spark components using the Spark configurable metrics system. Specifically, it shows how to set a new source and enable a sink. For detailed information about the Spark components available for metrics collection, including sinks supported out of the box, follow the documentation link above. Note There are several other ways to collect metrics to get insight into how a Spark job is performing, which are also not covered in this article: SparkStatusTracker (Source, API): monitor job, stage, or task progress StreamingQueryListener (Source, API): intercept streaming events SparkListener: intercept events from Spark scheduler For information about using other third-party tools to monitor Spark jobs in Azure Databricks, see Metrics. How does this metrics collection system work? Upon instantiation, each executor creates a connection to the driver to pass the metrics. The first step is to write a class that extends the Source trait: Scala Copy class MySource extends Source {\n  override val sourceName: String = \"MySource\"\n\n  override val metricRegistry: MetricRegistry = new MetricRegistry\n\n  val FOO: Histogram = metricRegistry.histogram(MetricRegistry.name(\"fooHistory\"))\n  val FOO_COUNTER: Counter = metricRegistry.counter(MetricRegistry.name(\"fooCounter\"))\n}\n The next step is to enable the sink. In this example, the metrics are printed to the console: Scala Copy val spark: SparkSession = SparkSession\n    .builder\n    .master(\"local[*]\")\n    .appName(\"MySourceDemo\")\n    .config(\"spark.driver.host\", \"localhost\")\n    .config(\"spark.metrics.conf.*.sink.console.class\", \"org.apache.spark.metrics.sink.ConsoleSink\")\n.getOrCreate()\n Note To sink metrics to Prometheus, you can use this third-party library: https://github.com/banzaicloud/spark-metrics. The last step is to instantiate the source and register it with SparkEnv: Scala Copy val source: MySource = new MySource\nSparkEnv.get.metricsSystem.registerSource(source)\n You can view a complete, buildable example at https://github.com/newroyker/meter.",
          "title" : "How to use Apache Spark metrics",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/metrics/spark-metrics"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/json-reader-parses-value-as-null",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents JSON reader parses values as null Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are attempting to read a JSON file. You know the file has data in it, but the Apache Spark JSON reader is returning a null value. Example code You can use this example code to reproduce the problem. Create a test JSON file in DBFS. Python Copy dbutils.fs.rm(\"dbfs:/tmp/json/parse_test.txt\")\ndbutils.fs.put(\"dbfs:/tmp/json/parse_test.txt\",\n\"\"\"\n{\"data_flow\":{\"upstream\":[{\"$\":{\"source\":\"input\"},\"cloud_type\":\"\"},{\"$\":{\"source\":\"File\"},\"cloud_type\":{\"azure\":\"cloud platform\",\"aws\":\"cloud service\"}}]}}\n\"\"\")\n Read the JSON file. Python Copy jsontest = spark.read.option(\"inferSchema\",\"true\").json(\"dbfs:/tmp/json/parse_test.txt\")\ndisplay(jsontest)\n The result is a null value. Cause In Spark 2.4 and below, the JSON parser allows empty strings. Only certain data types, such as IntegerType are treated as null when empty. In Spark 3.0 and above, the JSON parser does not allow empty strings. An exception is thrown for all data types, except BinaryType and StringType. For more information, review the Spark SQL Migration Guide. Example code The example code shows the error because the data has two identical classification fields. The first cloud_type entry is an empty string. The second cloud_type entry has data. JSON Copy \"cloud_type\":\"\"\n\"cloud_type\":{\"azure\":\"cloud platform\",\"aws\":\"cloud service\"}\n Because the JSON parser does not allow empty strings in Spark 3.0 and above, a null value is returned as output. Solution Set the Spark configuration value spark.sql.legacy.json.allowEmptyString.enabled to True. This configures the Spark 3.0 JSON parser to allow empty strings. You can set this configuration at the cluster level or the notebook level. Example code Python Copy spark.conf.set(\"spark.sql.legacy.json.allowEmptyString.enabled\", True)\n\njsontest1 = spark.read.option(\"inferSchema\",\"true\").json(\"dbfs:/tmp/json/parse_test.txt\")\ndisplay(jsontest1)",
          "title" : "JSON reader parses values as null",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/json-reader-parses-value-as-null"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/slowdown-from-root-disk-fill",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cluster slowdown due to Ganglia metrics filling root partition Article 03/11/2022 2 minutes to read 4 contributors In this article Problem Cause Solution Note This article applies to Databricks Runtime 7.3 LTS and below. Problem Clusters start slowing down and may show a combination of the following symptoms: Unhealthy cluster events are reported: Request timed out. Driver is temporarily unavailable. Metastore is down. DBFS is down. You do not see any high GC events or memory utilization associated with the driver process. When you use top on the driver node you see an intermittent high average load. The Ganglia related gmetad process shows intermittent high CPU utilization. The root disk shows high disk usage with df -h /. Specifically, /var/lib/ganglia/rrds shows high disk usage. The Ganglia UI is unable to show the load distribution. You can verify the issue by looking for files with local in the prefix in /var/lib/ganglia/rrds. Generally, this directory should only have files prefixed with application-<applicationId>. For example: Bash Copy %sh ls -ltrhR /var/lib/ganglia/rrds/  | grep -i local\n\nrw-rw-rw- 1 ganglia ganglia 616K Jun 29 18:00 local-1593453624916.driver.Databricks.directoryCommit.markerReadErrors.count.rrd -rw-rw-rw- 1 ganglia ganglia 616K Jun 29 18:00 local-1593453614595.driver.Databricks.directoryCommit.deletedFilesFiltered.count.rrd -rw-rw-rw- 1 ganglia ganglia 616K Jun 29 18:00 local-1593453614595.driver.Databricks.directoryCommit.autoVacuumCount.count.rrd -rw-rw-rw- 1 ganglia ganglia 616K Jun 29 18:00 local-1593453605184.driver.CodeGenerator.generatedMethodSize.min.rrd\n Cause Ganglia metrics typically use less than 10GB of disk space. However, under certain circumstances, a “data explosion” can occur, which causes the root partition to fill with Ganglia metrics. Data explosions also create a dirty cache. When this happens, the Ganglia metrics can consume more than 100GB of disk space on root. This “data explosion” can happen if you define the spark session variable as global in your Python file and then call functions defined in the same file to perform Apache Spark transformation on data. When this happens, the Spark session logic can be serialized, along with the required function definition, resulting in a Spark session being created on the worker node. For example, take the following Spark session definition: Python Copy from pyspark.sql import SparkSession\n\ndef get_spark():\n    \"\"\"Returns a spark session.\"\"\"\n    return SparkSession.builder.getOrCreate()\n\nif \"spark\" not in globals():\n  spark = get_spark()\n\ndef generator(partition):\n    print(globals()['spark'])\n    for row in partition:\n        yield [word.lower() for word in row[\"value\"]]\n If you use the following example commands, local prefixed files are created: Python Copy from repro import ganglia_test\ndf = spark.createDataFrame([([\"Hello\"], ), ([\"Spark\"], )], [\"value\"])\ndf.rdd.mapPartitions(ganglia_test.generator).toDF([\"value\"]).show()\n The print(globals()['spark']) statement in the generator() function doesn’t result in an error, because it is available as a global variable in the worker nodes. It may fail with an invalid key error in some cases, as that value is not available as a global variable. Streaming jobs that execute on short batch intervals are susceptible to this issue. Solution Ensure that you are not using SparkSession.builder.getOrCreate() to define a Spark session as a global variable. When you troubleshoot, you can use the timestamps on files with the local prefix to help determine when a problematic change was first introduced.",
          "title" : "Cluster slowdown due to Ganglia metrics filling root partition",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/slowdown-from-root-disk-fill"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/spark-shows-less-memory",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Apache Spark UI shows less than total node memory Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem The Executors tab in the Spark UI shows less memory than is actually available on the node: An F8s instance (16 GB, 4 core) for the driver node, shows 4.5 GB memory on the Executors tab An F4s instance (8 GB, 4 core) for the driver node, shows 710 GB memory on the Executors tab: Cause The total amount of memory shown is less than the memory on the cluster because some memory is occupied by kernel and node-level services. Solution To calculate the available amount of memory, you can use the formula used for executor memory allocation (all_memory_size * 0.97 - 4800MB) * 0.8, where: 0.97 accounts for kernel overhead. 4800 MB accounts for internal node-level services (node daemon, log daemon, and so on). 0.8 is a heuristic to ensure the LXC container running the Spark process doesn’t crash due to out-of-memory errors. Total available memory for storage on an F4s instance is (8192MB * 0.97 - 4800MB) * 0.8 - 1024 = 1.2 GB. Because the parameter spark.memory.fraction is by default 0.6, approximately (1.2 * 0.6) = ~710 MB is available for storage. You can change the spark.memory.fraction Spark configuration to adjust this parameter. Calculate the available memory for a new parameter as follows: If you use an F4s instance, which has 8192 MB memory, it has available memory 1.2 GB. If you specify a spark.memory.fraction of 0.8, the Executors tab in the Spark UI should show: (1.2 * 0.8) GB = ~960 MB.",
          "title" : "Apache Spark UI shows less than total node memory",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/spark-shows-less-memory"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/maven-library-version-mgmt",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to correctly update a Maven library in Azure Databricks Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You make a minor update to a library in the repository, but you don’t want to change the version number because it is a small change for testing purposes. When you attach the library to your cluster again, your code changes are not included in the library. Cause One strength of Azure Databricks is the ability to install third-party or custom libraries, such as from a Maven repository. However, when a library is updated in the repository, there is no automated way to update the corresponding library in the cluster. When you request Azure Databricks to download a library in order to attach it to a cluster, the following process occurs: In Azure Databricks, you request a library from a Maven repository. Azure Databricks checks the local cache for the library, and if it is not present, downloads the library from the Maven repository to a local cache. Azure Databricks then copies the library to DBFS (/FileStore/jars/maven/). Upon subsequent requests for the library, Azure Databricks uses the file that has already been copied to DBFS, and does not download a new copy. Solution To ensure that an updated version of a library (or a library that you have customized) is downloaded to a cluster, make sure to increment the build number or version number of the artifact in some way. For example, you can change libA_v1.0.0-SNAPSHOT to libA_v1.0.1-SNAPSHOT, and then the new library will download. You can then attach it to your cluster.",
          "title" : "How to correctly update a Maven library in Azure Databricks",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/maven-library-version-mgmt"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/incorrect-results-docs-as-input",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Incorrect results when using documents as inputs Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have a ML model that takes documents as inputs, specifically, an array of strings. You use a feature extractor like TfidfVectorizer to convert the documents to an array of strings and ingest the array into the model. The model is trained, and predictions happen in the notebook, but model serving doesn’t return the expected results for JSON inputs. Cause TfidfVectorizer expects an array of documents as an input. Azure Databricks converts inputs to Pandas DataFrames, which TfidfVectorizer does not process correctly. Solution You must create a custom transformer and add it to the head of the pipeline. For example, the following sample code checks the input for DataFrames. If it finds a DataFrame, the first column is converted to an array of documents. The array of documents is then passed to TfidfVectorizer before being ingested into the model. Python Copy class DataFrameToDocs():\n    def transform(self, input_df):\n        import pandas as pd\n        if isinstance(input_df, pd.DataFrame):\n          return input_df[0].values\n        else:\n          return input_df    def fit(self, X, y=None, **fit_params):\n        return self\n\nsteps = [('dftodocs', DataFrameToDocs()),('tfidf', TfidfVectorizer()), ('nb_clf', MultinomialNB())]\npipeline = Pipeline(steps)\n Note When input as JSON, both [\"Hello\", \"World\"] and [[\"Hello\"],[\"World\"]] return the same output.",
          "title" : "Incorrect results when using documents as inputs",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/incorrect-results-docs-as-input"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/kfold-cross-validation",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to perform group K-fold cross validation with Apache Spark Article 03/11/2022 2 minutes to read 3 contributors In this article Cross validation randomly splits the training data into a specified number of folds. To prevent data leakage where the same data shows up in multiple folds you can use groups. scikit-learn supports group K-fold cross validation to ensure that the folds are distinct and non-overlapping. On Spark you can use the spark-sklearn library, which distributes tuning of scikit-learn models, to take advantage of this method. This example tunes a scikit-learn random forest model with the group k-fold method on Spark with a grp variable: Python Copy from sklearn.ensemble import RandomForestClassifier\nfrom spark_sklearn import GridSearchCV\nfrom sklearn.model_selection import GroupKFold\nparam_grid = {\"max_depth\": [8, 12, None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [1, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"],\n              \"n_estimators\": [20, 40, 80]}\ngroup_kfold = GroupKFold(n_splits=3)\ngs = GridSearchCV(sc, estimator = RandomForestClassifier(random_state=42), param_grid=param_grid, cv = group_kfold)\ngs.fit(X1, y1 ,grp)\n Note The library that is used to run the grid search is called spark-sklearn, so you must pass in the Spark context (sc parameter) first. The X1 and y1 parameters must be pandas DataFrames. This grid search option only works on data that fits on the driver.",
          "title" : "How to perform group K-fold cross validation with Apache Spark",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/kfold-cross-validation"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/mlflow-fail-access-hive",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents MLflow project fails to access an Apache Hive table Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem You have an MLflow project that fails to access a Hive table and returns a Table or view not found error. Console Copy pyspark.sql.utils.AnalysisException: \"Table or view not found: `default`.`tab1`; line 1 pos 21;\\n'Aggregate [unresolvedalias(count(1), None)]\\n+- 'UnresolvedRelation `default`.`tab1`\\n\"\nxxxxx ERROR mlflow.cli: === Run (ID 'xxxxx') failed ===\n Cause This happens when the SparkSession object is created inside the MLflow project without Hive support. Solution Configure SparkSession with the .enableHiveSupport() option in the session builder. Do this as part of your MLflow project. Scala Copy val spark = SparkSession.builder.enableHiveSupport().getOrCreate()",
          "title" : "MLflow project fails to access an Apache Hive table",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/mlflow-fail-access-hive"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/python-cmd-fail-conda-cluster",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Python commands fail on Machine Learning clusters Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are using a Databricks Runtime for Machine Learning cluster and Python notebooks are failing. You find an invalid syntax error in the logs. Console Copy SyntaxError: invalid syntax\n  File \"/local_disk0/tmp/1593092990800-0/PythonShell.py\", line 363\n    def __init__(self, *args, condaMagicHandler=None, **kwargs):\n Cause Key values in the /etc/environment/ file are being overwritten by user environment variables. There are several default environment variables that should not be overwritten. For example, MLFLOW_CONDA_HOME=/databricks/conda is set by default. If you overwrite this value it can result in the invalid syntax error. This sample init script can cause the issue, because it is replacing, rather than appending a value. Python Copy dbutils.fs.put(\"/databricks/init-scripts/set-env.sh\", \"\"\"#!/bin/bash\nsudo echo VAR1=\"VAL1\" > /etc/environment\nsudo echo VAR2=\"VAL2\" > /etc/environment\nsudo echo VAR3=\"VAL3\" > /etc/environment\n\"\"\", true)\n Solution You should not overwrite any values in the /etc/environment/ file. You should always append variables to the /etc/environment/ file. This sample init script avoids the issue by appending every to value to the /etc/environment/ file. Python Copy dbutils.fs.put(\"/databricks/init-scripts/set-env.sh\", \"\"\"#!/bin/bash\nsudo echo VAR1=\"VAL1\" >> /etc/environment\nsudo echo VAR2=\"VAL2\" >> /etc/environment\nsudo echo VAR3=\"VAL3\" >> /etc/environment\n\"\"\", true)",
          "title" : "Python commands fail on Machine Learning clusters",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/machine-learning/python-cmd-fail-conda-cluster"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/cannot-import-tabularprediction",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Cannot import TabularPrediction from AutoGluon Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to import TabularPrediction from AutoGluon, but are getting an error message. Console Copy ImportError: cannot import name 'TabularPrediction' from 'autogluon' (unknown location)\n This happens when AutoGluon is installed via a notebook or as a cluster-installed library. You can reproduce the error by running the import command in your notebook: Python Copy import autogluon as ag\nfrom autogluon import TabularPrediction as task\n Cause There is a namespace collision in AutoGluon v0.0.14. autogluon==0.0.14 installs ‘gluoncv>=0.5.0,<1.0’. This results in gluoncv==0.9.0 getting installed, which creates the namespace collision. Solution The namespace collision was resolved in AutoGluon v0.0.15. Upgrade to AutoGluon v0.0.15 to use TabularPrediction. Specify autogluon==0.0.15 when installing AutoGluon as a cluster-installed library from PyPI. You can also install it via a notebook. Bash Copy pip install autogluon==0.0.15 autogluon.tabular \"mxnet<2.0.0\"\n After you have upgraded to AutoGluon v0.0.15, you can successfully import TabularPrediction. Python Copy import autogluon as ag\nfrom autogluon import TabularPrediction as task",
          "title" : "Cannot import TabularPrediction from AutoGluon",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/cannot-import-tabularprediction"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/install-cartopy-on-cluster",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Error when installing Cartopy on a cluster Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are trying to install Cartopy on a cluster and you receive a ManagedLibraryInstallFailed error message. Console Copy java.lang.RuntimeException: ManagedLibraryInstallFailed: org.apache.spark.SparkException: Process List(/databricks/python/bin/pip, install, cartopy==0.17.0, --disable-pip-version-check) exited with code 1.   ERROR: Command errored out with exit status 1:\n   command: /databricks/python3/bin/python3.7 /databricks/python3/lib/python3.7/site-packages/pip/_vendor/pep517/_in_process.py get_requires_for_build_wheel /tmp/tmpjoliwaky\n       cwd: /tmp/pip-install-t324easa/cartopy\n  Complete output (3 lines):\n  setup.py:171: UserWarning: Unable to determine GEOS version. Ensure you have 3.3.3 or later installed, or installation may fail.\n    '.'.join(str(v) for v in GEOS_MIN_VERSION), ))\n  Proj 4.9.0 must be installed.\n  ----------------------------------------\nERROR: Command errored out with exit status 1: /databricks/python3/bin/python3.7 /databricks/python3/lib/python3.7/site-packages/pip/_vendor/pep517/_in_process.py get_requires_for_build_wheel /tmp/tmpjoliwaky Check the logs for full command output.\n for library:PythonPyPiPkgId(cartopy,Some(0.17.0),None,List()),isSharedLibrary=false\n Cause Cartopy has dependencies on libgeos 3.3.3 and above and libproj 4.9.0. If libgeos and libproj are not installed, Cartopy fails to install. Solution Configure a cluster-scoped init script to automatically install Cartopy and the required dependencies. Create the base directory to store the init script in, if the base directory does not exist. Here, use dbfs:/databricks/<directory> as an example. Bash Copy dbutils.fs.mkdirs(\"dbfs:/databricks/<directory>/\")\n Create the script and save it to a file. Bash Copy dbutils.fs.put(\"dbfs:/databricks/<directory>/cartopy.sh\",\"\"\"\n#!/bin/bash\nsudo apt-get install libgeos++-dev -y\nsudo apt-get install libproj-dev -y\n/databricks/python/bin/pip install Cartopy\n\"\"\",True)\n Check that the script exists. Python Copy display(dbutils.fs.ls(\"dbfs:/databricks/<directory>/cartopy.sh\"))\n On the cluster configuration page, click the Advanced Options toggle. At the bottom of the page, click the Init Scripts tab. In the Destination drop-down, select DBFS, provide the file path to the script, and click Add. Restart the cluster.",
          "title" : "Error when installing Cartopy on a cluster",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/install-cartopy-on-cluster"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/install-pygraphviz",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Install PyGraphViz Article 03/11/2022 2 minutes to read 2 contributors In this article Install via notebook Install via init script PyGraphViz Python libraries are used to plot causal inference networks. If you try to install PyGraphViz as a standard library, it fails due to dependency errors. PyGraphViz has the following dependencies: python3-dev graphviz libgraphviz-dev pkg-config Install via notebook Install the dependencies with apt-get. Bash Copy sudo apt-get install -y python3-dev graphviz libgraphviz-dev pkg-config\n After the dependencies are installed, use pip to install PyGraphViz. Bash Copy pip install pygraphviz\n Install via init script Create the init script. Python Copy dbutils.fs.put(\"dbfs:/databricks/<init-script-folder>/install-pygraphviz.sh\",\n\"\"\"\n#!/bin/bash\n#install dependent packages\nsudo apt-get install -y python3-dev graphviz libgraphviz-dev pkg-config\npip install pygraphviz\"\"\", True)\n Install the init script that you just created as a cluster-scoped init script. You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/install-pygraphviz.sh). Restart the cluster after you have installed the init script.",
          "title" : "Install PyGraphViz",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/install-pygraphviz"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/install-tf21-dbr65ml",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Install TensorFlow 2.1 on Databricks Runtime 6.5 ML GPU clusters Article 03/11/2022 2 minutes to read 3 contributors In this article Install the init script Databricks Runtime ML includes versions of TensorFlow so you can use it without installing any packages. Databricks Runtime ML Version TensorFlow Version 7.0 2.2.0 6.3 - 6.6 1.15.0 You can install other versions of TensorFlow by using a cluster-scoped init script. In this article, you learn how to install TensorFlow 2.1 on Databricks Runtime 6.5 ML GPU clusters. Important Removing default libraries and installing new versions may cause instability or completely break your Azure Databricks cluster. You should thoroughly test any new library version in your environment before running production jobs. Install the init script Install the following cluster-scoped init script on your Databricks Runtime 6.5 ML GPU cluster. Scala Copy #!/bin/bash\nset -e\n\napt-get update\napt-get install -y --no-install-recommends --allow-downgrades \\\n  libnccl2=2.4.8-1+cuda10.1 \\\n  libnccl-dev=2.4.8-1+cuda10.1 \\\n  cuda-libraries-10-1 \\\n  libcudnn7=7.6.4.38-1+cuda10.1 \\\n  libcudnn7-dev=7.6.4.38-1+cuda10.1 \\\n  libnvinfer6=6.0.1-1+cuda10.1 \\\n  libnvinfer-dev=6.0.1-1+cuda10.1 \\\n  libnvinfer-plugin6=6.0.1-1+cuda10.1\napt-get clean\nln -sfn cuda-10.1 /usr/local/cuda\n\npip install tensorflow==2.1.* setuptools==41.* grpcio==1.24.*\n\n# This `conda list` is necessary to recognize the pip-installed packages.\nconda list\nconda install cudatoolkit=10.1\n Restart the cluster.",
          "title" : "Install TensorFlow 2.1 on Databricks Runtime 6.5 ML GPU clusters",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/install-tf21-dbr65ml"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-rate-limit",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Job fails due to job rate limit Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem An Azure Databricks notebook or Jobs API request returns the following error: Console Copy Error : {\"error_code\":\"INVALID_STATE\",\"message\":\"There were already 1000 jobs created in past 3600 seconds, exceeding rate limit: 1000 job creations per 3600 seconds.\"}\n Cause This error occurs because the number of jobs per hour exceeds the limit of 1000 established by Azure Databricks to prevent API abuses and ensure quality of service. Solution If you cannot ensure that the number of jobs created in your workspace is less than 1000 per hour, contact Azure Databricks Support to request a higher limit. A job rate limit increase requires at least 20 minutes of downtime. Azure Databricks can increase the job limit maximumJobCreationRate up to 2000. Currently running jobs will be affected while the limit is being increased.",
          "title" : "Job fails due to job rate limit",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-rate-limit"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Streaming: tips and troubleshooting Article 03/11/2022 2 minutes to read 4 contributors In this article These articles can help you with Structured Streaming and Spark Streaming (the legacy Apache Spark streaming feature). Append output is not supported without a watermark Apache Spark DStream is not supported Streaming with File Sink: Problems with recovery if you change checkpoint or output directories Get the path of files consumed by Auto Loader How to restart a structured streaming query from last written offset Kafka error: No resolvable bootstrap urls readStream() is not whitelisted error when running a query Checkpoint files not being deleted when using display() Checkpoint files not being deleted when using foreachBatch() Conflicting directory structures error RocksDB fails to acquire a lock Stream XML files using an auto-loader Streaming job gets stuck writing to checkpoint",
          "title" : "Streaming: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-fails-maxresultsize-exception",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Apache Spark job fails with maxResultSize exception Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem A Spark job fails with a maxResultSize exception: Console Copy org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized\nresults of XXXX tasks (X.0 GB) is bigger than spark.driver.maxResultSize (X.0 GB)\n Cause This error occurs because the configured size limit was exceeded. The size limit applies to the total serialized results for Spark actions across all partitions. The Spark actions include actions such as collect() to the driver node, toPandas(), or saving a large file to the driver local file system. Solution In some situations, you might have to refactor the code to prevent the driver node from collecting a large amount of data. You can change the code so that the driver node collects a limited amount of data or increase the driver instance memory size. For example you can call toPandas with Arrow enabled or writing files and then read those files instead of collecting large amounts of data back to the driver. If absolutely necessary you can set the property spark.driver.maxResultSize to a value <X>g higher than the value reported in the exception message in the cluster Spark configuration: ini Copy spark.driver.maxResultSize <X>g\n The default value is 4g. For details, see Application Properties If you set a high limit, out-of-memory errors can occur in the driver (depending on spark.driver.memory and the memory overhead of objects in the JVM). Set an appropriate limit to prevent out-of-memory errors.",
          "title" : "Apache Spark job fails with maxResultSize exception",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-fails-maxresultsize-exception"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Jobs: tips and troubleshooting Article 03/11/2022 2 minutes to read 3 contributors In this article These articles can help you with your Azure Databricks jobs. Distinguish active and dead jobs Spark job fails with Driver is temporarily unavailable How to delete all jobs using the REST API Identify less used jobs Jobs failing with shuffle fetch failures Job cluster limits on notebook output Job fails, but Apache Spark tasks finish Job fails due to job rate limit Create table in overwrite mode fails when interrupted Apache Spark Jobs hang due to non-deterministic custom UDF Apache Spark job fails with Failed to parse byte string Apache Spark UI shows wrong number of jobs Job fails with atypical errors message Apache Spark job fails with maxResultSize exception Azure Databricks job fails because library is not installed Jobs failing on Databricks Runtime 5.5 LTS with an SQLAlchemy package error Job failure due to Azure Data Lake Storage (ADLS) CREATE limits Job fails with invalid access token How to ensure idempotency for jobs Monitor running jobs with a Job Run dashboard Streaming job has degraded performance Task deserialization time is high",
          "title" : "Jobs: tips and troubleshooting",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/"
        }
      },
      {
        "_index" : "databricks_ms_azure_kb",
        "_type" : "ms_kb",
        "_id" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/import-custom-ca-cert",
        "_score" : 1.0,
        "_source" : {
          "body" : "Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to import a custom CA certificate Article 03/11/2022 2 minutes to read 5 contributors In this article When working with Python, you may want to import a custom CA certificate to avoid connection errors to your endpoints. Console Copy ConnectionError: HTTPSConnectionPool(host='my_server_endpoint', port=443): Max retries exceeded with url: /endpoint (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fb73dc3b3d0>: Failed to establish a new connection: [Errno 110] Connection timed out',))\n To import one or more custom CA certificates to your Azure Databricks cluster: Create an init script that adds the entire CA chain and sets the REQUESTS_CA_BUNDLE property. In this example, PEM format CA certificates are added to the file myca.crt which is located at /user/local/share/ca-certificates/. This file is referenced in the custom-cert.sh init script. Bash Copy dbutils.fs.put(\"/databricks/init-scripts/custom-cert.sh\", \"\"\"#!/bin/bash\n\ncat << 'EOF' > /usr/local/share/ca-certificates/myca.crt\n-----BEGIN CERTIFICATE-----\n<CA CHAIN 1 CERTIFICATE CONTENT>\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n<CA CHAIN 2 CERTIFICATE CONTENT>\n-----END CERTIFICATE-----\nEOF\n\nupdate-ca-certificates\n\nPEM_FILE=\"/etc/ssl/certs/myca.pem\"\nPASSWORD=\"<password>\"\nJAVA_HOME=$(readlink -f /usr/bin/java | sed \"s:bin/java::\")\nKEYSTORE=\"$JAVA_HOME/lib/security/cacerts\"\n\nCERTS=$(grep 'END CERTIFICATE' $PEM_FILE| wc -l)\n\n# To process multiple certs with keytool, you need to extract\n# each one from the PEM file and import it into the Java KeyStore.\n\nfor N in $(seq 0 $(($CERTS - 1))); do\n  ALIAS=\"$(basename $PEM_FILE)-$N\"\n  echo \"Adding to keystore with alias:$ALIAS\"\n  cat $PEM_FILE |\n    awk \"n==$N { print }; /END CERTIFICATE/ { n++ }\" |\n    keytool -noprompt -import -trustcacerts \\\n            -alias $ALIAS -keystore $KEYSTORE -storepass $PASSWORD\ndone\n\necho \"export REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt\" >> /databricks/spark/conf/spark-env.sh\n\"\"\")\n To use your custom CA certificates with DBFS FUSE, add this line to the bottom of your init script: Bash Copy /databricks/spark/scripts/restart_dbfs_fuse_daemon.sh\n Attach the init script to the cluster as a cluster-scoped init script. Restart the cluster.",
          "title" : "How to import a custom CA certificate",
          "view_href" : "https://docs.microsoft.com/en-us/azure/databricks/kb/python/import-custom-ca-cert"
        }
      }
    ]
  }
}
