{
  "took" : 18,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 307,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/security/troubleshoot-key-vault-access",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Security and permissions Troubleshoot key vault access issues Troubleshoot key vault access issues Troubleshoot Azure key vault access issues. Verify firewall. Enable secrets. Written by arvind.ravish Last published at: May 17th, 2022 You are trying to access secrets, when you get an error message. com.databricks.common.client.DatabricksServiceException: INVALID_STATE: Databricks could not access keyvault: https://xxxxxxx.vault.azure.net/. There is not a single root cause for this error message, so you will have to do some troubleshooting. Confirm permissions are correctly set on the key vault Load the Azure Portal. Open Key vaults. Click the key vault. Click Access policies. Verify the Get and List permissions are applied. Inspect the firewall configuration on the key vault Load the Azure Portal. Open Key vaults. Click the key vault. Click Networking. Click Firewalls and virtual networks. Select Private endpoint and selected networks. Verify that Allow trusted Microsoft services to bypass this firewall? is set to Yes. Attempt to access the secrets. If you can view the secrets, the issue is resolved. If you are still getting the INVALID_STATE: Databricks could not access keyvault error, continue troubleshooting. List all secrets in the secret scope Open a notebook. List all secrets in scope. %python\r\n\r\ndbutils.secrets.list(\"<scopename>\") Try to access individual secrets Try to access a few different, random secrets. %python\r\n\r\ndbutils.secrets.get(\"<KeyvaultSecretScope>\", \"<SecretName>\") If some secrets can be fetched, while others fail, the failed secrets are either disabled or inactive. Enable individual secrets Load the Azure Portal. Open Key vaults. Click the key vault. Click Secrets. Click the secret and verify that the status is set to Enabled. If the secret is disabled, enable it, or create a new version. Verify that individual secrets are working Try to access the previously failed secrets. %python\r\n\r\ndbutils.secrets.get(\"<KeyvaultSecretScope>\", \"<SecretName>\") You can fetch all of them. Was this article helpful? (8) (14) Additional Informations Related Articles Table creation fails with security exception Problem You attempt to create a table using a cluster that has Table ACLs enabled... Forbidden error while accessing S3 data Problem While trying to access S3 data using DBFS mount or directly in Spark APIs... Related Articles Table creation fails with security exception Problem You attempt to create a table using a cluster that has Table ACLs enabled... Forbidden error while accessing S3 data Problem While trying to access S3 data using DBFS mount or directly in Spark APIs... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Troubleshoot key vault access issues",
          "view_href" : "https://kb.databricks.com/en_US/security/troubleshoot-key-vault-access"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/cannot-view-table-serde-properties",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark Cannot view table SerDe properties Cannot view table SerDe properties SHOW CREATE TABLE only returns the Apache Spark DDL. It does not show the SerDe properties. Written by saritha.shivakumar Last published at: July 1st, 2022 Problem You are trying to view the SerDe properties on an Apache Hive table, but SHOW CREATE TABLE just returns the Apache Spark DDL. It does not show the SerDe properties. For example, given this sample code: %sql\r\n\r\nSHOW CREATE TABLE <table-identifier> You get a result that does not show the SerDe properties: Cause You are using Databricks Runtime 7.3 LTS or later, which uses Spark 3.0 and above. The usage of SHOW CREATE TABLE changed with Spark 3.0. Solution To view a table's SerDe properties in Spark 3.0 and above, you need to add the option AS SERDE at the end of the SHOW CREATE TABLE command. For example, given this sample code: SHOW CREATE TABLE <table-identifier> AS SERDE You get a result that shows the table's SerDe properties: Was this article helpful? (3) (5) Additional Informations Related Articles SHOW DATABASES command returns unexpected column name Problem You are using the SHOW DATABASES command and it returns an unexpected col... JDBC write fails with a PrimaryKeyViolation error Problem You are using JDBC to write to a SQL table that has primary key constrain... Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Inner join drops records in result Problem You perform an inner join, but the resulting joined table is missing data... Related Articles SHOW DATABASES command returns unexpected column name Problem You are using the SHOW DATABASES command and it returns an unexpected col... JDBC write fails with a PrimaryKeyViolation error Problem You are using JDBC to write to a SQL table that has primary key constrain... Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Inner join drops records in result Problem You perform an inner join, but the resulting joined table is missing data... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cannot view table SerDe properties",
          "view_href" : "https://kb.databricks.com/en_US/sql/cannot-view-table-serde-properties"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dev-tools/dbconnect-protoserializer-stackoverflow",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Developer tools ProtoSerializer stack overflow error in DBConnect ProtoSerializer stack overflow error in DBConnect A stack overflow error in DBConnect indicates that you need to allocate more memory on the local PC. Written by ashritha.laxminarayana Last published at: May 9th, 2022 Problem You are using DBConnect (AWS | Azure | GCP) to run a PySpark transformation on a DataFrame with more than 100 columns when you get a stack overflow error. py4j.protocol.Py4JJavaError: An error occurred while calling o945.count.\r\n: java.lang.StackOverflowError\r\n    at java.lang.Class.getEnclosingMethodInfo(Class.java:1072)\r\n    at java.lang.Class.getEnclosingClass(Class.java:1272)\r\n    at java.lang.Class.getSimpleBinaryName(Class.java:1443)\r\n    at java.lang.Class.getSimpleName(Class.java:1309)\r\n    at org.apache.spark.sql.types.DataType.typeName(DataType.scala:67)\r\n    at org.apache.spark.sql.types.DataType.simpleString(DataType.scala:82)\r\n    at org.apache.spark.sql.types.DataType.sql(DataType.scala:90)\r\n    at org.apache.spark.sql.util.ProtoSerializer.serializeDataType(ProtoSerializer.scala:3207)\r\n    at org.apache.spark.sql.util.ProtoSerializer.serializeAttrRef(ProtoSerializer.scala:3610)\r\n    at org.apache.spark.sql.util.ProtoSerializer.serializeAttr(ProtoSerializer.scala:3600)\r\n    at org.apache.spark.sql.util.ProtoSerializer.serializeNamedExpr(ProtoSerializer.scala:3537)\r\n    at org.apache.spark.sql.util.ProtoSerializer.serializeExpr(ProtoSerializer.scala:2323)\r\n    at org.apache.spark.sql.util.ProtoSerializer$$anonfun$$nestedInanonfun$serializeCanonicalizable$1$1.applyOrElse(ProtoSerializer.scala:3001)\r\n    at org.apache.spark.sql.util.ProtoSerializer$$anonfun$$nestedInanonfun$serializeCanonicalizable$1$1.applyOrElse(ProtoSerializer.scala:2998) Performing the same operation in a notebook works correctly and does not produce an error. Example code You can reproduce the error with this sample code. It creates a DataFrame with 200 columns and renames them all. This sample code runs correctly in a notebook, but results in an error when run in DBConnect. %python\r\n\r\ndf = spark.createDataFrame([{str(i) : i for i in range(2000)}])\r\ndf = spark.createDataFrame([{str(i) : i for i in range(200)}])\r\nfor col in df.columns:\r\n  df = df.withColumnRenamed(col, col + \"_a\")\r\ndf.collect() Cause When you run code in DBConnect, some functions are handled on the remote cluster driver, but some are handled locally on the client PC. If enough memory is not allocated on the local PC, you get an error. Solution You should increase the memory allocated to the Apache Spark driver on the local PC. Run databricks-connect get-spark-home on your local PC to get the ${spark_home} value. Navigate to the ${spark_home}/conf/ folder. Open the spark-defaults.conf file. Add the following settings to the spark-defaults.conffile: spark.driver.memory 4g\r\nspark.driver.extraJavaOptions -Xss32M Save the changes. Restart DBConnect. Delete Warning DBConnect only works with supported Databricks Runtime versions. Ensure that you are using a supported runtime on your cluster before using DBConnect. Was this article helpful? (9) (16) Additional Informations Related Articles Failed to create process error with Databricks CLI in Windows Problem While trying to access the Databricks CLI (AWS | Azure | GCP) in Windows,... Get Apache Spark config in DBConnect You can always view the Spark configuration (AWS | Azure | GCP) for your cluster ... Databricks Connect reports version error with Databricks Runtime 6.4 Problem You are using the Databricks Connect client with Databricks Runtime 6.4 a... Invalid Access Token error when running jobs with Airflow Problem When you run scheduled Airflow Databricks jobs, you get this error: Inval... Related Articles Failed to create process error with Databricks CLI in Windows Problem While trying to access the Databricks CLI (AWS | Azure | GCP) in Windows,... Get Apache Spark config in DBConnect You can always view the Spark configuration (AWS | Azure | GCP) for your cluster ... Databricks Connect reports version error with Databricks Runtime 6.4 Problem You are using the Databricks Connect client with Databricks Runtime 6.4 a... Invalid Access Token error when running jobs with Airflow Problem When you run scheduled Airflow Databricks jobs, you get this error: Inval... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "ProtoSerializer stack overflow error in DBConnect",
          "view_href" : "https://kb.databricks.com/en_US/dev-tools/dbconnect-protoserializer-stackoverflow"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/task-deserialization-time-is-high",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Task deserialization time is high Task deserialization time is high Configure cluster-installed libraries to install on executors at cluster launch vs executor launch to speed up your job task runs. Written by Adam Pavlacka Last published at: May 11th, 2022 Problem Your tasks are running slower than expected. You review the stage details in the Spark UI on your cluster and see that task deserialization time is high. Cause Cluster-installed libraries (AWS | Azure | GCP) are only installed on the driver when the cluster is started. These libraries are only installed on the executors when the first tasks are submitted. The time taken to install the PyPI libraries is included in the task deserialization time. Delete Info Library installation only occurs on an executor where a task is launched. If a second executor is given a task, the installation process is repeated. The more libraries you have installed, the more noticeable the delay time when a new executor is launched. Solution If you are using a large number of PyPI libraries, you should configure your cluster to install the libraries on all the executors when the cluster is started. This results in a slight increase to the cluster launch time, but allows your job tasks to run faster because you don’t have to wait for libraries to install on the executors after the initial launch. Add spark.databricks.libraries.enableSparkPyPI false to the cluster’s Spark config (AWS | Azure | GCP) and restart the cluster. Was this article helpful? (8) (12) Additional Informations Related Articles Apache Spark UI shows wrong number of jobs Problem You are reviewing the number of active Apache Spark jobs on a cluster in ... Job fails, but Apache Spark tasks finish Problem Your Databricks job reports a failed status, but all Spark jobs and tasks... Job fails due to job rate limit Problem A Databricks notebook or Jobs API request returns the following error: Er... Create table in overwrite mode fails when interrupted Problem When you attempt to rerun an Apache Spark write operation by cancelling t... Related Articles Apache Spark UI shows wrong number of jobs Problem You are reviewing the number of active Apache Spark jobs on a cluster in ... Job fails, but Apache Spark tasks finish Problem Your Databricks job reports a failed status, but all Spark jobs and tasks... Job fails due to job rate limit Problem A Databricks notebook or Jobs API request returns the following error: Er... Create table in overwrite mode fails when interrupted Problem When you attempt to rerun an Apache Spark write operation by cancelling t... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Task deserialization time is high",
          "view_href" : "https://kb.databricks.com/en_US/jobs/task-deserialization-time-is-high"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/install-cartopy-on-cluster",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Error when installing Cartopy on a cluster Error when installing Cartopy on a cluster Cartopy installation fails if libgeos and libproj are not installed. Written by prem.jayaraj Last published at: May 11th, 2022 Problem You are trying to install Cartopy on a cluster and you receive a ManagedLibraryInstallFailed error message. java.lang.RuntimeException: ManagedLibraryInstallFailed: org.apache.spark.SparkException: Process List(/databricks/python/bin/pip, install, cartopy==0.17.0, --disable-pip-version-check) exited with code 1.   ERROR: Command errored out with exit status 1:\r\n   command: /databricks/python3/bin/python3.7 /databricks/python3/lib/python3.7/site-packages/pip/_vendor/pep517/_in_process.py get_requires_for_build_wheel /tmp/tmpjoliwaky\r\n       cwd: /tmp/pip-install-t324easa/cartopy\r\n  Complete output (3 lines):\r\n  setup.py:171: UserWarning: Unable to determine GEOS version. Ensure you have 3.3.3 or later installed, or installation may fail.\r\n    '.'.join(str(v) for v in GEOS_MIN_VERSION), ))\r\n  Proj 4.9.0 must be installed.\r\n  ----------------------------------------\r\nERROR: Command errored out with exit status 1: /databricks/python3/bin/python3.7 /databricks/python3/lib/python3.7/site-packages/pip/_vendor/pep517/_in_process.py get_requires_for_build_wheel /tmp/tmpjoliwaky Check the logs for full command output.\r\n for library:PythonPyPiPkgId(cartopy,Some(0.17.0),None,List()),isSharedLibrary=false Cause Cartopy has dependencies on libgeos 3.3.3 and above and libproj 4.9.0. If libgeos and libproj are not installed, Cartopy fails to install. Solution Configure a cluster-scoped init script (AWS | Azure | GCP) to automatically install Cartopy and the required dependencies. Create the base directory to store the init script in, if the base directory does not exist. Here, use dbfs:/databricks/<directory>as an example. %sh\r\n\r\ndbutils.fs.mkdirs(\"dbfs:/databricks/<directory>/\") Create the script and save it to a file. %sh\r\n\r\ndbutils.fs.put(\"dbfs:/databricks/<directory>/cartopy.sh\",\"\"\"\r\n#!/bin/bash\r\nsudo apt-get install libgeos++-dev -y\r\nsudo apt-get install libproj-dev -y\r\n/databricks/python/bin/pip install Cartopy\r\n\"\"\",True) Check that the script exists. %python\r\n\r\ndisplay(dbutils.fs.ls(\"dbfs:/databricks/<directory>/cartopy.sh\")) On the cluster configuration page, click the Advanced Options toggle. At the bottom of the page, click the Init Scripts tab. In the Destination drop-down, select DBFS, provide the file path to the script, and click Add. Restart the cluster. Was this article helpful? (10) (16) Additional Informations Related Articles Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Install package using previous CRAN snapshot Problem You are trying to install a library package via CRAN, and are getting a L... Install PyGraphViz PyGraphViz Python libraries are used to plot causal inference networks. If you tr... Cannot uninstall library from UI Problem Usually, libraries can be uninstalled in the Clusters UI. If the checkbox... Related Articles Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Install package using previous CRAN snapshot Problem You are trying to install a library package via CRAN, and are getting a L... Install PyGraphViz PyGraphViz Python libraries are used to plot causal inference networks. If you tr... Cannot uninstall library from UI Problem Usually, libraries can be uninstalled in the Clusters UI. If the checkbox... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Error when installing Cartopy on a cluster",
          "view_href" : "https://kb.databricks.com/en_US/libraries/install-cartopy-on-cluster"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/cloud/failed-credential-validation-checks-error-with-terraform",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Cloud infrastructure Failed credential validation checks error with Terraform Failed credential validation checks error with Terraform You get a 'Failed credential validation checks' error message when using Terraform to deploy a Databricks workspace in AWS. Written by Cedric Law Last published at: July 1st, 2022 Problem You are using Terraform to deploy a workspace in AWS and you get a Failed credential validation checks error message. │ Error: MALFORMED_REQUEST: Failed credential validation checks: please use a valid cross account IAM role with permissions setup correctly \r\n│ \r\n│   with databricks_mws_credentials.this,\r\n│   on cross-account-role.tf line 29, in resource \"databricks_mws_credentials\" \"this\":\r\n│   29: resource \"databricks_mws_credentials\" \"this\" {\r\n│ Cause This issue can occur due to a race condition when the cross-account role configuration is applied by Terraform. If you re-run terraform apply after getting the Failed credential validation checks error, the operation is successful and does not result in an error message. Solution You should add an artificial delay as a dependency for the cross-account role configuration. This prevents the race condition from occurring when using Terraform. In this example cross-account role configuration file, timesleep.wait has been added as a dependency. // cross-account-role.tf\r\n\r\n// Properly configure the cross-account role for the creation of new workspaces within your AWS account.\r\n// See https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/mws_credentials\r\n\r\nresource \"databricks_mws_credentials\" \"this\" {\r\n  provider         = databricks.mws\r\n  account_id       = var.databricks_account_id\r\n  role_arn         = aws_iam_role.cross_account_role.arn\r\n  credentials_name = \"${local.prefix}-creds\"\r\n  depends_on = [\r\n    timesleep.wait\r\n  ]\r\n}\r\n\r\n The duration of the delay is set to 10 seconds. You can adjust the delay length as needed. resource \"time_sleep\" \"wait\" {\r\n  depends_on = [\r\n    aws_iam_role.cross_account_role\r\n  ]\r\n  create_duration = \"10s\"\r\n} Save the updated cross-account role configuration file. Run terraform init. Run terraform apply. After the artificial delay has been added to the cross-account role configuration you can resume normal deployments with Terraform. Review the Terraform time_sleep documentation for more information. You can also review the Databricks Terraform documentation. Was this article helpful? (5) (6) Additional Informations Related Articles Unable to load AWS credentials Problem When you try to access AWS resources like S3, SQS or Redshift, the operat... Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... How to analyze user interface performance issues Problem The Databricks user interface seems to be running slowly. Cause User inte... Vulnerability scan shows vulnerabilities in Databricks EC2 instances Problem The Corporate Information Security (CIS) Vulnerability Management team id... Related Articles Unable to load AWS credentials Problem When you try to access AWS resources like S3, SQS or Redshift, the operat... Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... How to analyze user interface performance issues Problem The Databricks user interface seems to be running slowly. Cause User inte... Vulnerability scan shows vulnerabilities in Databricks EC2 instances Problem The Corporate Information Security (CIS) Vulnerability Management team id... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Failed credential validation checks error with Terraform",
          "view_href" : "https://kb.databricks.com/en_US/cloud/failed-credential-validation-checks-error-with-terraform"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/init-script-fail-download-maven",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Init script fails to download Maven JAR Init script fails to download Maven JAR Cluster init script fails to download a Maven JAR when trying to install a library. Written by arvind.ravish Last published at: May 11th, 2022 Problem You have an init script that is attempting to install a library via Maven, but it fails when trying to download a JAR. https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark_2.12/0.4.1/rapids-4-spark_2.12-0.4.1.jar%0D\r\nResolving repo1.maven.org (repo1.maven.org)... 151.101.248.209\r\nConnecting to repo1.maven.org (repo1.maven.org)|151.101.248.209|:443... connected.\r\nHTTP request sent, awaiting response... 404 Not Found\r\n2021-07-30 01:31:11 ERROR 404: Not Found. Cause There is a carriage return (%0D) character at the end of one or more of the lines in the init script. This is usually caused by editing a file in Windows and then uploading it to your Databricks workspace without removing the excess carriage returns. Solution Remove the Windows carriage returns by running dos2unix on the file after you have uploaded it to the workspace. %sh\r\n\r\nsudo apt-get install dos2unix -y\r\ndos2unix file <initscript.sh> Once you have removed the Windows carriage returns from the file, you can configure the init script as normal. Was this article helpful? (8) (13) Additional Informations Related Articles Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Related Articles Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Init script fails to download Maven JAR",
          "view_href" : "https://kb.databricks.com/en_US/libraries/init-script-fail-download-maven"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/function-object-no-attribute",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark AttributeError: ‘function’ object has no attribute AttributeError: ‘function’ object has no attribute Using protected keywords from the DataFrame API as column names results in a function object has no attribute error message. Written by noopur.nigam Last published at: May 19th, 2022 Problem You are selecting columns from a DataFrame and you get an error message. ERROR: AttributeError: 'function' object has no attribute '_get_object_id' in job Cause The DataFrame API contains a small number of protected keywords. If a column in your DataFrame uses a protected keyword as the column name, you will get an error message. For example, summary is a protected keyword. If you use summary as a column name, you will see the error message. This sample code uses summary as a column name and generates the error message when run. %python\r\n\r\ndf=spark.createDataFrame([1,2], \"int\").toDF(\"id\")\r\ndf.show()\r\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\r\n\r\ndf1 = spark.createDataFrame(\r\n  [(10,), (11,), (13,)],\r\n  StructType([StructField(\"summary\", IntegerType(), True)]))\r\n\r\ndf1.show()\r\n\r\nResultDf = df1.join(df, df1.summary == df.id, \"inner\").select(df.id,df1.summary)\r\nResultDf.show() Solution You should not use DataFrame API protected keywords as column names. If you must use protected keywords, you should use bracket based column access when selecting columns from a DataFrame. Do not use dot notation when selecting columns that use protected keywords. %python\r\n\r\nResultDf = df1.join(df, df1[\"summary\"] == df.id, \"inner\").select(df.id,df1[\"summary\"]) Was this article helpful? (8) (14) Additional Informations Related Articles Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Install and compile Cython This document explains how to run Spark code with compiled Cython code. The steps... Reading large DBFS-mounted files using Python APIs This article explains how to resolve an error that occurs when you read large DBF... Related Articles Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Install and compile Cython This document explains how to run Spark code with compiled Cython code. The steps... Reading large DBFS-mounted files using Python APIs This article explains how to resolve an error that occurs when you read large DBF... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "AttributeError: ‘function’ object has no attribute",
          "view_href" : "https://kb.databricks.com/en_US/python/function-object-no-attribute"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data-sources/query-option-not-work-oracle",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data sources Apache Spark JDBC datasource query option doesn’t work for Oracle database Apache Spark JDBC datasource query option doesn’t work for Oracle database Learn how to resolve an error that occurs when using the Apache Spark JDBC datasource to connect to Oracle Database from Databricks. Written by Adam Pavlacka Last published at: June 1st, 2022 Problem When you use the query option with the Apache Spark JDBC datasource to connect to an Oracle Database, it fails with this error: java.sql.SQLSyntaxErrorException: ORA-00911: invalid character For example, if you run the following to make a JDBC connection: %scala\r\n\r\nval df = spark.read\r\n  .format(\"jdbc\")\r\n  .option(\"url\", \"<url>\")\r\n  .option(\"query\", \"SELECT * FROM oracle_test_table)\")\r\n  .option(\"user\", \"<user>\")\r\n  .option(\"password\", \"<password>\")\r\n  .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\")\r\n  .load()\r\ndf.show() You will see this error Message: java.sql.SQLSyntaxErrorException: ORA-00911: invalid character\r\nat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:447)\r\n      at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)\r\n      at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:951)\r\n      at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:513) Cause The error is due to a Spark-generated subquery alias (generated with the query option) that does not conform to Oracle Database identifier naming conventions. This bug is tracked in Spark Jira ticket SPARK-27596. Solution This issue is fixed in Apache Spark 2.4.4 and Databricks Runtime 5.4. For clusters running on earlier versions of Spark or Databricks Runtime, use the dbtable option instead of the query option. The query must be enclosed in parentheses as a subquery. %scala\r\n\r\nval df = spark.read\r\n  .format(\"jdbc\")\r\n  .option(\"url\", \"<url>\")\r\n  .option(\"dbtable\", \"(SELECT * FROM oracle_test_table)\")\r\n  .option(\"user\", \"<user>\")\r\n  .option(\"password\", \"<password>\")\r\n  .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\")\r\n  .load()\r\ndf.show() You can try the same workaround for other databases when the query option fails. Was this article helpful? (8) (15) Additional Informations Related Articles CosmosDB-Spark connector library conflict This article explains how to resolve an issue running applications that use the C... Delete table when underlying S3 bucket is deleted Problem You are trying to drop or alter a table when you get an error. Error in S... Failure to detect encoding in JSON Problem Spark job fails with an exception containing the message: Invalid UTF-32 ... Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... Related Articles CosmosDB-Spark connector library conflict This article explains how to resolve an issue running applications that use the C... Delete table when underlying S3 bucket is deleted Problem You are trying to drop or alter a table when you get an error. Error in S... Failure to detect encoding in JSON Problem Spark job fails with an exception containing the message: Invalid UTF-32 ... Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Apache Spark JDBC datasource query option doesn’t work for Oracle database",
          "view_href" : "https://kb.databricks.com/en_US/data-sources/query-option-not-work-oracle"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/global-temp-view-not-found",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark Error in SQL statement: AnalysisException: Table or view not found Error in SQL statement: AnalysisException: Table or view not found Learn how to resolve the AnalysisException SQL error \"Table or view not found\". Written by Adam Pavlacka Last published at: May 23rd, 2022 Problem When you try to query a table or view, you get this error: AnalysisException:Table or view not found when trying to query a global temp view Cause You typically create global temp views so they can be accessed from different sessions and kept alive until the application ends. You can create a global temp view with the following statement: %scala\r\n\r\ndf.createOrReplaceGlobalTempView(\"<global-view-name>\") Here, df is the DataFrame. Another way to create the view is with: %sql\r\n\r\nCREATE GLOBAL TEMP VIEW <global-view-name> All global temporary views are tied to a system temporary database named global_temp. If you query the global table or view without explicitly mentioning the global_temp database, then the error occurs. Solution Always use the qualified table name with the global_temp database, so that you can query the global view data successfully. For example: %sql\r\n\r\nselect * from global_temp.<global-view-name>; Was this article helpful? (16) (11) Additional Informations Related Articles Disable broadcast when query plan has BroadcastNestedLoopJoin This article explains how to disable broadcast when the query plan has BroadcastN... Duplicate columns in the metadata error Problem Your Apache Spark job is processing a Delta table when the job fails with... Date functions only accept int values in Apache Spark 3.0 Problem You are attempting to use the date_add() or date_sub() functions in Spark... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Related Articles Disable broadcast when query plan has BroadcastNestedLoopJoin This article explains how to disable broadcast when the query plan has BroadcastN... Duplicate columns in the metadata error Problem Your Apache Spark job is processing a Delta table when the job fails with... Date functions only accept int values in Apache Spark 3.0 Problem You are attempting to use the date_add() or date_sub() functions in Spark... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Error in SQL statement: AnalysisException: Table or view not found",
          "view_href" : "https://kb.databricks.com/en_US/sql/global-temp-view-not-found"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/metrics/explore-spark-metrics",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Metrics How to explore Apache Spark metrics with Spark listeners How to explore Apache Spark metrics with Spark listeners Learn how to explore Apache Spark metrics using Spark listeners with Databricks. Written by Adam Pavlacka Last published at: May 16th, 2022 Apache Spark provides several useful internal listeners that track metrics about tasks and jobs. During the development cycle, for example, these metrics can help you to understand when and why a task takes a long time to finish. Of course, you can leverage the Spark UI or History UI to see information for each task and stage, but there are some downsides. For instance, you can’t compare the statistics for two Spark jobs side by side, and the Spark History UI can take a long time to load for large Spark jobs. You can extract the metrics generated by Spark internal classes and persist them to disk as a table or a DataFrame. Then you can query the DataFrame just like any other data science table. You can use this SparkTaskMetrics package to explore how to use Spark listeners to extract metrics from tasks and jobs. Build the Spark Metrics package Use the following command to build the package. %sh\r\n\r\nsbt package Gather metrics Import TaskMetricsExplorer. Create the query sql(\"\"\"SELECT * FROM nested_data\"\"\").show(false) and pass it into runAndMeasure. The query should include at least one Spark action in order to trigger a Spark job. Spark does not generate any metrics until a Spark job is executed. For example: %scala\r\n\r\nimport com.databricks.TaskMetricsExplorer\r\n\r\nval t = new TaskMetricsExplorer(spark)\r\nsql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW nested_data AS\r\n       SELECT id AS key,\r\n       ARRAY(CAST(RAND(1) * 100 AS INT), CAST(RAND(2) * 100 AS INT), CAST(RAND(3) * 100 AS INT), CAST(RAND(4) * 100 AS INT), CAST(RAND(5) * 100 AS INT)) AS values,\r\n       ARRAY(ARRAY(CAST(RAND(1) * 100 AS INT), CAST(RAND(2) * 100 AS INT)), ARRAY(CAST(RAND(3) * 100 AS INT), CAST(RAND(4) * 100 AS INT), CAST(RAND(5) * 100 AS INT))) AS nested_values\r\n       FROM range(5)\"\"\")\r\nval query = sql(\"\"\"SELECT * FROM nested_data\"\"\").show(false)\r\nval res = t.runAndMeasure(query) The runAndMeasure method runs the command and gets the task’s internal metrics using a Spark listener. It then runs the query and returns the result: +---+-------------------+-----------------------+\r\n|key|values             |nested_values          |\r\n+---+-------------------+-----------------------+\r\n|0  |[26, 11, 66, 8, 47]|[[26, 11], [66, 8, 47]]|\r\n|1  |[66, 8, 47, 91, 6] |[[66, 8], [47, 91, 6]] |\r\n|2  |[8, 47, 91, 6, 70] |[[8, 47], [91, 6, 70]] |\r\n|3  |[91, 6, 70, 41, 19]|[[91, 6], [70, 41, 19]]|\r\n|4  |[6, 70, 41, 19, 12]|[[6, 70], [41, 19, 12]]|\r\n+---+-------------------+-----------------------+ The task metrics information is saved in a DataFrame. You can display it with this command: %scala\r\n\r\nres.select($\"stageId\", $\"taskType\", $\"taskLocality\", $\"executorRunTime\", $\"duration\", $\"executorId\", $\"host\", $\"jvmGCTime\").show(false) Then you get: +-------+----------+-------------+---------------+--------+----------+---------+---------+\r\n|stageId|taskType  |taskLocality |executorRunTime|duration|executorId| host    |jvmGCTime|\r\n+-------+----------+-------------+---------------+--------+----------+---------+---------+\r\n|3      |ResultTask|PROCESS_LOCAL|2              |9       |driver    |localhost|0        |\r\n|4      |ResultTask|PROCESS_LOCAL|3              |11      |driver    |localhost|0        |\r\n|4      |ResultTask|PROCESS_LOCAL|3              |16      |driver    |localhost|0        |\r\n|4      |ResultTask|PROCESS_LOCAL|2              |20      |driver    |localhost|0        |\r\n|4      |ResultTask|PROCESS_LOCAL|4              |22      |driver    |localhost|0        |\r\n|5      |ResultTask|PROCESS_LOCAL|2              |12      |driver    |localhost|0        |\r\n|5      |ResultTask|PROCESS_LOCAL|3              |17      |driver    |localhost|0        |\r\n|5      |ResultTask|PROCESS_LOCAL|7              |21      |driver    |localhost|0        |\r\n+-------+----------+-------------+---------------+--------+----------+---------+---------+ To view all available metrics names and data types, display the schema of the res DataFrame: %scala\r\n\r\nres.schema.treeString root\r\n |-- stageId: integer (nullable = false)\r\n |-- stageAttemptId: integer (nullable = false)\r\n |-- taskType: string (nullable = true)\r\n |-- index: long (nullable = false)\r\n |-- taskId: long (nullable = false)\r\n |-- attemptNumber: integer (nullable = false)\r\n |-- launchTime: long (nullable = false)\r\n |-- finishTime: long (nullable = false)\r\n |-- duration: long (nullable = false)\r\n |-- schedulerDelay: long (nullable = false)\r\n |-- executorId: string (nullable = true)\r\n |-- host: string (nullable = true)\r\n |-- taskLocality: string (nullable = true)\r\n |-- speculative: boolean (nullable = false)\r\n |-- gettingResultTime: long (nullable = false)\r\n |-- successful: boolean (nullable = false)\r\n |-- executorRunTime: long (nullable = false)\r\n |-- executorCpuTime: long (nullable = false)\r\n |-- executorDeserializeTime: long (nullable = false)\r\n |-- executorDeserializeCpuTime: long (nullable = false)\r\n |-- resultSerializationTime: long (nullable = false)\r\n |-- jvmGCTime: long (nullable = false)\r\n |-- resultSize: long (nullable = false)\r\n |-- numUpdatedBlockStatuses: integer (nullable = false)\r\n |-- diskBytesSpilled: long (nullable = false)\r\n |-- memoryBytesSpilled: long (nullable = false)\r\n |-- peakExecutionMemory: long (nullable = false)\r\n |-- recordsRead: long (nullable = false)\r\n |-- bytesRead: long (nullable = false)\r\n |-- recordsWritten: long (nullable = false)\r\n |-- bytesWritten: long (nullable = false)\r\n |-- shuffleFetchWaitTime: long (nullable = false)\r\n |-- shuffleTotalBytesRead: long (nullable = false)\r\n |-- shuffleTotalBlocksFetched: long (nullable = false)\r\n |-- shuffleLocalBlocksFetched: long (nullable = false)\r\n |-- shuffleRemoteBlocksFetched: long (nullable = false)\r\n |-- shuffleWriteTime: long (nullable = false)\r\n |-- shuffleBytesWritten: long (nullable = false)\r\n |-- shuffleRecordsWritten: long (nullable = false)\r\n |-- errorMessage: string (nullable = true) Was this article helpful? (10) (16) Additional Informations Related Articles How to use Apache Spark metrics This article gives an example of how to monitor Apache Spark components using the... Related Articles How to use Apache Spark metrics This article gives an example of how to monitor Apache Spark components using the... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to explore Apache Spark metrics with Spark listeners",
          "view_href" : "https://kb.databricks.com/en_US/metrics/explore-spark-metrics"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/access-s3-temp-session-token",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks Access S3 with temporary session credentials Access S3 with temporary session credentials Extract IAM session credentials and use them to access S3 storage via S3A URI. Requires Databricks Runtime 8.3 and above. Written by Gobinath.Viswanathan Last published at: May 16th, 2022 You can use IAM session tokens with Hadoop config support to access S3 storage in Databricks Runtime 8.3 and above. Delete Info You cannot mount the S3 path as a DBFS mount when using session credentials. You must use the S3A URI. Extract the session credentials from your cluster Extract the session credentials from your cluster. You will need the Instance Profile from your cluster. This can be found under Advanced Options in the cluster configuration. Use curl to display the AccessKeyId, SecretAccessKey, and Token. %sh\r\n\r\ncurl http://169.254.169.254/latest/meta-data/iam/security-credentials/<instance-profile> Alternatively, you can use a Python script. %python\r\n\r\nimport requests\r\nimport json\r\nresponse = requests.get(\"http://169.254.169.254/latest/meta-data/iam/security-credentials/<instance-profile>\")\r\ncredentials = response.json()\r\nprint(credentials) The IP address should not be modified. 169.254.169.254 is a link-local address and is valid only from the instance. Delete Info You can only extract a session token from a standard cluster. This will not work on a high concurrency cluster. Use session credentials in a notebook You can use the session credentials by entering them into a notebook. %python\r\n\r\nAccessKey = \"<AccessKeyId>\"\r\nSecret = \"<SecretAccessKey>\"\r\nToken = \"<Token>\"\r\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\r\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", AccessKey )\r\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", Secret)\r\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.session.token\", Token) Once the session credentials are loaded in the notebook, you can access files in the S3 bucket with a S3A URI. %python\r\n\r\ndbutils.fs.ls(\"s3a://<path-to-folder>/\") Use session credentials in the cluster config You can add the session credentials to the cluster Spark config. This makes them accessible to all notebooks on the cluster. fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\r\nfs.s3a.access.key <AccessKeyId>\r\nfs.s3a.secret.key <SecretAccessKey>\r\nfs.s3a.session.token <Token> Was this article helpful? (10) (48) Additional Informations Related Articles Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Enable s3cmd for notebooks s3cmd is a client library that allows you to perform all AWS S3 operations from a... How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... JSON reader parses values as null Problem You are attempting to read a JSON file. You know the file has data in it,... Common errors in notebooks There are some common issues that occur when using notebooks. This section outlin... Related Articles Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Enable s3cmd for notebooks s3cmd is a client library that allows you to perform all AWS S3 operations from a... How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... JSON reader parses values as null Problem You are attempting to read a JSON file. You know the file has data in it,... Common errors in notebooks There are some common issues that occur when using notebooks. This section outlin... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Access S3 with temporary session credentials",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/access-s3-temp-session-token"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/job-idle-before-start",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Job remains idle before starting Job remains idle before starting Apache Spark jobs remain idle for a long time before starting. Written by ashish Last published at: May 19th, 2022 Problem You have an Apache Spark job that is triggered correctly, but remains idle for a long time before starting. You have a Spark job that ran well for awhile, but goes idle for a long time before resuming. Symptoms include: Cluster downscales to the minimum number of worker nodes during idle time. Driver logs don’t show any Spark jobs during idle time, but does have repeated information about metadata. Ganglia shows activity only on the driver node. Executor logs show no activity. After some time passes, cluster scales up and Spark jobs start or resume. Cause These symptoms indicate that there are a lot of file scan operations happening during this period of the job. Tables are read and consumed in downstream operations. You see the file scan operation details when you review the SQL tab in the Spark UI. The queries appear to be completed, which makes it appear as though no work is being performed during this idle time. The driver node is busy because it is performing the file listing and processing data (metadata containing schema and other information). This work only happens on the driver node, which is why you only see driver node activity in the Ganglia metrics during this time. This issue becomes more pronounced if you have a large number of small files. Solution You should control the file size and number of files ingested at the source location by implementing a preprocessing step. You can also break down the ingestion into a number of smaller steps, so a smaller number of files have to be scanned at once. Another option is to migrate your data store to Delta Lake, which uses transactional logs as an index for all the underlying files. Was this article helpful? (7) (14) Additional Informations Related Articles How to import a custom CA certificate When working with Python, you may want to import a custom CA certificate to avoid... Use the HDFS API to read files in Python There may be times when you want to read files directly without using third party... Reading large DBFS-mounted files using Python APIs This article explains how to resolve an error that occurs when you read large DBF... Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Related Articles How to import a custom CA certificate When working with Python, you may want to import a custom CA certificate to avoid... Use the HDFS API to read files in Python There may be times when you want to read files directly without using third party... Reading large DBFS-mounted files using Python APIs This article explains how to resolve an error that occurs when you read large DBF... Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Job remains idle before starting",
          "view_href" : "https://kb.databricks.com/en_US/python/job-idle-before-start"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/administration/cname-migration",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Databricks administration SSO server redirects to original URL, not to vanity Databricks URL SSO server redirects to original URL, not to vanity Databricks URL Written by Adam Pavlacka Last published at: February 25th, 2022 Problem When you log into Databricks using a vanity URL (such as mycompany.cloud.databricks.com), you are redirected to a single sign-on (SSO) server for authentication. When that server redirects you back to the Databricks website, the URL changes from the vanity URL to the original deployment URL (such as dbc-XXXX.cloud.databricks.com). This can happen even if a CNAME record exists that points to the vanity URL. Cause This issue happens if the SSO administrator used the original deployment URL when they configured the Databricks application in the Identity Provider (IdP). Solution The SSO administrator should use the vanity Databricks URL as the base for target URL for the Identity Provider application. For example, https://mycompany.cloud.databricks.com/saml/consume. Was this article helpful? (16) (50) Additional Informations Related Articles SSO SAML authentication error with PingFederate Problem When using PingFederate to authenticate over a SSO connection with Databr... How to discover who deleted a cluster in Azure portal If a cluster in your workspace has disappeared or been deleted, you can identify ... How to discover who deleted a workspace in Azure portal If your workspace has disappeared or been deleted, you can identify which user de... Related Articles SSO SAML authentication error with PingFederate Problem When using PingFederate to authenticate over a SSO connection with Databr... How to discover who deleted a cluster in Azure portal If a cluster in your workspace has disappeared or been deleted, you can identify ... How to discover who deleted a workspace in Azure portal If your workspace has disappeared or been deleted, you can identify which user de... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "SSO server redirects to original URL, not to vanity Databricks URL",
          "view_href" : "https://kb.databricks.com/en_US/administration/cname-migration"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/readstream-is-not-whitelisted",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming readStream() is not whitelisted error when running a query readStream() is not whitelisted error when running a query readStream() is not whitelisted error on clusters that have table access control enabled. Written by mathan.pillai Last published at: May 19th, 2022 Problem You have table access control (AWS | Azure | GCP) enabled on your cluster. You are trying to run a structured streaming query and get and error message. py4j.security.Py4JSecurityException: Method public org.apache.spark.sql.streaming.DataStreamReader org.apache.spark.sql.SQLContext.readStream() is not whitelisted on class class org.apache.spark.sql.SQLContext Cause Streaming is not supported on clusters that have table access control enabled. Access control allows you to set permissions for data objects on a cluster. It requires user interaction to validate and refresh credentials. Because streaming queries run continuously, it is not supported on clusters with table access control. Solution You should use a cluster that does not have table access control enabled for streaming queries. Was this article helpful? (7) (12) Additional Informations Related Articles Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... Handling partition column values while using an SQS queue as a streaming source Problem If data in S3 is stored by partition, the partition column values are use... Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... Kafka error: No resolvable bootstrap urls Problem You are trying to read or write data to a Kafka stream when you get an er... Related Articles Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... Handling partition column values while using an SQS queue as a streaming source Problem If data in S3 is stored by partition, the partition column values are use... Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... Kafka error: No resolvable bootstrap urls Problem You are trying to read or write data to a Kafka stream when you get an er... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "readStream() is not whitelisted error when running a query",
          "view_href" : "https://kb.databricks.com/en_US/streaming/readstream-is-not-whitelisted"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/kafka-setup",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming How to set up Apache Kafka on Databricks How to set up Apache Kafka on Databricks Learn how to set up Apache Kafka on Databricks. Written by Adam Pavlacka Last published at: May 18th, 2022 This article explains how to set up Apache Kafka on AWS EC2 machines and connect them with Databricks. Following are the high level steps that are required to create a Kafka cluster and connect from Databricks notebooks. Update Table of Contents Step 1: Create a new VPC in AWSStep 2: Launch the EC2 instance in the new VPCStep 3: Install Kafka and ZooKeeper on the new EC2 instanceStep 4: Peer two VPCsStep 5: Access the Kafka broker from a notebook Step 1: Create a new VPC in AWS When creating the new VPC, set the new VPC CIDR range different than the Databricks VPC CIDR range. For example: Databricks VPC vpc-7f4c0d18 has CIDR IP range 10.205.0.0/16. New VPC vpc-8eb1faf7 has CIDR IP range 10.10.0.0/16. Create a new internet gateway and attach it to the route table of the new VPC. This allows you to ssh into the EC2 machines that you launch under this VPC. Create a new internet gateway. Attach it to VPC vpc-8eb1faf7. Step 2: Launch the EC2 instance in the new VPC Launch the EC2 instance inside the new VPC vpc-8eb1faf7 created in Step 1. Step 3: Install Kafka and ZooKeeper on the new EC2 instance SSH into the machine with the key pair. ssh -i keypair.pem ec2-user@ec2-xx-xxx-xx-xxx.us-west-2.compute.amazonaws.com Download Kafka and extract the archive. wget https://apache.claz.org/kafka/0.10.2.1/kafka_2.12-0.10.2.1.tgz\r\ntar -zxf kafka_2.12-0.10.2.1.tgz Start the ZooKeeper process. cd kafka_2.12-0.10.2.1\r\nbin/zookeeper-server-start.sh config/zookeeper.properties Edit the config/server.properties file and set 10.10.143.166as the private IP of the EC2 node. advertised.listeners=PLAINTEXT:/10.10.143.166:9092 Start the Kafka broker. cd kafka_2.12-0.10.2.1\r\nbin/kafka-server-start.sh config/server.properties Step 4: Peer two VPCs Create a new peering connection. Add the peering connection into the route tables of your Databricks VPC and new Kafka VPC created in Step 1. In the Kafka VPC, go to the route table and add the route to the Databricks VPC. In the Databricks VPC, go to the route table and add the route to the Kafka VPC. For more information, see VPC Peering. Step 5: Access the Kafka broker from a notebook Verify you can reach the EC2 instance running the Kafka broker with telnet. SSH to the Kafka broker. %sh\r\n\r\nssh -i keypair.pem ec2-user@ec2-xx-xxx-xx-xxx.us-west-2.compute.amazonaws.com Create a new topic in the Kafka broker from the command line. %sh\r\n\r\nbin/kafka-console-producer.sh --broker-list localhost:9092 --article wordcount < LICENSE Read data in a notebook. %scala\r\n\r\nimport org.apache.spark.sql.functions._\r\nval kafka = spark.readStream\r\n        .format(\"kafka\")\r\n        .option(\"kafka.bootstrap.servers\", \"10.10.143.166:9092\")\r\n        .option(\"subscribe\", \"wordcount\")\r\n        .option(\"startingOffsets\", \"earliest\")\r\ndisplay(kafka) Was this article helpful? (13) (45) Additional Informations Related Articles Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... Get the path of files consumed by Auto Loader When you process streaming files with Auto Loader (AWS | Azure | GCP), events are... Related Articles Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... Get the path of files consumed by Auto Loader When you process streaming files with Auto Loader (AWS | Azure | GCP), events are... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to set up Apache Kafka on Databricks",
          "view_href" : "https://kb.databricks.com/en_US/streaming/kafka-setup"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/bucketing",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management How to improve performance with bucketing How to improve performance with bucketing Learn how to improve Databricks performance by using bucketing. Written by Adam Pavlacka Last published at: March 4th, 2022 Bucketing is an optimization technique in Apache Spark SQL. Data is allocated among a specified number of buckets, according to values derived from one or more bucketing columns. Bucketing improves performance by shuffling and sorting data prior to downstream operations such as table joins. The tradeoff is the initial overhead due to shuffling and sorting, but for certain data transformations, this technique can improve performance by avoiding later shuffling and sorting. This technique is useful for dimension tables, which are frequently used tables containing primary keys. It is also useful when there are frequent join operations involving large and small tables. The example notebook below shows the differences in physical plans when performing joins of bucketed and unbucketed tables. Bucketing example notebook Open notebook in new tab. Was this article helpful? (14) (51) Additional Informations Related Articles Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... Related Articles Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to improve performance with bucketing",
          "view_href" : "https://kb.databricks.com/en_US/data/bucketing"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/install-package-cran-snapshot",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Install package using previous CRAN snapshot Install package using previous CRAN snapshot Avoid a package install error by installing from an earlier CRAN snapshot. Written by darshan.bargal Last published at: May 11th, 2022 Problem You are trying to install a library package via CRAN, and are getting a Library installation failed for library due to infra fault error message. Library installation failed for library due to infra fault for Some(cran {\r\npackage: \"<name-of-package>\"\r\n}\r\n). Error messages:\r\njava.lang.RuntimeException: Installation failed with message:\r\n\r\nError installing R package: Could not install package with error: installation of package <U+2018><name-of-package><U+2019> had non-zero exit status Cause CRAN maintains daily snapshots. If a snapshot is invalid for some reason, you get an error when trying to install a package. Solution Specify a previous snapshot when installing your library. Check the date of the most recent snapshot at https://cran.microsoft.com/snapshot/. Pick a date that is at least one day older than the most recent snapshot. For example, if the most recent snapshot is dated July 9, 2021, you should use July 8, 2021. Enter the full URL to your chosen snapshot in the Repository field when you install the package on your cluster (AWS | Azure | GCP). To use the July 8, 2021 snapshot enter https://cran.microsoft.com/snapshot/2021-07-08/ as the full URL for the repository. Was this article helpful? (10) (10) Additional Informations Related Articles How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Library unavailability causing job failures Problem You are launching jobs that import external libraries and get an Import E... Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Related Articles How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Library unavailability causing job failures Problem You are launching jobs that import external libraries and get an Import E... Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Install package using previous CRAN snapshot",
          "view_href" : "https://kb.databricks.com/en_US/libraries/install-package-cran-snapshot"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/tensorflow-fails-to-import",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries TensorFlow fails to import TensorFlow fails to import TensorFlow fails to import if you have an incompatible version of protobuf installed on your cluster. Written by kavya.parag Last published at: May 16th, 2022 Problem You have TensorFlow installed on your cluster. When you try to import TensorFlow, it fails with an Invalid Syntax or import error. Cause The version of protobuf installed on your cluster is not compatible with your version of TensorFlow. Solution Use a cluster-scoped init script to install TensorFlow with matching versions of NumPy and protobuf. Create the init script. %python\r\n\r\ndbutils.fs.put(\"/databricks/<init-script-folder>/install-tensorflow.sh\",\"\"\"\r\n#!/bin/bash\r\nset -e\r\n/databricks/python/bin/python -V\r\n/databricks/python/bin/pip install tensorflow protobuf==3.17.3 numpy==1.15.0\r\n\"\"\", True) Install the init script that you just created as a cluster-scoped init script (AWS | Azure | GCP). You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/install-tensorflow.sh). Restart the cluster after you have installed the init script. Delete Info Uninstall all existing versions of NumPy before installing the init script on your cluster. Was this article helpful? (6) (14) Additional Informations Related Articles Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... New job fails when adding a library from DBFS or S3 Problem You create a new job and attempt to add a library from DBFS or S3 storage... Install Turbodbc via init script Turbodbc is a Python module that uses the ODBC interface to access relational dat... Related Articles Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... New job fails when adding a library from DBFS or S3 Problem You create a new job and attempt to add a library from DBFS or S3 storage... Install Turbodbc via init script Turbodbc is a Python module that uses the ODBC interface to access relational dat... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "TensorFlow fails to import",
          "view_href" : "https://kb.databricks.com/en_US/libraries/tensorflow-fails-to-import"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/data-missing-vacuum-parallel-write",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Vaccuming with zero retention results in data loss Vaccuming with zero retention results in data loss Use varchar type in Databricks Runtime 8.0 and above. It can only be used in table schema. It cannot be used in functions or operators. Written by DD Sharma Last published at: May 10th, 2022 Problem You add data to a Delta table, but the data disappears without warning. There is no obvious error message. Cause This can happen when spark.databricks.delta.retentionDurationCheck.enabled is set to false and VACUUM is configured to retain 0 hours. %sql\r\n\r\nVACUUM <name-of-delta-table> RETAIN 0 HOURS When VACUUM is configured to retain 0 hours it can delete any file that is not part of the version that is being vacuumed. This includes committed files, uncommitted files, and temporary files for concurrent transactions. Consider the following example timeline: VACUUM starts running at 01:17 UTC on version 100. A data file named sample-data-part-0-1-2.parquet is added to version 101 at 01:18 UTC. Version 101 is committed at 01:19 UTC. VACUUM is still running on version 101 and deletes sample-data-part-0-1-2.parquet at 01:20 UTC. VACUUM completes at 01:22 UTC. In this example, VACUMM executed on version 100 and deleted everything that was added to version 101. Solution Databricks recommends that you set a VACUUM retention interval to be at least 7 days, because old snapshots and uncommitted files can still be in use by concurrent readers or writers to the table. Do not set spark.databricks.delta.retentionDurationCheck.enabled to false in your Spark config. If you do set spark.databricks.delta.retentionDurationCheck.enabled to false in your Spark config, you must choose an interval that is longer than the longest running concurrent transaction and the longest period that any stream can lag behind the most recent update to the table. Review the Databricks VACUMM documentation for more information. Was this article helpful? (12) (13) Additional Informations Related Articles Access denied when writing Delta Lake tables to S3 Problem Writing DataFrame contents in Delta Lake format to an S3 location can cau... Unable to cast string to varchar Problem You are trying to cast a string type column to varchar but it isn’t worki... Identify duplicate data on append operations A common issue when performing append operations on Delta tables is duplicate dat... HIVE_CURSOR_ERROR when reading a table in Athena Problem You create an external table in Athena and integrate it with Delta Lake u... Related Articles Access denied when writing Delta Lake tables to S3 Problem Writing DataFrame contents in Delta Lake format to an S3 location can cau... Unable to cast string to varchar Problem You are trying to cast a string type column to varchar but it isn’t worki... Identify duplicate data on append operations A common issue when performing append operations on Delta tables is duplicate dat... HIVE_CURSOR_ERROR when reading a table in Athena Problem You create an external table in Athena and integrate it with Delta Lake u... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Vaccuming with zero retention results in data loss",
          "view_href" : "https://kb.databricks.com/en_US/delta/data-missing-vacuum-parallel-write"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/enable-retry-init-script",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Enable retries in init script Enable retries in init script Add a retry function to your init script. Written by arjun.kaimaparambilrajan Last published at: March 4th, 2022 Init scripts are commonly used to configure Databricks clusters. There are some scenarios where you may want to implement retries in an init script. Example init script This sample init script shows you how to implement a retry for a basic copy operation. You can use this sample code as a base for implementing retries in your own init script. %scala\r\n\r\ndbutils.fs.put(\"dbfs:/databricks/<path-to-init-script>/retry-example-init.sh\", \"\"\"#!/bin/bash\r\n\r\necho \"starting script at `date`\"\r\n\r\nfunction fail {\r\n  echo $1 >&2\r\n  exit 1\r\n}\r\n\r\nfunction retry {\r\n  local n=1\r\n  local max=5\r\n  local delay=5\r\n  while true; do\r\n    \"$@\" && break || {\r\n      if [[ $n -lt $max ]]; then\r\n        ((n++))\r\n        echo \"Command failed. Attempt $n/$max: `date`\"\r\n        sleep $delay;\r\n      else\r\n        echo \"Collecting additional info for debugging..\"\r\n        ps aux > /tmp/ps_info.txt \r\n        debug_log_file=debug_logs_${HOSTNAME}_$(date +\"%Y-%m-%d--%H-%M\").zip\r\n        zip -r /tmp/${debug_log_file} /var/log/ /tmp/ps_info.txt /databricks/data/logs/\r\n        cp /tmp/${debug_log_file} /dbfs/tmp/\r\n        fail \"The command has failed after $n attempts. `date`\"\r\n      fi\r\n    }\r\n  done\r\n}\r\n\r\nsleep 15s\r\necho \"starting Copying at `date`\"\r\nretry cp -rv /dbfs/libraries/xyz.jar /databricks/jars/\r\n\r\necho \"Finished script at `date`\"\r\n\"\"\", true) Was this article helpful? (14) (11) Additional Informations Related Articles Auto termination is disabled when starting a job cluster Problem You are trying to start a job cluster, but the job creation fails with an... How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... How to overwrite log4j configurations on Databricks clusters Warning This article describes steps related to customer use of Log4j 1.x within ... EBS leaked volumes Problem After a cluster is terminated on AWS, some EBS volumes are not deleted au... Related Articles Auto termination is disabled when starting a job cluster Problem You are trying to start a job cluster, but the job creation fails with an... How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... How to overwrite log4j configurations on Databricks clusters Warning This article describes steps related to customer use of Log4j 1.x within ... EBS leaked volumes Problem After a cluster is terminated on AWS, some EBS volumes are not deleted au... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Enable retries in init script",
          "view_href" : "https://kb.databricks.com/en_US/clusters/enable-retry-init-script"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/execution/increase-tasks-per-stage",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Job execution Increase the number of tasks per stage Increase the number of tasks per stage Learn how to increase the number of tasks per stage when using the spark-xml package with Databricks. Written by Adam Pavlacka Last published at: May 11th, 2022 When using the spark-xml package, you can increase the number of tasks per stage by changing the configuration setting spark.hadoop.mapred.max.split.size to a lower value in the cluster’s Spark config (AWS | Azure). This configuration setting controls the input block size. When data is read from DBFS, it is divided into input blocks, which are then sent to different executors. This configuration controls the size of these input blocks. By default, it is 128 MB (128000000 bytes). Setting this value in the notebook with spark.conf.set() is not effective. In the following example, the Spark config field shows that the input block size is 32 MB. Was this article helpful? (10) (13) Additional Informations Related Articles Maximum execution context or notebook attachment limit reached Problem Notebook or job execution stops and returns either of the following error... Serialized task is too large If you see the follow error message, you may be able to fix this error by changin... Related Articles Maximum execution context or notebook attachment limit reached Problem Notebook or job execution stops and returns either of the following error... Serialized task is too large If you see the follow error message, you may be able to fix this error by changin... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Increase the number of tasks per stage",
          "view_href" : "https://kb.databricks.com/en_US/execution/increase-tasks-per-stage"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/file-transaction-log-not-found",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake A file referenced in the transaction log cannot be found A file referenced in the transaction log cannot be found A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. Written by Adam Pavlacka Last published at: May 10th, 2022 Problem Your job fails with an error message: A file referenced in the transaction log cannot be found. Example stack trace: Error in SQL statement: SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 106, XXX.XXX.XXX.XXX, executor 0): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/mnt/<path>/part-00000-da504c51-3bb4-4406-bb99-3566c0e2f743-c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.databricks.com/delta/delta-intro.html#frequently-asked-questions ... Caused by: java.io.FileNotFoundException: dbfs:/mnt/<path>/part-00000-da504c51-3bb4-4406-bb99-3566c0e2f743-c000.snappy.parquet ... Cause There are three common causes for this error message. Cause 1: You start the Delta streaming job, but before the streaming job starts processing, the underlying data is deleted. Cause 2: You perform updates to the Delta table, but the transaction files are not updated with the latest details. Cause 3: You attempt multi-cluster read or update operations on the same Delta table, resulting in a cluster referring to files on a cluster that was deleted and recreated. Solution Cause 1: You should use a new checkpoint directory, or set the Spark property spark.sql.files.ignoreMissingFiles to true in the cluster’s Spark Config. Cause 2: Wait for the data to load, then refresh the table. You can also run fsck to update the transaction files with the latest details. Delete Info fsck removes any file entries that cannot be found in the underlying file system from the transaction log of a Delta table. Cause 3: When tables have been deleted and recreated, the metadata cache in the driver is incorrect. You should not delete a table, you should always overwrite a table. If you do delete a table, you should clear the metadata cache to mitigate the issue. You can use a Python or Scala notebook command to clear the cache. %python\r\n\r\nspark._jvm.com.databricks.sql.transaction.tahoe.DeltaLog.clearCache() %scala\r\n\r\ncom.databricks.sql.transaction.tahoe.DeltaLog.clearCache() Was this article helpful? (7) (16) Additional Informations Related Articles Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... Converting from Parquet to Delta Lake fails Problem You are attempting to convert a Parquet file to a Delta Lake file. The di... Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... Related Articles Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... Converting from Parquet to Delta Lake fails Problem You are attempting to convert a Parquet file to a Delta Lake file. The di... Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "A file referenced in the transaction log cannot be found",
          "view_href" : "https://kb.databricks.com/en_US/delta/file-transaction-log-not-found"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/conda-fails-to-download-packages-from-anaconda",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning Conda fails to download packages from Anaconda Conda fails to download packages from Anaconda Conda fails to download packages with PackagesNotFoundError when you try to install packages from Anaconda. Written by mathan.pillai Last published at: May 16th, 2022 Problem You are attempting to download packages from the Anaconda repository and get a PackagesNotFoundError error message. This error can occur when using %conda, or %sh conda in notebooks, and when using Conda in an init script. Cause Anaconda Inc. updated the terms of service for repo.anaconda.com and anaconda.org/anaconda. Based on the Anaconda terms of service you may require a commercial license if you rely on Anaconda’s packaging and distribution. You should review the Anaconda Commercial Edition FAQ for more information. Delete Info Your use of any Anaconda channels is governed by the Anaconda terms of service. As a result, the default channel configuration for the Conda package manager was removed in Databricks Runtime 7.3 LTS for Machine Learning and above. Solution You should review the Anaconda terms of service and determine if you require a commercial license. Once you have verified that you have a valid license, you must specify a channel to install or update packages with Conda. You can specify a Conda channel with -c <name-of-channel>. For example, %conda install matplotlib returns an error, while %conda install -c defaults matplotlib installs matplotlib. Was this article helpful? (10) (12) Additional Informations Related Articles Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... How to extract feature information for tree-based Apache SparkML pipeline models When you are fitting a tree-based model, such as a decision tree, random forest, ... Fitting an Apache SparkML model throws error Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.... H2O.ai Sparkling Water cluster not reachable Problem You are trying to initialize H2O.ai’s Sparkling Water on Databricks Runti... How to perform group K-fold cross validation with Apache Spark Cross validation randomly splits the training data into a specified number of fol... Related Articles Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... How to extract feature information for tree-based Apache SparkML pipeline models When you are fitting a tree-based model, such as a decision tree, random forest, ... Fitting an Apache SparkML model throws error Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.... H2O.ai Sparkling Water cluster not reachable Problem You are trying to initialize H2O.ai’s Sparkling Water on Databricks Runti... How to perform group K-fold cross validation with Apache Spark Cross validation randomly splits the training data into a specified number of fol... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Conda fails to download packages from Anaconda",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/conda-fails-to-download-packages-from-anaconda"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/sql-in-python",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark How to run SQL queries from Python scripts How to run SQL queries from Python scripts Learn how to run SQL queries using Python scripts. Written by arjun.kaimaparambilrajan Last published at: May 19th, 2022 You may want to access your tables outside of Databricks notebooks. Besides connecting BI tools via JDBC (AWS | Azure), you can also access tables by using Python scripts. You can connect to a Spark cluster via JDBC using PyHive and then run a script. You should have PyHive installed on the machine where you are running the Python script. Delete Info Python 2 is considered end-of-life. You should use Python 3 to run the script provided in this article. If you have both Python 2 and Python 3 running on your system, you should make sure your version of pip is linked to Python 3 before you proceed. You can check your version of pip by running pip -V at the command prompt. This command returns the version of pip and the version of Python it is using. Install PyHive and Thrift Use pip to install PyHive and Thrift. %sh\r\n\r\npip install pyhive thrift Run SQL script This sample Python script sends the SQL query show tables to your cluster and then displays the result of the query. Do the following before you run the script: Replace <token> with your Databricks API token. Replace <databricks-instance> with the domain name of your Databricks deployment. Replace <workspace-id> with the Workspace ID. Replace <cluster-id> with a cluster ID. To get the API token, see Generate a token (AWS | Azure). To determine the other values, see How to get Workspace, Cluster, Notebook, and Job Details (AWS | Azure). %python\r\n\r\n#!/usr/bin/python\r\n\r\n\r\nimport os\r\nimport sys\r\nfrom pyhive import hive\r\nfrom thrift.transport import THttpClient\r\nimport base64\r\n\r\n\r\nTOKEN = \"<token>\"\r\nWORKSPACE_URL = \"<databricks-instance>\"\r\nWORKSPACE_ID = \"<workspace-id>\"\r\nCLUSTER_ID = \"<cluster-id>\"\r\n\r\n\r\nconn = 'https://%s/sql/protocolv1/o/%s/%s' % (WORKSPACE_URL, WORKSPACE_ID, CLUSTER_ID)\r\nprint(conn)\r\n\r\n\r\ntransport = THttpClient.THttpClient(conn)\r\n\r\n\r\nauth = \"token:%s\" % TOKEN\r\nPY_MAJOR = sys.version_info[0]\r\n\r\n\r\nif PY_MAJOR < 3:\r\n  auth = base64.standard_b64encode(auth)\r\nelse:\r\n  auth = base64.standard_b64encode(auth.encode()).decode()\r\n\r\n\r\ntransport.setCustomHeaders({\"Authorization\": \"Basic %s\" % auth})\r\n\r\n\r\ncursor = hive.connect(thrift_transport=transport).cursor()\r\n\r\n\r\ncursor.execute('show tables',async_=True)\r\n\r\n\r\npending_states = (\r\n        hive.ttypes.TOperationState.INITIALIZED_STATE,\r\n        hive.ttypes.TOperationState.PENDING_STATE,\r\n        hive.ttypes.TOperationState.RUNNING_STATE)\r\n\r\n\r\nwhile cursor.poll().operationState in pending_states:\r\n    print(\"Pending...\")\r\n\r\n\r\nprint(\"Done. Results:\")\r\n\r\n\r\nfor table in cursor.fetchall():\r\n    print(table) Was this article helpful? (9) (13) Additional Informations Related Articles Cluster cancels Python command execution due to library conflict Problem The cluster returns Cancelled in a Python notebook. Notebooks in all othe... How to import a custom CA certificate When working with Python, you may want to import a custom CA certificate to avoid... Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... AttributeError: ‘function’ object has no attribute Problem You are selecting columns from a DataFrame and you get an error message. ... Related Articles Cluster cancels Python command execution due to library conflict Problem The cluster returns Cancelled in a Python notebook. Notebooks in all othe... How to import a custom CA certificate When working with Python, you may want to import a custom CA certificate to avoid... Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... AttributeError: ‘function’ object has no attribute Problem You are selecting columns from a DataFrame and you get an error message. ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to run SQL queries from Python scripts",
          "view_href" : "https://kb.databricks.com/en_US/python/sql-in-python"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/best-practice-cache-count-take",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Best practice for cache(), count(), and take() Best practice for cache(), count(), and take() Learn best practices for using `cache()`, `count()`, and `take()` with a Spark DataFrame. Written by ram.sankarasubramanian Last published at: May 20th, 2022 cache() is an Apache Spark transformation that can be used on a DataFrame, Dataset, or RDD when you want to perform more than one action. cache() caches the specified DataFrame, Dataset, or RDD in the memory of your cluster’s workers. Since cache() is a transformation, the caching operation takes place only when a Spark action (for example, count(), show(), take(), or write()) is also used on the same DataFrame, Dataset, or RDD in a single action. Calling cache() and count() separately %scala\r\n\r\ndf1=spark.read.parquet(input_path1)\r\ndf2=spark.read.parquet(input_path2)\r\ndf1.cache()                                         # Cache DataFrame df1\r\n\r\njoined_df = df1.join(df2, df1.id==df2.id, ‘inner’)  # Join DataFrame df1 and df2\r\nfiltered_df = joined_df.filter(“name == ‘John’”)    # Filter the joined DataFrame for the name “John”\r\ndf1.count()                                         # Call count() on the cached DataFrame\r\nfiltered_df.show()                                  # Show the filtered DataFrame filtered_df In this example, DataFrame df1 is cached into memory when df1.count() is executed. df1.cache() does not initiate the caching operation on DataFrame df1. Calling take() on a cached DataFrame %scala\r\n\r\ndf=spark.table(“input_table_name”)\r\ndf.cache.take(5)                   # Call take(5) on the DataFrame df, while also caching it\r\ndf.count()                         # Call count() on the DataFrame df In this example, DataFrame df is cached into memory when take(5) is executed. Only one partition of DataFrame df is cached in this case, because take(5) only processes 5 records. Only the partition from which the records are fetched is processed, and only that processed partition is cached. Other partitions of DataFrame df are not cached. As a result, when df.count() is called, DataFrame df is created again, since only one partition is available in the cluster’s cache. Calling take(5) in the example only caches 14% of the DataFrame. Calling count() on a cached DataFrame %scala\r\n\r\ndf=spark.table(“input_table_name”)\r\ndf.cache.count()                    # Call count() on the DataFrame df, while also caching it\r\ndf.count()                          # Call count() on the DataFrame df\r\ndf.filter(“name==’John’”).count() In this example, DataFrame df is cached into memory when df.count() is executed. To return the count of the dataframe, all the partitions are processed. This means that all the partitions are cached. As a result, when df.count() and df.filter(“name==’John'”).count() are called as subsequent actions, DataFrame df is fetched from the cluster’s cache, rather than getting created again. Calling count() in the example caches 100% of the DataFrame. Summary You should call count() or write() immediately after calling cache() so that the entire DataFrame is processed and cached in memory. If you only cache part of the DataFrame, the entire DataFrame may be recomputed when a subsequent action is performed on the DataFrame. Delete Info The advice for cache() also applies to persist(). Was this article helpful? (14) (40) Additional Informations Related Articles Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Related Articles Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Best practice for cache(), count(), and take()",
          "view_href" : "https://kb.databricks.com/en_US/scala/best-practice-cache-count-take"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/spark-executor-memory",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Apache Spark executor memory allocation Apache Spark executor memory allocation Understand how Spark executor memory allocation works in a Databricks cluster. Written by Adam Pavlacka Last published at: March 4th, 2022 By default, the amount of memory available for each executor is allocated within the Java Virtual Machine (JVM) memory heap. This is controlled by the spark.executor.memory property. However, some unexpected behaviors were observed on instances with a large amount of memory allocated. As JVMs scale up in memory size, issues with the garbage collector become apparent. These issues can be resolved by limiting the amount of memory under garbage collector management. Selected Databricks cluster types enable the off-heap mode, which limits the amount of memory under garbage collector management. This is why certain Spark clusters have the spark.executor.memory value set to a fraction of the overall cluster memory. The off-heap mode is controlled by the properties spark.memory.offHeap.enabled and spark.memory.offHeap.size which are available in Spark 1.6.0 and above. AWS The following Databricks cluster types enable the off-heap memory policy: c5d.18xlarge c5d.9xlarge i3.16xlarge i3en.12xlarge i3en.24xlarge i3en.2xlarge i3en.3xlarge i3en.6xlarge i3en.large i3en.xlarge m4.16xlarge m5.24xlarge m5a.12xlarge m5a.16xlarge m5a.24xlarge m5a.8xlarge m5d.12xlarge m5d.24xlarge m5d.4xlarge r4.16xlarge r5.12xlarge r5.16xlarge r5.24xlarge r5.2xlarge r5.4xlarge r5.8xlarge r5a.12xlarge r5a.16xlarge r5a.24xlarge r5a.2xlarge r5a.4xlarge r5a.8xlarge r5d.12xlarge r5d.24xlarge r5d.2xlarge r5d.4xlarge z1d.2xlarge z1d.3xlarge z1d.6xlarge z1d.6xlarge Delete Azure The following Azure Databricks cluster types enable the off-heap memory policy: Standard_L8s_v2 Standard_L16s_v2 Standard_L32s_v2 Standard_L32s_v2 Standard_L80s_v2 Delete Was this article helpful? (14) (11) Additional Informations Related Articles Admin user cannot restart cluster to run job Problem When a user who has permission to start a cluster, such as a Databricks A... Cluster fails to start with dummy does not exist error Problem You try to start a cluster, but it fails to start. You get an Apache Spar... Set Apache Hadoop core-site.xml properties You have a scenario that requires Apache Hadoop properties to be set. You would n... Persist Apache Spark CSV metrics to a DBFS location Spark has a configurable metrics system that supports a number of sinks, includin... Related Articles Admin user cannot restart cluster to run job Problem When a user who has permission to start a cluster, such as a Databricks A... Cluster fails to start with dummy does not exist error Problem You try to start a cluster, but it fails to start. You get an Apache Spar... Set Apache Hadoop core-site.xml properties You have a scenario that requires Apache Hadoop properties to be set. You would n... Persist Apache Spark CSV metrics to a DBFS location Spark has a configurable metrics system that supports a number of sinks, includin... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Apache Spark executor memory allocation",
          "view_href" : "https://kb.databricks.com/en_US/clusters/spark-executor-memory"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dev-tools/invalid-access-token-airflow",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Developer tools Invalid Access Token error when running jobs with Airflow Invalid Access Token error when running jobs with Airflow Learn what to do when you receive an Invalid Access Token error when using Databricks jobs with Airflow. Written by Adam Pavlacka Last published at: May 9th, 2022 Problem When you run scheduled Airflow Databricks jobs, you get this error: Invalid Access Token : 403 Forbidden Error Cause To run or schedule Databricks jobs through Airflow, you need to configure the Databricks connection using the Airflow web UI. Any of the following incorrect settings can cause the error: Set the host field to the Databricks workspace hostname. Set the login field to token. Set the password field to the Databricks-generated personal access token. Set the Extra field to a JSON string, where the key is token and the value is your personal access token. The Databricks-generated personal access token is normally valid for 90 days. If the token expires, then this 403 Forbidden Error occurs. Solution Verify that the Extra field is correctly configured with the JSON string: {\"token\": \"<your personal access token>\"} Verify that the token is mentioned in both the password field and the Extra field. Verify that the host, login, and password fields are configured correctly. Verify that the personal access token has not expired. If necessary, generate a new token (AWS | Azure). Was this article helpful? (8) (11) Additional Informations Related Articles Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... Failed to create process error with Databricks CLI in Windows Problem While trying to access the Databricks CLI (AWS | Azure | GCP) in Windows,... How to Sort S3 files By Modification Time in Databricks Notebooks Problem When you use the dbutils utility to list the files in a S3 location, the ... GeoSpark undefined function error with DBConnect Problem You are trying to use the GeoSpark function st_geofromwkt with DBConnect ... Related Articles Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... Failed to create process error with Databricks CLI in Windows Problem While trying to access the Databricks CLI (AWS | Azure | GCP) in Windows,... How to Sort S3 files By Modification Time in Databricks Notebooks Problem When you use the dbutils utility to list the files in a S3 location, the ... GeoSpark undefined function error with DBConnect Problem You are trying to use the GeoSpark function st_geofromwkt with DBConnect ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Invalid Access Token error when running jobs with Airflow",
          "view_href" : "https://kb.databricks.com/en_US/dev-tools/invalid-access-token-airflow"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/driver-unavailable",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Spark job fails with Driver is temporarily unavailable Spark job fails with Driver is temporarily unavailable Learn how to distinguish between active and dead Databricks jobs. Written by Adam Pavlacka Last published at: May 10th, 2022 Problem A Databricks notebook returns the following error: Driver is temporarily unavailable This issue can be intermittent or not. A related error message is: Lost connection to cluster. The notebook may have been detached. Cause One common cause for this error is that the driver is undergoing a memory bottleneck. When this happens, the driver crashes with an out of memory (OOM) condition and gets restarted or becomes unresponsive due to frequent full garbage collection. The reason for the memory bottleneck can be any of the following: The driver instance type is not optimal for the load executed on the driver. There are memory-intensive operations executed on the driver. There are many notebooks or jobs running in parallel on the same cluster. Solution The solution varies from case to case. The easiest way to resolve the issue in the absence of specific details is to increase the driver memory. You can increase driver memory simply by upgrading the driver node type on the cluster edit page in your Databricks workspace. Other points to consider: Avoid memory intensive operations like: collect() operator, which brings a large amount of data to the driver. Conversion of a large DataFrame to Pandas If these operations are essential, ensure that enough driver memory is available. Avoid running batch jobs on a shared interactive cluster. Distribute the workloads into different clusters. No matter how big the cluster is, the functionalities of the Spark driver cannot be distributed within a cluster. Was this article helpful? (10) (53) Additional Informations Related Articles Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... Related Articles Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Spark job fails with Driver is temporarily unavailable",
          "view_href" : "https://kb.databricks.com/en_US/jobs/driver-unavailable"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/replace-default-jar-new-jar",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Replace a default library jar Replace a default library jar Learn how to replace a default Java or Scala library jar with another version. Written by ram.sankarasubramanian Last published at: May 16th, 2022 Databricks includes a number of default Java and Scala libraries. You can replace any of these libraries with another version by using a cluster-scoped init script to remove the default library jar and then install the version you require. Delete Warning Removing default libraries and installing new versions may cause instability or completely break your Databricks cluster. You should thoroughly test any new library version in your environment before running production jobs. Identify the artifact id To identify the name of the jar file you want to remove: Click the Databricks Runtime version you are using from the list of supported releases (AWS | Azure | GCP). Navigate to the Java and Scala libraries section. Identify the Artifact ID for the library you want to remove. Use the artifact id to find the jar filename Use the ls -l command in a notebook to find the jar that contains the artifact id. For example, to find the jar filename for the spark-snowflake_2.12 artifact id in Databricks Runtime 7.0 you can use the following code: %sh\r\n\r\nls -l /databricks/jars/*spark-snowflake_2.12* This returns the jar filename `----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.5.9-spark_2.4.jar`. Upload the replacement jar file Upload your replacement jar file to a DBFS path. Create the init script Use the following template to create a cluster-scoped init script. %sh\r\n\r\n#!/bin/bash\r\nrm -rf /databricks/jars/<jar_filename_to_remove>.jar\r\ncp /dbfs/<path_to_replacement_jar>/<replacement_jar_filename>.jar /databricks/jars/ Using the spark-snowflake_2.12 example from the prior step would result in an init script similar to the following: %sh\r\n\r\n#!/bin/bash\r\nrm -rf /databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.5.9-spark_2.4.jar\r\ncp /dbfs/FileStore/jars/e43fe9db_c48d_412b_b142_cdde10250800-spark_snowflake_2_11_2_7_1_spark_2_4-b2adc.jar /databricks/jars/ Install the init script and restart Install the cluster-scoped init script on the cluster, following the instructions in Configure a cluster-scoped init script (AWS | Azure | GCP). Restart the cluster. Was this article helpful? (7) (13) Additional Informations Related Articles How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Libraries fail with dependency exception Problem You have a Python function that is defined in a custom egg or wheel file ... Install Turbodbc via init script Turbodbc is a Python module that uses the ODBC interface to access relational dat... Library unavailability causing job failures Problem You are launching jobs that import external libraries and get an Import E... Related Articles How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Libraries fail with dependency exception Problem You have a Python function that is defined in a custom egg or wheel file ... Install Turbodbc via init script Turbodbc is a Python module that uses the ODBC interface to access relational dat... Library unavailability causing job failures Problem You are launching jobs that import external libraries and get an Import E... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Replace a default library jar",
          "view_href" : "https://kb.databricks.com/en_US/libraries/replace-default-jar-new-jar"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/failure-when-mounting-storage",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks Failure when accessing or mounting storage Failure when accessing or mounting storage Do not mount storage to the root mount path. Written by kiran.bharathi Last published at: May 16th, 2022 Problem You are trying to access an existing mount point, or create a new mount point, and it fails with an error message. Invalid Mount Exception:The backend could not get tokens for path /mnt. Cause The root mount path (/mnt) is also mounted to a storage location. You can verify that something is mounted to the root path by listing all mount points with DBUtils (AWS | Azure | GCP). %python\r\n\r\ndbutils.fs.mounts() If /mnt is listed with a source, you have storage incorrectly mounted to the root path.. Solution You should unmount the root mount path. %python\r\n\r\ndbutils.fs.unmount(\"/mnt\") You can now access existing mount points, or create new mount points. Was this article helpful? (8) (22) Additional Informations Related Articles Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... Error: Received command c on object id p0 Problem You have imported Python libraries, but when you try to execute Python co... display() does not show microseconds correctly Problem You want to display a timestamp value with microsecond precision, but whe... Common errors in notebooks There are some common issues that occur when using notebooks. This section outlin... Related Articles Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... Error: Received command c on object id p0 Problem You have imported Python libraries, but when you try to execute Python co... display() does not show microseconds correctly Problem You want to display a timestamp value with microsecond precision, but whe... Common errors in notebooks There are some common issues that occur when using notebooks. This section outlin... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Failure when accessing or mounting storage",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/failure-when-mounting-storage"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/troubleshoot-cancel-command",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks Troubleshooting unresponsive Python notebooks or canceled commands Troubleshooting unresponsive Python notebooks or canceled commands Learn how to troubleshoot unresponsive Python notebooks and cancelled commands in Databricks notebooks. Written by Adam Pavlacka Last published at: May 17th, 2022 This article provides an overview of troubleshooting steps you can take if a notebook is unresponsive or cancels commands. Check metastore connectivity Problem Simple commands in newly-attached notebooks fail, but succeed in notebooks that were attached to the same cluster earlier. Troubleshooting steps Check metastore connectivity. The inability to connect to the Hive metastore can cause REPL initialization to hang, making the cluster appear unresponsive. Are you are using the Databricks metastore or your own external metastore? If you are using an external metastore, have you changed anything recently? Did you upgrade your metastore version? Rotate passwords or configurations? Change security group rules? See Metastore for more troubleshooting tips and solutions. Check for conflicting libraries Problem Python library conflicts can result in cancelled commands. The Databricks support organization sees conflicts most often with versions of ipython, numpy, scipy, and pandas. Troubleshooting steps Review the Cluster cancels Python command execution due to library conflict KB article for more information. For more notebook troubleshooting information, see Notebooks. Was this article helpful? (6) (15) Additional Informations Related Articles Notebook autosave fails due to file size limits Problem Notebook autosaving fails with the following error message: Failed to sav... display() does not show microseconds correctly Problem You want to display a timestamp value with microsecond precision, but whe... Access notebooks owned by a deleted user When you remove a user (AWS | Azure) from Databricks, a special backup folder is ... Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... Related Articles Notebook autosave fails due to file size limits Problem Notebook autosaving fails with the following error message: Failed to sav... display() does not show microseconds correctly Problem You want to display a timestamp value with microsecond precision, but whe... Access notebooks owned by a deleted user When you remove a user (AWS | Azure) from Databricks, a special backup folder is ... Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Troubleshooting unresponsive Python notebooks or canceled commands",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/troubleshoot-cancel-command"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/jdbc-write-fails-primarykeyviolation",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark JDBC write fails with a PrimaryKeyViolation error JDBC write fails with a PrimaryKeyViolation error JDBC write to a SQL database fails with a `PrimaryKeyViolation` error or results in duplicate data Written by harikrishnan.kunhumveettil Last published at: May 24th, 2022 Problem You are using JDBC to write to a SQL table that has primary key constraints, and the job fails with a PrimaryKeyViolation error. Alternatively, you are using JDBC to write to a SQL table that does not have primary key constraints, and you see duplicate entries in recently written tables. Cause When Apache Spark performs a JDBC write, one partition of the DataFrame is written to a SQL table. This is generally done as a single JDBC transaction, in order to avoid repeatedly inserting data. However, if the transaction fails after the commit occurs, but before the final stage completes, it is possible for duplicate data to be copied into the SQL table. The PrimaryKeyViolation error occurs when a write operation is attempting to insert a duplicate entry for the primary key. Solution You should use a temporary table to buffer the write, and ensure there is no duplicate data. Verify that speculative execution is disabled in your Spark configuration: spark.speculation false. This is disabled by default. Create a temporary table on your SQL database. Modify your Spark code to write to the temporary table. After the Spark writes have completed, check the temporary table to ensure there is no duplicate data. Merge the temporary table with the target table on your SQL database. Delete the temporary table. Delete Info This workaround should only be used if you encounter the listed data duplication issue, as there is a small performance penalty when compared to Spark jobs that write directly to the target table. Was this article helpful? (7) (12) Additional Informations Related Articles Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Find the size of a table This article explains how to find the size of a table. The command used depends o... Error in SQL statement: AnalysisException: Table or view not found Problem When you try to query a table or view, you get this error: AnalysisExcept... Date functions only accept int values in Apache Spark 3.0 Problem You are attempting to use the date_add() or date_sub() functions in Spark... Related Articles Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Find the size of a table This article explains how to find the size of a table. The command used depends o... Error in SQL statement: AnalysisException: Table or view not found Problem When you try to query a table or view, you get this error: AnalysisExcept... Date functions only accept int values in Apache Spark 3.0 Problem You are attempting to use the date_add() or date_sub() functions in Spark... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "JDBC write fails with a PrimaryKeyViolation error",
          "view_href" : "https://kb.databricks.com/en_US/sql/jdbc-write-fails-primarykeyviolation"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/common-errors-in-notebooks",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks Common errors in notebooks Common errors in notebooks Learn about common errors from Databricks notebooks. Written by Adam Pavlacka Last published at: May 16th, 2022 There are some common issues that occur when using notebooks. This section outlines some of the frequently asked questions and best practices that you should follow. Spark job fails with java.lang.NoClassDefFoundError Sometimes you may come across an error like: %scala\r\n\r\njava.lang.NoClassDefFoundError: Could not initialize class line.....$read$ This can occur with a Spark Scala 2.11 cluster and a Scala notebook, if you mix together a case class definition and Dataset/DataFrame operations in the same notebook cell, and later use the case class in a Spark job in a different cell. For example, in the first cell, say you define a case class MyClass and also created a Dataset. %scala\r\n\r\ncase class MyClass(value: Int)\r\n\r\nval dataset = spark.createDataset(Seq(1)) Then in a later cell, you create instances of MyClass inside a Spark job. %scala\r\n\r\ndataset.map { i => MyClass(i) }.count() Solution Move the case class definition to a cell of its own. %scala\r\n\r\ncase class MyClass(value: Int)   // no other code in this cell %scala\r\n\r\nval dataset = spark.createDataset(Seq(1))\r\ndataset.map { i => MyClass(i) }.count() Spark job fails with java.lang.UnsupportedOperationException Sometimes you may come across an error like: java.lang.UnsupportedOperationException: Accumulator must be registered before send to executor This can occur with a Spark Scala 2.10 cluster and a Scala notebook. The reason and solution for this error are same as the prior Spark job fails with java.lang.NoClassDefFoundError. Was this article helpful? (14) (18) Additional Informations Related Articles Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... JSON reader parses values as null Problem You are attempting to read a JSON file. You know the file has data in it,... Enable s3cmd for notebooks s3cmd is a client library that allows you to perform all AWS S3 operations from a... Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Related Articles Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... JSON reader parses values as null Problem You are attempting to read a JSON file. You know the file has data in it,... Enable s3cmd for notebooks s3cmd is a client library that allows you to perform all AWS S3 operations from a... Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Common errors in notebooks",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/common-errors-in-notebooks"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dbfs/dbfs-s3-bucket-costs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Databricks File System (DBFS) How to calculate the Databricks file system (DBFS) S3 API call cost How to calculate the Databricks file system (DBFS) S3 API call cost Learn how to calculate the Databricks file system (DBFS) S3 API call cost. Written by Adam Pavlacka Last published at: March 8th, 2022 The cost of a DBFS S3 bucket is primarily driven by the number of API calls, and secondarily by the cost of storage. You can use the AWS CloudTrail logs to create a table, count the number of API calls, and thereby calculate the exact cost of the API requests. Obtain the following information. You may need to contact your AWS Administrator to get it. API call cost for calls involving List, Put, Copy, or Post (the example script uses the price per thousand calls: 0.005/1000) API call cost for calls involving Head, Get, or Select (below, 0.0004/1000) Account ID for the Databricks control plane account (below, 414351767826) Copy the CloudTrail logs to an S3 bucket and use the following Apache Spark code to read the logs and create a table: %python\r\n\r\nspark.read.json(\"s3://dbc-root-cloudwatch/*/*/*/*/*/*/*\").createOrReplaceTempView(\"f_cloudwatch\")\t Substitute the accountIDand the API call costs into the following query. This query takes the CloudTrail results collected during a specific time interval, counts the number of API calls being made from the Databricks control plane account, and calculates the cost. %sql\r\n\r\nselect\r\nRecords.userIdentity.accountId,\r\nRecords.eventName,\r\ncount(*) as api_calls,\r\n(case when Records.eventName like 'List%' or Records.eventName like 'Put%' or Records.eventName like 'Copy%' or Records.eventName like 'Post%' then 0.005/1000\r\n when Records.eventName like 'Head%' or Records.eventName like 'Get%' or Records.eventName like 'Select%' then 0.0004/1000\r\n else 0 end) * count(*) as api_cost\r\nfrom\r\n(select explode(Records) as Records\r\nfrom f_cloudwatch\r\nwhere Records is not null)\r\n-- where Records.userIdentity.accountId = '414351767826'\r\ngroup by 1,2\r\norder by 4 desc\r\nlimit 10; Run the query to generate a table. The resulting table shows the number of API calls and the cost of those calls. Additional API costs are often due to checkpointing directories for streaming jobs. Databricks recommends deleting old checkpointing directories if they are no longer referenced. Was this article helpful? (11) (14) Additional Informations Related Articles Cannot read Databricks objects stored in the DBFS root directory Problem An Access Denied error returns when you attempt to read Databricks object... Cannot access objects written by Databricks from outside Databricks Problem When you attempt to access an object in an S3 location written by Databri... Related Articles Cannot read Databricks objects stored in the DBFS root directory Problem An Access Denied error returns when you attempt to read Databricks object... Cannot access objects written by Databricks from outside Databricks Problem When you attempt to access an object in an S3 location written by Databri... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to calculate the Databricks file system (DBFS) S3 API call cost",
          "view_href" : "https://kb.databricks.com/en_US/dbfs/dbfs-s3-bucket-costs"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/dbfs-file-size-limit",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Reading large DBFS-mounted files using Python APIs Reading large DBFS-mounted files using Python APIs Learn how to resolve errors when reading large DBFS-mounted files using Python APIs. Written by Adam Pavlacka Last published at: May 19th, 2022 This article explains how to resolve an error that occurs when you read large DBFS-mounted files using local Python APIs. Problem If you mount a folder onto dbfs:// and read a file larger than 2GB in a Python API like pandas, you will see following error: /databricks/python/local/lib/python2.7/site-packages/pandas/parser.so in pandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)()\r\n/databricks/python/local/lib/python2.7/site-packages/pandas/parser.so in pandas.parser.TextReader._setup_parser_source (pandas/parser.c:6883)()\r\nIOError: Initializing from file failed Cause The error occurs because one argument in the Python method to read a file is a signed int, the length of the file is an int, and if the object is a file larger than 2GB, the length can be larger than maximum signed int. Solution Move the file from dbfs:// to local file system (file://). Then read using the Python API. For example: Copy the file from dbfs:// to file://: %fs cp dbfs:/mnt/large_file.csv file:/tmp/large_file.csv Read the file in the pandasAPI: %python\r\n\r\nimport pandas as pd\r\npd.read_csv('file:/tmp/large_file.csv',).head() Was this article helpful? (8) (14) Additional Informations Related Articles Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... Install and compile Cython This document explains how to run Spark code with compiled Cython code. The steps... Related Articles Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... Install and compile Cython This document explains how to run Spark code with compiled Cython code. The steps... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Reading large DBFS-mounted files using Python APIs",
          "view_href" : "https://kb.databricks.com/en_US/python/dbfs-file-size-limit"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/backfill-delta-table-cols",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake How to populate or update columns in an existing Delta table How to populate or update columns in an existing Delta table Learn how to populate or update columns in an existing Delta table. Written by Adam Pavlacka Last published at: May 10th, 2022 Problem You have an existing Delta table, with a few empty columns. You need to populate or update those columns with data from a raw Parquet file. Solution In this example, there is a customers table, which is an existing Delta table. It has an address column with missing values. The updated data exists in Parquet format. Create a DataFrame from the Parquet file using an Apache Spark API statement: %python\r\n\r\nupdatesDf = spark.read.parquet(\"/path/to/raw-file\") View the contents of the updatesDF DataFrame: %python\r\n\r\ndisplay(updatesDf) Create a table from the updatesDf DataFrame. In this example, it is named updates. %python\r\n\r\nupdatesDf.createOrReplaceTempView(\"updates\") Check the contents of the updates table, and compare it to the contents of customers: %python\r\n\r\ndisplay(customers) Use the MERGE INTO statement to merge the data from the updates table into the original customers table. %sql\r\n\r\nMERGE INTO customers\r\nUSING updates\r\nON customers.customerId = source.customerId\r\nWHEN MATCHED THEN\r\n  UPDATE SET address = updates.address\r\nWHEN NOT MATCHED\r\n  THEN INSERT (customerId, address) VALUES (updates.customerId, updates.address) Here, customers is the original Delta table that has an address column with missing values. updates is the table created from the DataFrame updatesDf, which is created by reading data from the raw file. The address column of the original Delta table is populated with the values from updates, overwriting any existing values in the address column. If updates contains customers that are not already in the customers table, then the command adds these new customer records. For more examples of using MERGE INTO, see Merge Into (Delta Lake) (AWS | Azure | GCP). Was this article helpful? (10) (43) Additional Informations Related Articles How Delta cache behaves on an autoscaling cluster This article is about how Delta cache (AWS | Azure | GCP) behaves on an auto-scal... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Converting from Parquet to Delta Lake fails Problem You are attempting to convert a Parquet file to a Delta Lake file. The di... Related Articles How Delta cache behaves on an autoscaling cluster This article is about how Delta cache (AWS | Azure | GCP) behaves on an auto-scal... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Converting from Parquet to Delta Lake fails Problem You are attempting to convert a Parquet file to a Delta Lake file. The di... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to populate or update columns in an existing Delta table",
          "view_href" : "https://kb.databricks.com/en_US/delta/backfill-delta-table-cols"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/install-pyodbc-on-cluster",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Error when installing pyodbc on a cluster Error when installing pyodbc on a cluster Learn how to troubleshoot an error when installing pyodbc on a Databricks cluster. Written by Adam Pavlacka Last published at: May 11th, 2022 Problem One of the following errors occurs when you use pip to install the pyodbc library. java.lang.RuntimeException: Installation failed with message: Collecting pyodbc \"Library installation is failing due to missing dependencies. sasl and thrift_sasl are optional dependencies for SASL or Kerberos support\" Cause Although sasl and thrift_sasl are optional dependencies for SASL or Kerberos support, they need to be present for pyodbc installation to succeed. Solution Cluster-scoped init script method You can put these commands into a single init script and attach it to the cluster. This ensures that the dependent libraries for pyodbc are installed before the cluster starts. Create the base directory to store the init script in, if the base directory does not exist. Here, use dbfs:/databricks/<directory>as an example. %sh\r\n\r\ndbutils.fs.mkdirs(\"dbfs:/databricks/<directory>/\") Create the script and save it to a file. %sh\r\n\r\ndbutils.fs.put(\"dbfs:/databricks/<directory>/tornado.sh\",\"\"\"\r\n#!/bin/bash\r\npip list | egrep 'thrift-sasl|sasl'\r\npip install --upgrade thrift\r\ndpkg -l | egrep 'thrift_sasl|libsasl2-dev|gcc|python-dev'\r\nsudo apt-get -y install unixodbc-dev libsasl2-dev gcc python-dev\r\n\"\"\",True) Check that the script exists. %python\r\n\r\ndisplay(dbutils.fs.ls(\"dbfs:/databricks/<directory>/tornado.sh\")) On the cluster configuration page, click the Advanced Options toggle. At the bottom of the page, click the Init Scripts tab. In the Destination drop-down, select DBFS, provide the file path to the script, and click Add. Restart the cluster. For more details about cluster-scoped init scripts, see Cluster-scoped init scripts (AWS | Azure | GCP). Notebook method In a notebook, check the version of thrift and upgrade to the latest version. %sh\r\n\r\npip list | egrep 'thrift-sasl|sasl'\r\npip install --upgrade thrift Ensure that dependent packages are installed. %sh\r\n\r\ndpkg -l | egrep 'thrift_sasl|libsasl2-dev|gcc|python-dev' Install nnixodbc before installing pyodbc. %sh\r\n\r\nsudo apt-get -y install unixodbc-dev libsasl2-dev gcc python-dev Was this article helpful? (13) (15) Additional Informations Related Articles Error when installing Cartopy on a cluster Problem You are trying to install Cartopy on a cluster and you receive a ManagedL... Install PyGraphViz PyGraphViz Python libraries are used to plot causal inference networks. If you tr... Init script fails to download Maven JAR Problem You have an init script that is attempting to install a library via Maven... Library unavailability causing job failures Problem You are launching jobs that import external libraries and get an Import E... Related Articles Error when installing Cartopy on a cluster Problem You are trying to install Cartopy on a cluster and you receive a ManagedL... Install PyGraphViz PyGraphViz Python libraries are used to plot causal inference networks. If you tr... Init script fails to download Maven JAR Problem You have an init script that is attempting to install a library via Maven... Library unavailability causing job failures Problem You are launching jobs that import external libraries and get an Import E... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Error when installing pyodbc on a cluster",
          "view_href" : "https://kb.databricks.com/en_US/libraries/install-pyodbc-on-cluster"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/display-not-show-microseconds",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks display() does not show microseconds correctly display() does not show microseconds correctly Use show() to display timestamp values with microsecond precision. display() is limited to millisecond precision. Written by harikrishnan.kunhumveettil Last published at: May 16th, 2022 Problem You want to display a timestamp value with microsecond precision, but when you use display() it does not show the value past milliseconds. For example, this Apache Spark SQL display() command: %sql\r\n\r\ndisplay(spark.sql(\"select cast('2021-08-10T09:08:56.740436' as timestamp) as test\")) Returns a truncated value: 2021-08-10T09:08:56.740+0000 Cause The DataFrame is converted to HTML internally before the output is rendered. This limits the displayed results to millisecond precision. It does not affect the stored value. Solution You should use show() instead of using display(). For example, this Apache Spark SQL show() command: %sql\r\n\r\nspark.sql(\"select cast('2021-08-10T09:08:56.740436' as timestamp) as test\").show(truncate=False) Returns the correct value: 2021-08-10 09:08:56.740436 As an alternative, you can create a second column and copy the value to the column as a string. After conversion to a string, display() shows the full value. Was this article helpful? (9) (22) Additional Informations Related Articles Common errors in notebooks There are some common issues that occur when using notebooks. This section outlin... Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... Enable s3cmd for notebooks s3cmd is a client library that allows you to perform all AWS S3 operations from a... Related Articles Common errors in notebooks There are some common issues that occur when using notebooks. This section outlin... Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... Enable s3cmd for notebooks s3cmd is a client library that allows you to perform all AWS S3 operations from a... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "display() does not show microseconds correctly",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/display-not-show-microseconds"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/import-custom-ca-cert",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark How to import a custom CA certificate How to import a custom CA certificate Learn how to import a custom CA certificate into your Databricks cluster for Python use. Written by arjun.kaimaparambilrajan Last published at: May 19th, 2022 When working with Python, you may want to import a custom CA certificate to avoid connection errors to your endpoints. ConnectionError: HTTPSConnectionPool(host='my_server_endpoint', port=443): Max retries exceeded with url: /endpoint (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fb73dc3b3d0>: Failed to establish a new connection: [Errno 110] Connection timed out',)) To import one or more custom CA certificates to your Databricks cluster: Create an init script that adds the entire CA chain and sets the REQUESTS_CA_BUNDLE property. In this example, PEM format CA certificates are added to the file myca.crt which is located at /user/local/share/ca-certificates/. This file is referenced in the custom-cert.sh init script. %sh\r\n\r\n\r\ndbutils.fs.put(\"/databricks/init-scripts/custom-cert.sh\", \"\"\"#!/bin/bash\r\n\r\n\r\ncat << 'EOF' > /usr/local/share/ca-certificates/myca.crt\r\n-----BEGIN CERTIFICATE-----\r\n<CA CHAIN 1 CERTIFICATE CONTENT>\r\n-----END CERTIFICATE-----\r\n-----BEGIN CERTIFICATE-----\r\n<CA CHAIN 2 CERTIFICATE CONTENT>\r\n-----END CERTIFICATE-----\r\nEOF\r\n\r\n\r\nupdate-ca-certificates\r\n\r\n\r\nPEM_FILE=\"/etc/ssl/certs/myca.pem\"\r\nPASSWORD=\"<password>\"\r\nJAVA_HOME=$(readlink -f /usr/bin/java | sed \"s:bin/java::\")\r\nKEYSTORE=\"$JAVA_HOME/lib/security/cacerts\"\r\n\r\n\r\nCERTS=$(grep 'END CERTIFICATE' $PEM_FILE| wc -l)\r\n\r\n\r\n# To process multiple certs with keytool, you need to extract\r\n# each one from the PEM file and import it into the Java KeyStore.\r\n\r\n\r\nfor N in $(seq 0 $(($CERTS - 1))); do\r\n  ALIAS=\"$(basename $PEM_FILE)-$N\"\r\n  echo \"Adding to keystore with alias:$ALIAS\"\r\n  cat $PEM_FILE |\r\n    awk \"n==$N { print }; /END CERTIFICATE/ { n++ }\" |\r\n    keytool -noprompt -import -trustcacerts \\\r\n            -alias $ALIAS -keystore $KEYSTORE -storepass $PASSWORD\r\ndone\r\n\r\n\r\necho \"export REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt\" >> /databricks/spark/conf/spark-env.sh\r\n\"\"\") To use your custom CA certificates with DBFS FUSE (AWS | Azure | GCP), add this line to the bottom of your init script: /databricks/spark/scripts/restart_dbfs_fuse_daemon.sh Attach the init script to the cluster as a cluster-scoped init script (AWS | Azure | GCP). Restart the cluster. Was this article helpful? (7) (13) Additional Informations Related Articles Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Use the HDFS API to read files in Python There may be times when you want to read files directly without using third party... Reading large DBFS-mounted files using Python APIs This article explains how to resolve an error that occurs when you read large DBF... Related Articles Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Use the HDFS API to read files in Python There may be times when you want to read files directly without using third party... Reading large DBFS-mounted files using Python APIs This article explains how to resolve an error that occurs when you read large DBF... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to import a custom CA certificate",
          "view_href" : "https://kb.databricks.com/en_US/python/import-custom-ca-cert"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/incorrect-results-docs-as-input",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning Incorrect results when using documents as inputs Incorrect results when using documents as inputs Your model does not return expected results when documents are input using TfidfVectorizer. JSON array Written by pradeepkumar.palaniswamy Last published at: May 16th, 2022 Problem You have a ML model that takes documents as inputs, specifically, an array of strings. You use a feature extractor like TfidfVectorizer to convert the documents to an array of strings and ingest the array into the model. The model is trained, and predictions happen in the notebook, but model serving doesn’t return the expected results for JSON inputs. Cause TfidfVectorizer expects an array of documents as an input. Databricks converts inputs to Pandas DataFrames, which TfidfVectorizer does not process correctly. Solution You must create a custom transformer and add it to the head of the pipeline. For example, the following sample code checks the input for DataFrames. If it finds a DataFrame, the first column is converted to an array of documents. The array of documents is then passed to TfidfVectorizer before being ingested into the model. %python\r\n\r\nclass DataFrameToDocs():\r\n    def transform(self, input_df):\r\n        import pandas as pd\r\n        if isinstance(input_df, pd.DataFrame):\r\n          return input_df[0].values\r\n        else:\r\n          return input_df    def fit(self, X, y=None, **fit_params):\r\n        return self\r\n\r\n\r\nsteps = [('dftodocs', DataFrameToDocs()),('tfidf', TfidfVectorizer()), ('nb_clf', MultinomialNB())]\r\npipeline = Pipeline(steps) Delete Info When input as JSON, both [\"Hello\", \"World\"] and [[\"Hello\"],[\"World\"]] return the same output. Was this article helpful? (8) (14) Additional Informations Related Articles Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... MLflow project fails to access an Apache Hive table Problem You have an MLflow project that fails to access a Hive table and returns ... Error when importing OneHotEncoderEstimator Problem You have migrated a notebook from Databricks Runtime 6.4 for Machine Lear... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... Related Articles Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... MLflow project fails to access an Apache Hive table Problem You have an MLflow project that fails to access a Hive table and returns ... Error when importing OneHotEncoderEstimator Problem You have migrated a notebook from Databricks Runtime 6.4 for Machine Lear... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Incorrect results when using documents as inputs",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/incorrect-results-docs-as-input"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/match-parquet-schema",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management How to handle corrupted Parquet files with different schema How to handle corrupted Parquet files with different schema Learn how to read Parquet files with a specific schema using Databricks. Written by Adam Pavlacka Last published at: May 31st, 2022 Problem Let’s say you have a large list of essentially independent Parquet files, with a variety of different schemas. You want to read only those files that match a specific schema and skip the files that don’t match. One solution could be to read the files in sequence, identify the schema, and union the DataFrames together. However, this approach is impractical when there are hundreds of thousands of files. Solution Set the Apache Spark property spark.sql.files.ignoreCorruptFiles to true and then read the files with the desired schema. Files that don’t match the specified schema are ignored. The resultant dataset contains only data from those files that match the specified schema. Set the Spark property using spark.conf.set: spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\") Alternatively, you can set this property in your Spark config (AWS | Azure | GCP). Was this article helpful? (6) (15) Additional Informations Related Articles How to list and delete files faster in Databricks Scenario Suppose you need to delete a table that is partitioned by year, month, d... Prevent duplicated columns when joining two DataFrames If you perform a join in Spark and don’t specify your join correctly you’ll end u... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... Related Articles How to list and delete files faster in Databricks Scenario Suppose you need to delete a table that is partitioned by year, month, d... Prevent duplicated columns when joining two DataFrames If you perform a join in Spark and don’t specify your join correctly you’ll end u... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to handle corrupted Parquet files with different schema",
          "view_href" : "https://kb.databricks.com/en_US/data/match-parquet-schema"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data-sources/cosmosdb-connector-lib-conf",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data sources CosmosDB-Spark connector library conflict CosmosDB-Spark connector library conflict Learn how to resolve conflicts that arise when using the CosmosDB-Spark connector library with Databricks. Written by Adam Pavlacka Last published at: June 1st, 2022 This article explains how to resolve an issue running applications that use the CosmosDB-Spark connector in the Databricks environment. Problem Normally if you add a Maven dependency to your Spark cluster, your app should be able to use the required connector libraries. But currently, if you simply specify the CosmosDB-Spark connector’s Maven co-ordinates as a dependency for the cluster, you will get the following exception: java.lang.NoClassDefFoundError: Could not initialize class com.microsoft.azure.cosmosdb.Document Cause This occurs because Spark 2.3 uses jackson-databind-2.6.7.1, whereas the CosmosDB-Spark connector uses jackson-databind-2.9.5. This creates a library conflict, and at the executor level you observe the following exception: java.lang.NoSuchFieldError: ALLOW_TRAILING_COMMA\r\nat com.microsoft.azure.cosmosdb.internal.Utils.<clinit>(Utils.java:69) Solution To avoid this problem: Directly download the CosmosDB-Spark connector Uber JAR: azure-cosmosdb-spark_2.3.0_2.11-1.2.2-uber.jar. Upload the downloaded JAR to Databricks following the instructions in Upload a Jar, Python egg, or Python wheel (AWS | Azure). Install the uploaded library as a Cluster-installed library (AWS | Azure) For more information, see Azure Cosmos DB (AWS | Azure). Was this article helpful? (10) (14) Additional Informations Related Articles Unable to read files and list directories in a WASB filesystem Problem When you try reading a file on WASB with Spark, you get the following exc... Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2 Problem Info In general, you should use Databricks Runtime 5.2 and above, which i... Optimize read performance from JDBC data sources Problem Reading data from an external JDBC database is slow. How can I improve re... Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... Related Articles Unable to read files and list directories in a WASB filesystem Problem When you try reading a file on WASB with Spark, you get the following exc... Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2 Problem Info In general, you should use Databricks Runtime 5.2 and above, which i... Optimize read performance from JDBC data sources Problem Reading data from an external JDBC database is slow. How can I improve re... Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "CosmosDB-Spark connector library conflict",
          "view_href" : "https://kb.databricks.com/en_US/data-sources/cosmosdb-connector-lib-conf"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/wrong-data-read-from-snowflake",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark Data is incorrect when read from Snowflake Data is incorrect when read from Snowflake Data read from Snowflake is incorrect when time zone value is not set correctly. Written by DD Sharma Last published at: May 24th, 2022 Problem You have a job that is using Apache Spark to read from a Snowflake table, but the time data that appears in the Dataframe is incorrect. If you run the same query directly on Snowflake, the correct time data is returned. Cause The time zone value was not correctly set. A mismatch between the time zone value of the Databricks cluster and Snowflake can result in incorrect time values, as explained in Snowflake’s working with timestamps and time zones documentation. Solution Set the time zone in Databricks and do not explicitly set a time zone in Snowflake. Option 1: Set the time zone for SQL statements in Databricks Open the Databricks workspace. Select Clusters. Select the cluster you want to modify. Select Edit. Select Advanced Options. Enter spark.sql.session.timeZone <timezone> in the Spark config field. Select Confirm. Option 2: Set the time zone for all nodes with an init script Create the init script with the following command: %python\r\n\r\ndbutils.fs.put(\"/databricks/scripts/set_timezone.sh\",\"\"\"\r\n#!/bin/bash\r\ntimedatectl set-timezone America/Los_Angeles\r\n\"\"\", True) Verify the full path of the init script. %python\r\n\r\n%fs ls /databricks/scripts/set_timezone.sh Open the Databricks workspace. Select Clusters. Select the cluster you want to modify. Select Edit. Select Advanced Options. Select Init Scripts. Enter the Init Script Path. Select Add. Select Confirm. Was this article helpful? (6) (16) Additional Informations Related Articles Generate unique increasing numeric values This article shows you how to use Apache Spark functions to generate unique incre... Find the size of a table This article explains how to find the size of a table. The command used depends o... Date functions only accept int values in Apache Spark 3.0 Problem You are attempting to use the date_add() or date_sub() functions in Spark... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Related Articles Generate unique increasing numeric values This article shows you how to use Apache Spark functions to generate unique incre... Find the size of a table This article explains how to find the size of a table. The command used depends o... Date functions only accept int values in Apache Spark 3.0 Problem You are attempting to use the date_add() or date_sub() functions in Spark... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Data is incorrect when read from Snowflake",
          "view_href" : "https://kb.databricks.com/en_US/sql/wrong-data-read-from-snowflake"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/update-job-perms-multiple-users",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks Update job permissions for multiple users Update job permissions for multiple users Use the job permissions API to update permissions for multiple users. Written by Atanu.Sarkar Last published at: May 17th, 2022 When you are running jobs, you might want to update user permissions for multiple users. You can do this by using the Databricks job permissions API (AWS | Azure | GCP) and a bit of Python code. Instructions Copy the example code into a notebook. Enter the <job-id> (or multiple job ids) into the array arr[]. Enter your payload{}. In this example, we are using the <username> and <permission> that we want to grant. Enter the <workspace-url> into the url field. Enter the <token> under Bearer. Run the notebook cell with the updated code. If the update is successful, the code returns a response of 200 (OK). Example code %python\r\n\r\nimport requests\r\nimport json\r\n\r\narr=[<job-id-1>,<job-id-2>]\r\nfor j in arr :\r\n  def requestcall():\r\n      payload = {\"access_control_list\": [{\"user_name\": \"<username>\",\"permission_level\": \"<permission>\"}]}\r\n      url='https://<workspace-url>/api/2.0/permissions/jobs/'+str(j)\r\n      myResponse = requests.patch(url=url, headers={'Authorization': 'Bearer <token>'}, verify=True, data=json.dumps(payload))\r\n      print(myResponse.status_code)\r\n      print(myResponse.content)\r\n        # If the API call is successful, the response code is 200 (OK).\r\n      if myResponse.ok:\r\n            # Extracting data in JSON format.\r\n       data = myResponse.json()\r\n       return data\r\n  requestcall() Was this article helpful? (7) (12) Additional Informations Related Articles Error: Received command c on object id p0 Problem You have imported Python libraries, but when you try to execute Python co... How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... Common errors in notebooks There are some common issues that occur when using notebooks. This section outlin... JSON reader parses values as null Problem You are attempting to read a JSON file. You know the file has data in it,... Related Articles Error: Received command c on object id p0 Problem You have imported Python libraries, but when you try to execute Python co... How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... Common errors in notebooks There are some common issues that occur when using notebooks. This section outlin... JSON reader parses values as null Problem You are attempting to read a JSON file. You know the file has data in it,... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Update job permissions for multiple users",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/update-job-perms-multiple-users"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/library-install-latency",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Library unavailability causing job failures Library unavailability causing job failures Learn how to resolve Databricks job failures caused by unavailable libraries. Written by Adam Pavlacka Last published at: May 11th, 2022 Problem You are launching jobs that import external libraries and get an Import Error. When a job causes a node to restart, the job fails with the following error message: ImportError: No module named XXX Cause The Cluster Manager is part of the Databricks service that manages customer Apache Spark clusters. It sends commands to install Python and R libraries when it restarts each node. Sometimes, library installation or downloading of artifacts from the internet can take more time than expected. This occurs due to network latency, or it occurs if the library that is being attached to the cluster has many dependent libraries. The library installation mechanism guarantees that when a notebook attaches to a cluster, it can import installed libraries. When library installation through PyPI takes excessive time, the notebook attaches to the cluster before the library installation completes. In this case, the notebook is unable to import the library. Solution Method 1 Use notebook-scoped library installation commands in the notebook. You can enter the following commands in one cell, which ensures that all of the specified libraries are installed. %sh\r\n\r\ndbutils.library.installPyPI(\"mlflow\")\r\ndbutils.library.restartPython() Method 2 AWS To avoid delay in downloading the libraries from the internet repositories, you can cache the libraries in DBFS or S3. For example, you can download the wheel or egg file for a Python library to a DBFS or S3 location. You can use the REST API or cluster-scoped init scripts to install libraries from DBFS or S3. First, download the wheel or egg file from the internet to the DBFS or S3 location. This can be performed in a notebook as follows: Delete Azure To avoid delay in downloading the libraries from the internet repositories, you can cache the libraries in DBFS or Azure Blob Storage. For example, you can download the wheel or egg file for a Python library to a DBFS or Azure Blob Storage location. You can use the REST API or cluster-scoped init scripts to install libraries from DBFS or Azure Blob Storage. First, download the wheel or egg file from the internet to the DBFS or Azure Blob Storage location. This can be performed in a notebook as follows: Delete %sh\r\n\r\ncd /dbfs/mnt/library\r\nwget <whl/egg-file-location-from-pypi-repository> After the wheel or egg file download completes, you can install the library to the cluster using the REST API, UI, or init script commands. Was this article helpful? (6) (14) Additional Informations Related Articles Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... Related Articles Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Library unavailability causing job failures",
          "view_href" : "https://kb.databricks.com/en_US/libraries/library-install-latency"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/xlsx-file-not-supported-xlrd",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Reading .xlsx files with xlrd fails Reading .xlsx files with xlrd fails xlrd no longer supports .xlsx files. Use openpyxl to read .xlsx files. Written by prakash.jha Last published at: May 12th, 2022 Problem You are have xlrd installed on your cluster and are attempting to read files in the Excel .xlsx format when you get an error. XLRDError: Excel xlsx file; not supported Cause xlrd 2.0.0 and above can only read .xls files. Support for .xlsx files was removed from xlrd due to a potential security vulnerability. Solution Use openpyxl to open .xlsx files instead of xlrd. Install the openpyxl library on your cluster (AWS | Azure | GCP). Confirm that you are using pandasversion 1.0.1 or above. %python\r\n\r\nimport pandas as pd\r\nprint(pd.__version__) Specify openpyxl when reading .xlsx files with pandas. %python\r\n\r\nimport pandas\r\ndf = pandas.read_excel(`<name-of-file>.xlsx`, engine=`openpyxl`) Refer to the openpyxl documentation for more information. Was this article helpful? (7) (13) Additional Informations Related Articles Install PyGraphViz PyGraphViz Python libraries are used to plot causal inference networks. If you tr... Libraries failing due to transient Maven issue Problem Job fails because libraries cannot be installed. Library resolution faile... Libraries fail with dependency exception Problem You have a Python function that is defined in a custom egg or wheel file ... Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... Related Articles Install PyGraphViz PyGraphViz Python libraries are used to plot causal inference networks. If you tr... Libraries failing due to transient Maven issue Problem Job fails because libraries cannot be installed. Library resolution faile... Libraries fail with dependency exception Problem You have a Python function that is defined in a custom egg or wheel file ... Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Reading .xlsx files with xlrd fails",
          "view_href" : "https://kb.databricks.com/en_US/libraries/xlsx-file-not-supported-xlrd"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/update-nested-column",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management How to update nested columns How to update nested columns Learn how to update nested columns in Databricks. Written by Adam Pavlacka Last published at: May 31st, 2022 Spark doesn’t support adding new columns or dropping existing columns in nested structures. In particular, the withColumn and drop methods of the Dataset class don’t allow you to specify a column name different from any top level columns. For example, suppose you have a dataset with the following schema: %scala\r\n\r\nval schema = (new StructType)\r\n      .add(\"metadata\",(new StructType)\r\n             .add(\"eventid\", \"string\", true)\r\n             .add(\"hostname\", \"string\", true)\r\n             .add(\"timestamp\", \"string\", true)\r\n           , true)\r\n      .add(\"items\", (new StructType)\r\n             .add(\"books\", (new StructType).add(\"fees\", \"double\", true), true)\r\n             .add(\"paper\", (new StructType).add(\"pages\", \"int\", true), true)\r\n           ,true)\r\nschema.treeString The schema looks like: root\r\n |-- metadata: struct (nullable = true)\r\n |    |-- eventid: string (nullable = true)\r\n |    |-- hostname: string (nullable = true)\r\n |    |-- timestamp: string (nullable = true)\r\n |-- items: struct (nullable = true)\r\n |    |-- books: struct (nullable = true)\r\n |    |    |-- fees: double (nullable = true)\r\n |    |-- paper: struct (nullable = true)\r\n |    |    |-- pages: integer (nullable = true) Suppose you have the DataFrame: %scala\r\n\r\nval rdd: RDD[Row] = sc.parallelize(Seq(Row(\r\n  Row(\"eventid1\", \"hostname1\", \"timestamp1\"),\r\n  Row(Row(100.0), Row(10)))))\r\nval df = spark.createDataFrame(rdd, schema)\r\ndisplay(df) You want to increase the fees column, which is nested under books, by 1%. To update the fees column, you can reconstruct the dataset from existing columns and the updated column as follows: %scala\r\n\r\nval updated = df.selectExpr(\"\"\"\r\n    named_struct(\r\n        'metadata', metadata,\r\n        'items', named_struct(\r\n          'books', named_struct('fees', items.books.fees * 1.01),\r\n          'paper', items.paper\r\n        )\r\n    ) as named_struct\r\n\"\"\").select($\"named_struct.metadata\", $\"named_struct.items\")\r\nupdated.show(false) Then you will get the result: +-----------------------------------+-----------------+\r\n| metadata                          | items           |\r\n+===================================+=================+\r\n| [eventid1, hostname1, timestamp1] | [[101.0], [10]] |\r\n+-----------------------------------+-----------------+ Was this article helpful? (8) (14) Additional Informations Related Articles Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... Behavior of the randomSplit method When using randomSplit on a DataFrame, you could potentially observe inconsistent... Revoke all user privileges When user permissions are explicitly granted for individual tables and views, the... Generate schema from case class Spark provides an easy way to generate a schema from a Scala case class. For case... Related Articles Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... Behavior of the randomSplit method When using randomSplit on a DataFrame, you could potentially observe inconsistent... Revoke all user privileges When user permissions are explicitly granted for individual tables and views, the... Generate schema from case class Spark provides an easy way to generate a schema from a Scala case class. For case... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to update nested columns",
          "view_href" : "https://kb.databricks.com/en_US/data/update-nested-column"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/display-file-timestamp-details",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Display file and directory timestamp details Display file and directory timestamp details Display file creation date and modification date using Python. Written by rakesh.parija Last published at: May 19th, 2022 In this article we show you how to display detailed timestamps, including the date and time when a file was created or modified. Use ls command The simplest way to display file timestamps is to use the ls -lt <path> command in a bash shell. For example, this sample command displays basic timestamps for files and directories in the /dbfs/ folder. %sh\r\n\r\nls -lt /dbfs/ Output: total 36\r\ndrwxrwxrwx 2 root root 4096 Jul  1 12:49 FileStore\r\ndrwxrwxrwx 2 root root 4096 Jul  1 12:49 databricks\r\ndrwxrwxrwx 2 root root 4096 Jul  1 12:49 databricks-datasets\r\ndrwxrwxrwx 2 root root 4096 Jul  1 12:49 databricks-results\r\ndrwxrwxrwx 2 root root 4096 Jul  1 12:49 ml\r\ndrwxrwxrwx 2 root root 4096 Jul  1 12:49 tmp\r\ndrwxrwxrwx 2 root root 4096 Jul  1 12:49 user\r\ndrwxrwxrwx 2 root root 4096 Jun  9  2020 dbfs\r\ndrwxrwxrwx 2 root root 4096 May 20  2020 local_disk0 Use Python commands to display creation date and modification date The ls command is an easy way to display basic information. If you want more detailed timestamps, you should use Python API calls. For example, this sample code uses datetime functions to display the creation date and modified date of all listed files and directories in the /dbfs/ folder. Replace /dbfs/ with the full path to the files you want to display. %python\r\n\r\nimport os\r\nfrom datetime import datetime\r\npath = '/dbfs/'\r\nfdpaths = [path+\"/\"+fd for fd in os.listdir(path)]\r\nprint(\" file_path \" + \" create_date \" + \" modified_date \")\r\nfor fdpath in fdpaths:\r\n  statinfo = os.stat(fdpath)\r\n  create_date = datetime.fromtimestamp(statinfo.st_ctime)\r\n  modified_date = datetime.fromtimestamp(statinfo.st_mtime)\r\n  print(fdpath, create_date, modified_date) Output:  file_path  create_date  modified_date\r\n/dbfs//FileStore 2021-07-01 12:49:45.264730 2021-07-01 12:49:45.264730\r\n/dbfs//databricks 2021-07-01 12:49:45.264730 2021-07-01 12:49:45.264730\r\n/dbfs//databricks-datasets 2021-07-01 12:49:45.264730 2021-07-01 12:49:45.264730\r\n/dbfs//databricks-results 2021-07-01 12:49:45.264730 2021-07-01 12:49:45.264730\r\n/dbfs//dbfs 2020-06-09 21:11:24 2020-06-09 21:11:24\r\n/dbfs//local_disk0 2020-05-20 22:32:05 2020-05-20 22:32:05\r\n/dbfs//ml 2021-07-01 12:49:45.264730 2021-07-01 12:49:45.264730\r\n/dbfs//tmp 2021-07-01 12:49:45.264730 2021-07-01 12:49:45.264730\r\n/dbfs//user 2021-07-01 12:49:45.264730 2021-07-01 12:49:45.264730 Was this article helpful? (11) (11) Additional Informations Related Articles Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... AttributeError: ‘function’ object has no attribute Problem You are selecting columns from a DataFrame and you get an error message. ... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... Related Articles Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... AttributeError: ‘function’ object has no attribute Problem You are selecting columns from a DataFrame and you get an error message. ... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Display file and directory timestamp details",
          "view_href" : "https://kb.databricks.com/en_US/python/display-file-timestamp-details"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/date-int-only-spark-30",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark Date functions only accept int values in Apache Spark 3.0 Date functions only accept int values in Apache Spark 3.0 Date functions only accept int values in Apache Spark 3.0; fractional and string values return AnalysisException error. Written by Adam Pavlacka Last published at: May 23rd, 2022 Problem You are attempting to use the date_add() or date_sub() functions in Spark 3.0, but they are returning an Error in SQL statement: AnalysisException error message. In Spark 2.4 and below, both functions work as normal. %sql\r\n\r\nselect date_add(cast('1964-05-23' as date), '12.34') Cause You are attempting to use a fractional or string value as the second argument. In Spark 2.4 and below, if the second argument is a fractional or string value, it is coerced to an int value before date_add() or date_sub() is evaluated. Using the example code listed above, the value 12.34 is converted to 12 before date_add() is evaluated. In Spark 3.0, if the second argument is a fractional or string value, it returns an error. Solution Use int, smallint, or tinyint values as the second argument for the date_add() or date_sub() functions in Spark 3.0. %sql\r\n\r\nselect date_add(cast('1964-05-23' as date), '12')\r\nSQL\r\nCopy to clipboardCopy\r\nselect date_add(cast('1964-05-23' as date), 12) Both of these examples work properly in Spark 3.0. Delete Info If you are importing this data from another source, you should create a routine to sanitize the values and ensure the data is in integer form before passing it to one of the date functions. Was this article helpful? (10) (14) Additional Informations Related Articles Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Related Articles Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Date functions only accept int values in Apache Spark 3.0",
          "view_href" : "https://kb.databricks.com/en_US/sql/date-int-only-spark-30"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/random-split-behavior",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management Behavior of the randomSplit method Behavior of the randomSplit method Learn about inconsistent behaviors when using the randomSplit method in Databricks. Written by Adam Pavlacka Last published at: May 31st, 2022 When using randomSplit on a DataFrame, you could potentially observe inconsistent behavior. Here is an example: %python\r\n\r\ndf = spark.read.format('inconsistent_data_source').load()\r\na,b = df.randomSplit([0.5, 0.5])\r\na.join(broadcast(b), on='id', how='inner').count() Typically this query returns 0. However, depending on the underlying data source or input DataFrame, in some cases the query could result in more than 0 records. This unexpected behavior is explained by the fact that data distribution across RDD partitions is not idempotent, and could be rearranged or updated during the query execution, thus affecting the output of the randomSplit method. Delete Info Spark DataFrames and RDDs preserve partitioning order; this problem only exists when query output depends on the actual data distribution across partitions, for example, values from files 1, 2 and 3 always appear in partition 1. The issue could also be observed when using Delta cache (AWS | Azure | GCP). All solutions listed below are still applicable in this case. Solution Do one of the following: Use explicit Apache Spark RDD caching %python\r\n\r\ndf = inputDF.cache()\r\na,b = df.randomSplit([0.5, 0.5]) Repartition by a column or a set of columns %python\r\n\r\ndf = inputDF.repartition(100, 'col1')\r\na,b = df.randomSplit([0.5, 0.5]) Apply an aggregate function %python\r\n\r\ndf = inputDF.groupBy('col1').count()\r\na,b = df.randomSplit([0.5, 0.5]) These operations persist or shuffle data resulting in the consistent data distribution across partitions in Spark jobs. Was this article helpful? (7) (14) Additional Informations Related Articles How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... How to handle corrupted Parquet files with different schema Problem Let’s say you have a large list of essentially independent Parquet files,... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... Revoke all user privileges When user permissions are explicitly granted for individual tables and views, the... Related Articles How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... How to handle corrupted Parquet files with different schema Problem Let’s say you have a large list of essentially independent Parquet files,... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... Revoke all user privileges When user permissions are explicitly granted for individual tables and views, the... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Behavior of the randomSplit method",
          "view_href" : "https://kb.databricks.com/en_US/data/random-split-behavior"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/skew-hints-in-join",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management How to specify skew hints in dataset and DataFrame-based join commands How to specify skew hints in dataset and DataFrame-based join commands Learn how to specify skew hints in Dataset and DataFrame-based join commands in Databricks. Written by Adam Pavlacka Last published at: May 31st, 2022 When you perform a join command with DataFrame or Dataset objects, if you find that the query is stuck on finishing a small number of tasks due to data skew, you can specify the skew hint with the hint(\"skew\") method: df.hint(\"skew\"). The skew join optimization (AWS | Azure | GCP) is performed on the DataFrame for which you specify the skew hint. In addition to the basic hint, you can specify the hint method with the following combinations of parameters: column name, list of column names, and column name and skew value. DataFrame and column name. The skew join optimization is performed on the specified column of the DataFrame. %python\r\n\r\ndf.hint(\"skew\", \"col1\") DataFrame and multiple columns. The skew join optimization is performed for multiple columns in the DataFrame. %python\r\n\r\ndf.hint(\"skew\", [\"col1\",\"col2\"]) DataFrame, column name, and skew value. The skew join optimization is performed on the data in the column with the skew value. %python\r\n\r\ndf.hint(\"skew\", \"col1\", \"value\") Example This example shows how to specify the skew hint for multiple DataFrame objects involved in a join operation: %scala\r\n\r\nval joinResults = ds1.hint(\"skew\").as(\"L\").join(ds2.hint(\"skew\").as(\"R\"), $\"L.col1\" === $\"R.col1\") Was this article helpful? (8) (13) Additional Informations Related Articles Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... Behavior of the randomSplit method When using randomSplit on a DataFrame, you could potentially observe inconsistent... Hive UDFs This article shows how to create a Hive UDF, register it in Spark, and use it in ... Revoke all user privileges When user permissions are explicitly granted for individual tables and views, the... Related Articles Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... Behavior of the randomSplit method When using randomSplit on a DataFrame, you could potentially observe inconsistent... Hive UDFs This article shows how to create a Hive UDF, register it in Spark, and use it in ... Revoke all user privileges When user permissions are explicitly granted for individual tables and views, the... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to specify skew hints in dataset and DataFrame-based join commands",
          "view_href" : "https://kb.databricks.com/en_US/data/skew-hints-in-join"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/mlflow-artifacts-legacy-storage",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning Experiment warning when legacy artifact storage location is used Experiment warning when legacy artifact storage location is used Resolve experiment warnings when a legacy artifact storage location is used instead of the MLflow managed location. Written by Adam Pavlacka Last published at: May 16th, 2022 Problem A new icon appears on the MLflow Experiments page with the following open access warning: Cause MLflow experiment permissions (AWS | Azure | GCP) are enforced on artifacts in MLflow Tracking, enabling you to easily control access to datasets, models, and other files. In MLflow 1.11 and above, new experiments store artifacts in an MLflow-managed location (dbfs:/databricks/mlflow-tracking/) that enforces experiment access controls. Certain older experiments use a legacy storage location (dbfs:/databricks/mlflow/) that can be accessed by all users of your workspace. This warning indicates that your experiment uses a legacy artifact storage location. Solution You should always use the MLflow-managed DBFS storage locations when logging artifacts to experiments. This protects against unintended or unauthorized access to your MLflow artifacts. Was this article helpful? (10) (13) Additional Informations Related Articles MLflow project fails to access an Apache Hive table Problem You have an MLflow project that fails to access a Hive table and returns ... How to extract feature information for tree-based Apache SparkML pipeline models When you are fitting a tree-based model, such as a decision tree, random forest, ... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... Incorrect results when using documents as inputs Problem You have a ML model that takes documents as inputs, specifically, an arra... Related Articles MLflow project fails to access an Apache Hive table Problem You have an MLflow project that fails to access a Hive table and returns ... How to extract feature information for tree-based Apache SparkML pipeline models When you are fitting a tree-based model, such as a decision tree, random forest, ... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... Incorrect results when using documents as inputs Problem You have a ML model that takes documents as inputs, specifically, an arra... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Experiment warning when legacy artifact storage location is used",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/mlflow-artifacts-legacy-storage"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/mlflow-artifacts-no-client-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning Errors when accessing MLflow artifacts without using the MLflow client Errors when accessing MLflow artifacts without using the MLflow client Resolve errors when attempting to access MLflow artifacts without using the MLflow client Written by Adam Pavlacka Last published at: May 16th, 2022 MLflow experiment permissions (AWS | Azure) are now enforced on artifacts in MLflow Tracking, enabling you to easily control access to your datasets, models, and other files. Invalid mount exception Problem When trying to access an MLflow run artifact using Databricks File System (DBFS) commands, such as dbutils.fs, you get the following error: com.databricks.backend.daemon.data.common.InvalidMountException: Error while using path /databricks/mlflow-tracking/<experiment-id>/<run-id>/artifacts for resolving path &#39;/<experiment-id>/<run-id>/artifacts&#39; within mount at &#39;/databricks/mlflow-tracking&#39;. Cause With the extension of MLflow experiment permissions to artifacts, DBFS access APIs for run artifacts stored in dbfs:/databricks/mlflow-tracking/ are no longer supported. Solution Upgrade to MLflow client version 1.9.1 or above to download, list, or upload artifacts stored in dbfs:/databricks/mlflow-tracking/. %sh\r\n\r\npip install --upgrade mlflow FileNotFoundError Problem When trying to access an MLflow run artifact using %sh/os.listdir(), you get the following error: FileNotFoundError: [Errno 2] No such file or directory: '/databricks/mlflow-tracking/' Cause With the extension of MLflow experiment permissions to artifacts, run artifacts stored in dbfs:/databricks/mlflow-tracking/ can only be accessed using MLflow client version 1.9.1 or above. Solution Upgrade to MLflow client version 1.9.1 or above to download, list, or upload artifacts stored in dbfs:/databricks/mlflow-tracking/. %sh\r\n\r\npip install --upgrade mlflow Was this article helpful? (12) (15) Additional Informations Related Articles Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... How to speed up cross-validation Hyperparameter tuning of Apache SparkML models takes a very long time, depending ... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... How to perform group K-fold cross validation with Apache Spark Cross validation randomly splits the training data into a specified number of fol... Related Articles Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... How to speed up cross-validation Hyperparameter tuning of Apache SparkML models takes a very long time, depending ... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... How to perform group K-fold cross validation with Apache Spark Cross validation randomly splits the training data into a specified number of fol... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Errors when accessing MLflow artifacts without using the MLflow client",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/mlflow-artifacts-no-client-error"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/delta-cache-autoscaling",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake How Delta cache behaves on an autoscaling cluster How Delta cache behaves on an autoscaling cluster Learn how Delta cache behaves on an autoscaling cluster. Written by Adam Pavlacka Last published at: May 10th, 2022 This article is about how Delta cache (AWS | Azure | GCP) behaves on an auto-scaling cluster, which removes or adds nodes as needed. When a cluster downscales and terminates nodes: A Delta cache behaves in the same way as an RDD cache. Whenever a node goes down, all of the cached data in that particular node is lost. Delta cache data is not moved from the lost node. When a cluster upscales and adds new nodes: Whenever a cluster adds a new node, data is not moved between caches. Lost data is re-cached the next time an application accesses the data or tables again. Was this article helpful? (10) (14) Additional Informations Related Articles Delete your streaming query checkpoint and restart Problem Your job fails with a Delta table <value> doesn't exist. Please del... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... Converting from Parquet to Delta Lake fails Problem You are attempting to convert a Parquet file to a Delta Lake file. The di... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... Related Articles Delete your streaming query checkpoint and restart Problem Your job fails with a Delta table <value> doesn't exist. Please del... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... Converting from Parquet to Delta Lake fails Problem You are attempting to convert a Parquet file to a Delta Lake file. The di... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How Delta cache behaves on an autoscaling cluster",
          "view_href" : "https://kb.databricks.com/en_US/delta/delta-cache-autoscaling"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/checkpoint-no-cleanup-display",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming Checkpoint files not being deleted when using display() Checkpoint files not being deleted when using display() Learn how to prevent display(streamingDF) checkpoint files from using a large amount of storage. Written by Adam Pavlacka Last published at: May 19th, 2022 Problem You have a streaming job using display() to display DataFrames. %scala\r\n\r\nval streamingDF = spark.readStream.schema(schema).parquet(<input_path>)\r\ndisplay(streamingDF) Checkpoint files are being created, but are not being deleted. You can verify the problem by navigating to the root directory and looking in the /local_disk0/tmp/ folder. Checkpoint files remain in the folder. Cause The command display(streamingDF) is a memory sink implementation that can display the data from the streaming DataFrame for every micro-batch. A checkpoint directory is required to track the streaming updates. If you have not specified a custom checkpoint location, a default checkpoint directory is created at /local_disk0/tmp/. Databricks uses the checkpoint directory to ensure correct and consistent progress information. When a stream is shut down, either purposely or accidentally, the checkpoint directory allows Databricks to restart and pick up exactly where it left off. If a stream is shut down by cancelling the stream from the notebook, the Databricks job attempts to clean up the checkpoint directory on a best-effort basis. If the stream is terminated in any other way, or if the job is terminated, the checkpoint directory is not cleaned up. This is as designed. Solution You can prevent unwanted checkpoint files with the following guidelines. You should not use display(streamingDF) in production jobs. If display(streamingDF) is mandatory for your use case, you should manually specify the checkpoint directory by using the Apache Spark config option spark.sql.streaming.checkpointLocation. If you manually specify the checkpoint directory, you should periodically delete any remaining files in this directory. This can be done on a weekly basis. Was this article helpful? (8) (13) Additional Informations Related Articles Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... Handling partition column values while using an SQS queue as a streaming source Problem If data in S3 is stored by partition, the partition column values are use... How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Kafka error: No resolvable bootstrap urls Problem You are trying to read or write data to a Kafka stream when you get an er... Related Articles Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... Handling partition column values while using an SQS queue as a streaming source Problem If data in S3 is stored by partition, the partition column values are use... How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Kafka error: No resolvable bootstrap urls Problem You are trying to read or write data to a Kafka stream when you get an er... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Checkpoint files not being deleted when using display()",
          "view_href" : "https://kb.databricks.com/en_US/streaming/checkpoint-no-cleanup-display"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/sqs-stream-partition-values",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming Handling partition column values while using an SQS queue as a streaming source Handling partition column values while using an SQS queue as a streaming source Written by Adam Pavlacka Last published at: May 18th, 2022 Problem If data in S3 is stored by partition, the partition column values are used to name folders in the source directory structure. However, if you use an SQS queue as a streaming source, the S3-SQS source cannot detect the partition column values. For example, if you save the following DataFrame to S3 in JSON format: %scala\r\n\r\nval df = spark.range(10).withColumn(\"date\",current_date())\r\ndf.write.partitionBy(\"date\").json(\"s3a://bucket-name/json\") The file structure underneath will be: %scala\r\n\r\ns3a://bucket-name/json/_SUCCESS\r\ns3a://bucket-name/json/date=2018-10-25/<individual JSON files> Let’s say you have an S3-SQS input stream created from the queue configured for this S3 bucket. If you directly load the data from this S3-SQS input stream using the following code: %scala\r\n\r\nimport org.apache.spark.sql.types._\r\nval schema = StructType(List(StructField(\"id\", IntegerType, false),StructField(\"date\", DateType, false)))\r\n\r\ndisplay(spark.readStream\r\n          .format(\"s3-sqs\")\r\n          .option(\"fileFormat\", \"json\")\r\n          .option(\"queueUrl\", \"https://sqs.us-east-1.amazonaws.com/826763667205/sqs-queue\")\r\n          .option(\"sqsFetchInterval\", \"1m\")\r\n          .option(\"ignoreFileDeletion\", true)\r\n          .schema(schema)\r\n          .load()) The output will be: You can see the date column values are not populated correctly. Solution You can use a combination of input_file_name() and regexp_extract() UDFs to extract the date values properly, as in the following code snippet: %scala\r\n\r\nimport org.apache.spark.sql.functions._\r\nval df = spark.readStream\r\n          .format(\"s3-sqs\")\r\n          .option(\"fileFormat\", \"json\")\r\n          .option(\"queueUrl\", \"https://sqs.us-east-1.amazonaws.com/826763667205/sqs-queue\")\r\n          .option(\"sqsFetchInterval\", fetch_interval)\r\n          .option(\"ignoreFileDeletion\", true)\r\n          .schema(schema)\r\n          .load()\r\ndisplay(df.withColumn(\"date\",regexp_extract(input_file_name(), \"/date=(\\\\d{4}-\\\\d{2}-\\\\d{2})/\", 1))) Now you can see the correct values for the date column in the following output: Was this article helpful? (6) (13) Additional Informations Related Articles How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Get the path of files consumed by Auto Loader When you process streaming files with Auto Loader (AWS | Azure | GCP), events are... Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... Related Articles How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Get the path of files consumed by Auto Loader When you process streaming files with Auto Loader (AWS | Azure | GCP), events are... Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Handling partition column values while using an SQS queue as a streaming source",
          "view_href" : "https://kb.databricks.com/en_US/streaming/sqs-stream-partition-values"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dbsql/display-null-as-nan",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Databricks SQL Null column values display as NaN Null column values display as NaN Null column values correctly display as NaN in Databricks SQL. Written by Adam Pavlacka Last published at: March 4th, 2022 Problem You have a table with null values in some columns. When you query the table using a select statement in Databricks, the null values appear as null. When you query the table using the same select statement in Databricks SQL, the null values appear as NaN. %sql\r\n\r\nselect * from default.<table-name> where <column-name> is null Databricks Databricks SQL Cause NaN is short for not a number. This is how null values are displayed in Databricks SQL. Solution This is not a problem. Databricks SQL is working as designed. The representation of null values in Databricks SQL is different from the representation of null values in Databricks, but the data itself is not changed. Was this article helpful? (12) (21) Additional Informations Related Articles Retrieve queries owned by a disabled user When a Databricks SQL user is removed from an organization, the queries owned by ... Related Articles Retrieve queries owned by a disabled user When a Databricks SQL user is removed from an organization, the queries owned by ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Null column values display as NaN",
          "view_href" : "https://kb.databricks.com/en_US/dbsql/display-null-as-nan"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/redshift-fails-decimal-write",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management Job fails when using Spark-Avro to write decimal values to AWS Redshift Job fails when using Spark-Avro to write decimal values to AWS Redshift Learn how to resolve job failures when writing decimal values to AWS Redshift with Spark-Avro. Written by Adam Pavlacka Last published at: May 31st, 2022 Problem In Databricks Runtime versions 5.x and above, when writing decimals to Amazon Redshift using Spark-Avro as the default temp file format, either the write operation fails with the exception: Error (code 1207) while loading data into Redshift: \"Invalid digit, Value '\"', Pos 0, Type: Decimal\" or the write operation writes nulls in place of the decimal values. Cause When writing to Redshift, data is first stored in a temp folder in S3 before being loaded into Redshift. The default format used for storing temp data between Apache Spark and Redshift is Spark-Avro. However, Spark-Avro stores a decimal as a binary, which is interpreted by Redshift as empty strings or nulls. Solution Change the temp file format to CSV using the tempformat option. You can use this sample Scala code: %scala\r\n\r\n//Create sample data\r\ncase class createDec(value: BigDecimal)\r\nval df = Seq(createDec(45.24)).toDS\r\n//Write to Redshift\r\n(df.write\r\n  .format(\"com.databricks.spark.redshift\")\r\n  .option(\"url\", jdbcUrl)\r\n  .option(\"tempdir\", tempDir)\r\n  .option(\"dbtable\", \"testtable\")\r\n  .option(\"aws_iam_role\", \"your_aws_iam_role\")\r\n  .option(\"tempformat\", \"CSV\")\r\n  .mode(\"append\")\r\n  .save()) Was this article helpful? (8) (13) Additional Informations Related Articles How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... Get and set Apache Spark configuration properties in a notebook In most cases, you set the Spark config (AWS | Azure ) at the cluster level. Howe... Behavior of the randomSplit method When using randomSplit on a DataFrame, you could potentially observe inconsistent... Related Articles How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... Get and set Apache Spark configuration properties in a notebook In most cases, you set the Spark config (AWS | Azure ) at the cluster level. Howe... Behavior of the randomSplit method When using randomSplit on a DataFrame, you could potentially observe inconsistent... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Job fails when using Spark-Avro to write decimal values to AWS Redshift",
          "view_href" : "https://kb.databricks.com/en_US/data/redshift-fails-decimal-write"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dbsql/retrieve-queries-disabled-user",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Databricks SQL Retrieve queries owned by a disabled user Retrieve queries owned by a disabled user How to retrieve queries owned by a disabled user in Databricks SQL. Written by John.Lourdu Last published at: March 4th, 2022 When a Databricks SQL user is removed from an organization, the queries owned by the user remain, but they are only visible to those who already have permission to access them. A Databricks SQL admin can transfer ownership to other users, as well as delete alerts, dashboards, and queries owned by the disabled user account. Clone a query A Databricks SQL admin has view access to all queries by default. As an admin, you can view and delete any query. You cannot edit a query, if it is not shared with you. This includes admin users. The solution is to clone a query, and then edit the permissions. Open Databricks SQL. Click Queries. Click Admin View. Select the query you want to clone. Click the vertical ellipsis and select Clone. You can now edit the copy of the original query as needed. Delete a query Open Databricks SQL. Click Queries. Click Admin View. Select the query you want to delete. Click the vertical ellipsis and select Move to Trash. Click Move to Trash to confirm. Delete Warning When you delete a query, all alerts and dashboard widgets created with its visualizations are also deleted. Was this article helpful? (9) (13) Additional Informations Related Articles Null column values display as NaN Problem You have a table with null values in some columns. When you query the tab... Related Articles Null column values display as NaN Problem You have a table with null values in some columns. When you query the tab... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Retrieve queries owned by a disabled user",
          "view_href" : "https://kb.databricks.com/en_US/dbsql/retrieve-queries-disabled-user"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/knn-model-pyfunc-modulenotfounderror",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning KNN model using pyfunc returns ModuleNotFoundError or FileNotFoundError KNN model using pyfunc returns ModuleNotFoundError or FileNotFoundError Predictions using pyfunc on a KNN model returns a ModuleNotFoundError or FileNotFoundError. Written by pradeepkumar.palaniswamy Last published at: May 16th, 2022 Problem You have created a Sklearn model using KNeighborsClassifier and are using pyfunc to run a prediction. For example: %python\r\n\r\nimport mlflow.pyfunc\r\npyfunc_udf = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri, result_type='string')\r\npredicted_df = merge.withColumn(\"prediction\", pyfunc_udf(*merge.columns[1:]))\r\npredicted_df.collect() The prediction returns a ModuleNotFoundError: No module named 'sklearn.neighbors._classification' error message. The prediction may also return a FileNotFoundError: [Errno 2] No usable temporary directory found error message. Cause When a KNN model is logged, all of the data points used for training are saved as part of the pickle file. If the model is trained with millions of records, all of that data is added to the model, which can dramatically increase its size. A model trained on millions of records can easily total multiple GBs. pyfunc attempts to load the entire model into the executor’s cache when running a prediction. If the model is too big to fit into memory, it results in one of the above error messages. Solution You should use a tree-based algorithm, such as Random Forest or XGBoost to downsample the data in a KNN model. If you have unbalanced data, attempt a sampling method like SMOTE, when training a tree-based algorithm. Was this article helpful? (6) (14) Additional Informations Related Articles Fitting an Apache SparkML model throws error Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.... Experiment warning when custom artifact storage location is used Problem When you create an MLflow experiment with a custom artifact location, you... Experiment warning when legacy artifact storage location is used Problem A new icon appears on the MLflow Experiments page with the following open... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... Related Articles Fitting an Apache SparkML model throws error Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.... Experiment warning when custom artifact storage location is used Problem When you create an MLflow experiment with a custom artifact location, you... Experiment warning when legacy artifact storage location is used Problem A new icon appears on the MLflow Experiments page with the following open... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "KNN model using pyfunc returns ModuleNotFoundError or FileNotFoundError",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/knn-model-pyfunc-modulenotfounderror"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/h2o-cluster-not-reachable-exception",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning H2O.ai Sparkling Water cluster not reachable H2O.ai Sparkling Water cluster not reachable H2O.ai Sparkling Water cluster not reachable if the version of the Sparkling Water package does not match the version of Spark used on your cluster. Written by shanmugavel.chandrakasu Last published at: May 16th, 2022 Problem You are trying to initialize H2O.ai’s Sparkling Water on Databricks Runtime 7.0 and above when you get a H2OClusterNotReachableException error message. %python\r\n\r\nimport ai.h2o.sparkling._\r\nval h2oContext = H2OContext.getOrCreate() ai.h2o.sparkling.backend.exceptions.H2OClusterNotReachableException: H2O cluster X.X.X.X:54321 - sparkling-water-root_app-20210720231748-0000 is not reachable. Cause This error occurs when you are trying to use a version of the Sparkling Water package which is not compatible with the version of Apache Spark used on your Databricks cluster. Solution Make sure you are downloading the correct version of Sparkling Water from the Sparkling Water download page. By default, the download page provides the latest version of Sparkling Water. If you are still having trouble, you may want to try rolling back to a prior version of Sparkling Water that is compatible with your Spark version. If you are still having trouble configuring Sparkling Water, open a case with H20.ai support. Was this article helpful? (5) (15) Additional Informations Related Articles Fitting an Apache SparkML model throws error Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.... Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... How to extract feature information for tree-based Apache SparkML pipeline models When you are fitting a tree-based model, such as a decision tree, random forest, ... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... Related Articles Fitting an Apache SparkML model throws error Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.... Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... How to extract feature information for tree-based Apache SparkML pipeline models When you are fitting a tree-based model, such as a decision tree, random forest, ... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "H2O.ai Sparkling Water cluster not reachable",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/h2o-cluster-not-reachable-exception"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/metastore/drop-database-no-delete",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Metastore Drop database without deletion Drop database without deletion Use Hive commands to drop a database without deleting the underlying storage folder. Written by arvind.ravish Last published at: May 24th, 2022 By default, the DROP DATABASE (AWS | Azure | GCP) command drops the database and deletes the directory associated with the database from the file system. Sometimes you may want to drop the database, but keep the underlying database directory intact. Example code You can use this example code to drop the database without dropping the underlying storage folder. %scala\r\n\r\nimport scala.collection.JavaConverters._\r\nimport org.apache.hadoop.hive.ql.metadata.Hive\r\nimport org.apache.hadoop.hive.conf.HiveConf\r\nimport org.apache.hadoop.hive.ql.session.SessionState\r\n\r\nval hiveConf = new HiveConf(classOf[SessionState])\r\nsc.hadoopConfiguration.iterator().asScala.foreach { kv =>\r\nhiveConf.set(kv.getKey, kv.getValue)\r\n}\r\nsc.getConf.getAll.foreach {\r\ncase (k, v) => hiveConf.set(k, v)\r\n}\r\n\r\nhiveConf.setBoolean(\"hive.cbo.enable\", false)\r\nval state = new SessionState(hiveConf)\r\nval hive = Hive.get(state.getConf)\r\nprintln(state.getConf)\r\n\r\nhive.dropDatabase(\"<database-name>\", false, false, true) For more information on org.apache.hadoop.hive.ql.metadata.Hive, please review the Hive documentation. Was this article helpful? (10) (10) Additional Informations Related Articles Autoscaling is slow with an external metastore Problem You have an external metastore configured on your cluster and autoscaling... Data too long for column error Problem You are trying to insert a struct into a table, but you get a java.sql.SQ... Parquet timestamp requires Hive metastore 1.2 or above Problem You are trying to create a Parquet table using TIMESTAMP, but you get an ... Error in CREATE TABLE with external Hive metastore Problem You are connecting to an external MySQL metastore and attempting to creat... Related Articles Autoscaling is slow with an external metastore Problem You have an external metastore configured on your cluster and autoscaling... Data too long for column error Problem You are trying to insert a struct into a table, but you get a java.sql.SQ... Parquet timestamp requires Hive metastore 1.2 or above Problem You are trying to create a Parquet table using TIMESTAMP, but you get an ... Error in CREATE TABLE with external Hive metastore Problem You are connecting to an external MySQL metastore and attempting to creat... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Drop database without deletion",
          "view_href" : "https://kb.databricks.com/en_US/metastore/drop-database-no-delete"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/flatten-nested-columns-dynamically",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Convert nested JSON to a flattened DataFrame Convert nested JSON to a flattened DataFrame How to convert a flattened DataFrame to nested JSON using a nested case class. Written by Adam Pavlacka Last published at: May 20th, 2022 This article shows you how to flatten nested JSON, using only $\"column.*\" and explode methods. Sample JSON file Pass the sample JSON string to the reader. %scala\r\n\r\nval json =\"\"\"\r\n{\r\n        \"id\": \"0001\",\r\n        \"type\": \"donut\",\r\n        \"name\": \"Cake\",\r\n        \"ppu\": 0.55,\r\n        \"batters\":\r\n                {\r\n                        \"batter\":\r\n                                [\r\n                                        { \"id\": \"1001\", \"type\": \"Regular\" },\r\n                                        { \"id\": \"1002\", \"type\": \"Chocolate\" },\r\n                                        { \"id\": \"1003\", \"type\": \"Blueberry\" },\r\n                                        { \"id\": \"1004\", \"type\": \"Devil's Food\" }\r\n                                ]\r\n                },\r\n        \"topping\":\r\n                [\r\n                        { \"id\": \"5001\", \"type\": \"None\" },\r\n                        { \"id\": \"5002\", \"type\": \"Glazed\" },\r\n                        { \"id\": \"5005\", \"type\": \"Sugar\" },\r\n                        { \"id\": \"5007\", \"type\": \"Powdered Sugar\" },\r\n                        { \"id\": \"5006\", \"type\": \"Chocolate with Sprinkles\" },\r\n                        { \"id\": \"5003\", \"type\": \"Chocolate\" },\r\n                        { \"id\": \"5004\", \"type\": \"Maple\" }\r\n                ]\r\n}\r\n\"\"\" Convert to DataFrame Add the JSON string as a collection type and pass it as an input to spark.createDataset. This converts it to a DataFrame. The JSON reader infers the schema automatically from the JSON string. This sample code uses a list collection type, which is represented as json :: Nil. You can also use other Scala collection types, such as Seq (Scala Sequence). %scala\r\n\r\nimport org.apache.spark.sql.functions._\r\nimport spark.implicits._\r\nval DF= spark.read.json(spark.createDataset(json :: Nil)) Extract and flatten Use $\"column.*\" and explode methods to flatten the struct and array types before displaying the flattened DataFrame. %scala\r\n\r\ndisplay(DF.select($\"id\" as \"main_id\",$\"name\",$\"batters\",$\"ppu\",explode($\"topping\")) // Exploding the topping column using explode as it is an array type\r\n        .withColumn(\"topping_id\",$\"col.id\") // Extracting topping_id from col using DOT form\r\n        .withColumn(\"topping_type\",$\"col.type\") // Extracting topping_tytpe from col using DOT form\r\n        .drop($\"col\")\r\n        .select($\"*\",$\"batters.*\") // Flattened the struct type batters tto array type which is batter\r\n        .drop($\"batters\")\r\n        .select($\"*\",explode($\"batter\"))\r\n        .drop($\"batter\")\r\n        .withColumn(\"batter_id\",$\"col.id\") // Extracting batter_id from col using DOT form\r\n        .withColumn(\"battter_type\",$\"col.type\") // Extracting battter_type from col using DOT form\r\n        .drop($\"col\")\r\n       ) Delete Warning Make sure to use $ for all column names, otherwise you may get an error message: overloaded method value select with alternatives. Example notebook Run the Nested JSON to DataFrame example notebook to view the sample code and results. Was this article helpful? (7) (15) Additional Informations Related Articles Best practice for cache(), count(), and take() cache() is an Apache Spark transformation that can be used on a DataFrame, Datase... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Related Articles Best practice for cache(), count(), and take() cache() is an Apache Spark transformation that can be used on a DataFrame, Datase... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Convert nested JSON to a flattened DataFrame",
          "view_href" : "https://kb.databricks.com/en_US/scala/flatten-nested-columns-dynamically"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/rocksdb-fails-to-acquire-a-lock",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming RocksDB fails to acquire a lock RocksDB fails to acquire a lock When using RocksDB as a state store, you may need to increase the acquire timeout in the SQL config. Written by Adam Pavlacka Last published at: May 19th, 2022 Problem You are trying to use RocksDB as a state store for your structured streaming application, when you get an error message saying that the instance could not be acquired. Caused by: java.lang.IllegalStateException: RocksDB instance could not be acquired by [ThreadId: 742, task: 140.3 in stage 3152, TID 553193] as it was not released by [ThreadId: 42, task: 140.1 in stage 3152, TID 553083] after 10009 ms\r\nStateStoreId(opId=0,partId=140,name=default) Cause Two concurrent tasks cannot modify the same RocksDBStateStore instance. Concurrent tasks attempting to access the same state store (the state store tied to the same partition of state maintained by flatMapGroupsWithState) should be extremely rare. It can only happen if the task updating the store instance was restarted by the driver before the previous attempt had terminated. Delete Info Abrupt node termination, like when a spot instance terminates, can also cause this error. Solution This error prevents the state from being corrupted. Restart the query if you encounter this error. If zombie tasks are taking too long to clean up their resources, when the next task tries to acquire a lock, it will also fail. In this case, you should allow more time for the thread to clean up. Set the wait time for the thread by configuring rocksdb.lockAcquireTimeoutMs in your SQL configuration. The value is in milliseconds. %scala\r\nspark.sql(\"set spark.sql.streaming.stateStore.rocksdb.lockAcquireTimeoutMs = 20000\") Was this article helpful? (6) (12) Additional Informations Related Articles Handling partition column values while using an SQS queue as a streaming source Problem If data in S3 is stored by partition, the partition column values are use... How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... How to switch a SNS streaming job to a new SQS queue Problem You have a Structured Streaming job running via the S3-SQS connector. Sup... Related Articles Handling partition column values while using an SQS queue as a streaming source Problem If data in S3 is stored by partition, the partition column values are use... How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... How to switch a SNS streaming job to a new SQS queue Problem You have a Structured Streaming job running via the S3-SQS connector. Sup... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "RocksDB fails to acquire a lock",
          "view_href" : "https://kb.databricks.com/en_US/streaming/rocksdb-fails-to-acquire-a-lock"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/apache-spark-jobs-fail-with-environment-directory-not-found-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Apache Spark jobs fail with Environment directory not found error Apache Spark jobs fail with Environment directory not found error Spark jobs appear to time out after you install a library because security rules are preventing workers from resolving the Python executable path. Written by Adam Pavlacka Last published at: July 1st, 2022 Problem After you install a Python library (via the cluster UI or by using pip), your Apache Spark jobs fail with an Environment directory not found error message. org.apache.spark.SparkException: Environment directory not found at\r\n/local_disk0/.ephemeral_nfs/cluster_libraries/python Cause Libraries are installed on a Network File System (NFS) on the cluster's driver node. If any security group rules prevent the workers from communicating with the NFS server, Spark commands cannot resolve the Python executable path. Solution You should make sure that your security groups are configured with appropriate security rules (AWS | Azure | GCP). Was this article helpful? (2) (21) Additional Informations Related Articles New job fails when adding a library from DBFS or S3 Problem You create a new job and attempt to add a library from DBFS or S3 storage... Libraries fail with dependency exception Problem You have a Python function that is defined in a custom egg or wheel file ... Install Turbodbc via init script Turbodbc is a Python module that uses the ODBC interface to access relational dat... Install PyGraphViz PyGraphViz Python libraries are used to plot causal inference networks. If you tr... Related Articles New job fails when adding a library from DBFS or S3 Problem You create a new job and attempt to add a library from DBFS or S3 storage... Libraries fail with dependency exception Problem You have a Python function that is defined in a custom egg or wheel file ... Install Turbodbc via init script Turbodbc is a Python module that uses the ODBC interface to access relational dat... Install PyGraphViz PyGraphViz Python libraries are used to plot causal inference networks. If you tr... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Apache Spark jobs fail with Environment directory not found error",
          "view_href" : "https://kb.databricks.com/en_US/libraries/apache-spark-jobs-fail-with-environment-directory-not-found-error"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dbfs/errno95-operation-not-supported",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Databricks File System (DBFS) Operation not supported during append Operation not supported during append Written by Adam Pavlacka Last published at: July 7th, 2022 Problem You are attempting to append data to a file saved on an external storage mount point and are getting an error message: OSError: [Errno 95] Operation not supported. The error occurs when trying to append to a file from both Python and R. Cause Direct appends and random writes are not supported in FUSE v2, which is available in Databricks Runtime 6.0 and above. This is by design. The underlying storage that is mounted to DBFS does not support append. This means that Databricks would have to download the data, run the append, and reupload the data in order to support the command. This works for small files, but quickly becomes an issue as file size increases. Because the DBFS mount is shared between driver and worker nodes, appending to a file from multiple nodes can cause data corruption. Solution As a workaround, you should run your append on a local disk, such as /tmp, and move the entire file at the end of the operation. If you need to perform cross-session appends, please contact your account team to discuss enabling an NFS mount on your clusters. Was this article helpful? (0) (4) Additional Informations Related Articles Cannot read Databricks objects stored in the DBFS root directory Problem An Access Denied error returns when you attempt to read Databricks object... How to specify the DBFS path When working with Databricks you will sometimes have to access the Databricks Fil... How to calculate the Databricks file system (DBFS) S3 API call cost The cost of a DBFS S3 bucket is primarily driven by the number of API calls, and ... Cannot access objects written by Databricks from outside Databricks Problem When you attempt to access an object in an S3 location written by Databri... Related Articles Cannot read Databricks objects stored in the DBFS root directory Problem An Access Denied error returns when you attempt to read Databricks object... How to specify the DBFS path When working with Databricks you will sometimes have to access the Databricks Fil... How to calculate the Databricks file system (DBFS) S3 API call cost The cost of a DBFS S3 bucket is primarily driven by the number of API calls, and ... Cannot access objects written by Databricks from outside Databricks Problem When you attempt to access an object in an S3 location written by Databri... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Operation not supported during append",
          "view_href" : "https://kb.databricks.com/en_US/dbfs/errno95-operation-not-supported"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/change-cluster-config-for-delta-live-table-pipeline",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Change cluster config for Delta Live Table pipeline Change cluster config for Delta Live Table pipeline Customize the cluster configuration when using a Delta Live Table pipeline. Written by pratik.bhawsar Last published at: July 1st, 2022 Problem You are using Delta Live Tables and want to change the cluster configuration. You create a pipeline, but only have options to enable or disable Photon and select the number of workers. Cause When you create a Delta Live Table pipeline, most parameters are configured with default values. These values cannot be configured before the pipeline is created. Solution You can change the cluster configuration after the pipeline is created. Click Workflows in the sidebar. Click the Delta Live Tables tab. Click the name of your pipeline. Click Settings. On the Edit Pipeline Settings pop-up, click JSON. Edit the JSON to specify your cluster configuration. You can update all of the Delta Live Table settings (AWS | Azure | GCP) in the JSON file. Click Save. Click Start to start your pipeline with the new cluster configuration. Was this article helpful? (4) (6) Additional Informations Related Articles Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... Delta Lake write job fails with java.lang.UnsupportedOperationException Problem Delta Lake write jobs sometimes fail with the following exception: java.l... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... How to improve performance of Delta Lake MERGE INTO queries using partition pruning This article explains how to trigger partition pruning in Delta Lake MERGE INTO (... Related Articles Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... Delta Lake write job fails with java.lang.UnsupportedOperationException Problem Delta Lake write jobs sometimes fail with the following exception: java.l... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... How to improve performance of Delta Lake MERGE INTO queries using partition pruning This article explains how to trigger partition pruning in Delta Lake MERGE INTO (... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Change cluster config for Delta Live Table pipeline",
          "view_href" : "https://kb.databricks.com/en_US/delta/change-cluster-config-for-delta-live-table-pipeline"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dev-tools/geospark-undefined-function-error-dbconnect",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Developer tools GeoSpark undefined function error with DBConnect GeoSpark undefined function error with DBConnect Use GeoSpark code with a DBConnect session. Written by arjun.kaimaparambilrajan Last published at: June 1st, 2022 Problem You are trying to use the GeoSpark function st_geofromwkt with DBConnect (AWS | Azure | GCP) and you get an Apache Spark error message. Error: org.apache.spark.sql.AnalysisException: Undefined function: 'st_geomfromwkt'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; This example code fails with the error when used with DBConnect. %scala\r\n\r\nval sc = spark.sparkContext\r\nsc.setLogLevel(\"DEBUG\")\r\n\r\nval sqlContext = spark.sqlContext\r\nspark.sparkContext.addJar(\"~/jars/geospark-sql_2.3-1.2.0.jar\")\r\nspark.sparkContext.addJar(\"~/jars/geospark-1.2.0.jar\")\r\n\r\nGeoSparkSQLRegistrator.registerAll(sqlContext)\r\nprintln(spark.sessionState.functionRegistry.listFunction)\r\n\r\nspark.sql(\"select ST_GeomFromWKT(area) AS geometry from polygon\").show() Cause DBConnect does not support auto-sync of client side UDFs to the server. Solution You can use a custom utility jar with code that registers the UDF on the cluster using the SparkSessionExtensions class. Create a utility jar that registers GeoSpark functions using SparkSessionExtensions. This utility class definition can be built into a utility jar. %scala\r\n\r\npackage com.databricks.spark.utils\r\n\r\nimport org.apache.spark.sql.SparkSessionExtensions\r\nimport org.datasyslab.geosparksql.utils.GeoSparkSQLRegistrator\r\n\r\nclass GeoSparkUdfExtension extends (SparkSessionExtensions => Unit) {\r\n  def apply(e: SparkSessionExtensions): Unit = {\r\n    e.injectCheckRule(spark => {\r\n      println(\"INJECTING UDF\")\r\n      GeoSparkSQLRegistrator.registerAll(spark)\r\n      _ => Unit\r\n    })\r\n  }\r\n} Copy the GeoSpark jars and your utility jar to DBFS at dbfs:/databricks/geospark-extension-jars/. Create an init script (set_geospark_extension_jar.sh) that copies the jars from the DBFS location to the Spark class path and sets the spark.sql.extensions to the utility class. %scala\r\n\r\ndbutils.fs.put(\r\n    \"dbfs:/databricks/<init-script-folder>/set_geospark_extension_jar.sh\",\r\n    \"\"\"#!/bin/sh\r\n      |sleep 10s\r\n      |# Copy the extension and GeoSpark dependency jars to /databricks/jars.\r\n      |cp -v /dbfs/databricks/geospark-extension-jars/{spark_geospark_extension_2_11_0_1.jar,geospark_sql_2_3_1_2_0.jar,geospark_1_2_0.jar} /databricks/jars/\r\n      |# Set the extension.\r\n      |cat << 'EOF' > /databricks/driver/conf/00-custom-spark.conf\r\n      |[driver] {\r\n      |    \"spark.sql.extensions\" = \"com.databricks.spark.utils.GeoSparkUdfExtension\"\r\n      |}\r\n      |EOF\r\n      |\"\"\".stripMargin,\r\n    overwrite = true\r\n) Install the init script as a cluster-scoped init script (AWS | Azure | GCP). You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/set_geospark_extension_jar.sh). Reboot your cluster. You can now use GeoSpark code with DBConnect. Was this article helpful? (10) (16) Additional Informations Related Articles Databricks Connect reports version error with Databricks Runtime 6.4 Problem You are using the Databricks Connect client with Databricks Runtime 6.4 a... Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... Failed to create process error with Databricks CLI in Windows Problem While trying to access the Databricks CLI (AWS | Azure | GCP) in Windows,... ProtoSerializer stack overflow error in DBConnect Problem You are using DBConnect (AWS | Azure | GCP) to run a PySpark transformati... Related Articles Databricks Connect reports version error with Databricks Runtime 6.4 Problem You are using the Databricks Connect client with Databricks Runtime 6.4 a... Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... Failed to create process error with Databricks CLI in Windows Problem While trying to access the Databricks CLI (AWS | Azure | GCP) in Windows,... ProtoSerializer stack overflow error in DBConnect Problem You are using DBConnect (AWS | Azure | GCP) to run a PySpark transformati... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "GeoSpark undefined function error with DBConnect",
          "view_href" : "https://kb.databricks.com/en_US/dev-tools/geospark-undefined-function-error-dbconnect"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/update-query-fails",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Delta Lake UPDATE query fails with IllegalState exception Delta Lake UPDATE query fails with IllegalState exception Learn how to resolve an issue with Delta Lake UPDATE, DELETE, or MERGE queries that use Python UDFs. Written by Adam Pavlacka Last published at: May 10th, 2022 Problem When you execute a Delta Lake UPDATE, DELETE, or MERGE query that uses Python UDFs in any of its transformations, it fails with the following exception: AWS java.lang.UnsupportedOperationException: Error in SQL statement:\r\nIllegalStateException: File (s3a://xxx/table1) to be rewritten not found among candidate files:\r\ns3a://xxx/table1/part-00001-39cae1bb-9406-49d2-99fb-8c865516fbaa-c000.snappy.parquet Delete Azure java.lang.UnsupportedOperationException: Error in SQL statement:\r\nIllegalStateException: File (adl://xxx/table1) to be rewritten not found among candidate files:\r\nadl://xxx/table1/part-00001-39cae1bb-9406-49d2-99fb-8c865516fbaa-c000.snappy.parquet Delete Version This problem occurs on Databricks Runtime 5.5 and below. Cause Delta Lake internally depends on the input_file_name() function for operations like UPDATE, DELETE, and MERGE. input_file_name() returns an empty value if you use it in a SELECT statement that evaluates a Python UDF. UPDATE calls SELECT internally, which then fails to return file names and leads to the error. This error does not occur with Scala UDFs. Solution You have two options: Use Databricks Runtime 6.0 or above, which includes the resolution to this issue: [SPARK-28153]. If you can’t use Databricks Runtime 6.0 or above, use Scala UDFs instead of Python UDFs. Was this article helpful? (9) (13) Additional Informations Related Articles Best practices for dropping a managed Delta Lake table Regardless of how you drop a managed table, it can take a significant amount of t... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... Optimize a Delta sink in a structured streaming application You are using a Delta table as the sink for your structured streaming application... Delta Lake write job fails with java.lang.UnsupportedOperationException Problem Delta Lake write jobs sometimes fail with the following exception: java.l... Related Articles Best practices for dropping a managed Delta Lake table Regardless of how you drop a managed table, it can take a significant amount of t... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... Optimize a Delta sink in a structured streaming application You are using a Delta table as the sink for your structured streaming application... Delta Lake write job fails with java.lang.UnsupportedOperationException Problem Delta Lake write jobs sometimes fail with the following exception: java.l... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Delta Lake UPDATE query fails with IllegalState exception",
          "view_href" : "https://kb.databricks.com/en_US/delta/update-query-fails"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data-sources/delete-table-if-s3-bucket-deleted",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data sources Delete table when underlying S3 bucket is deleted Delete table when underlying S3 bucket is deleted Do not delete the contents of a S3 bucket before dropping a table that stores data in the bucket. Written by Jose Gonzalez Last published at: May 31st, 2022 Problem You are trying to drop or alter a table when you get an error. Error in SQL statement: IOException: Bucket_name … does not exist You can reproduce the error with a DROP TABLE or ALTER TABLE command. %sql\r\n\r\nDROP TABLE <database-name.table-name>; %sql\r\n\r\nALTER TABLE <database-name.table-name> SET LOCATION \"<file-system-location>\"; Cause You deleted the contents of the underlying S3 bucket before dropping the tables. Because the data no longer exists, you get an error when trying to drop the table. Solution You can use spark.sessionState.catalog.externalCatalog.dropTable to delete the table. %scala\r\nimport org.apache.spark.sql.hive.HiveUtils\r\nspark.sessionState.catalog.externalCatalog.dropTable(\"<database-name>\", \"<table-name>\", ignoreIfNotExists = false, purge = false) Was this article helpful? (8) (14) Additional Informations Related Articles Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... Related Articles Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Delete table when underlying S3 bucket is deleted",
          "view_href" : "https://kb.databricks.com/en_US/data-sources/delete-table-if-s3-bucket-deleted"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/administration/sso-pingfederate",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Databricks administration SSO SAML authentication error with PingFederate SSO SAML authentication error with PingFederate Written by ashwin Last published at: February 25th, 2022 Problem When using PingFederate to authenticate over a SSO connection with Databricks, the redirection fails with the following error:   19/12/21 01:27:01 ERROR SamlAuthenticator[root=ServiceMain-6c710d1c1fca0002 parent=ServiceMain-6c710d1c1fca0002 op=HttpServer-6c710d1c1fdf2812]: SAML login failed unexpectedly\r\n  java.lang.IllegalArgumentException: com.google.common.io.BaseEncoding$DecodingException: Unrecognized character: :\r\n          at com.google.common.io.BaseEncoding.decode(BaseEncoding.java:248)\r\n          at com.databricks.common.authentication.saml.SamlAuthenticator.validateRequest(SamlAuthenticator.scala:154)\r\n          at com.databricks.common.web.WrappingAuthenticator.validateRequest(WrappingAuthenticator.scala:98)\r\n          at com.databricks.common.web.CustomErrorAuthenticator.validateRequest(CustomErrorAuthenticator.scala:45) Cause This happens because PingFederate uses multiple authentication sources simultaneously, to fulfill various policy requirements depending on the user and device context. When you are using LDAP for backend authentication, PingFederate acts as an adapter that passes along communications to the LDAP server. The SAML response is generated based on the LDAP settings, which override the PingFederate SSO settings. Solution In order to authenticate over SSO, your LDAP SAML issuer must be entered as the Identity Provider Entity ID in the Databricks Admin Console. Locate the LDAP SAML issuer in your PingFederate settings. Log in to the Databricks workspace. Open the Admin Console. Click the Single Sign On tab. Enter the SAML issuer information from your PingFederate settings in the Identity Provider Entity ID field. Was this article helpful? (13) (42) Additional Informations Related Articles SSO server redirects to original URL, not to vanity Databricks URL Problem When you log into Databricks using a vanity URL (such as mycompany.cloud.... How to discover who deleted a workspace in Azure portal If your workspace has disappeared or been deleted, you can identify which user de... How to discover who deleted a cluster in Azure portal If a cluster in your workspace has disappeared or been deleted, you can identify ... Related Articles SSO server redirects to original URL, not to vanity Databricks URL Problem When you log into Databricks using a vanity URL (such as mycompany.cloud.... How to discover who deleted a workspace in Azure portal If your workspace has disappeared or been deleted, you can identify which user de... How to discover who deleted a cluster in Azure portal If a cluster in your workspace has disappeared or been deleted, you can identify ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "SSO SAML authentication error with PingFederate",
          "view_href" : "https://kb.databricks.com/en_US/administration/sso-pingfederate"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/multi-part-upload",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Multi-part upload failure Multi-part upload failure Learn how to resolve a multi-part upload failure. Written by Adam Pavlacka Last published at: March 4th, 2022 Problem You observe a job failure with the exception: com.amazonaws.SdkClientException: Unable to complete multi-part upload. Individual part upload failed :\r\nUnable to execute HTTP request: Timeout waiting for connection from pool\r\norg.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\r\n...\r\ncom.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1190) Cause This error originates in the Amazon SDK internal implementation of multi-part upload, which takes all of the multi-part upload requests and submits them as Futures to a thread pool. There is no back pressure control here. All of the pieces are submitted in parallel. Thus the only limit on the actual parallelism of execution is the size of the thread pool itself. In this case, the thread pool is a BlockingThreadPoolExecutorService a class internal to S3A that queues requests rather than rejecting them once the pool has reached its maximum thread capacity. There are two parallelism limits here: The size of the thread pool used by S3A The size of the HTTPClient connection pool inside AmazonS3Client If the S3A thread pool is smaller than the HTTPClient connection pool, then we could imagine a situation where threads become starved when trying to get a connection from the pool. We could see this happening if hundreds of running commands end up thrashing. Solution You can tune the sizes of the S3A thread pool and HTTPClient connection pool. One plausible approach would be to reduce the size of the S3A thread pool to be smaller than the HTTPClient pool size. However, this isn’t without risk: in HADOOP-13826 it was reported that sizing the pool too small can cause deadlocks during multi-part upload. There’s a related bug referencing that one on the AWS Java SDK itself: issues/939. Given this, we don’t recommend reducing this pool size. Instead, we recommend that you increase the HTTPClient pool size to match the number of threads in the S3A pool (it is 256 currently). The HTTPClient connection pool is ultimately configured by fs.s3a.connection.maximum which is now hardcoded to 200. To solve the problem, set the following Spark configuration properties. The properties will be applied to all jobs running in the cluster: spark.hadoop.fs.s3a.multipart.threshold 2097152000\r\nspark.hadoop.fs.s3a.multipart.size 104857600\r\nspark.hadoop.fs.s3a.connection.maximum 500\r\nspark.hadoop.fs.s3a.connection.timeout 600000 Was this article helpful? (9) (13) Additional Informations Related Articles EBS leaked volumes Problem After a cluster is terminated on AWS, some EBS volumes are not deleted au... How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Failed to create cluster with invalid tag value Problem You are trying to create a cluster, but it is failing with an invalid tag... Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... Related Articles EBS leaked volumes Problem After a cluster is terminated on AWS, some EBS volumes are not deleted au... How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Failed to create cluster with invalid tag value Problem You are trying to create a cluster, but it is failing with an invalid tag... Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Multi-part upload failure",
          "view_href" : "https://kb.databricks.com/en_US/clusters/multi-part-upload"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/metrics/spark-metrics",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Metrics How to use Apache Spark metrics How to use Apache Spark metrics Learn how to use Apache Spark metrics with Databricks. Written by Adam Pavlacka Last published at: May 16th, 2022 This article gives an example of how to monitor Apache Spark components using the Spark configurable metrics system. Specifically, it shows how to set a new source and enable a sink. For detailed information about the Spark components available for metrics collection, including sinks supported out of the box, follow the documentation link above. Delete Info There are several other ways to collect metrics to get insight into how a Spark job is performing, which are also not covered in this article: SparkStatusTracker (Source, API): monitor job, stage, or task progress StreamingQueryListener (Source, API): intercept streaming events SparkListener (Source): intercept events from Spark scheduler For information about using other third-party tools to monitor Spark jobs in Databricks, see Monitor performance (AWS | Azure). How does this metrics collection system work? Upon instantiation, each executor creates a connection to the driver to pass the metrics. The first step is to write a class that extends the Source trait: %scala\r\n\r\nclass MySource extends Source {\r\n  override val sourceName: String = \"MySource\"\r\n\r\n  override val metricRegistry: MetricRegistry = new MetricRegistry\r\n\r\n  val FOO: Histogram = metricRegistry.histogram(MetricRegistry.name(\"fooHistory\"))\r\n  val FOO_COUNTER: Counter = metricRegistry.counter(MetricRegistry.name(\"fooCounter\"))\r\n} The next step is to enable the sink. In this example, the metrics are printed to the console: %scala\r\n\r\nval spark: SparkSession = SparkSession\r\n    .builder\r\n    .master(\"local[*]\")\r\n    .appName(\"MySourceDemo\")\r\n    .config(\"spark.driver.host\", \"localhost\")\r\n    .config(\"spark.metrics.conf.*.sink.console.class\", \"org.apache.spark.metrics.sink.ConsoleSink\")\r\n.getOrCreate() Delete Info To sink metrics to Prometheus, you can use this third-party library: https://github.com/banzaicloud/spark-metrics. The last step is to instantiate the source and register it with SparkEnv: %scala\r\n\r\nval source: MySource = new MySource\r\nSparkEnv.get.metricsSystem.registerSource(source) You can view a complete, buildable example at https://github.com/newroyker/meter. Was this article helpful? (13) (56) Additional Informations Related Articles How to explore Apache Spark metrics with Spark listeners Apache Spark provides several useful internal listeners that track metrics about ... Related Articles How to explore Apache Spark metrics with Spark listeners Apache Spark provides several useful internal listeners that track metrics about ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to use Apache Spark metrics",
          "view_href" : "https://kb.databricks.com/en_US/metrics/spark-metrics"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/delete-checkpoint-restart",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Delete your streaming query checkpoint and restart Delete your streaming query checkpoint and restart Delta table doesn't exist. Please delete your streaming query checkpoint and restart. Written by Adam Pavlacka Last published at: May 10th, 2022 Problem Your job fails with a Delta table <value> doesn't exist. Please delete your streaming query checkpoint and restart. error message. Cause Two different streaming sources are configured to use the same checkpoint directory. This is not supported. For example, assume streaming query A streams data from Delta table A, and uses the directory /checkpoint/A as a checkpoint. If streaming query B streams data from Delta table B, but attempts to use the directory /checkpoint/A as a checkpoint, the reservoirId of the Delta tables doesn’t match and the query fails with an exception. AWS Delete Info A similar issue can occur with S3-SQS if you attempt to share the checkpoint directory. This is because S3-SQS uses an internal Delta table to maintain the event messages. Delete Azure Delete Info A similar issue can occur with ABS-AQS if you attempt to share the checkpoint directory. This is because ABS-AQS uses an internal Delta table to maintain the event messages. Delete Solution You should not share checkpoint directories between different streaming queries. Use a new checkpoint directory for every new streaming query. Was this article helpful? (6) (18) Additional Informations Related Articles Converting from Parquet to Delta Lake fails Problem You are attempting to convert a Parquet file to a Delta Lake file. The di... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Related Articles Converting from Parquet to Delta Lake fails Problem You are attempting to convert a Parquet file to a Delta Lake file. The di... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Delete your streaming query checkpoint and restart",
          "view_href" : "https://kb.databricks.com/en_US/delta/delete-checkpoint-restart"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data-sources/abfs-client-hang",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data sources ABFS client hangs if incorrect client ID or wrong path used ABFS client hangs if incorrect client ID or wrong path used Trying to access an Azure Blob File System (ABFS) path results in a hung command when using Azure Data Lake Storage Gen2 (ADLS). Written by Adam Pavlacka Last published at: June 1st, 2022 Problem You are using Azure Data Lake Storage (ADLS) Gen2. When you try to access an Azure Blob File System (ABFS) path from a Databricks cluster, the command hangs. Enable the debug log and you can see the following stack trace in the driver logs: Caused by: java.io.IOException: Server returned HTTP response code: 400 for URL: https://login.microsoftonline.com/b9b831a9-6c10-40bf-86f3-489ed83c81e8/oauth2/token\r\n  at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1894)\r\n  at sun.net.www.protocol.http.HttpURLConnection.access$200(HttpURLConnection.java:91)\r\n  at sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1484)\r\n  at sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1482)\r\n  at java.security.AccessController.doPrivileged(Native Method)\r\n  at java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:782)\r\n  at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1481)\r\n  at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480)\r\n  at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347)\r\n  at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenSingleCall(AzureADAuthenticator.java:254)\r\n  ... 31 more Cause If ABFS is configured on a cluster with a wrong value for property fs.azure.account.oauth2.client.id, or if you try to access an explicit path of the form abfss://myContainer@myStorageAccount.dfs.core.windows.net/... where myStorageAccount does not exist, then the ABFS driver ends up in a retry loop and becomes unresponsive. The command will eventually fail, but because it retries so many times, it appears to be a hung command. If you try to access an incorrect path with an existing storage account, you will see a 404 error message. The system does not hang in this case. Solution You must verify the accuracy of all credentials when accessing ABFS data. You must also verify the ABFS path you are trying to access exists. If either of these are incorrect, the problem occurs. Was this article helpful? (12) (49) Additional Informations Related Articles Inconsistent timestamp results with JDBC applications Problem When using JDBC applications with Databricks clusters you see inconsisten... Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... Accessing Redshift fails with NullPointerException Problem Sometimes when you read a Redshift table: %scala val original_df = spark.... Apache Spark JDBC datasource query option doesn’t work for Oracle database Problem When you use the query option with the Apache Spark JDBC datasource to co... Related Articles Inconsistent timestamp results with JDBC applications Problem When using JDBC applications with Databricks clusters you see inconsisten... Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... Accessing Redshift fails with NullPointerException Problem Sometimes when you read a Redshift table: %scala val original_df = spark.... Apache Spark JDBC datasource query option doesn’t work for Oracle database Problem When you use the query option with the Apache Spark JDBC datasource to co... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "ABFS client hangs if incorrect client ID or wrong path used",
          "view_href" : "https://kb.databricks.com/en_US/data-sources/abfs-client-hang"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/enable-gcm-cipher",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Enable GCM cipher suites Enable GCM cipher suites Enable AES-GCM encryption (GCM cipher suites) for use with SSL connections to other clusters. Resolve javax.net.ssl.SSLHandshakeException error. Written by Adam Pavlacka Last published at: March 4th, 2022 Databricks clusters do not have GCM (Galois/Counter Mode) cipher suites enabled by default. You must enable GCM cipher suites on your cluster to connect to an external server that requires GCM cipher suites. Verify required cipher suites Use the nmap utility to verify which cipher suites are required by the external server. %sh\r\n\r\nnmap --script ssl-enum-ciphers -p <port> <hostname> Delete Note If nmap is not installed, run sudo apt-get install -y nmap to install it on your cluster. Create an init script to enable GCM cipher suites Use the example code to create an init script that enables GCM cipher suites on your cluster. %python\r\n\r\ndbutils.fs.put(\"/<path-to-init-script>/enable-gcm.sh\", \"\"\"#!/bin/bash\r\nsed -i 's/, GCM//g' /databricks/spark/dbconf/java/extra.security\r\n\"\"\",True) %scala\r\n\r\ndbutils.fs.put(\"/<path-to-init-script>/enable-gcm.sh\", \"\"\"#!/bin/bash\r\nsed -i 's/, GCM//g' /databricks/spark/dbconf/java/extra.security\r\n\"\"\",true) Remember the path to the init script. You will need it when configuring your cluster. Configure cluster with init script Follow the documentation to configure a cluster-scoped init script. You must specify the path to the init script. After configuring the init script, restart the cluster. Verify that GCM cipher suites are enabled This example code queries the cluster for all supported cipher suites and then prints the output. %scala\r\n\r\nimport java.util.Map;\r\nimport java.util.TreeMap;\r\nimport javax.net.ssl.SSLServerSocketFactory\r\nimport javax.net.ssl._\r\nSSLContext.getDefault.getDefaultSSLParameters.getProtocols.foreach(println)\r\nSSLContext.getDefault.getDefaultSSLParameters.getCipherSuites.foreach(println) If the GCM cipher suites are enabled, you see the following AES-GCM ciphers listed in the output. TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\r\nTLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\r\nTLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\r\nTLS_RSA_WITH_AES_256_GCM_SHA384\r\nTLS_ECDH_ECDSA_WITH_AES_256_GCM_SHA384\r\nTLS_ECDH_RSA_WITH_AES_256_GCM_SHA384\r\nTLS_DHE_RSA_WITH_AES_256_GCM_SHA384\r\nTLS_DHE_DSS_WITH_AES_256_GCM_SHA384\r\nTLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\r\nTLS_RSA_WITH_AES_128_GCM_SHA256\r\nTLS_ECDH_ECDSA_WITH_AES_128_GCM_SHA256\r\nTLS_ECDH_RSA_WITH_AES_128_GCM_SHA256\r\nTLS_DHE_RSA_WITH_AES_128_GCM_SHA256\r\nTLS_DHE_DSS_WITH_AES_128_GCM_SHA256 Connect to the external server Once you have verified that GCM cipher suites are installed on your cluster, make a connection to the external server. Was this article helpful? (13) (15) Additional Informations Related Articles Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Custom Docker image requires root Problem You are trying to launch a Databricks cluster with a custom Docker contai... Auto termination is disabled when starting a job cluster Problem You are trying to start a job cluster, but the job creation fails with an... Cluster fails to start with dummy does not exist error Problem You try to start a cluster, but it fails to start. You get an Apache Spar... Related Articles Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Custom Docker image requires root Problem You are trying to launch a Databricks cluster with a custom Docker contai... Auto termination is disabled when starting a job cluster Problem You are trying to start a job cluster, but the job creation fails with an... Cluster fails to start with dummy does not exist error Problem You try to start a cluster, but it fails to start. You get an Apache Spar... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Enable GCM cipher suites",
          "view_href" : "https://kb.databricks.com/en_US/clusters/enable-gcm-cipher"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/spark-job-not-starting",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Apache Spark job doesn’t start Apache Spark job doesn’t start Learn how to troubleshoot a Databricks Spark job that won't start. Written by Adam Pavlacka Last published at: March 4th, 2022 Problem No Spark jobs start, and the driver logs contain the following error: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources Cause This error can occur when the executor memory and number of executor cores are set explicitly on the Spark Config tab. Here is a sample config: AWS In this example, the executor is set to a i3.xLarge node, and the Spark Config is set to: spark.executor.cores 5\r\nspark.executor.memory 6G The i3.xLarge cluster type only has 4 cores but a user has set 5 cores per executor explicitly. Spark does not start any tasks, and enters the following error messages into the driver logs: WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\r\nWARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\r\nWARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources Delete GCP In this example, the executor is set to a n1-standard-4 node, and the Spark Config is set to: spark.executor.cores 5\r\nspark.executor.memory 6G The n1-standard-4 cluster type only has 4 cores but a user has set 5 cores per executor explicitly. Spark does not start any tasks, and enters the following error messages into the driver logs: WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\r\nWARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\r\nWARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources Delete Solution You should never specify cores greater than the available number of cores on the node that you chose for a cluster. Was this article helpful? (9) (13) Additional Informations Related Articles Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Log delivery fails with AssumeRole Problem You are using AssumeRole to send cluster logs to a S3 bucket in another a... Custom Docker image requires root Problem You are trying to launch a Databricks cluster with a custom Docker contai... Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... Related Articles Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Log delivery fails with AssumeRole Problem You are using AssumeRole to send cluster logs to a S3 bucket in another a... Custom Docker image requires root Problem You are trying to launch a Databricks cluster with a custom Docker contai... Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Apache Spark job doesn’t start",
          "view_href" : "https://kb.databricks.com/en_US/clusters/spark-job-not-starting"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/s3-permissions-delta",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Access denied when writing Delta Lake tables to S3 Access denied when writing Delta Lake tables to S3 Learn how to resolve an access denied 403 Forbidden error when writing Delta Lake tables to S3. Written by Adam Pavlacka Last published at: May 10th, 2022 Problem Writing DataFrame contents in Delta Lake format to an S3 location can cause an error: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3;\r\nStatus Code: 403; Error Code: 403 Forbidden; Request ID: C827672D85516BA9; S3 Extended Request ID: Cause A write operation involving the Delta Lake format requires permissions that other file formats do not need. For example, Delta Lake requires creation of a _delta_log directory. The write operation also needs to check the latest version of the commit logs. You need to add extra permissions to IAM and bucket roles to enable the write operation to complete successfully. Solution Add the following permissions to enable writing of Delta tables: Add these permissions to the IAM policy JSON: [\"s3:PutObject\",\"s3:DeleteObject\", \"s3:ListBucket\", \"s3:GetObject\", \"s3: PutObjectAcl\"] Add these permissions to the bucket policy JSON: [\"s3:GetObject\",\"s3:GetObjectVersion\",\"s3:PutObject\",\"s3:DeleteObject\",\"s3:ListBucket\",\"s3:GetBucketLocation\"] Alternatively, you can add permissions using an IAM policy in JSON format, as shown here: {\r\n  \"Version\": \"2012-10-17\",\r\n  \"Statement\": [\r\n    {\r\n      \"Sid\": \"VisualEditor0\",\r\n      \"Effect\": \"Allow\",\r\n      \"Action\": \"s3:ListBucket\",\r\n      \"Resource\": \"arn:aws:s3:::my-bucket\"\r\n    },\r\n    {\r\n      \"Sid\": \"VisualEditor1\",\r\n      \"Effect\": \"Allow\",\r\n      \"Action\": [\r\n          \"s3:PutObject\",\r\n          \"s3:GetObject\",\r\n          \"s3:DeleteObject\",\r\n          \"s3:PutObjectAcl\"\r\n        ],\r\n    \"Resource\": \"arn:aws:s3:::my-bucket/subfolder/*\"\r\n    }\r\n  ]\r\n} Was this article helpful? (6) (15) Additional Informations Related Articles A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... How to improve performance of Delta Lake MERGE INTO queries using partition pruning This article explains how to trigger partition pruning in Delta Lake MERGE INTO (... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... Related Articles A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... How to improve performance of Delta Lake MERGE INTO queries using partition pruning This article explains how to trigger partition pruning in Delta Lake MERGE INTO (... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Access denied when writing Delta Lake tables to S3",
          "view_href" : "https://kb.databricks.com/en_US/delta/s3-permissions-delta"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/intermittent-nullpointerexception-aqe-enabled",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Intermittent NullPointerException when AQE is enabled Intermittent NullPointerException when AQE is enabled When adaptive query execution (AQE) is enabled, and cluster scales down and loses shuffle data, you can get a `NullPointerException` error. Written by mathan.pillai Last published at: May 23rd, 2022 Problem You get an intermittent NullPointerException error when saving your data. Py4JJavaError: An error occurred while calling o2892.save.\r\n: java.lang.NullPointerException\r\n    at org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin.$anonfun$getMapSizesForReduceId$1(OptimizeSkewedJoin.scala:167)\r\n    at org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin.$anonfun$getMapSizesForReduceId$1$adapted(OptimizeSkewedJoin.scala:167)\r\n    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n    at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n    ....\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke(Method.java:498)\r\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\r\n    at py4j.Gateway.invoke(Gateway.java:295)\r\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n    at py4j.GatewayConnection.run(GatewayConnection.java:251)\r\n    at java.lang.Thread.run(Thread.java:748) Cause This error can occur if adaptive query execution (AQE) (AWS | Azure) is enabled and you are joining data. If AQE is enabled, skew join is also enabled. If any of the shuffle data fails due to a cluster scaling down event it generates a NullPointerException error. Solution Set spark.sql.adaptive.skewJoin.enabled to false in your Spark config (AWS | Azure). Was this article helpful? (11) (9) Additional Informations Related Articles Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Create a DataFrame from a JSON string or Python dictionary In this article we are going to review how you can create an Apache Spark DataFra... Best practice for cache(), count(), and take() cache() is an Apache Spark transformation that can be used on a DataFrame, Datase... Cannot modify the value of an Apache Spark config Problem You are trying to SET the value of a Spark config in a notebook and get a... Related Articles Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Create a DataFrame from a JSON string or Python dictionary In this article we are going to review how you can create an Apache Spark DataFra... Best practice for cache(), count(), and take() cache() is an Apache Spark transformation that can be used on a DataFrame, Datase... Cannot modify the value of an Apache Spark config Problem You are trying to SET the value of a Spark config in a notebook and get a... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Intermittent NullPointerException when AQE is enabled",
          "view_href" : "https://kb.databricks.com/en_US/scala/intermittent-nullpointerexception-aqe-enabled"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data-sources/redshift-jdbc-driver-conflict",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data sources Redshift JDBC driver conflict issue Redshift JDBC driver conflict issue Learn how to resolve a Redshift JDBC SQLDriverWrapper driver conflict. Written by Adam Pavlacka Last published at: June 1st, 2022 Problem If you attach multiple Redshift JDBC drivers to a cluster, and use the Redshift connector, the notebook REPL might hang or crash with a SQLDriverWrapper error message. 19/11/14 01:01:44 ERROR SQLDriverWrapper: Fatal non-user error thrown in ReplId-9d455-9b970-b2042\r\njava.lang.NoSuchFieldError: PG_SUBPROTOCOL_NAMES\r\n        at com.amazon.redshift.jdbc.Driver.getSubProtocols(Unknown Source)\r\n        at com.amazon.redshift.jdbc.Driver.acceptsSubProtocol(Unknown Source)\r\n        at com.amazon.jdbc.common.BaseConnectionFactory.acceptsURL(Unknown Source)\r\n        at com.amazon.jdbc.common.AbstractDriver.connect(Unknown Source)\r\n        at org.apache.spark.sql.execution.datasources.jdbc.DriverWrapper.connect(DriverWrapper.scala:45)\r\n        at com.databricks.spark.redshift.JDBCWrapper.getConnector(RedshiftJDBCWrapper.scala:355)\r\n        at com.databricks.spark.redshift.JDBCWrapper.getConnector(RedshiftJDBCWrapper.scala:376)\r\n        at com.databricks.spark.redshift.RedshiftRelation$$anonfun$schema$1.apply(RedshiftRelation.scala:75)\r\n        at com.databricks.spark.redshift.RedshiftRelation$$anonfun$schema$1.apply(RedshiftRelation.scala:72) Cause Databricks Runtime does not include a Redshift JDBC driver. If you are using Redshift, you must attach the correct driver to your cluster. If you attach multiple Redshift JDBC drivers to a single cluster they may be incompatible, which results in a hang or a crash. For example, the following Redshift JDBC jars are incompatible: RedshiftJDBC41-1.1.7.1007.jar RedshiftJDBC42-no-awssdk-1.2.20.1043.jar If you attach both of these to the same cluster, the SQLDriverWrapper error message will appear when you try to access Redshift. Solution You should only have one Redshift JDBC driver attached to a cluster. Review the Redshift JDBC Driver documentation to choose the correct driver for your cluster. Was this article helpful? (7) (37) Additional Informations Related Articles Optimize read performance from JDBC data sources Problem Reading data from an external JDBC database is slow. How can I improve re... Failure to detect encoding in JSON Problem Spark job fails with an exception containing the message: Invalid UTF-32 ... Kafka client terminated with OffsetOutOfRangeException Problem You have an Apache Spark application that is trying to fetch messages fro... CosmosDB-Spark connector library conflict This article explains how to resolve an issue running applications that use the C... Related Articles Optimize read performance from JDBC data sources Problem Reading data from an external JDBC database is slow. How can I improve re... Failure to detect encoding in JSON Problem Spark job fails with an exception containing the message: Invalid UTF-32 ... Kafka client terminated with OffsetOutOfRangeException Problem You have an Apache Spark application that is trying to fetch messages fro... CosmosDB-Spark connector library conflict This article explains how to resolve an issue running applications that use the C... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Redshift JDBC driver conflict issue",
          "view_href" : "https://kb.databricks.com/en_US/data-sources/redshift-jdbc-driver-conflict"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/python-cmd-fails-tornado-version",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Cluster cancels Python command execution after installing Bokeh Cluster cancels Python command execution after installing Bokeh Learn what to do when your Databricks cluster cancels Python command execution after you install Bokeh. Written by Adam Pavlacka Last published at: May 19th, 2022 Problem The cluster returns Cancelled in a Python notebook. Inspect the driver log (std.err) in the Cluster Configuration page for a stack trace and error message similar to the following: log4j:WARN No appenders could be found for logger (com.databricks.conf.trusted.ProjectConf$).\r\nlog4j:WARN Please initialize the log4j system properly.\r\nlog4j:WARN See https://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\r\nOpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0\r\nTraceback (most recent call last):\r\n  File \"/local_disk0/tmp/1551693540856-0/PythonShell.py\", line 30, in <module>\r\n    from IPython.nbconvert.filters.ansi import ansi2html\r\n  File \"/databricks/python/lib/python3.5/site-packages/IPython/nbconvert/__init__.py\", line 6, in <module>\r\n    from . import postprocessors\r\n  File \"/databricks/python/lib/python3.5/site-packages/IPython/nbconvert/postprocessors/__init__.py\", line 6, in <module>\r\n    from .serve import ServePostProcessor\r\n  File \"/databricks/python/lib/python3.5/site-packages/IPython/nbconvert/postprocessors/serve.py\", line 29, in <module>\r\n    class ProxyHandler(web.RequestHandler):\r\n  File \"/databricks/python/lib/python3.5/site-packages/IPython/nbconvert/postprocessors/serve.py\", line 31, in ProxyHandler\r\n    @web.asynchronous\r\nAttributeError: module 'tornado.web' has no attribute 'asynchronous' Cause When you install the bokeh library, by default tornado version 6.0a1 is installed, which is an alpha release. The alpha release causes this error, so the solution is to revert to the stable version of tornado. Solution Follow the steps below to create a cluster-scoped init script (AWS | Azure | GCP). The init script removes the newer version of tornado and installs the stable version. If the init script does not already exist, create a base directory to store it: %sh\r\n\r\ndbutils.fs.mkdirs(\"dbfs:/databricks/<directory>/\") Create the following script: %sh\r\n\r\ndbutils.fs.put(\"dbfs:/databricks/<directory>/tornado.sh\",\"\"\"\r\n#!/bin/bash\r\npip uninstall --yes tornado\r\nrm -rf /home/ubuntu/databricks/python/lib/python3.5/site-packages/tornado*\r\nrm -rf /databricks/python/lib/python3.5/site-packages/tornado*\r\n/usr/bin/yes | /home/ubuntu/databricks/python/bin/pip install tornado==5.1.1\r\n\"\"\",True) Confirm that the script exists: %sh\r\n\r\ndisplay(dbutils.fs.ls(\"dbfs:/databricks/<directory>/tornado.sh\")) Go to the cluster configuration page (AWS | Azure | GCP) and click the Advanced Options toggle. At the bottom of the page, click the Init Scripts tab: In the Destination drop-down, select DBFS, provide the file path to the script, and click Add. Restart the cluster. For more information, see: CI failures with tornado 6.0a1 Convert proxy handler from callback to coroutine Was this article helpful? (11) (8) Additional Informations Related Articles Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Job remains idle before starting Problem You have an Apache Spark job that is triggered correctly, but remains idl... Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... Related Articles Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Job remains idle before starting Problem You have an Apache Spark job that is triggered correctly, but remains idl... Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cluster cancels Python command execution after installing Bokeh",
          "view_href" : "https://kb.databricks.com/en_US/python/python-cmd-fails-tornado-version"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/r/verify-r-packages-installed-init",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles R with Apache Spark Verify R packages installed via init script Verify R packages installed via init script Verify that R packages successfully installed via an init script. List all R packages that failed to install. Written by kavya.parag Last published at: May 20th, 2022 When you configure R packages to install via an init script, it is possible for a package install to fail if dependencies are not installed. You can use the R commands in a notebook to check that all of the packages correctly installed. Delete Info This article does require you to provide a list of packages to check against. List installed packages Make a list of all R package names that you have listed in your init script or scripts. Enter the list of packages in this sample code. %r\r\n\r\nmy_packages <- list(\"<package-1>\", \"<package-2>\", \"<package-3>\" )\r\nfind.package(my_packages, quiet=TRUE) The output is a list of all installed packages. Verify the output against the input list to ensure that all packages were successfully installed. List packages that did not install Make a list of all R package names that you have listed in your init script or scripts. Enter the list of packages in this sample code. %r\r\n\r\nmy_packages <- c(\"<package-1>\", \"<package-2>\", \"<package-3>\" )\r\nnot_installed <- my_packages[!(my_packages %in% installed.packages()[ , \"Package\"])]\r\nprint(not_installed) The output is a list of all packages that failed to install. If you have packages that are consistently failing to install, you should enable cluster log delivery and review the cluster logs for failures. Was this article helpful? (9) (11) Additional Informations Related Articles Rendering an R markdown file containing sparklyr code fails Problem After you install and configure RStudio in the Databricks environment, wh... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... RStudio server backend connection error Problem You get a backend connection error when using RStudio server. Error in Sy... Related Articles Rendering an R markdown file containing sparklyr code fails Problem After you install and configure RStudio in the Databricks environment, wh... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... RStudio server backend connection error Problem You get a backend connection error when using RStudio server. Error in Sy... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Verify R packages installed via init script",
          "view_href" : "https://kb.databricks.com/en_US/r/verify-r-packages-installed-init"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data-sources/access-blob-fails-wasb",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data sources Failure when mounting or accessing Azure Blob storage Failure when mounting or accessing Azure Blob storage Learn how to resolve a failure when mounting or accessing Azure Blob storage from Databricks. Written by Adam Pavlacka Last published at: May 31st, 2022 Problem When you try to access an already created mount point or create a new mount point, it fails with the error: WASB: Fails with java.lang.NullPointerException Cause This error can occur when the root mount path (such as /mnt/) is also mounted to blob storage. Run the following command to check if the root path is also mounted: %python\r\n\r\ndbutils.fs.mounts() Check if /mnt appears in the list. Solution Unmount the /mnt/ mount point using the command: %python\r\n\r\ndbutils.fs.unmount(\"/mnt\") Now you should be able to access your existing mount points and create new ones. Was this article helpful? (11) (47) Additional Informations Related Articles Delete table when underlying S3 bucket is deleted Problem You are trying to drop or alter a table when you get an error. Error in S... Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... Related Articles Delete table when underlying S3 bucket is deleted Problem You are trying to drop or alter a table when you get an error. Error in S... Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Failure when mounting or accessing Azure Blob storage",
          "view_href" : "https://kb.databricks.com/en_US/data-sources/access-blob-fails-wasb"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/delta-merge-into",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake How to improve performance of Delta Lake MERGE INTO queries using partition pruning How to improve performance of Delta Lake MERGE INTO queries using partition pruning Learn how to use partition pruning to improve the performance of Delta Lake MERGE INTO queries. Written by Adam Pavlacka Last published at: May 10th, 2022 This article explains how to trigger partition pruning in Delta Lake MERGE INTO (AWS | Azure | GCP) queries from Databricks. Partition pruning is an optimization technique to limit the number of partitions that are inspected by a query. Discussion MERGE INTO is an expensive operation when used with Delta tables. If you don’t partition the underlying data and use it appropriately, query performance can be severely impacted. The main lesson is this: if you know which partitions a MERGE INTO query needs to inspect, you should specify them in the query so that partition pruning is performed. Demonstration: no partition pruning Here is an example of a poorly performing MERGE INTO query without partition pruning. Start by creating the following Delta table, called delta_merge_into: %scala\r\n\r\nval df = spark.range(30000000)\r\n    .withColumn(\"par\", ($\"id\" % 1000).cast(IntegerType))\r\n    .withColumn(\"ts\", current_timestamp())\r\n    .write\r\n      .format(\"delta\")\r\n      .mode(\"overwrite\")\r\n      .partitionBy(\"par\")\r\n      .saveAsTable(\"delta_merge_into\") Then merge a DataFrame into the Delta table to create a table called update: %scala\r\n\r\nval updatesTableName = \"update\"\r\nval targetTableName = \"delta_merge_into\"\r\nval updates = spark.range(100).withColumn(\"id\", (rand() * 30000000 * 2).cast(IntegerType))\r\n    .withColumn(\"par\", ($\"id\" % 2).cast(IntegerType))\r\n    .withColumn(\"ts\", current_timestamp())\r\n    .dropDuplicates(\"id\")\r\nupdates.createOrReplaceTempView(updatesTableName) The update table has 100 rows with three columns, id, par, and ts. The value of par is always either 1 or 0. Let’s say you run the following simple MERGE INTO query: %scala\r\n\r\nspark.sql(s\"\"\"\r\n    |MERGE INTO $targetTableName\r\n    |USING $updatesTableName\r\n     |ON $targetTableName.id = $updatesTableName.id\r\n     |WHEN MATCHED THEN\r\n     |  UPDATE SET $targetTableName.ts = $updatesTableName.ts\r\n    |WHEN NOT MATCHED THEN\r\n    |  INSERT (id, par, ts) VALUES ($updatesTableName.id, $updatesTableName.par, $updatesTableName.ts)\r\n \"\"\".stripMargin) The query takes 13.16 minutes to complete: The physical plan for this query contains PartitionCount: 1000, as shown below. This means Apache Spark is scanning all 1000 partitions in order to execute the query. This is not an efficient query, because the update data only has partition values of 1 and 0: == Physical Plan ==\r\n*(5) HashAggregate(keys=[], functions=[finalmerge_count(merge count#8452L) AS count(1)#8448L], output=[count#8449L])\r\n+- Exchange SinglePartition\r\n   +- *(4) HashAggregate(keys=[], functions=[partial_count(1) AS count#8452L], output=[count#8452L])\r\n    +- *(4) Project\r\n       +- *(4) Filter (isnotnull(count#8440L) && (count#8440L > 1))\r\n          +- *(4) HashAggregate(keys=[_row_id_#8399L], functions=[finalmerge_sum(merge sum#8454L) AS sum(cast(one#8434 as bigint))#8439L], output=[count#8440L])\r\n             +- Exchange hashpartitioning(_row_id_#8399L, 200)\r\n                +- *(3) HashAggregate(keys=[_row_id_#8399L], functions=[partial_sum(cast(one#8434 as bigint)) AS sum#8454L], output=[_row_id_#8399L, sum#8454L])\r\n                   +- *(3) Project [_row_id_#8399L, UDF(_file_name_#8404) AS one#8434]\r\n                      +- *(3) BroadcastHashJoin [cast(id#7514 as bigint)], [id#8390L], Inner, BuildLeft, false\r\n                         :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\r\n                         :  +- *(2) HashAggregate(keys=[id#7514], functions=[], output=[id#7514])\r\n                         :     +- Exchange hashpartitioning(id#7514, 200)\r\n                         :        +- *(1) HashAggregate(keys=[id#7514], functions=[], output=[id#7514])\r\n                         :           +- *(1) Filter isnotnull(id#7514)\r\n                         :              +- *(1) Project [cast(((rand(8188829649009385616) * 3.0E7) * 2.0) as int) AS id#7514]\r\n                         :                 +- *(1) Range (0, 100, step=1, splits=36)\r\n                         +- *(3) Filter isnotnull(id#8390L)\r\n                            +- *(3) Project [id#8390L, _row_id_#8399L, input_file_name() AS _file_name_#8404]\r\n                               +- *(3) Project [id#8390L, monotonically_increasing_id() AS _row_id_#8399L]\r\n                                  +- *(3) Project [id#8390L, par#8391, ts#8392]\r\n                                     +- *(3) FileScan parquet [id#8390L,ts#8392,par#8391] Batched: true, DataFilters: [], Format: Parquet, Location: TahoeBatchFileIndex[dbfs:/user/hive/warehouse/delta_merge_into], PartitionCount: 1000, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,ts:timestamp> Solution Rewrite the query to specify the partitions. This MERGE INTO query specifies the partitions directly: %scala\r\n\r\nspark.sql(s\"\"\"\r\n     |MERGE INTO $targetTableName\r\n     |USING $updatesTableName\r\n     |ON $targetTableName.par IN (1,0) AND $targetTableName.id = $updatesTableName.id\r\n     |WHEN MATCHED THEN\r\n     |  UPDATE SET $targetTableName.ts = $updatesTableName.ts\r\n     |WHEN NOT MATCHED THEN\r\n     |  INSERT (id, par, ts) VALUES ($updatesTableName.id, $updatesTableName.par, $updatesTableName.ts)\r\n \"\"\".stripMargin) Now the query takes just 20.54 seconds to complete on the same cluster: The physical plan for this query contains PartitionCount: 2, as shown below. With only minor changes, the query is now more than 40X faster: == Physical Plan ==\r\n\r\n*(5) HashAggregate(keys=[], functions=[finalmerge_count(merge count#7892L) AS count(1)#7888L], output=[count#7889L])\r\n+- Exchange SinglePartition\r\n   +- *(4) HashAggregate(keys=[], functions=[partial_count(1) AS count#7892L], output=[count#7892L])\r\n    +- *(4) Project\r\n       +- *(4) Filter (isnotnull(count#7880L) && (count#7880L > 1))\r\n          +- *(4) HashAggregate(keys=[_row_id_#7839L], functions=[finalmerge_sum(merge sum#7894L) AS sum(cast(one#7874 as bigint))#7879L], output=[count#7880L])\r\n             +- Exchange hashpartitioning(_row_id_#7839L, 200)\r\n                +- *(3) HashAggregate(keys=[_row_id_#7839L], functions=[partial_sum(cast(one#7874 as bigint)) AS sum#7894L], output=[_row_id_#7839L, sum#7894L])\r\n                   +- *(3) Project [_row_id_#7839L, UDF(_file_name_#7844) AS one#7874]\r\n                      +- *(3) BroadcastHashJoin [cast(id#7514 as bigint)], [id#7830L], Inner, BuildLeft, false\r\n                         :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\r\n                         :  +- *(2) HashAggregate(keys=[id#7514], functions=[], output=[id#7514])\r\n                         :     +- Exchange hashpartitioning(id#7514, 200)\r\n                         :        +- *(1) HashAggregate(keys=[id#7514], functions=[], output=[id#7514])\r\n                         :           +- *(1) Filter isnotnull(id#7514)\r\n                         :              +- *(1) Project [cast(((rand(8188829649009385616) * 3.0E7) * 2.0) as int) AS id#7514]\r\n                         :                 +- *(1) Range (0, 100, step=1, splits=36)\r\n                         +- *(3) Project [id#7830L, _row_id_#7839L, _file_name_#7844]\r\n                            +- *(3) Filter (par#7831 IN (1,0) && isnotnull(id#7830L))\r\n                               +- *(3) Project [id#7830L, par#7831, _row_id_#7839L, input_file_name() AS _file_name_#7844]\r\n                                  +- *(3) Project [id#7830L, par#7831, monotonically_increasing_id() AS _row_id_#7839L]\r\n                                     +- *(3) Project [id#7830L, par#7831, ts#7832]\r\n                                        +- *(3) FileScan parquet [id#7830L,ts#7832,par#7831] Batched: true, DataFilters: [], Format: Parquet, Location: TahoeBatchFileIndex[dbfs:/user/hive/warehouse/delta_merge_into], PartitionCount: 2, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,ts:timestamp> Was this article helpful? (18) (48) Additional Informations Related Articles Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... Converting from Parquet to Delta Lake fails Problem You are attempting to convert a Parquet file to a Delta Lake file. The di... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... Related Articles Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... Converting from Parquet to Delta Lake fails Problem You are attempting to convert a Parquet file to a Delta Lake file. The di... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to improve performance of Delta Lake MERGE INTO queries using partition pruning",
          "view_href" : "https://kb.databricks.com/en_US/delta/delta-merge-into"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/anaconda-environment",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Create a cluster with Conda Create a cluster with Conda Learn how to create a Databricks cluster with Conda. Written by Adam Pavlacka Last published at: May 19th, 2022 Conda is a popular open source package management system for the Anaconda repo. Databricks Runtime for Machine Learning (Databricks Runtime ML) uses Conda to manage Python library dependencies. If you want to use Conda, you should use Databricks Runtime ML. Attempting to install Anaconda or Conda for use with Databricks Runtime is not supported. Follow the Create a cluster using Databricks Runtime ML (AWS | Azure) instructions to create a cluster with Conda. Once the cluster has been created, you can use Conda to manage Python packages on the cluster. Was this article helpful? (11) (42) Additional Informations Related Articles AttributeError: ‘function’ object has no attribute Problem You are selecting columns from a DataFrame and you get an error message. ... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... Related Articles AttributeError: ‘function’ object has no attribute Problem You are selecting columns from a DataFrame and you get an error message. ... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Create a cluster with Conda",
          "view_href" : "https://kb.databricks.com/en_US/python/anaconda-environment"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/hdfs-to-read-files",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Use the HDFS API to read files in Python Use the HDFS API to read files in Python Learn how to read files directly by using the HDFS API in Python. Written by arjun.kaimaparambilrajan Last published at: May 19th, 2022 There may be times when you want to read files directly without using third party libraries. This can be useful for reading small files when your regular storage blobs and buckets are not available as local DBFS mounts. AWS Use the following example code for S3 bucket storage. %python\r\n\r\nURI = sc._gateway.jvm.java.net.URI\r\nPath = sc._gateway.jvm.org.apache.hadoop.fs.Path\r\nFileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\r\nconf = sc._jsc.hadoopConfiguration()\r\nfs = Path('s3a://<bucket-name>/<file-path>').getFileSystem(sc._jsc.hadoopConfiguration())\r\nistream = fs.open(Path('s3a://<bucket-name>/<file-path>'))\r\n\r\nreader = sc._gateway.jvm.java.io.BufferedReader(sc._jvm.java.io.InputStreamReader(istream))\r\n\r\nwhile True:\r\n  thisLine = reader.readLine()\r\n  if thisLine is not None:\r\n    print(thisLine)\r\n  else:\r\n    break\r\n\r\nistream.close() where <bucket-name> is the name of the S3 bucket. <file-path> is the full path to the file. Delete Azure Use the following example code for Azure Blob storage. %python\r\n\r\nURI = sc._gateway.jvm.java.net.URI\r\nPath = sc._gateway.jvm.org.apache.hadoop.fs.Path\r\nFileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\r\nconf = sc._jsc.hadoopConfiguration()\r\n\r\nconf.set(\r\n  \"fs.azure.account.key.<account-name>.blob.core.windows.net,\r\n  \"<account-access-key>\")\r\n\r\nfs = Path('wasbs://<container-name>@<account-name>.blob.core.windows.net/<file-path>/').getFileSystem(sc._jsc.hadoopConfiguration())\r\nistream = fs.open(Path('wasbs://<container-name>@<account-name>.blob.core.windows.net/<file-path>/'))\r\n\r\nreader = sc._gateway.jvm.java.io.BufferedReader(sc._jvm.java.io.InputStreamReader(istream))\r\n\r\nwhile True:\r\n  thisLine = reader.readLine()\r\n  if thisLine is not None:\r\n    print(thisLine)\r\n  else:\r\n    break\r\n\r\nistream.close() where <account-name> is your Azure account name. <container-name> is the container name. <file-path> is the full path to the file. <account-access-key> is the account access key. Delete Was this article helpful? (7) (14) Additional Informations Related Articles Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Install and compile Cython This document explains how to run Spark code with compiled Cython code. The steps... Reading large DBFS-mounted files using Python APIs This article explains how to resolve an error that occurs when you read large DBF... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... Related Articles Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Install and compile Cython This document explains how to run Spark code with compiled Cython code. The steps... Reading large DBFS-mounted files using Python APIs This article explains how to resolve an error that occurs when you read large DBF... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Use the HDFS API to read files in Python",
          "view_href" : "https://kb.databricks.com/en_US/python/hdfs-to-read-files"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/enable-openjsse-tls13",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Enable OpenJSSE and TLS 1.3 Enable OpenJSSE and TLS 1.3 Add OpenJSSE to allow the use of TLS 1.3 for encrypted data transmission. Written by Adam Pavlacka Last published at: March 2nd, 2022 Queries and transformations are encrypted before being send to your clusters. By default, the data exchanged between worker nodes in a cluster is not encrypted. If you require that data is encrypted at all times, you can encrypt traffic between cluster worker nodes using AES 128 over a TLS 1.2 connection. In some cases, you may want to use TLS 1.3 instead of TLS 1.2 because it allows for stronger ciphers. To use TLS 1.3 on your clusters, you must enable OpenJSSE in the cluster’s Apache Spark configuration. Add spark.driver.extraJavaOptions -XX:+UseOpenJSSE to your Spark Config. Restart your cluster. OpenJSSE and TLS 1.3 are now enabled on your cluster and can be used in notebooks. Was this article helpful? (14) (11) Additional Informations Related Articles How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... Related Articles How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Enable OpenJSSE and TLS 1.3",
          "view_href" : "https://kb.databricks.com/en_US/clusters/enable-openjsse-tls13"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/cloud/s3-part-number-limit",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Cloud infrastructure S3 part number must be between 1 and 10000 inclusive S3 part number must be between 1 and 10000 inclusive Learn how to resolve the S3 part number must be between 1 and 10000 inclusive error. Written by Adam Pavlacka Last published at: February 25th, 2022 Problem When you copy a large file from the local file system to DBFS on S3, the following exception can occur: Amazon.S3.AmazonS3Exception: Part number must be an integer between 1 and 10000, inclusive Cause This is an S3 limit on segment count. Part files can only be numbered from 1 to 10000, inclusive. Solution To prevent this exception from occurring, increase the size of each part file. Set the following property at the cluster level or notebook level. Cluster Level (Bash): you must restart the cluster after setting this property. spark.hadoop.fs.s3a.multipart.size 104857600 Notebook Level (Python): spark.conf.set(\"spark.hadoop.fs.s3a.multipart.size\", \"104857600\") Delete Note If the error still occurs, increase the multipart size even more. Was this article helpful? (9) (16) Additional Informations Related Articles Vulnerability scan shows vulnerabilities in Databricks EC2 instances Problem The Corporate Information Security (CIS) Vulnerability Management team id... Unable to load AWS credentials Problem When you try to access AWS resources like S3, SQS or Redshift, the operat... Access denied when writing logs to an S3 bucket Problem When you try to write log files to an S3 bucket, you get the error: com.a... Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... Related Articles Vulnerability scan shows vulnerabilities in Databricks EC2 instances Problem The Corporate Information Security (CIS) Vulnerability Management team id... Unable to load AWS credentials Problem When you try to access AWS resources like S3, SQS or Redshift, the operat... Access denied when writing logs to an S3 bucket Problem When you try to write log files to an S3 bucket, you get the error: com.a... Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "S3 part number must be between 1 and 10000 inclusive",
          "view_href" : "https://kb.databricks.com/en_US/cloud/s3-part-number-limit"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/r/change-r-version",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles R with Apache Spark Change version of R (r-base) Change version of R (r-base) Learn how to change the version of R on your Databricks cluster. Written by Adam Pavlacka Last published at: May 20th, 2022 These instructions describe how to install a different version of R (r-base) on a cluster. You can check the default r-base version that each Databricks Runtime version is installed with in the System environment section of each Databricks Runtime release note (AWS | Azure | GCP). List available r-base-core versions To list the versions of r-base-core that can be installed and the version format: Paste the following shell command in a notebook cell: %sh\r\n\r\nadd-apt-repository 'deb [arch=amd64,i386] https://cran.rstudio.com/bin/linux/ubuntu xenial//'\r\napt-get -y update\r\napt-cache madison r-base-core Run the cell. For example, you can install version 3.3.3 by specifying 3.3.3-1xenial0. Install a specific R version Paste the following shell command into a notebook cell. Set <r-version> to the R version to be installed. Set <init-script-path> to a file path under /dbfswhere this init script will be saved. %sh\r\n\r\nR_VERSION='<r-version>'\r\nINIT_SCRIPT_PATH='<init-script-path>'\r\n\r\nmkdir -p $(dirname $INIT_SCRIPT_PATH)\r\n\r\necho \"set -e\r\n\r\n# Add the repository containing another version of R\r\nadd-apt-repository 'deb [arch=amd64,i386] https://cran.rstudio.com/bin/linux/ubuntu xenial//'\r\napt-get -y update\r\n\r\n# Uninstall current R version\r\napt-get remove -y r-base-core\r\n\r\n# Install another version of R\r\napt-get install -y r-base-core=$R_VERSION\r\n\r\n# Must install Rserve to use Databricks notebook\r\nR -e \\\"install.packages('Rserve', repos='https://rforge.net/', type = 'source')\\\"\r\nR -e \\\"install.packages('hwriterPlus', repos='https://mran.revolutionanalytics.com/snapshot/2017-02-26')\\\"\" > $INIT_SCRIPT_PATH Run the notebook cell to save the init script to a file on DBFS. Configure a cluster with a cluster-scoped init script (AWS | Azure | GCP). When specifying the init script path in the cluster creation UI, modify the format of the init script path to change /dbfs to dbfs:/. For example, if <init-script-path> is set to /dbfs/examplepath/change-r-base.sh, then in the cluster creation UI specify the init script path dbfs:/examplepath/change-r-base.sh. After the cluster starts up, verify that the desired R version is installed by running %r R.version in a notebook cell. Was this article helpful? (15) (42) Additional Informations Related Articles Fix the version of R packages When you use the install.packages() function to install CRAN packages, you cannot... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... How to parallelize R code with spark.lapply Parallelization of R code is difficult, because R code runs on the driver and R d... How to persist and share code in RStudio Problem Unlike a Databricks notebook that has version control built in, code deve... Install rJava and RJDBC libraries This article explains how to install rJava and RJBDC libraries. Problem When you ... Related Articles Fix the version of R packages When you use the install.packages() function to install CRAN packages, you cannot... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... How to parallelize R code with spark.lapply Parallelization of R code is difficult, because R code runs on the driver and R d... How to persist and share code in RStudio Problem Unlike a Databricks notebook that has version control built in, code deve... Install rJava and RJDBC libraries This article explains how to install rJava and RJBDC libraries. Problem When you ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Change version of R (r-base)",
          "view_href" : "https://kb.databricks.com/en_US/r/change-r-version"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data-sources/json-unicode",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data sources Failure to detect encoding in JSON Failure to detect encoding in JSON Learn how to resolve a failure to detect encoding of input JSON files when using BOM with Databricks. Written by Adam Pavlacka Last published at: June 1st, 2022 Problem Spark job fails with an exception containing the message: Invalid UTF-32 character 0x1414141(above 10ffff)  at char #1, byte #7)\r\nAt org.apache.spark.sql.catalyst.json.JacksonParser.parse Cause The JSON data source reader is able to automatically detect encoding of input JSON files using BOM at the beginning of the files. However, BOM is not mandatory by Unicode standard and prohibited by RFC 7159. For example, section 8.1 says, \"Implementations MUST NOT add a byte order mark to the beginning of a JSON text.\" As a consequence, Spark is not always able to detect the charset correctly and read the JSON file. Solution To solve the issue, disable the charset auto-detection mechanism and explicitly set the charset using the encoding option: %scala\r\n\r\n.option(\"encoding\", \"UTF-16LE\") Was this article helpful? (16) (39) Additional Informations Related Articles Optimize read performance from JDBC data sources Problem Reading data from an external JDBC database is slow. How can I improve re... CosmosDB-Spark connector library conflict This article explains how to resolve an issue running applications that use the C... Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... Related Articles Optimize read performance from JDBC data sources Problem Reading data from an external JDBC database is slow. How can I improve re... CosmosDB-Spark connector library conflict This article explains how to resolve an issue running applications that use the C... Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Failure to detect encoding in JSON",
          "view_href" : "https://kb.databricks.com/en_US/data-sources/json-unicode"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/job-cluster-limit-nb-output",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Job cluster limits on notebook output Job cluster limits on notebook output Job clusters have a maximum notebook output size of 20 MB. If the output is larger, it results in an error. Written by Jose Gonzalez Last published at: May 10th, 2022 Problem You are running a notebook on a job cluster and you get an error message indicating that the output is too large. The output of the notebook is too large. Cause: rpc response (of 20975548 bytes) exceeds limit of 20971520 bytes Cause This error message can occur in a job cluster whenever the notebook output is greater then 20 MB. If you are using multiple display(), displayHTML(), show() commands in your notebook, this increases the amount of output. Once the output exceeds 20 MB, the error occurs. If you are using multiple print() commands in your notebook, this can increase the output to stdout. Once the output exceeds 20 MB, the error occurs. If you are running a streaming job and enable awaitAnyTermination in the cluster’s Spark config (AWS | Azure | GCP), it tries to fetch the entire output in a single request. If this exceeds 20 MB, the error occurs. Solution Remove any unnecessary display(), displayHTML(), print(), and show(), commands in your notebook. These can be useful for debugging, but they are not recommended for production jobs. If your job output is exceeding the 20 MB limit, try redirecting your logs to log4j or disable stdout by setting spark.databricks.driver.disableScalaOutput true in the cluster’s Spark config. For more information, please review the documentation on output size limits (AWS | Azure | GCP). Was this article helpful? (9) (13) Additional Informations Related Articles How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... Related Articles How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Job cluster limits on notebook output",
          "view_href" : "https://kb.databricks.com/en_US/jobs/job-cluster-limit-nb-output"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/job-run-dash",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Monitor running jobs with a Job Run dashboard Monitor running jobs with a Job Run dashboard Learn about using the Job Run dashboard in a workspace. Written by Adam Pavlacka Last published at: May 11th, 2022 The Job Run dashboard is a notebook that displays information about all of the jobs currently running in your workspace. To configure the dashboard, you must have permission to attach a notebook to an all-purpose cluster in the workspace you want to monitor. If an all-purpose cluster does not exist, you must have permission to create one. Once the dashboard is configured, you can manage job permissions (AWS | Azure) and assign Can View permissions to users in your organization. These users can view the dashboard, but cannot modify it. Job Run dashboard notebook Review the Job Run dashboard notebook. Attach the dashboard Because the Job Run dashboard is a notebook, no special steps are required to attach the notebook to a cluster (AWS | Azure). Attach it to an all-purpose cluster. Run the dashboard as a scheduled job After attaching the notebook to a cluster in your workspace, configure it to run as a scheduled job that runs every minute. Open the notebook. Click Schedule in the notebook toolbar. Click New in the Schedule job pane. Select Every and minute in the Create Schedule dialog box. Click OK. Click Job Run dashboard in the Schedule job pane. Click Edit next to the Cluster option on the job details (AWS | Azure) page. Select an existing all-purpose cluster. Click Confirm. Display dashboard Go to the job details page for the scheduled job. Check to make sure at least one successful run has occurred. Click Latest successful run (refreshes automatically). Select the Job Run Dashboard view. The dashboard is now in presentation mode. It updates automatically after each scheduled run completes. You can share the dashboard URL with any user who has view permissions. Results listed The Job Run dashboard results are split into two sections: Job Runs - Displays all of the scheduled jobs that are currently running. Run Submits - Displays all of the running jobs that were invoked via an API call. The dashboard displays the following components for each job: Job ID - This is the unique ID number for the job. You can use this to view all of the job data by entering it into a job URL (AWS | Azure). Run Page - This is the ID number of the specific run for a given job. It is formatted as a clickable hyperlink, so you can navigate directly to the run page from the Job Run dashboard. You can access previous run pages by navigating to the job URL and then clicking the specific run page from the list of completed runs. Run Name - This is the name of the notebook associated with the job. Start Time - This is the time the job run began. Time is displayed in DD-MM-YYYY HH:MM:SS format, using a 24 hour clock. Time is in UTC. Created By - This is the email address of the user who owns the job. Was this article helpful? (12) (9) Additional Informations Related Articles How to ensure idempotency for jobs When you submit jobs through the Databricks Jobs REST API, idempotency is not gua... Create table in overwrite mode fails when interrupted Problem When you attempt to rerun an Apache Spark write operation by cancelling t... Apache Spark job fails with Failed to parse byte string Problem Spark-submit jobs fail with a Failed to parse byte string: -1 error messa... Apache Spark job fails with a Connection pool shut down error Problem A Spark job fails with the error message java.lang.IllegalStateException:... Related Articles How to ensure idempotency for jobs When you submit jobs through the Databricks Jobs REST API, idempotency is not gua... Create table in overwrite mode fails when interrupted Problem When you attempt to rerun an Apache Spark write operation by cancelling t... Apache Spark job fails with Failed to parse byte string Problem Spark-submit jobs fail with a Failed to parse byte string: -1 error messa... Apache Spark job fails with a Connection pool shut down error Problem A Spark job fails with the error message java.lang.IllegalStateException:... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Monitor running jobs with a Job Run dashboard",
          "view_href" : "https://kb.databricks.com/en_US/jobs/job-run-dash"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/spark-overwrite-cancel",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Create table in overwrite mode fails when interrupted Create table in overwrite mode fails when interrupted Learn how to troubleshoot failures that occur when you rerun an Apache Spark write operation by cancelling the currently running job. Written by Adam Pavlacka Last published at: May 10th, 2022 Problem When you attempt to rerun an Apache Spark write operation by cancelling the currently running job, the following error occurs: Error: org.apache.spark.sql.AnalysisException: Cannot create the managed table('`testdb`.` testtable`').\r\nThe associated location ('dbfs:/user/hive/warehouse/testdb.db/metastore_cache_ testtable) already exists.; Cause This problem is due to a change in the default behavior of Spark in version 2.4. This problem can occur if: The cluster is terminated while a write operation is in progress. A temporary network issue occurs. The job is interrupted. Once the metastore data for a particular table is corrupted, it is hard to recover except by dropping the files in that location manually. Basically, the problem is that a metadata directory called _STARTED isn’t deleted automatically when Databricks tries to overwrite it. You can reproduce the problem by following these steps: Create a DataFrame: val df = spark.range(1000) Write the DataFrame to a location in overwrite mode: df.write.mode(SaveMode.Overwrite).saveAsTable(\"testdb.testtable\") Cancel the command while it is executing. Re-run the write command. Solution Set the flag spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation to true. This flag deletes the _STARTED directory and returns the process to the original state. For example, you can set it in the notebook: %python\r\n\r\nspark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\") Or you can set it in the cluster level Spark config (AWS | Azure | GCP): spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation true Another option is to manually clean up the data directory specified in the error message. You can do this with dbutils.fs.rm. %scala\r\n\r\ndbutils.fs.rm(\"<path-to-directory>\", true) Was this article helpful? (7) (13) Additional Informations Related Articles Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... Job cluster limits on notebook output Problem You are running a notebook on a job cluster and you get an error message ... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Related Articles Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... Job cluster limits on notebook output Problem You are running a notebook on a job cluster and you get an error message ... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Create table in overwrite mode fails when interrupted",
          "view_href" : "https://kb.databricks.com/en_US/jobs/spark-overwrite-cancel"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/metastore/hive-metastore-troubleshooting",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Metastore How to troubleshoot several Apache Hive metastore problems How to troubleshoot several Apache Hive metastore problems Learn how to troubleshoot Apache Hive metastore problems. Written by Adam Pavlacka Last published at: May 16th, 2022 Problem 1: External metastore tables not available When you inspect the driver logs, you see a stack trace that includes the error Required table missing: WARN Query: Query for candidates of org.apache.hadoop.hive.metastore.model.MDatabase and subclasses resulted in no possible candidates\r\n\r\nRequired table missing: \"DBS\" in Catalog \"\" Schema \"\". DataNucleus requires this table to perform its\r\npersistence operations. Either your MetaData is incorrect, or you need to enable\r\n\"datanucleus.schema.autoCreateTables\"\r\n\r\norg.datanucleus.store.rdbms.exceptions.MissingTableException: Required table missing : \"DBS\" in Catalog \"\"  Schema \"\". DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable\r\n\"datanucleus.schema.autoCreateTables\"\r\n\r\n   at\r\n\r\norg.datanucleus.store.rdbms.table.AbstractTable.exists(AbstractTable.java:606)\r\n\r\n   at\r\n\r\norg.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.performTablesValidation(RDBMSStoreManager.java:33\r\n85) Cause The database is present, but there are no metastore tables. Solution If the external metastore version is Hive 2.0 or above, use the Hive Schema Tool to create the metastore tables. For versions below Hive 2.0, add the metastore tables with the following configurations in your existing init script: spark.hadoop.datanucleus.autoCreateSchema=true\r\nspark.hadoop.datanucleus.fixedDatastore=false You can also set these configurations in the Apache Spark config (AWS | Azure) directly: datanucleus.autoCreateSchema true\r\ndatanucleus.fixedDatastore false Problem 2: Hive metastore verification failed When you inspect the driver logs, you see a stack trace that includes an error like the following: 18/09/24 14:51:07 ERROR RetryingHMSHandler: HMSHandler Fatal error:\r\nMetaException(message:Version information not found in metastore. )\r\n\r\n   at\r\norg.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore\r\n.java:7564)\r\n\r\n   at\r\norg.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.\r\njava:7542)\r\n\r\n   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) Cause The VERSION table in the metastore is empty. Solution Do one of the following: Populate the VERSION table with the correct version values using an INSERT query. Set the following configurations to turn off the metastore verification in the Spark configuration of the cluster: hive.metastore.schema.verification false\r\nhive.metastore.schema.verification.record.version false Problem 3: Metastore connection limit exceeded Commands run on the cluster fail with the following stack trace in the driver logs: Unable to open a test connection to the given\r\ndatabase. JDBC url =\r\njdbc:<jdbcURL>?trustServerCertificate=true&useSS\r\nL=true, username = <REDACTED>. Terminating\r\nconnection pool (set lazyInit to true if you\r\nexpect to start your database after your app).\r\nOriginal Exception: ------\r\n\r\njava.sql.SQLSyntaxErrorException: User\r\n'<userId>' has exceeded the\r\n'max_user_connections' resource (current value:\r\n100)\r\nat\r\norg.mariadb.jdbc.internal.util.exceptions.Except\r\nionMapper.get(ExceptionMapper.java:163)\r\nat\r\norg.mariadb.jdbc.internal.util.exceptions.Except\r\nionMapper.getException(ExceptionMapper.java:106)\r\nat\r\norg.mariadb.jdbc.internal.protocol.AbstractConne\r\nctProtocol.connectWithoutProxy(AbstractConnectPr\r\notocol.java:1036) Cause The metastore configuration allows only 100 connections. When the connection limit is reached, new connections are not allowed, and commands fail with this error. Each cluster in the Databricks workspace establishes a connection with the metastore. If you have a large number of clusters running, then this issue can occur. Additionally, incorrect configurations can cause a connection leak, causing the number of connections to keep increasing until the limit is reached. Solution Correct the problem with one of the following actions: If you are using an external metastore and you have a large number of clusters running, then increase the connection limit on your external metastore. If you are not using an external metastore, ensure that you do not have any custom Hive metastore configurations on your cluster. When using the metastore provided by Databricks, you should use the default configurations on the cluster for the Hive metastore. If you are using the default configuration and still encounter this issue, contact Databricks Support. Depending on the configuration of your Databricks workspace, it might be possible to increase the number of connections allowed to the internal metastore. Problem 4: Table actions fail because column has too much metadata When the quantity of metadata for a single column exceeds 4000 characters, table actions fail with an error like this: Error in SQL statement: IllegalArgumentException:\r\nError: type expected at the position 3998 of 'struct<num_ad_accounts:bigint,num_benchmarks:bigint,num_days_range:string,num_days_in_history:string,num_fb_pages:bigint,num_g_profiles:bigint,num_ga_views:bigint,num_groups:bigint,num_ig_profiles:bigint,num_li_pages:bigint,num_labels:string,num_labels_added:bigint,num_labels_ Cause This is a bug that was fixed in Hive Metastore version 2.3.0 (HIVE-12274). Databricks uses an earlier version of Hive Metastore (version 0.13), so this bug occurs when there is too much metadata for a column, such as an imported JSON schema. Solution As a workaround, set up an external Hive metastore (AWS | Azure) that uses version 2.3.0 or above. Then delete the existing table with the following command: %scala\r\n\r\nspark.sessionState\r\n  .catalog\r\n  .externalCatalog\r\n  .dropTable(\"default\", \"test_table_tabledrop_1\", ignoreIfNotExists = false, purge = false) Was this article helpful? (8) (11) Additional Informations Related Articles Error in CREATE TABLE with external Hive metastore Problem You are connecting to an external MySQL metastore and attempting to creat... Autoscaling is slow with an external metastore Problem You have an external metastore configured on your cluster and autoscaling... How to create table DDLs to import into an external metastore Databricks supports using external metastores instead of the default Hive metasto... Drop database without deletion By default, the DROP DATABASE (AWS | Azure | GCP) command drops the database and ... Related Articles Error in CREATE TABLE with external Hive metastore Problem You are connecting to an external MySQL metastore and attempting to creat... Autoscaling is slow with an external metastore Problem You have an external metastore configured on your cluster and autoscaling... How to create table DDLs to import into an external metastore Databricks supports using external metastores instead of the default Hive metasto... Drop database without deletion By default, the DROP DATABASE (AWS | Azure | GCP) command drops the database and ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to troubleshoot several Apache Hive metastore problems",
          "view_href" : "https://kb.databricks.com/en_US/metastore/hive-metastore-troubleshooting"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/cluster-terminated-driver-down",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Cluster fails to start with dummy does not exist error Cluster fails to start with dummy does not exist error Cluster is not starting due to a `dummy does not exist` Apache Spark error message. Written by arvind.ravish Last published at: March 4th, 2022 Problem You try to start a cluster, but it fails to start. You get an Apache Spark error message. Internal error message: Spark error: Driver down You review the cluster driver and worker logs and see an error message containing java.io.FileNotFoundException: File file:/databricks/driver/dummy does not exist. 21/07/14 21:44:06 ERROR DriverDaemon$: XXX Fatal uncaught exception. Terminating driver.\r\njava.io.FileNotFoundException: File file:/databricks/driver/dummy does not exist\r\n   at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\r\n   at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\r\n   at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\r\n   at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\r\n   at org.apache.spark.SparkContext.addFile(SparkContext.scala:1668)\r\n   at org.apache.spark.SparkContext.addFile(SparkContext.scala:1632)\r\n   at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:511)\r\n   at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:511)\r\n   at scala.collection.immutable.List.foreach(List.scala:392) Cause You have spark.files dummy set in your Spark Config, but no such file exists. Spark interprets the dummy configuration value as a valid file path and tries to find it on the local file system. If the file does not exist, it generates the error message. java.io.FileNotFoundException: File file:/databricks/driver/dummy does not exist Solution Option 1: Delete spark.files dummy from your Spark Config if you are not passing actual files to Spark. Option 2: Create a dummy file and place it on the cluster. You can do this with an init script. Create the init script. %python\r\ndbutils.fs.put(\"dbfs:/databricks/<init-script-folder>/create_dummy_file.sh\",\r\n\"\"\"\r\n#!/bin/bash\r\ntouch /databricks/driver/dummy\"\"\", True) Install the init script that you just created as a cluster-scoped init script. You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/create_dummy_file.sh). Restart the cluster Restart your cluster after you have installed the init script. Was this article helpful? (10) (17) Additional Informations Related Articles How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Admin user cannot restart cluster to run job Problem When a user who has permission to start a cluster, such as a Databricks A... Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Related Articles How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Admin user cannot restart cluster to run job Problem When a user who has permission to start a cluster, such as a Databricks A... Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cluster fails to start with dummy does not exist error",
          "view_href" : "https://kb.databricks.com/en_US/clusters/cluster-terminated-driver-down"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/r/namespace-onload",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles R with Apache Spark Resolving package or namespace loading error Resolving package or namespace loading error Learn how to resolve package or namespace loading errors in a Databricks notebook. Written by Adam Pavlacka Last published at: May 20th, 2022 This article explains how to resolve a package or namespace loading error. Problem When you install and load some libraries in a notebook cell, like: %r\r\n\r\nlibrary(BreakoutDetection) You may get a package or namespace error: Loading required package: BreakoutDetection:\r\n\r\nError : package or namespace load failed for ‘BreakoutDetection’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\r\nnamespace ‘rlang’ 0.3.1 is already loaded, but >= 0.3.4 is required Cause While a notebook is attached to a cluster, the R namespace cannot be refreshed. When an R package depends on a newer package version, the required package is downloaded but not loaded. When you load the package, you can observe this error. Solution To resolve this error, install the required package as a cluster-installed library (AWS | Azure | GCP). Was this article helpful? (9) (12) Additional Informations Related Articles How to persist and share code in RStudio Problem Unlike a Databricks notebook that has version control built in, code deve... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... Rendering an R markdown file containing sparklyr code fails Problem After you install and configure RStudio in the Databricks environment, wh... Related Articles How to persist and share code in RStudio Problem Unlike a Databricks notebook that has version control built in, code deve... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... Rendering an R markdown file containing sparklyr code fails Problem After you install and configure RStudio in the Databricks environment, wh... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Resolving package or namespace loading error",
          "view_href" : "https://kb.databricks.com/en_US/r/namespace-onload"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/security/forbidden-access-to-s3-data",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Security and permissions Forbidden error while accessing S3 data Forbidden error while accessing S3 data Written by Adam Pavlacka Last published at: May 17th, 2022 Problem While trying to access S3 data using DBFS mount or directly in Spark APIs, the command fails with an exception similar to the following: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; Request ID: XXXXXXXXXXXXX,\r\nExtended Request ID: XXXXXXXXXXXXXXXXXXX, Cloud Provider: AWS, Instance ID: XXXXXXXXXX\r\n(Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: XXXXXXXXXXX; S3\r\nExtended Request ID: Cause Below are the common causes: AWS keys are used in addition to the IAM role. Using global init scripts to set the AWS keys can cause this behavior. The IAM role has the required permission to access the S3 data, but AWS keys are set in the Spark configuration. For example, setting spark.hadoop.fs.s3a.secret.key can conflict with the IAM role. Setting AWS keys at environment level on the driver node from an interactive cluster through a notebook. DBFS mount points were created earlier with AWS keys and now trying to access using an IAM role. The files are written outside Databricks, and the bucket owner does not have read permission (see Step 7: Update cross-account S3 object ACLs). The IAM role is not attached to the cluster. The IAM role with read permission was attached, but you are trying to perform a write operation. That is, the IAM role does not have adequate permission for the operation you are trying to perform. Solution Below are the recommendations and best practices to avoid this issue: Use IAM roles instead of AWS keys. If you are trying to switch the configuration from AWS keys to IAM roles, unmount the DBFS mount points for S3 buckets created using AWS keys and remount using the IAM role. Avoid using global init script to set AWS keys. Always use a cluster-scoped init script if required. Avoid setting AWS keys in a notebook or cluster Spark configuration. Was this article helpful? (15) (44) Additional Informations Related Articles Table creation fails with security exception Problem You attempt to create a table using a cluster that has Table ACLs enabled... Troubleshoot key vault access issues You are trying to access secrets, when you get an error message. com.databricks.c... Related Articles Table creation fails with security exception Problem You attempt to create a table using a cluster that has Table ACLs enabled... Troubleshoot key vault access issues You are trying to access secrets, when you get an error message. com.databricks.c... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Forbidden error while accessing S3 data",
          "view_href" : "https://kb.databricks.com/en_US/security/forbidden-access-to-s3-data"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/visualizations/save-plotly-to-dbfs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Visualizations How to save Plotly files and display From DBFS How to save Plotly files and display From DBFS Learn how to save Plotly files and display them from DBFS. Written by Adam Pavlacka Last published at: May 19th, 2022 You can save a chart generated with Plotly to the driver node as a jpg or png file. Then, you can display it in a notebook by using the displayHTML() method. By default, you save Plotly charts to the /databricks/driver/ directory on the driver node in your cluster. Use the following procedure to display the charts at a later time. Generate a sample plot: %python\r\n\r\ndata = {'data': [{'y': [4, 2, 3, 4]}],\r\n            'layout': {'title': 'Test Plot',\r\n                       'font': dict(size=16)}}\r\np = plot(data,output_type='div')\r\ndisplayHTML(p) Save the generated plot to a file with plotly.io.write_image(): %sh\r\n\r\nplotly.io.write_image(fig=data,file=\"/databricks/driver/plotly_images/<imageName>.jpg\", format=\"jpeg\",scale=None, width=None, height=None) Copy the file from the driver node and save it to DBFS: %sh\r\n\r\ndbutils.fs.cp(\"file:/databricks/driver/plotly_images/<imageName>.jpg\", \"dbfs:/FileStore/<your_folder_name>/<imageName>.jpg\") Display the image using displayHTML(): %sh\r\n\r\ndisplayHTML('''<img src=\"/files/<your_folder_name>/<imageName>.jpg\">''') See also Plotly in Python and R Notebooks. Was this article helpful? (13) (18) Additional Informations Related Articles Related Articles © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to save Plotly files and display From DBFS",
          "view_href" : "https://kb.databricks.com/en_US/visualizations/save-plotly-to-dbfs"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/python-repl-fails-dcs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Python REPL fails to start in Docker Python REPL fails to start in Docker Learn how to fix a Python virtualenv error that prevents REPL from starting in a Docker container Written by arjun.kaimaparambilrajan Last published at: May 19th, 2022 Problem When you use a Docker container that includes prebuilt Python libraries, Python commands fail and the virtual environment is not created. The following error message is visible in the driver logs. 20/02/29 16:38:35 WARN PythonDriverWrapper: Failed to start repl ReplId-5b591-0ce42-78ef3-7\r\njava.io.IOException: Cannot run program \"/local_disk0/pythonVirtualEnvDirs/virtualEnv-56a5be60-3e71-486f-ac04-08e8f2491032/bin/python\" (in directory \".\"): error=2, No such file or directory\r\n        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n        at org.apache.spark.util.Utils$.executeCommand(Utils.scala:1367)\r\n        at org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:1393)\r\n        at org.apache.spark.util.Utils$.executePythonAndGetOutput(Utils.scala:\r\n…\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: error=2, No such file or directory\r\n        at java.lang.UNIXProcess.forkAndExec(Native Method)\r\n        at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\r\n        at java.lang.ProcessImpl.start(ProcessImpl.java:134)\r\n        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n        ... 17 more You can confirm the issue by running the following command in a notebook: %sh\r\n\r\nvirtualenv --no-site-packages The result is an error message similar to the following: usage: virtualenv [--version] [--with-traceback] [-v | -q] [--discovery {builtin}] [-p py] [--creator {builtin,cpython3-posix,venv}] [--seeder {app-data,pip}] [--no-seed] [--activators comma_separated_list] [--clear]\r\n                  [--system-site-packages] [--symlinks | --copies] [--download | --no-download] [--extra-search-dir d [d ...]] [--pip version] [--setuptools version] [--wheel version] [--no-pip] [--no-setuptools] [--no-wheel]\r\n                  [--clear-app-data] [--symlink-app-data] [--prompt prompt] [-h]\r\n                  dest\r\nvirtualenv: error: the following arguments are required: dest The virtualenv command does not recognize the --no-site-packages option. Version The problem affects all current Databricks Runtime versions, except for Databricks Runtime versions that include Conda. It affects virtualenv library version 20.0.0 and above. Cause This issue is caused by using a Python virtualenv library version in the Docker container that does not support the --no-site-packages option. Databricks Runtime requires a virtualenv library that supports the --no-site-packages option. This option was removed in virtualenv library version 20.0.0 and above. You can verify your virtualenv library version by running the following command in a notebook: %sh \r\n\r\nvirtualenv --version Solution You can resolve the issue by specifying a compatible version when you install the virtualenv library. For example, setting virtualenv==16.0.0 in the Dockerfile installs virtualenv library version 16.0.0. This version of the library supports the required option. Was this article helpful? (7) (13) Additional Informations Related Articles Cluster cancels Python command execution after installing Bokeh Problem The cluster returns Cancelled in a Python notebook. Inspect the driver lo... Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... Cluster cancels Python command execution due to library conflict Problem The cluster returns Cancelled in a Python notebook. Notebooks in all othe... Job remains idle before starting Problem You have an Apache Spark job that is triggered correctly, but remains idl... Related Articles Cluster cancels Python command execution after installing Bokeh Problem The cluster returns Cancelled in a Python notebook. Inspect the driver lo... Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... Cluster cancels Python command execution due to library conflict Problem The cluster returns Cancelled in a Python notebook. Notebooks in all othe... Job remains idle before starting Problem You have an Apache Spark job that is triggered correctly, but remains idl... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Python REPL fails to start in Docker",
          "view_href" : "https://kb.databricks.com/en_US/python/python-repl-fails-dcs"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/bi/jdbc-odbc-troubleshooting",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Business intelligence tools Troubleshooting JDBC and ODBC connections Troubleshooting JDBC and ODBC connections Learn how to troubleshoot Databricks JDBC and ODBC connection errors. Written by Adam Pavlacka Last published at: June 23rd, 2022 This article provides information to help you troubleshoot the connection between your Databricks JDBC/ODBC server and BI tools and data sources. Fetching result set is slow after statement execution After a query execution, you can fetch result rows by calling the next() method on the returned ResultSet repeatedly. This method triggers a request to the driver Thrift server to fetch a batch of rows back if the buffered ones are exhausted. We found the size of the batch significantly affects the performance. The default value in the most of the JDBC/ODBC drivers is too conservative, and we recommend that you set it to at least 100,000. Contact the BI tool provider if you cannot access this configuration. Timeout/Exception when creating the connection Once you have the server hostname, you can run the following tests from a terminal to check for connectivity to the warehouse. curl https://<server-hostname>:<port>/sql/protocolv1/o/0/<cluster-id> -H \"Authorization: Basic $(echo -n 'token:<personal-access-token>' | base64)\" If the connection times out, check whether your network settings of the connection are correct. TTransportException If the response contains a TTransportException (the error is expected) like the following, it means that the gateway is functioning properly and you have passed in valid credentials. If you are not able to connect with the same credentials, check that the client you are using is properly configured and is using the latest Simba drivers (version >= 2.6.22): <h2>HTTP ERROR: 500</h2>\r\n<p>Problem accessing /cliservice. Reason:\r\n<pre> javax.servlet.ServletException: org.apache.thrift.transport.TTransportException</pre></p> Referencing temporary views If the response contains the message Table or view not found: SPARK..temp_view it means that a temporary view is not properly referenced in the client application. Simba has an internal configuration parameter called UseNativeQuery that decides whether the query is translated or not before being submitted to the Thrift server. By default, the parameter is set to 0, in which case Simba can modify the query. In particular, Simba creates a custom #temp schema for temporary views and it expects the client application to reference a temporary view with this schema. You can avoid using this special alias by setting UseNativeQuery=1, which prevents Simba from modifying the query. In this case, Simba sends the query directly to the Thrift server. However, the client needs to make sure that the queries are written in the dialect that Spark expects, that is, HiveQL. To sum up, you have the following options to handle temporary views over Simba and Spark: UseNativeQuery=0 and reference the view by prefixing its name with #temp. UseNativeQuery=1 and make sure the query is written in the dialect that Spark expects. Other errors If you get the error 401 Unauthorized, check the credentials you are using: <h2>HTTP ERROR: 401</h2>\r\n<p>Problem accessing /sql/protocolv1/o/0/test-cluster. Reason:\r\n<pre>    Unauthorized</pre></p>If you use a personal access token to authenticate, verify that the username is token (not your username) and the password is a personal access token (the token should start with dapi). Responses such as 404, Not Foundusually indicate problems with locating the specified cluster: <h2>HTTP ERROR: 404</h2>\r\n<p>Problem accessing /sql/protocolv1/o/0/missing-cluster. Reason:\r\n<pre>    RESOURCE_DOES_NOT_EXIST: No cluster found matching: missing-cluster</pre></p> If you see the following errors in your application log4j logs: log4j:ERROR A \"org.apache.log4j.FileAppender\" object is not assignable to a \"com.simba.spark.jdbc42.internal.apache.log4j.Appender\" variable.You can ignore these errors. The Simba internal log4j library is shaded to avoid conflicts with the log4j library in your application. However, Simba may still load the log4j configuration of your application, and attempt to use some custom log4j appenders. This attempt fails with the shaded library. Relevant information is still captured in the logs. Was this article helpful? (10) (38) Additional Informations Related Articles Configure Simba ODBC driver with a proxy in Windows In this article you learn how to configure the Databricks ODBC Driver when your l... Related Articles Configure Simba ODBC driver with a proxy in Windows In this article you learn how to configure the Databricks ODBC Driver when your l... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Troubleshooting JDBC and ODBC connections",
          "view_href" : "https://kb.databricks.com/en_US/bi/jdbc-odbc-troubleshooting"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/set-instance-profile-arn-optional-policy",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Set instance_profile_arn as optional with a cluster policy Set instance_profile_arn as optional with a cluster policy Use a cluster policy to set the AWS attribute instance_profile_arn as optional. Written by ravirahul.padmanabhan Last published at: March 4th, 2022 In this article, we review the steps to create a cluster policy for the AWS attribute instance_profile_arn and define it as optional. This allows you to start a cluster with a specific AWS instance profile. You can also start a cluster without an instance profile. Delete Note You must be an admin user in order to manage cluster policies. Create a new cluster policy Open your Databricks workspace. Click Compute. Click Cluster Policies. Click Create Cluster Policy. Enter a Name for the policy. Enter this JSON code in the Definitions field. {\r\n  \"aws_attributes.instance_profile_arn\": {\r\n    \"type\": \"allowlist\",\r\n    \"values\": [\r\n      \"arn:aws:iam::123456789012:instance-profile/allow-this-role\"\r\n    ],\r\n    \"isOptional\": true\r\n  }\r\n} Click Permissions. Assign the new policy to users in your workspace. Click Create. You can now create a new cluster using the policy. Edit an existing cluster policy Open your Databricks workspace. Click Compute. Click Cluster Policies. Click on an existing policy. Click Edit. Add this JSON code to the policy in the Definitions field. {\r\n  \"aws_attributes.instance_profile_arn\": {\r\n    \"type\": \"allowlist\",\r\n    \"values\": [\r\n      \"arn:aws:iam::123456789012:instance-profile/allow-this-role\"\r\n    ],\r\n    \"isOptional\": true\r\n  }\r\n} Click Permissions. Verify that the policy is assigned to the correct users in your workspace. Click Update. You can now create a new cluster using the policy. Was this article helpful? (9) (13) Additional Informations Related Articles IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... Cluster Apache Spark configuration not applied Problem Your cluster’s Spark configuration values are not applied. Cause This hap... Failed to create cluster with invalid tag value Problem You are trying to create a cluster, but it is failing with an invalid tag... Persist Apache Spark CSV metrics to a DBFS location Spark has a configurable metrics system that supports a number of sinks, includin... Related Articles IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... Cluster Apache Spark configuration not applied Problem Your cluster’s Spark configuration values are not applied. Cause This hap... Failed to create cluster with invalid tag value Problem You are trying to create a cluster, but it is failing with an invalid tag... Persist Apache Spark CSV metrics to a DBFS location Spark has a configurable metrics system that supports a number of sinks, includin... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Set instance_profile_arn as optional with a cluster policy",
          "view_href" : "https://kb.databricks.com/en_US/clusters/set-instance-profile-arn-optional-policy"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data-sources/create-table-json-serde",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data sources Create tables on JSON datasets Create tables on JSON datasets Create tables on JSON datasets; requires SerDe JAR. Written by ram.sankarasubramanian Last published at: May 31st, 2022 In this article we cover how to create a table on JSON datasets using SerDe. Download the JSON SerDe JAR Open the hive-json-serde 1.3.8 download page. Click on json-serde-1.3.8-jar-with-dependencies.jar to download the file json-serde-1.3.8-jar-with-dependencies.jar. Delete Info You can review the Hive-JSON-Serde GitHub repo for more information on the JAR, including source code. Install the JSON SerDe JAR on your cluster Select your cluster in the workspace. Click the Libraries tab. Click Install new. In the Library Source button list, select Upload. In the Library Type button list, select JAR. Click Drop JAR here. Select the json-serde-1.3.8-jar-with-dependencies.jar file. Click Install. Configure SerDe properties in the create table statement %sql\r\n\r\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\r\nSTORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\r\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\r\nLOCATION '<path-to-json-files>' For example: %sql\r\n\r\ncreate table <name-of-table> (timestamp_unix string, comments string, start_date string, end_date string)\r\npartitioned by (yyyy string, mm string, dd string)\r\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\r\nSTORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\r\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\r\nLOCATION '<path-to-json-files>'\r\nThis example creates a table that is partitioned by the columns yyyy, mm, and dd. Run a repair table statement after the table is created For example: %sql\r\n\r\nmsck repair table <name-of-table> Was this article helpful? (11) (19) Additional Informations Related Articles Delete table when underlying S3 bucket is deleted Problem You are trying to drop or alter a table when you get an error. Error in S... Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... Unable to read files and list directories in a WASB filesystem Problem When you try reading a file on WASB with Spark, you get the following exc... Optimize read performance from JDBC data sources Problem Reading data from an external JDBC database is slow. How can I improve re... Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2 Problem Info In general, you should use Databricks Runtime 5.2 and above, which i... Related Articles Delete table when underlying S3 bucket is deleted Problem You are trying to drop or alter a table when you get an error. Error in S... Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... Unable to read files and list directories in a WASB filesystem Problem When you try reading a file on WASB with Spark, you get the following exc... Optimize read performance from JDBC data sources Problem Reading data from an external JDBC database is slow. How can I improve re... Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2 Problem Info In general, you should use Databricks Runtime 5.2 and above, which i... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Create tables on JSON datasets",
          "view_href" : "https://kb.databricks.com/en_US/data-sources/create-table-json-serde"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/running-c-plus-plus-code",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Run C++ code in Python Run C++ code in Python Learn how to run C++ code in Python. Written by Adam Pavlacka Last published at: May 19th, 2022 Run C++ from Python example notebook Review the Run C++ from Python notebook to learn how to compile C++ code and run it on a cluster. Was this article helpful? (10) (9) Additional Informations Related Articles Cluster cancels Python command execution after installing Bokeh Problem The cluster returns Cancelled in a Python notebook. Inspect the driver lo... Job remains idle before starting Problem You have an Apache Spark job that is triggered correctly, but remains idl... Install and compile Cython This document explains how to run Spark code with compiled Cython code. The steps... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... Related Articles Cluster cancels Python command execution after installing Bokeh Problem The cluster returns Cancelled in a Python notebook. Inspect the driver lo... Job remains idle before starting Problem You have an Apache Spark job that is triggered correctly, but remains idl... Install and compile Cython This document explains how to run Spark code with compiled Cython code. The steps... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Run C++ code in Python",
          "view_href" : "https://kb.databricks.com/en_US/python/running-c-plus-plus-code"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/list-delete-files-faster",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management How to list and delete files faster in Databricks How to list and delete files faster in Databricks Learn how to list and delete files faster in Databricks. Written by Adam Pavlacka Last published at: May 31st, 2022 Scenario Suppose you need to delete a table that is partitioned by year, month, date, region, and service. However, the table is huge, and there will be around 1000 part files per partition. You can list all the files in each partition and then delete them using an Apache Spark job. For example, suppose you have a table that is partitioned by a, b, and c: %scala\r\n\r\nSeq((1,2,3,4,5),\r\n  (2,3,4,5,6),\r\n  (3,4,5,6,7),\r\n  (4,5,6,7,8))\r\n  .toDF(\"a\", \"b\", \"c\", \"d\", \"e\")\r\n  .write.mode(\"overwrite\")\r\n  .partitionBy(\"a\", \"b\", \"c\")\r\n  .parquet(\"/mnt/path/table\") List files You can list all the part files using this function: %scala\r\n\r\nimport org.apache.hadoop.conf.Configuration\r\nimport org.apache.hadoop.fs.{Path, FileSystem}\r\nimport org.apache.spark.deploy.SparkHadoopUtil\r\nimport org.apache.spark.sql.execution.datasources.InMemoryFileIndex\r\nimport java.net.URI\r\n\r\ndef listFiles(basep: String, globp: String): Seq[String] = {\r\n  val conf = new Configuration(sc.hadoopConfiguration)\r\n  val fs = FileSystem.get(new URI(basep), conf)\r\n\r\n  def validated(path: String): Path = {\r\n    if(path startsWith \"/\") new Path(path)\r\n    else new Path(\"/\" + path)\r\n  }\r\n\r\n  val fileCatalog = InMemoryFileIndex.bulkListLeafFiles(\r\n    paths = SparkHadoopUtil.get.globPath(fs, Path.mergePaths(validated(basep), validated(globp))),\r\n    hadoopConf = conf,\r\n    filter = null,\r\n    sparkSession = spark, areRootPaths=true)\r\n\r\n // If you are using Databricks Runtime 6.x and below,\r\n // remove <areRootPaths=true> from the bulkListLeafFiles function parameter.\r\n\r\n  fileCatalog.flatMap(_._2.map(_.path))\r\n}\r\n\r\nval root = \"/mnt/path/table\"\r\nval globp = \"[^_]*\" // glob pattern, e.g. \"service=webapp/date=2019-03-31/*log4j*\"\r\n\r\nval files = listFiles(root, globp)\r\nfiles.toDF(\"path\").show() +------------------------------------------------------------------------------------------------------------------------------+\r\n|path                                                                                                                          |\r\n+------------------------------------------------------------------------------------------------------------------------------+\r\n|dbfs:/mnt/path/table/a=1/b=2/c=3/part-00000-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-5.c000.snappy.parquet|\r\n|dbfs:/mnt/path/table/a=2/b=3/c=4/part-00001-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-6.c000.snappy.parquet|\r\n|dbfs:/mnt/path/table/a=3/b=4/c=5/part-00002-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-7.c000.snappy.parquet|\r\n|dbfs:/mnt/path/table/a=4/b=5/c=6/part-00003-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-8.c000.snappy.parquet|\r\n+------------------------------------------------------------------------------------------------------------------------------+ The listFiles function takes a base path and a glob path as arguments, scans the files and matches with the glob pattern, and then returns all the leaf files that were matched as a sequence of strings. The function also uses the utility function globPath from the SparkHadoopUtil package. This function lists all the paths in a directory with the specified prefix, and does not further list leaf children (files). The list of paths is passed into InMemoryFileIndex.bulkListLeafFiles method, which is a Spark internal API for distributed file listing. Neither of these listing utility functions work well alone. By combining them you can get a list of top-level directories that you want to list using globPath function, which will run on the driver, and you can distribute the listing for all child leaves of the top-level directories into Spark workers using bulkListLeafFiles. The speed-up can be around 20-50x faster according to Amdahl’s law. The reason is that, you can easily control the glob path according to the real file physical layout and control the parallelism through spark.sql.sources.parallelPartitionDiscovery.parallelism for InMemoryFileIndex. Delete files When you delete files or partitions from an unmanaged table, you can use the Databricks utility function dbutils.fs.rm. This function leverages the native cloud storage file system API, which is optimized for all file operations. However, you can’t delete a gigantic table directly using dbutils.fs.rm(\"path/to/the/table\"). You can list files efficiently using the script above. For smaller tables, the collected paths of the files to delete fit into the driver memory, so you can use a Spark job to distribute the file deletion task. For gigantic tables, even for a single top-level partition, the string representations of the file paths cannot fit into the driver memory. The easiest way to solve this problem is to collect the paths of the inner partitions recursively, list the paths, and delete them in parallel. %scala\r\n\r\nimport scala.util.{Try, Success, Failure}\r\n\r\ndef delete(p: String): Unit = {\r\n  dbutils.fs.ls(p).map(_.path).toDF.foreach { file =>\r\n    dbutils.fs.rm(file(0).toString, true)\r\n    println(s\"deleted file: $file\")\r\n  }\r\n}\r\n\r\nfinal def walkDelete(root: String)(level: Int): Unit = {\r\n  dbutils.fs.ls(root).map(_.path).foreach { p =>\r\n    println(s\"Deleting: $p, on level: ${level}\")\r\n    val deleting = Try {\r\n      if(level == 0) delete(p)\r\n      else if(p endsWith \"/\") walkDelete(p)(level-1)\r\n      //\r\n      // Set only n levels of recursion, so it won't be a problem\r\n      //\r\n      else delete(p)\r\n    }\r\n    deleting match {\r\n      case Success(v) => {\r\n        println(s\"Successfully deleted $p\")\r\n        dbutils.fs.rm(p, true)\r\n      }\r\n      case Failure(e) => println(e.getMessage)\r\n    }\r\n  }\r\n} The code deletes inner partitions while ensuring that the partition that is being deleted is small enough. It does this by searching through the partitions recursively by each level, and only starts deleting when it hits the level you set. For instance, if you want to start with deleting the top-level partitions, use walkDelete(root)(0). Spark will delete all the files under dbfs:/mnt/path/table/a=1/, then delete .../a=2/, following the pattern until it is exhausted. The Spark job distributes the deletion task using the delete function shown above, listing the files with dbutils.fs.ls with the assumption that the number of child partitions at this level is small. You can also be more efficient by replacing the dbutils.fs.ls function with the listFiles function shown above, with only slight modification. Summary These two approaches highlight methods for listing and deleting gigantic tables. They use some Spark utility functions and functions specific to the Databricks environment. Even if you cannot use them directly, you can create your own utility functions to solve the problem in an analogous way. Was this article helpful? (14) (45) Additional Informations Related Articles How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... Hive UDFs This article shows how to create a Hive UDF, register it in Spark, and use it in ... Related Articles How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... Hive UDFs This article shows how to create a Hive UDF, register it in Spark, and use it in ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to list and delete files faster in Databricks",
          "view_href" : "https://kb.databricks.com/en_US/data/list-delete-files-faster"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/job-rate-limit",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Job fails due to job rate limit Job fails due to job rate limit Learn how to resolve Databricks job failures due to job rate limits. Written by Adam Pavlacka Last published at: May 10th, 2022 Problem A Databricks notebook or Jobs API request returns the following error: Error : {\"error_code\":\"INVALID_STATE\",\"message\":\"There were already 1000 jobs created in past 3600 seconds, exceeding rate limit: 1000 job creations per 3600 seconds.\"} Cause This error occurs because the number of jobs per hour exceeds the limit of 1000 established by Databricks to prevent API abuses and ensure quality of service. Solution If you cannot ensure that the number of jobs created in your workspace is less than 1000 per hour, contact Databricks Support to request a higher limit. A job rate limit increase requires at least 20 minutes of downtime. Databricks can increase the job limit maximumJobCreationRate up to 2000. Currently running jobs will be affected while the limit is being increased. Was this article helpful? (11) (19) Additional Informations Related Articles Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... Job cluster limits on notebook output Problem You are running a notebook on a job cluster and you get an error message ... How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... Related Articles Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... Job cluster limits on notebook output Problem You are running a notebook on a job cluster and you get an error message ... How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Job fails due to job rate limit",
          "view_href" : "https://kb.databricks.com/en_US/jobs/job-rate-limit"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/error-download-full-results",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark Error when downloading full results after join Error when downloading full results after join If you have duplicate columns after a join, you will get an error when trying to download the full results. Written by manjunath.swamy Last published at: May 23rd, 2022 Problem You are working with two tables in a notebook. You perform a join. You can preview the output, but when you try to Download full results you get an error. Error in SQL statement: AnalysisException: Found duplicate column(s) when inserting into dbfs:/databricks-results/ Reproduce error Create two tables. %python\r\n\r\nfrom pyspark.sql.functions import *\r\n\r\ndf = spark.range(12000)\r\ndf = df.withColumn(\"col2\",lit(\"test\"))\r\ndf.createOrReplaceTempView(\"table1\")\r\n\r\ndf1 = spark.range(5)\r\ndf1.createOrReplaceTempView(\"table2\") Perform left outer join on the tables. %sql\r\n\r\nselect * from table1 t1 left join table2 t2 on t1.id = t2.id Click Download preview. A CSV file downloads. Click Download full results. An error is generated. Cause Download preview works because this is a frontend only operation that runs in the browser. No constraints are checked and only 1000 rows are included in the CSV file. Download full results re-executes the query in Apache Spark and writes the CSV file internally. The error occurs when duplicate columns are found after a join operation. Solution Option 1 If you select all the required columns, and avoid duplicate columns after the join operation, you will not get the error and can download the full result. %sql\r\n\r\nselect t1.id, t1.col2 from table1 t1 left join table2 t2 on t1.id = t2.id Option 2 You can use DataFrames to prevent duplicated columns. If there are no duplicated columns after the join operation, you will not get the error and can download the full result. %python\r\n\r\nresult_df = df.join(df1, [\"id\"],\"left\")\r\ndisplay(result_df) Was this article helpful? (13) (9) Additional Informations Related Articles Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Duplicate columns in the metadata error Problem Your Apache Spark job is processing a Delta table when the job fails with... Error in SQL statement: AnalysisException: Table or view not found Problem When you try to query a table or view, you get this error: AnalysisExcept... Generate unique increasing numeric values This article shows you how to use Apache Spark functions to generate unique incre... Related Articles Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Duplicate columns in the metadata error Problem Your Apache Spark job is processing a Delta table when the job fails with... Error in SQL statement: AnalysisException: Table or view not found Problem When you try to query a table or view, you get this error: AnalysisExcept... Generate unique increasing numeric values This article shows you how to use Apache Spark functions to generate unique incre... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Error when downloading full results after join",
          "view_href" : "https://kb.databricks.com/en_US/sql/error-download-full-results"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/cluster-failed-launch",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Cluster failed to launch Cluster failed to launch Learn how to resolve cluster launch failures. Written by Adam Pavlacka Last published at: March 4th, 2022 Update Table of Contents Cluster timeoutGlobal or cluster-specific init scriptsToo many libraries installed in cluster UICloud provider limitCloud provider shutdownInstances unreachable (Azure) This article describes several scenarios in which a cluster fails to launch, and provides troubleshooting steps for each scenario based on error messages found in logs. Cluster timeout Error messages: Driver failed to start in time\r\nINTERNAL_ERROR: The Spark driver failed to start within 300 seconds\r\nCluster failed to be healthy within 200 seconds Cause The cluster can fail to launch if it has a connection to an external Hive metastore and it tries to download all the Hive metastore libraries from a Maven repo. A cluster downloads almost 200 JAR files, including dependencies. If the Databricks cluster manager cannot confirm that the driver is ready within 5 minutes, then cluster launch fails. This can occur because JAR downloading is taking too much time. Solution Store the Hive libraries in DBFS and access them locally from the DBFS location. See Spark Options. Global or cluster-specific init scripts Error message: The cluster could not be started in 50 minutes. Cause: Timed out with exception after <xxx> attempts Cause Init scripts that run during the cluster spin-up stage send an RPC (remote procedure call) to each worker machine to run the scripts locally. All RPCs must return their status before the process continues. If any RPC hits an issue and doesn’t respond back (due to a transient networking issue, for example), then the 1-hour timeout can be hit, causing the cluster setup job to fail. Solution Use a cluster-scoped init script instead of global or cluster-named init scripts. With cluster-scoped init scripts, Databricks does not use synchronous blocking of RPCs to fetch init script execution status. Too many libraries installed in cluster UI Error message: Library installation timed out after 1800 seconds. Libraries that are not yet installed: Cause This is usually an intermittent problem due to network problems. Solution Usually you can fix this problem by re-running the job or restarting the cluster. The library installer is configured to time out after 3 minutes. While fetching and installing jars, a timeout can occur due to network problems. To mitigate this issue, you can download the libraries from Maven to a DBFS location and install it from there. Cloud provider limit Error message: Cluster terminated. Reason: Cloud Provider Limit Cause This error is usually returned by the cloud provider. Solution See the cloud provider error information in cluster unexpected termination. Cloud provider shutdown Error message: Cluster terminated. Reason: Cloud Provider Shutdown Cause This error is usually returned by the cloud provider. Solution See the cloud provider error information in cluster unexpected termination. Instances unreachable (Azure) Error message: Cluster terminated. Reason: Instances Unreachable\r\nAn unexpected error was encountered while setting up the cluster. Please retry and contact Azure Databricks if the problem persists. Internal error message: Timeout while placing node Cause This error is usually returned by the cloud provider. Typically, it occurs when you have an Azure Databricks workspace deployed to your own virtual network (VNet) (as opposed to the default VNet created when you launch a new Azure Databricks workspace). If the virtual network where the workspace is deployed is already peered or has an ExpressRoute connection to on-premises resources, the virtual network cannot make an ssh connection to the cluster node when Azure Databricks is attempting to create a cluster. Solution Add a user-defined route (UDR) to give the Azure Databricks control plane ssh access to the cluster instances, Blob Storage instances, and artifact resources. This custom UDR allows outbound connections and does not interfere with cluster creation. For detailed UDR instructions, see Step 3: Create user-defined routes and associate them with your Azure Databricks virtual network subnets. For more VNet-related troubleshooting information, see Troubleshooting. Was this article helpful? (7) (18) Additional Informations Related Articles Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... Cluster Apache Spark configuration not applied Problem Your cluster’s Spark configuration values are not applied. Cause This hap... Related Articles Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... Cluster Apache Spark configuration not applied Problem Your cluster’s Spark configuration values are not applied. Cause This hap... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cluster failed to launch",
          "view_href" : "https://kb.databricks.com/en_US/clusters/cluster-failed-launch"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dbfs/cross-account-write-perms",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Databricks File System (DBFS) Cannot access objects written by Databricks from outside Databricks Cannot access objects written by Databricks from outside Databricks Learn how to resolve a HeadObject operation error and access objects written by Databricks from outside Databricks. Written by Adam Pavlacka Last published at: March 8th, 2022 Problem When you attempt to access an object in an S3 location written by Databricks using the AWS CLI, the following error occurs: ubuntu@0213-174944-clean111-10-93-15-150:~$ aws s3 cp s3://<bucket>/<location>/0/delta/sandbox/deileringDemo__m2/_delta_log/00000000000000000000.json .\r\nfatal error: An error occurred (403) when calling the HeadObject operation: Forbidden Cause S3 access fails because the bucket ACL allows access only to the bucket owner (\"DisplayName\": \"bigdata_dataservices\") or your account (\"DisplayName\": \"infra\"). This is expected behavior if you are trying to access Databricks objects stored in the Databricks File System (DBFS) root directory. The DBFS root bucket is assigned to Databricks for storing metadata, libraries, and so on. Therefore, the object owner (within the Databricks AWS account) is the canonical user ID assigned to the customer. Objects written from a Databricks notebook into the DBFS root bucket receive the following object permissions: {\r\n  \"Owner\": {\r\n    \"DisplayName\": \"infra\",\r\n    \"ID\": \"f65635fc2d277e71b19495a2a74d8170dd035d3e8aa6fc7187696eb42c6c276c\"\r\n  }\r\n} The \"ID\" value identifies a Databricks customer, and by extension the customer's objects in the Databricks account. Solution To access objects in DBFS, use the Databricks CLI, DBFS API, Databricks Utilities, or Apache Spark APIs from within a Databricks notebook. If you need to access data from outside Databricks, migrate the data from the DBFS root bucket to another bucket where the bucket owner can have full control. Indeed, Databricks does not recommend using the DBFS root directory for storing any user files or objects. It is always a best practice to create a different S3 directory and mount it to DBFS. There are two migration scenarios: Scenario 1: The destination Databricks data plane and S3 bucket are in the same AWS account Make sure to attach the IAM role to the cluster where the data is currently located. The cluster needs the IAM role to enable it to write to the destination. Configure Amazon S3 ACL as BucketOwnerFullControl in the Spark configuration: spark.hadoop.fs.s3a.acl.default BucketOwnerFullControl BucketOwnerFullControl recursively calls the putObjectACL property as well. Now you have the correct permissions on the file and can use S3 commands to perform backups. Scenario 2: The destination Databricks data plane and S3 bucket are in different AWS accounts The objects are still owned by Databricks because it is a cross-account write. To avoid this scenario, you can assume a role using instance profiles with an AssumeRole policy. Tips for migrating across accounts using the AWS API or CLI If you are using IAM role instantiation and writing to a cross-account bucket where the Databricks data plane and S3 bucket are in different accounts, call the putObject and the putObject ACL as part of the aws s3api cp command: aws s3api put-object-acl --bucket bucketname --key keyname --acl bucket-owner-full-control Was this article helpful? (16) (45) Additional Informations Related Articles Cannot read Databricks objects stored in the DBFS root directory Problem An Access Denied error returns when you attempt to read Databricks object... How to calculate the Databricks file system (DBFS) S3 API call cost The cost of a DBFS S3 bucket is primarily driven by the number of API calls, and ... How to specify the DBFS path When working with Databricks you will sometimes have to access the Databricks Fil... Operation not supported during append Problem You are attempting to append data to a file saved on an external storage ... Parallelize filesystem operations When you need to speed up copy and move operations, parallelizing them is usually... Related Articles Cannot read Databricks objects stored in the DBFS root directory Problem An Access Denied error returns when you attempt to read Databricks object... How to calculate the Databricks file system (DBFS) S3 API call cost The cost of a DBFS S3 bucket is primarily driven by the number of API calls, and ... How to specify the DBFS path When working with Databricks you will sometimes have to access the Databricks Fil... Operation not supported during append Problem You are attempting to append data to a file saved on an external storage ... Parallelize filesystem operations When you need to speed up copy and move operations, parallelizing them is usually... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cannot access objects written by Databricks from outside Databricks",
          "view_href" : "https://kb.databricks.com/en_US/dbfs/cross-account-write-perms"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/job-fails-maxresultsize-exception",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Apache Spark job fails with maxResultSize exception Apache Spark job fails with maxResultSize exception Learn what to do when an Apache Spark job fails with a maxResultSize exception. Written by Adam Pavlacka Last published at: May 11th, 2022 Problem A Spark job fails with a maxResultSize exception: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of XXXX tasks (X.0 GB) is bigger than spark.driver.maxResultSize (X.0 GB) Cause This error occurs because the configured size limit was exceeded. The size limit applies to the total serialized results for Spark actions across all partitions. The Spark actions include actions such as collect() to the driver node, toPandas(), or saving a large file to the driver local file system. Solution In some situations, you might have to refactor the code to prevent the driver node from collecting a large amount of data. You can change the code so that the driver node collects a limited amount of data or increase the driver instance memory size. For example you can call toPandas with Arrow enabled or writing files and then read those files instead of collecting large amounts of data back to the driver. If absolutely necessary you can set the property spark.driver.maxResultSize to a value <X>g higher than the value reported in the exception message in the cluster Spark config (AWS | Azure): spark.driver.maxResultSize <X>g The default value is 4g. For details, see Application Properties. If you set a high limit, out-of-memory errors can occur in the driver (depending on spark.driver.memory and the memory overhead of objects in the JVM). Set an appropriate limit to prevent out-of-memory errors. Was this article helpful? (6) (17) Additional Informations Related Articles Job fails, but Apache Spark tasks finish Problem Your Databricks job reports a failed status, but all Spark jobs and tasks... Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Apache Spark Jobs hang due to non-deterministic custom UDF Problem Sometimes Apache Spark jobs hang indefinitely due to the non-deterministi... Related Articles Job fails, but Apache Spark tasks finish Problem Your Databricks job reports a failed status, but all Spark jobs and tasks... Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Apache Spark Jobs hang due to non-deterministic custom UDF Problem Sometimes Apache Spark jobs hang indefinitely due to the non-deterministi... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Apache Spark job fails with maxResultSize exception",
          "view_href" : "https://kb.databricks.com/en_US/jobs/job-fails-maxresultsize-exception"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/decimal-is-fractional-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Decimal$DecimalIsFractional assertion error Decimal$DecimalIsFractional assertion error Using `round()` or casing a double to decimal results in a `Decimal$DecimalIsFractional` assertion error. java.lang.AssertionError assertion failed Written by saikrishna.pujari Last published at: May 23rd, 2022 Problem You are running a job on Databricks Runtime 7.x or above when you get a java.lang.AssertionError: assertion failed: Decimal$DecimalIsFractional error message. Example stack trace: java.lang.AssertionError: assertion failed:\r\n Decimal$DecimalIsFractional\r\n  while compiling: <notebook>\r\n   during phase: globalPhase=terminal, enteringPhase=jvm\r\n  library version: version 2.12.10\r\n compiler version: version 2.12.10\r\nreconstructed args: -deprecation -classpath .....\r\n*** WARNING: skipped 126593 bytes of output *** This error message only occurs on the first run of your notebook. Subsequent runs complete without error. Cause There are two common use cases that can trigger this error message. Cause 1: You are trying to use the round() function on a decimal column that contains null values in a notebook. Cause 2: You are casting a double column to a decimal column in a notebook. This example code can be used to reproduce the error: %scala\r\n\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.types._\r\nimport org.apache.spark.sql.{DataFrame, SparkSession}\r\nimport org.apache.spark.sql.Column\r\n\r\n//Sample data with decimal values\r\n\r\nval updateData =  Seq(\r\n                     Row(BigDecimal.decimal(123.456), 123.456),\r\n                     Row(BigDecimal.decimal(123.456), 123.456))\r\n\r\nval updateSchema = List(\r\n                    StructField(\"amt_decimal\", DecimalType(14,3), true),\r\n                    StructField(\"amt_double\", DoubleType, true))\r\n\r\nval testDF =  spark.createDataFrame(\r\n  spark.sparkContext.parallelize(updateData),\r\n  StructType(updateSchema)\r\n)\r\n\r\n// Cause 1:\r\n// round() on the Decimal column reproduces the error\r\n\r\ntestDF.withColumn(\"round_amt_decimal\",round(col(\"amt_decimal\"),2)).show()\r\n\r\n// Cause 2:\r\n// CAST() on the Double column to Decimal reproduces the error\r\n\r\ntestDF.createOrReplaceTempView(\"dec_table\")\r\nspark.sql(\"select CAST(amt_double AS DECIMAL(3,3)) AS dec_col from dec_table\").show() Solution This is a known issue and can be safely ignored. The error message does not halt the notebook run and it should not result in any data loss. Was this article helpful? (15) (14) Additional Informations Related Articles Cannot modify the value of an Apache Spark config Problem You are trying to SET the value of a Spark config in a notebook and get a... Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Related Articles Cannot modify the value of an Apache Spark config Problem You are trying to SET the value of a Spark config in a notebook and get a... Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Decimal$DecimalIsFractional assertion error",
          "view_href" : "https://kb.databricks.com/en_US/scala/decimal-is-fractional-error"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/spark-jar-job-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Multiple Apache Spark JAR jobs fail when run concurrently Multiple Apache Spark JAR jobs fail when run concurrently Apache Spark JAR jobs failing with an AnalysisException error when run concurrently. Written by Adam Pavlacka Last published at: May 23rd, 2022 Problem If you run multiple Apache Spark JAR jobs concurrently, some of the runs might fail with the error: org.apache.spark.sql.AnalysisException: Table or view not found: xxxxxxx; line 1 pos 48 Cause This error occurs due to a bug in Scala. When an object extends App, its val fields are no longer immutable and they can be changed when the main method is called. If you run JAR jobs multiple times, a val field containing a DataFrame can be changed inadvertently. As a result, when any one of the concurrent runs finishes, it wipes out the temporary views of the other runs. Scala issue 11576 provides more detail. Solution To work around this bug, call the main() method explicitly. As an example, if you have code similar to this: %scala\r\n\r\n  object MainTest extends App {\r\n    ...\r\n  } You can replace it with code that does not extend App: %scala\r\n\r\n  object MainTest {\r\n    def main(args: Array[String]) {\r\n    ......\r\n    }\r\n  } Was this article helpful? (6) (14) Additional Informations Related Articles Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Convert nested JSON to a flattened DataFrame This article shows you how to flatten nested JSON, using only $\"column.*\" and exp... Intermittent NullPointerException when AQE is enabled Problem You get an intermittent NullPointerException error when saving your data.... Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... Related Articles Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Convert nested JSON to a flattened DataFrame This article shows you how to flatten nested JSON, using only $\"column.*\" and exp... Intermittent NullPointerException when AQE is enabled Problem You get an intermittent NullPointerException error when saving your data.... Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Multiple Apache Spark JAR jobs fail when run concurrently",
          "view_href" : "https://kb.databricks.com/en_US/scala/spark-jar-job-error"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/schema-from-case-class",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management Generate schema from case class Generate schema from case class Learn how to generate a schema from a Scala case class. Written by Adam Pavlacka Last published at: May 31st, 2022 Spark provides an easy way to generate a schema from a Scala case class. For case class A, use the method ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]. For example: %scala\r\n\r\nimport org.apache.spark.sql.types.StructType\r\nimport org.apache.spark.sql.catalyst.ScalaReflection\r\n\r\ncase class A(key: String, time: java.sql.Timestamp, date: java.sql.Date, decimal: java.math.BigDecimal, map: Map[String, Int], nested: Seq[Map[String, Seq[Int]]])\r\nval schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]\r\nschema.printTreeString Was this article helpful? (7) (17) Additional Informations Related Articles Prevent duplicated columns when joining two DataFrames If you perform a join in Spark and don’t specify your join correctly you’ll end u... Hive UDFs This article shows how to create a Hive UDF, register it in Spark, and use it in ... How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... Revoke all user privileges When user permissions are explicitly granted for individual tables and views, the... Related Articles Prevent duplicated columns when joining two DataFrames If you perform a join in Spark and don’t specify your join correctly you’ll end u... Hive UDFs This article shows how to create a Hive UDF, register it in Spark, and use it in ... How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... Revoke all user privileges When user permissions are explicitly granted for individual tables and views, the... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Generate schema from case class",
          "view_href" : "https://kb.databricks.com/en_US/data/schema-from-case-class"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/hive-udf",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management Hive UDFs Hive UDFs Learn how to create and use a Hive UDF for Databricks. Written by Adam Pavlacka Last published at: May 31st, 2022 This article shows how to create a Hive UDF, register it in Spark, and use it in a Spark SQL query. Here is a Hive UDF that takes a long as an argument and returns its hexadecimal representation. %scala\r\n\r\nimport org.apache.hadoop.hive.ql.exec.UDF\r\nimport org.apache.hadoop.io.LongWritable\r\n\r\n// This UDF takes a long integer and converts it to a hexadecimal string.\r\n\r\nclass ToHex extends UDF {\r\n  def evaluate(n: LongWritable): String = {\r\n    Option(n)\r\n    .map { num =>\r\n        // Use Scala string interpolation. It's the easiest way, and it's\r\n        // type-safe, unlike String.format().\r\n        f\"0x${num.get}%x\"\r\n    }\r\n    .getOrElse(\"\")\r\n  }\r\n} Register the function: %scala\r\n\r\nspark.sql(\"CREATE TEMPORARY FUNCTION to_hex AS 'com.ardentex.spark.hiveudf.ToHex'\") Use your function as any other registered function: %scala\r\n\r\nspark.sql(\"SELECT first_name, to_hex(code) as hex_code FROM people\") You can find more examples and compilable code at the Sample Hive UDF project. Was this article helpful? (9) (40) Additional Informations Related Articles How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... Get and set Apache Spark configuration properties in a notebook In most cases, you set the Spark config (AWS | Azure ) at the cluster level. Howe... Related Articles How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... Get and set Apache Spark configuration properties in a notebook In most cases, you set the Spark config (AWS | Azure ) at the cluster level. Howe... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Hive UDFs",
          "view_href" : "https://kb.databricks.com/en_US/data/hive-udf"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/enable-s3cmd-for-notebooks",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks Enable s3cmd for notebooks Enable s3cmd for notebooks Use an init script to enable s3cmd for use in notebooks. Written by pavan.kumarchalamcharla Last published at: May 16th, 2022 s3cmd is a client library that allows you to perform all AWS S3 operations from any machine. s3cmd is not installed on Databricks clusters by default. You must install it via a cluster-scoped init script before it can be used. Delete Info The sample init script stores the path to a secret in an environment variable. You should store secrets in this fashion because these environment variables are not accessible from other programs running in Apache Spark. Create the init script Run this sample script in a notebook to create the init script on your cluster. %python\r\n\r\ndbutils.fs.put(\"dbfs:/databricks/<path-to-init-script>/s3cmd-init.sh\",\"\"\"\r\n#!/bin/bash\r\n# Purpose: s3cmd installation and configuration\r\n\r\nsudo apt-get -y install s3cmd\r\ncat > /root/.s3cfg <<EOF\r\naccess_key = $ACCESS_KEY\r\nsecret_key = $SECRET_KEY\r\nEOF\r\ns3cmd ls\r\n\r\n\"\"\",True) Remember the path to the init script. You will need it when configuring your cluster. Configure the init script Follow the documentation to configure a cluster-scoped init script. Specify the path to the init script. Use the same path that you used in the sample script (dbfs:/databricks/<directory>/s3cmd-init.sh). Add secret environment variables Avoid storing secrets directly in your init script. Instead, store the path to a secret in an environment variable. ACCESS_KEY={{secrets/<scope-name>/<secret-name>}}\r\nSECRET_KEY={{secrets/<scope-name>/<secret-name>}} After you have configured the environment variables, your init script can use them. Restart the cluster After configuring the init script, restart the cluster. You can now use s3cmd in notebooks with the %sh magic command. Was this article helpful? (16) (25) Additional Informations Related Articles Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... Related Articles Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Enable s3cmd for notebooks",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/enable-s3cmd-for-notebooks"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/disable-broadcast-when-broadcastnestedloopjoin",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark Disable broadcast when query plan has BroadcastNestedLoopJoin Disable broadcast when query plan has BroadcastNestedLoopJoin How to disable broadcast when the query plan has BroadcastNestedLoopJoin. Written by Adam Pavlacka Last published at: May 23rd, 2022 This article explains how to disable broadcast when the query plan has BroadcastNestedLoopJoin in the physical plan. You expect the broadcast to stop after you disable the broadcast threshold, by setting spark.sql.autoBroadcastJoinThreshold to -1, but Apache Spark tries to broadcast the bigger table and fails with a broadcast error. This behavior is NOT a bug, however it can be unexpected. We are going to review the expected behavior and provide a mitigation option for this issue. Create tables Start by creating two tables, one with null values table_withNull and the other without null values tblA_NoNull. %sql\r\n\r\nsql(\"SELECT id FROM RANGE(10)\").write.mode(\"overwrite\").saveAsTable(\"tblA_NoNull\")\r\nsql(\"SELECT id FROM RANGE(50) UNION SELECT NULL\").write.mode(\"overwrite\").saveAsTable(\"table_withNull\") Attempt to disable broadcast We attempt to disable broadcast by setting spark.sql.autoBroadcastJoinThreshold for the query, which has a sub-query with an in clause. %sql\r\n\r\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\r\nsql(\"select * from table_withNull where id not in (select id from tblA_NoNull)\").explain(true) If you review the query plan, BroadcastNestedLoopJoin is the last possible fallback in this situation. It appears even after attempting to disable the broadcast. == Physical Plan ==\r\n*(2) BroadcastNestedLoopJoin BuildRight, LeftAnti, ((id#2482L = id#2483L) || isnull((id#2482L = id#2483L)))\r\n:- *(2) FileScan parquet default.table_withnull[id#2482L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/table_withnull], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>\r\n+- BroadcastExchange IdentityBroadcastMode, [id=#2586]\r\n   +- *(1) FileScan parquet default.tbla_nonull[id#2483L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/tbla_nonull], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint> If the data being processed is large enough, this results in broadcast errors when Spark attempts to broadcast the table. Rewrite query using not exists instead of in You can resolve the issue by rewriting the query with not exists instead of in. %sql\r\n\r\n// It can be rewritten into a NOT EXISTS, which will become a regular join:\r\nsql(\"select * from table_withNull where not exists (select 1 from tblA_NoNull where table_withNull.id = tblA_NoNull.id)\").explain(true) By using not exists, the query runs with SortMergeJoin. == Physical Plan ==\r\nSortMergeJoin [id#2482L], [id#2483L], LeftAnti\r\n:- Sort [id#2482L ASC NULLS FIRST], false, 0\r\n:  +- Exchange hashpartitioning(id#2482L, 200), [id=#2653]\r\n:     +- *(1) FileScan parquet default.table_withnull[id#2482L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/table_withnull], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>\r\n+- Sort [id#2483L ASC NULLS FIRST], false, 0\r\n   +- Exchange hashpartitioning(id#2483L, 200), [id=#2656]\r\n      +- *(2) Project [id#2483L]\r\n         +- *(2) Filter isnotnull(id#2483L)\r\n            +- *(2) FileScan parquet default.tbla_nonull[id#2483L] Batched: true, DataFilters: [isnotnull(id#2483L)], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/tbla_nonull], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint> Explanation Spark doesn’t do this automatically, because Spark and SQL have slightly different semantics for null handling. In SQL, not in means that if there is any null value in the not in values, the result is empty. This is why it can only be executed with BroadcastNestedLoopJoin. All not in values must be known in order to ensure there is no null value in the set. Example notebook This notebook has a complete example, showing why Spark does not automatically switch BroadcastNestedLoopJoin to SortMergeJoin. Review the BroadcastNestedLoopJoin example notebook. Was this article helpful? (10) (14) Additional Informations Related Articles Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Date functions only accept int values in Apache Spark 3.0 Problem You are attempting to use the date_add() or date_sub() functions in Spark... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Related Articles Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Date functions only accept int values in Apache Spark 3.0 Problem You are attempting to use the date_add() or date_sub() functions in Spark... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Disable broadcast when query plan has BroadcastNestedLoopJoin",
          "view_href" : "https://kb.databricks.com/en_US/sql/disable-broadcast-when-broadcastnestedloopjoin"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/job-fails-spark-finishes",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Job fails, but Apache Spark tasks finish Job fails, but Apache Spark tasks finish Your job fails, but all of the Apache Spark tasks have completed successfully. You are using spark.stop() or System.exit(0) in your code. Written by harikrishnan.kunhumveettil Last published at: May 10th, 2022 Problem Your Databricks job reports a failed status, but all Spark jobs and tasks have successfully completed. Cause You have explicitly called spark.stop() or System.exit(0) in your code. If either of these are called, the Spark context is stopped, but the graceful shutdown and handshake with the Databricks job service does not happen. Solution Do not call spark.stop() or System.exit(0) in Spark code that is running on a Databricks cluster. Was this article helpful? (8) (12) Additional Informations Related Articles Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... Job cluster limits on notebook output Problem You are running a notebook on a job cluster and you get an error message ... Related Articles Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... Job cluster limits on notebook output Problem You are running a notebook on a job cluster and you get an error message ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Job fails, but Apache Spark tasks finish",
          "view_href" : "https://kb.databricks.com/en_US/jobs/job-fails-spark-finishes"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/python-cmd-fail-high-con-cluster",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Python commands fail on high concurrency clusters Python commands fail on high concurrency clusters Python commands fail on high concurrency clusters with Apache Spark process isolation and shared session enabled. WARN error message. Written by xin.wang Last published at: May 19th, 2022 Problem You are attempting to run Python commands on a high concurrency cluster. All Python commands fail with a WARN error message. WARN PythonDriverWrapper: Failed to start repl ReplId-61bef-9fc33-1f8f6-2\r\nExitCodeException exitCode=1: chown: invalid user: ‘spark-9fcdf4d2-045d-4f3b-9293-0f’ Cause Both spark.databricks.pyspark.enableProcessIsolation true and spark.databricks.session.share true are set in the Apache Spark configuration on the cluster. These two Spark properties conflict with each other and prevent the cluster from running Python commands. Solution You can only have one of these two Spark properties enabled on your cluster at a time. You must choose process isolation or a Spark shared session based on your needs. Disable the other option. Was this article helpful? (5) (15) Additional Informations Related Articles Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... How to import a custom CA certificate When working with Python, you may want to import a custom CA certificate to avoid... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... Related Articles Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... How to import a custom CA certificate When working with Python, you may want to import a custom CA certificate to avoid... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Python commands fail on high concurrency clusters",
          "view_href" : "https://kb.databricks.com/en_US/python/python-cmd-fail-high-con-cluster"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dev-tools/get-spark-config-in-dbconnect",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Developer tools Get Apache Spark config in DBConnect Get Apache Spark config in DBConnect Use a REST API call and DBConnect to get the Apache Spark configuration for your cluster. Written by arvind.ravish Last published at: May 9th, 2022 You can always view the Spark configuration (AWS | Azure | GCP) for your cluster by reviewing the cluster details in the workspace. If you are using DBConnect (AWS | Azure | GCP) you may want to quickly review the current Spark configuration details without switching over to the workspace UI. This example code shows you how to get the current Spark configuration for your cluster by making a REST API call in DBConnect. %python\r\n\r\nimport json\r\nimport requests\r\nimport base64\r\n\r\nwith open(\"/<path-to-dbconnect-config>/.databricks-connect\") as readconfig:\r\n    conf = json.load(readconfig)\r\n\r\nCLUSTER_ID = conf[\"cluster_id\"]\r\nTOKEN = conf[\"token\"]\r\nAPI_URL = conf[\"host\"]\r\n\r\nheaders = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + TOKEN}\r\npayload =  {'cluster_id': '' + CLUSTER_ID}\r\nresponse = requests.get(API_URL + \"/api/2.0/clusters/get/?cluster_id=\"+CLUSTER_ID, headers=headers, json = payload)\r\nsparkconf = response.json()[\"spark_conf\"]\r\n\r\nfor config_key, config_value in sparkconf.items():\r\n    print(config_key, config_value) Delete Warning DBConnect only works with supported Databricks Runtime versions. Ensure that you are using a supported runtime on your cluster before using DBConnect. Was this article helpful? (8) (14) Additional Informations Related Articles GeoSpark undefined function error with DBConnect Problem You are trying to use the GeoSpark function st_geofromwkt with DBConnect ... Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... Failed to create process error with Databricks CLI in Windows Problem While trying to access the Databricks CLI (AWS | Azure | GCP) in Windows,... Databricks Connect reports version error with Databricks Runtime 6.4 Problem You are using the Databricks Connect client with Databricks Runtime 6.4 a... Related Articles GeoSpark undefined function error with DBConnect Problem You are trying to use the GeoSpark function st_geofromwkt with DBConnect ... Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... Failed to create process error with Databricks CLI in Windows Problem While trying to access the Databricks CLI (AWS | Azure | GCP) in Windows,... Databricks Connect reports version error with Databricks Runtime 6.4 Problem You are using the Databricks Connect client with Databricks Runtime 6.4 a... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Get Apache Spark config in DBConnect",
          "view_href" : "https://kb.databricks.com/en_US/dev-tools/get-spark-config-in-dbconnect"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/r/pin-r-packages",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles R with Apache Spark Fix the version of R packages Fix the version of R packages Learn how to fix the version of R packages. Written by Adam Pavlacka Last published at: May 20th, 2022 When you use the install.packages() function to install CRAN packages, you cannot specify the version of the package, because the expectation is that you will install the latest version of the package and it should be compatible with the latest version of its dependencies. If you have an outdated dependency installed, it will be updated as well. Sometimes you want to fix the version of an R package. There are several ways to do this: Use the devtools package. Download and install a package file from a CRAN archive. Use a CRAN snapshot. When you use the Libraries UI or API (AWS | Azure | GCP) to install R packages on all the instances of a cluster, we recommend the third option. The Microsoft R Application Network maintains a CRAN Time Machine that stores a snapshot of CRAN every night. The snapshots are available at https://cran.microsoft.com/snapshot/<date> where <date> is the date of the desired snapshot, for example, 2019-05-01. To install specific versions of R packages, specify this URL as the repository of your CRAN library (AWS | Azure | GCP)when you create the library. Was this article helpful? (11) (9) Additional Informations Related Articles Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... Related Articles Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Fix the version of R packages",
          "view_href" : "https://kb.databricks.com/en_US/r/pin-r-packages"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/kfold-cross-validation",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning How to perform group K-fold cross validation with Apache Spark How to perform group K-fold cross validation with Apache Spark Learn how to perform group K-fold cross validation with Apache Spark on Databricks. Written by Adam Pavlacka Last published at: May 16th, 2022 Cross validation randomly splits the training data into a specified number of folds. To prevent data leakage where the same data shows up in multiple folds you can use groups. scikit-learn supports group K-fold cross validation to ensure that the folds are distinct and non-overlapping. On Spark you can use the spark-sklearn library, which distributes tuning of scikit-learn models, to take advantage of this method. This example tunes a scikit-learn random forest model with the group k-fold method on Spark with a grp variable: %python\r\n\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom spark_sklearn import GridSearchCV\r\nfrom sklearn.model_selection import GroupKFold\r\nparam_grid = {\"max_depth\": [8, 12, None],\r\n              \"max_features\": [1, 3, 10],\r\n              \"min_samples_split\": [1, 3, 10],\r\n              \"min_samples_leaf\": [1, 3, 10],\r\n              \"bootstrap\": [True, False],\r\n              \"criterion\": [\"gini\", \"entropy\"],\r\n              \"n_estimators\": [20, 40, 80]}\r\ngroup_kfold = GroupKFold(n_splits=3)\r\ngs = GridSearchCV(sc, estimator = RandomForestClassifier(random_state=42), param_grid=param_grid, cv = group_kfold)\r\ngs.fit(X1, y1 ,grp) Delete Info The library that is used to run the grid search is called spark-sklearn, so you must pass in the Spark context (sc parameter) first. The X1 and y1 parameters must be pandas DataFrames. This grid search option only works on data that fits on the driver. Was this article helpful? (8) (11) Additional Informations Related Articles H2O.ai Sparkling Water cluster not reachable Problem You are trying to initialize H2O.ai’s Sparkling Water on Databricks Runti... Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... Fitting an Apache SparkML model throws error Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... Related Articles H2O.ai Sparkling Water cluster not reachable Problem You are trying to initialize H2O.ai’s Sparkling Water on Databricks Runti... Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... Fitting an Apache SparkML model throws error Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to perform group K-fold cross validation with Apache Spark",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/kfold-cross-validation"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/verify-log4j-version",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Verify the version of Log4j on your cluster Verify the version of Log4j on your cluster Verify the version of Log4j installed on your cluster and upgrade if required. Written by Adam Pavlacka Last published at: May 16th, 2022 Databricks recently published a blog on Log4j 2 Vulnerability (CVE-2021-44228) Research and Assessment. Databricks does not directly use a version of Log4j known to be affected by this vulnerability within the Databricks platform in a way we understand may be vulnerable. If you are using Log4j within your cluster (for example, if you are processing user-controlled strings through Log4j), your use may be potentially vulnerable to the exploit if you have installed, and are using, an affected version or have installed services that transitively depend on an affected version. This article explains how to check your cluster for installed versions of Log4j 2 and how to upgrade those instances. Delete Warning DISCLAIMER: The suggestions provided in this article reflect Databricks’s best understanding of the ways to make these determinations at this time. Because we do not control your code, we cannot guarantee that if you fail to find Log4j by following these directions or using the suggested scanners, that affected Log4j code is not present in your code. Check to see if Log4j 2 is installed Check for a manual install Manually review the libraries installed on your cluster (AWS | Azure | GCP). If you have explicitly installed a version of Log4j 2 via Maven, it is listed under Libraries in the cluster UI (AWS | Azure | GCP). Scan the classpath Scan your classpath to check for a version of Log4j 2. Start your cluster. Attach a notebook to your cluster. Run this code to scan your classpath: %scala\r\n\r\n{\r\n  import scala.util.{Try, Success, Failure}\r\n  import java.lang.ClassNotFoundException\r\n  Try(Class.forName(\"org.apache.logging.log4j.core.Logger\", false, this.getClass.getClassLoader)) match {\r\n    case Success(loggerCls) =>\r\n      Option(loggerCls.getPackage) match {\r\n          case Some(pkg) =>\r\n            println(s\"Version: ${pkg.getSpecificationTitle} ${pkg.getSpecificationVersion}\")\r\n          case None =>\r\n            println(\"Could not determine Log4J 2 version\")\r\n      }\r\n    case Failure(e: ClassNotFoundException) =>\r\n      println(\"Could not load Log4J 2 class\")\r\n    case Failure(e) =>\r\n      println(s\"Unexpected Error: $e\")\r\n      throw e\r\n  }\r\n} If Log4j 2 is NOT PRESENT on your classpath, you see a result like this: Could not load Log4J 2 class If Log4j 2 is PRESENT on your classpath, you should see a result like this, which includes the Log4j 2 version: Version: Apache Log4j Core 2.15.0 Delete Info This method does not identify cases where Log4j classes are shaded or included transitively. Scan all user installed jars Locate all of the user installed jar files on your cluster and run a scanner to check for vulnerable Log4j 2 versions. Start your cluster. Attach a notebook to your cluster. Run this code to identify the location of the jar files: %scala\r\n\r\nimport org.apache.spark._\r\n\r\nval sparkEnv = SparkEnv.get\r\nval field = SparkEnv.get.getClass.getDeclaredField(\"driverTmpDir\")\r\nfield.setAccessible(true)\r\nprintln(s\"Your jars are installed under ${field.get(sparkEnv).asInstanceOf[Option[String]].get}\\n\") The code displays the location of your jar files. Your jars are installed under /local_disk0/spark-1a6be695-9318-463c-b966-256c32e3771c/userFiles-582ca64b-93c9-444c-85b8-7779bd2c5e52 Download the jar files to your local machine. Run a scanner like Logpresso to check for vulnerable Log4j 2 versions. Delete Warning DISCLAIMER: The Logpresso scanner is open source software provided by a third party. Databricks makes no representations of any kind regarding the function or quality of Logpresso. Upgrade your Log4j 2 version Upgrade via cluster UI If you manually installed Log4j 2 via the cluster UI, ensure that it is version 2.17 or above. In this case, no action is required. If you manually installed Log4j 2 via the cluster UI, and it is 2.16 or below, you should uninstall the library from the cluster (AWS | Azure | GCP) and install version 2.17 or above. Delete Info If Log4j 2 is a transitive dependency for another library, upgrade the library that uses Log4j 2 to a secure version. You can also exclude the Log4j 2 package when pulling in an outdated library, and explicitly include a secure version of Log4j 2. This is not guaranteed to work. Upgrade via command line If you have installed Log4j 2 via command line (or via SSH), use the same method to upgrade Log4j 2 to a secure version. Upgrade custom built jar If you include Log4j 2 in a custom built jar, upgrade Log4j 2 to a secure version and rebuild your jar. Re-attach the updated jar to your cluster. Restart your cluster after upgrading Restart your cluster after upgrading Log4j 2. Was this article helpful? (7) (13) Additional Informations Related Articles PyPMML fails with Could not find py4j jar error Problem PyPMML is a Python PMML scoring library. After installing PyPMML in a Dat... Remove Log4j 1.x JMSAppender and SocketServer classes from classpath Databricks recently published a blog on Log4j 2 Vulnerability (CVE-2021-44228) Re... Init script fails to download Maven JAR Problem You have an init script that is attempting to install a library via Maven... Install package using previous CRAN snapshot Problem You are trying to install a library package via CRAN, and are getting a L... Related Articles PyPMML fails with Could not find py4j jar error Problem PyPMML is a Python PMML scoring library. After installing PyPMML in a Dat... Remove Log4j 1.x JMSAppender and SocketServer classes from classpath Databricks recently published a blog on Log4j 2 Vulnerability (CVE-2021-44228) Re... Init script fails to download Maven JAR Problem You have an init script that is attempting to install a library via Maven... Install package using previous CRAN snapshot Problem You are trying to install a library package via CRAN, and are getting a L... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Verify the version of Log4j on your cluster",
          "view_href" : "https://kb.databricks.com/en_US/libraries/verify-log4j-version"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/blob-data-in-xml",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management How to handle blob data contained in an XML file How to handle blob data contained in an XML file Learn how to handle blob data contained in an XML file. Written by Adam Pavlacka Last published at: March 4th, 2022 If you log events in XML format, then every XML event is recorded as a base64 string. In order to run analytics on this data using Apache Spark, you need to use the spark_xml library and the BASE64DECODER API to transform the data for analysis. Problem You need to analyze base64-encoded strings from an XML-formatted log file using Spark. For example, the following file input.xml shows this type of format: <?xml version=\"1.0\" encoding=\"UTF-8\" ?>\r\n<!DOCTYPE log [<!ENTITY % log SYSTEM \"instance\">%log;]>\r\n<log systemID=\"MF2018\" timeZone=\"UTC\" timeStamp=\"Mon Mar 25 16:00:01 2018\">\r\n  <message source=\"message.log\" time=\"Mon Mar 25 16:00:01 2018\" type=\"sysMSG\"><text/>\r\n    <detail>\r\n      <blob>aW5zdGFuY2VJZCxzdGFydFRpbWUsZGVsZXRlVGltZSxob3Vycw0KaS0wMjdmYTdjY2RhMjEwYjRmNCwyLzE3LzE3VDIwOjIxLDIvMTcvMTdUMjE6MTEsNQ0KaS0wN2NkNzEwMGUzZjU0YmY2YSwyLzE3LzE3VDIwOjE5LDIvMTcvMTdUMjE6MTEsNA0KaS0wYTJjNGFkYmYwZGMyNTUxYywyLzE3LzE3VDIwOjE5LDIvMTcvMTdUMjE6MTEsMg0KaS0wYjQwYjE2MjM2Mzg4OTczZiwyLzE3LzE3VDIwOjE4LDIvMTcvMTdUMjE6MTEsNg0KaS0wY2ZkODgwNzIyZTE1ZjE5ZSwyLzE3LzE3VDIwOjE4LDIvMTcvMTdUMjE6MTEsMg0KaS0wY2YwYzczZWZlZWExNGY3NCwyLzE3LzE3VDE2OjIxLDIvMTcvMTdUMTc6MTEsMQ0KaS0wNTA1ZTk1YmZlYmVjZDZlNiwyLzE3LzE3VDE2OjIxLDIvMTcvMTdUMTc6MTEsOA==\r\n      </blob>\r\n    </detail>\r\n  </message>\r\n</log> Solution To parse the XML file: Load the XML data. Use the spark_xml library and create a raw DataFrame. Apply a base64 decoder on the blob column using the BASE64Decoder API. Save the decoded data in a text file (optional). Load the text file using the Spark DataFrame and parse it. Create the DataFrame as a Spark SQL table. The following Scala code processes the file: val xmlfile = \"/mnt/<path>/input.xml\"\r\nval readxml = spark.read.format(\"com.databricks.spark.xml\").option(\"rowTag\",\"message\").load(xmlfile)\r\n\r\nval decoded = readxml.selectExpr(\"_source as source\",\"_time as time\",\"_type as type\",\"detail.blob\")\r\n\r\ndecoded.show() //Displays the raw blob data\r\n\r\n//Apply base64 decoder on every piece of blob data as shown below\r\nval decodethisxmlblob = decoded.rdd\r\n    .map(str => str(3).toString)\r\n    .map(str1 => new String(new sun.misc.BASE64Decoder()\r\n    .decodeBuffer(str1)))\r\n\r\n//Store it in a text file temporarily\r\ndecodethisxmlblob.saveAsTextFile(\"/mnt/vgiri/ec2blobtotxt\")\r\n\r\n//Parse the text file as required using Spark DataFrame.\r\n\r\nval readAsDF = spark.sparkContext.textFile(\"/mnt/vgiri/ec2blobtotxt\")\r\nval header = readAsDF.first()\r\nval finalTextFile = readAsDF.filter(row => row != header)\r\n\r\nval finalDF = finalTextFile.toDF()\r\n    .selectExpr(\r\n    (\"split(value, ',')[0] as instanceId\"),\r\n    (\"split(value, ',')[1] as startTime\"),\r\n    (\"split(value, ',')[2] as deleteTime\"),\r\n    (\"split(value, ',')[3] as hours\")\r\n    )\r\n\r\nfinalDF.show() The Spark code generates the following output: 18/03/24 22:54:31 INFO DAGScheduler: ResultStage 4 (show at SparkXMLBlob.scala:42) finished in 0.016 s\r\n18/03/24 22:54:31 INFO DAGScheduler: Job 4 finished: show at SparkXMLBlob.scala:42, took 0.019120 s\r\n18/03/24 22:54:31 INFO SparkContext: Invoking stop() from shutdown hook\r\n+-------------------+-------------+-------------+-----+\r\n| instanceId        | startTime   | deleteTime  |hours|\r\n+-------------------+-------------+-------------+-----+\r\n|i-027fa7ccda210b4f4|2/17/17T20:21|2/17/17T21:11|    5|\r\n|i-07cd7100e3f54bf6a|2/17/17T20:19|2/17/17T21:11|    4|\r\n|i-0a2c4adbf0dc2551c|2/17/17T20:19|2/17/17T21:11|    2|\r\n|i-0b40b16236388973f|2/17/17T20:18|2/17/17T21:11|    6|\r\n|i-0cfd880722e15f19e|2/17/17T20:18|2/17/17T21:11|    2|\r\n|i-0cf0c73efeea14f74|2/17/17T16:21|2/17/17T17:11|    1|\r\n|i-0505e95bfebecd6e6|2/17/17T16:21|2/17/17T17:11|    8|\r\n+-------------------+-------------+-------------+-----+ Was this article helpful? (13) (52) Additional Informations Related Articles Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... Related Articles Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to handle blob data contained in an XML file",
          "view_href" : "https://kb.databricks.com/en_US/data/blob-data-in-xml"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/revoke-all-user-privileges",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management Revoke all user privileges Revoke all user privileges Use a regex and a series of for loops to revoke all privileges for a single user. Written by pavan.kumarchalamcharla Last published at: May 31st, 2022 When user permissions are explicitly granted for individual tables and views, the selected user can access those tables and views even if they don’t have permission to access the underlying database. If you want to revoke a user’s access, you can do so with the REVOKE command. However, the REVOKE command is explicit, and is strictly scoped to the object specified in the command. For example: %sql\r\n\r\nREVOKE ALL PRIVILEGES ON DATABASE <database-name> FROM `<user>@<domain-name>`\r\nREVOKE SELECT ON <table-name> FROM `<user>@<domain-name>` If you want to revoke all privileges for a single user you can do it with a series of multiple commands, or you can use a regular expression and a series of for loops to automate the process. Example code This example code matches the <search-string> pattern to the database name and the table name and then revokes the user’s privileges. The search is recursive. %python\r\n\r\nfrom re import search\r\ndatabaseQuery = sqlContext.sql(\"show databases\")\r\ndatabaseList = databaseQuery.collect()\r\n# This loop revokes at the database level.\r\nfor db in databaseList:\r\n  listTables = sqlContext.sql(\"show tables from \"+db['databaseName'])\r\n  tableRows = listTables.collect()\r\n  if search(<search-string>, db['databaseName']):\r\n    revokeDatabase=sqlContext.sql(\"REVOKE ALL PRIVILAGES ON DATABASE \"+db['databaseName']+\" to `<username>`\")\r\n    display(revokeDatabase)\r\n    print(\"Ran the REVOKE query on \"+db['databaseName']+\" for <username>\")\r\n  # This loop revokes at the table level.\r\n  for table in tableRows:\r\n    if search(<search-string>,table['tableName']):\r\n      revokeCommand=sqlContext.sql(\"REVOKE SELECT ON \"+table['database']+\".\"+table['tableName']+\" FROM `<username>`\")\r\n      display(revokeCommand)\r\n      print(\"Revoked the SELECT permissions on \"+table['database']+\".\"+table['tableName']+\" for <username>\") Delete Info These commands only work if you have enabled table access control for the cluster (AWS | Azure | GCP). Was this article helpful? (6) (16) Additional Informations Related Articles Hive UDFs This article shows how to create a Hive UDF, register it in Spark, and use it in ... How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... Related Articles Hive UDFs This article shows how to create a Hive UDF, register it in Spark, and use it in ... How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Revoke all user privileges",
          "view_href" : "https://kb.databricks.com/en_US/data/revoke-all-user-privileges"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/wrong-schema-in-files",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management Incompatible schema in some files Incompatible schema in some files Learn how to resolve incompatible schema in Parquet files with Databricks. Written by Adam Pavlacka Last published at: May 31st, 2022 Problem The Spark job fails with an exception like the following while reading Parquet files: Error in SQL statement: SparkException: Job aborted due to stage failure:\r\nTask 20 in stage 11227.0 failed 4 times, most recent failure: Lost task 20.3 in stage 11227.0\r\n(TID 868031, 10.111.245.219, executor 31):\r\njava.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\r\n    at org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:52) Cause The java.lang.UnsupportedOperationException in this instance is caused by one or more Parquet files written to a Parquet folder with an incompatible schema. Solution Find the Parquet files and rewrite them with the correct schema. Try to read the Parquet dataset with schema merging enabled: %scala\r\n\r\nspark.read.option(\"mergeSchema\", \"true\").parquet(path) or %scala\r\n\r\nspark.conf.set(\"spark.sql.parquet.mergeSchema\", \"true\")\r\nspark.read.parquet(path) If you do have Parquet files with incompatible schemas, the snippets above will output an error with the name of the file that has the wrong schema. You can also check if two schemas are compatible by using the merge method. For example, let’s say you have these two schemas: %scala\r\n\r\nimport org.apache.spark.sql.types._\r\n\r\nval struct1 = (new StructType)\r\n  .add(\"a\", \"int\", true)\r\n  .add(\"b\", \"long\", false)\r\n\r\nval struct2 = (new StructType)\r\n  .add(\"a\", \"int\", true)\r\n  .add(\"b\", \"long\", false)\r\n  .add(\"c\", \"timestamp\", true) Then you can test if they are compatible: %scala\r\n\r\nstruct1.merge(struct2).treeString This will give you: %scala\r\n\r\nres0: String =\r\n\"root\r\n|-- a: integer (nullable = true)\r\n|-- b: long (nullable = false)\r\n|-- c: timestamp (nullable = true)\r\n\" However, if struct2 has the following incompatible schema: %scala\r\n\r\nval struct2 = (new StructType)\r\n  .add(\"a\", \"int\", true)\r\n  .add(\"b\", \"string\", false) Then the test will give you the following SparkException: org.apache.spark.SparkException: Failed to merge fields 'b' and 'b'. Failed to merge incompatible data types LongType and StringType Was this article helpful? (6) (19) Additional Informations Related Articles Revoke all user privileges When user permissions are explicitly granted for individual tables and views, the... How to update nested columns Spark doesn’t support adding new columns or dropping existing columns in nested s... How to list and delete files faster in Databricks Scenario Suppose you need to delete a table that is partitioned by year, month, d... Job fails when using Spark-Avro to write decimal values to AWS Redshift Problem In Databricks Runtime versions 5.x and above, when writing decimals to Am... Related Articles Revoke all user privileges When user permissions are explicitly granted for individual tables and views, the... How to update nested columns Spark doesn’t support adding new columns or dropping existing columns in nested s... How to list and delete files faster in Databricks Scenario Suppose you need to delete a table that is partitioned by year, month, d... Job fails when using Spark-Avro to write decimal values to AWS Redshift Problem In Databricks Runtime versions 5.x and above, when writing decimals to Am... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Incompatible schema in some files",
          "view_href" : "https://kb.databricks.com/en_US/data/wrong-schema-in-files"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/compare-versions-delta-table",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Compare two versions of a Delta table Compare two versions of a Delta table Use time travel to compare two versions of a Delta table. Written by mathan.pillai Last published at: May 10th, 2022 Delta Lake supports time travel, which allows you to query an older snapshot of a Delta table. One common use case is to compare two versions of a Delta table in order to identify what changed. For more details on time travel, please review the Delta Lake time travel documentation (AWS | Azure | GCP). Identify all differences You can use a SQL SELECT query to identify all differences between two versions of a Delta table. You need to know the name of the table and the version numbers of the snapshots you want to compare. %sql\r\n\r\nselect * from <table-name>@v<version-number>\r\nexcept all\r\nselect * from\r\n<table-name>@v<version-number> For example, if you had a table named “schedule” and you wanted to compare version 2 with the original version, your query would look like this: %sql\r\n\r\nselect * from schedule@v2\r\nexcept all\r\nselect * from\r\nschedule@v0 Identify files added to a specific version You can use a Scala query to retrieve a list of files that were added to a specific version of the Delta table. %scala\r\n\r\ndisplay(spark.read.json(\"dbfs:/<path-to-delta-table>/_delta_log/00000000000000000002.json\").where(\"add is not null\").select(\"add.path\")) In this example, we are getting a list of all files that were added to version 2 of the Delta table. 00000000000000000002.json contains the list of all files in version 2. After reading in the full list, we are excluding files that already existed, so the displayed list only includes files added to version 2. Was this article helpful? (11) (14) Additional Informations Related Articles A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... Related Articles A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Compare two versions of a Delta table",
          "view_href" : "https://kb.databricks.com/en_US/delta/compare-versions-delta-table"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/mlflow-artifacts-custom-storage",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning Experiment warning when custom artifact storage location is used Experiment warning when custom artifact storage location is used Resolve experiment warnings when a custom artifact storage location is used instead of the MLflow managed location. Written by Adam Pavlacka Last published at: May 16th, 2022 Problem When you create an MLflow experiment with a custom artifact location, you get the following warning: Cause MLflow experiment permissions (AWS | Azure | GCP) are enforced on artifacts in MLflow Tracking, enabling you to easily control access to datasets, models, and other files. MLflow cannot guarantee the enforcement of access controls on artifacts stored in custom locations. Solution Databricks recommends using the default artifact location when creating an MLflow experiment. The default storage location is backed by access controls. Was this article helpful? (12) (12) Additional Informations Related Articles How to speed up cross-validation Hyperparameter tuning of Apache SparkML models takes a very long time, depending ... How to perform group K-fold cross validation with Apache Spark Cross validation randomly splits the training data into a specified number of fol... MLflow project fails to access an Apache Hive table Problem You have an MLflow project that fails to access a Hive table and returns ... Error when importing OneHotEncoderEstimator Problem You have migrated a notebook from Databricks Runtime 6.4 for Machine Lear... Related Articles How to speed up cross-validation Hyperparameter tuning of Apache SparkML models takes a very long time, depending ... How to perform group K-fold cross validation with Apache Spark Cross validation randomly splits the training data into a specified number of fol... MLflow project fails to access an Apache Hive table Problem You have an MLflow project that fails to access a Hive table and returns ... Error when importing OneHotEncoderEstimator Problem You have migrated a notebook from Databricks Runtime 6.4 for Machine Lear... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Experiment warning when custom artifact storage location is used",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/mlflow-artifacts-custom-storage"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/metastore/create-table-error-external-hive",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Metastore Error in CREATE TABLE with external Hive metastore Error in CREATE TABLE with external Hive metastore CREATE TABLE error with MySQL 8.0 in external Hive metastore due to charset. Written by jordan.hicks Last published at: May 16th, 2022 Problem You are connecting to an external MySQL metastore and attempting to create a table when you get an error. AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException:\r\nMetaException(message:An exception was thrown while adding/validating class(es) : (conn=21)\r\nColumn length too big for column 'PARAM_VALUE' (max = 16383); use BLOB or TEXT instead. Cause This is a known issue with MySQL 8.0 when the default charset is utfmb4. You can confirm this by running a query on the database with the error. %sql\r\n\r\nSELECT default_character_set_name FROM information_schema.SCHEMATA S WHERE schema_name = \"<database-name>\" Solution You need to update or recreate the database and set the charset to latin1. Option 1 Manually run create statements in the Hive database with DEFAULT CHARSET=latin1 at the end of each CREATE TABLEstatement. %sql\r\n\r\nCREATE TABLE `TABLE_PARAMS`\r\n(\r\n    `TBL_ID` BIGINT NOT NULL,\r\n    `PARAM_KEY` VARCHAR(256) BINARY NOT NULL,\r\n    `PARAM_VALUE` VARCHAR(4000) BINARY NULL,\r\n    CONSTRAINT `TABLE_PARAMS_PK` PRIMARY KEY (`TBL_ID`,`PARAM_KEY`)\r\n) ENGINE=INNODB DEFAULT CHARSET=latin1; Restart the Hive metastore and repeat until all creation errors have been resolved. Option 2 Setup the database and user accounts. Create the database and run alter database hive character set latin1; before you launch the metastore. This command sets the default CHARSET for the database. It is applied when the metastore creates tables. Was this article helpful? (11) (13) Additional Informations Related Articles Data too long for column error Problem You are trying to insert a struct into a table, but you get a java.sql.SQ... Autoscaling is slow with an external metastore Problem You have an external metastore configured on your cluster and autoscaling... Drop database without deletion By default, the DROP DATABASE (AWS | Azure | GCP) command drops the database and ... Drop tables with corrupted metadata from the metastore Problem Sometimes you cannot drop a table from the Databricks UI. Using %sql or s... Related Articles Data too long for column error Problem You are trying to insert a struct into a table, but you get a java.sql.SQ... Autoscaling is slow with an external metastore Problem You have an external metastore configured on your cluster and autoscaling... Drop database without deletion By default, the DROP DATABASE (AWS | Azure | GCP) command drops the database and ... Drop tables with corrupted metadata from the metastore Problem Sometimes you cannot drop a table from the Databricks UI. Using %sql or s... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Error in CREATE TABLE with external Hive metastore",
          "view_href" : "https://kb.databricks.com/en_US/metastore/create-table-error-external-hive"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/execution/serialized-task-is-too-large",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Job execution Serialized task is too large Serialized task is too large Learn what to do when a serialized task is too large in Databricks. Written by Adam Pavlacka Last published at: May 11th, 2022 If you see the follow error message, you may be able to fix this error by changing the Spark config (AWS | Azure) when you start the cluster. Serialized task XXX:XXX was XXX bytes, which exceeds max allowed: spark.rpc.message.maxSize (XXX bytes).\r\nConsider increasing spark.rpc.message.maxSize or using broadcast variables for large values. To change the Spark config, set the property: spark.rpc.message.maxSize While tuning the configuration is one option, typically this error message means that you send some large objects from the driver to executors, e.g., call parallelize with a large list, or convert a large R DataFrame to a Spark DataFrame. If so, we recommend first auditing your code to remove large objects that you use, or leverage broadcast variables instead. If that does not resolve this error, you can increase the partition number to split the large list to multiple small ones to reduce the Spark RPC message size. Here are examples for Python and Scala: Python largeList = [...] # This is a large list\r\npartitionNum = 100 # Increase this number if necessary\r\nrdd = sc.parallelize(largeList, partitionNum)\r\nds = rdd.toDS() Delete Scala val largeList = Seq(...) // This is a large list\r\nval partitionNum = 100 // Increase this number if necessary\r\nval rdd = sc.parallelize(largeList, partitionNum)\r\nval ds = rdd.toDS() Delete R users need to increase the Spark configuration spark.default.parallelism to increase the partition number at cluster initialization. You cannot set this configuration after cluster creation. Was this article helpful? (8) (13) Additional Informations Related Articles Maximum execution context or notebook attachment limit reached Problem Notebook or job execution stops and returns either of the following error... Increase the number of tasks per stage When using the spark-xml package, you can increase the number of tasks per stage ... Related Articles Maximum execution context or notebook attachment limit reached Problem Notebook or job execution stops and returns either of the following error... Increase the number of tasks per stage When using the spark-xml package, you can increase the number of tasks per stage ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Serialized task is too large",
          "view_href" : "https://kb.databricks.com/en_US/execution/serialized-task-is-too-large"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/generate-browser-har-files",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks Generate browser HAR files Generate browser HAR files Learn how to record HAR files in your web browser. These are very useful when troubleshooting UI issues. Written by vivian.wilfred Last published at: July 1st, 2022 When troubleshooting UI issues, it is sometimes necessary to obtain additional information about the network requests that are generated in your browser. If this is needed, our support team will ask you to generate a HAR file. This article describes how to generate a HAR file with each of the major web browsers. Delete Warning HAR files contain sensitive data, including the content of the pages you download while recording, along with your browser cookies. This is sensitive data and should not be shared publicly. Chrome Open Google Chrome and navigate to the page you want to record. Right-click on the page, and then click Inspect. Click the Network tab. Look for the Record button () in the upper left corner of the frame. It should be red. If the Record button is grey, click it once to start recording. Check the Preserve log box. Click the Clear button (). This removes any existing logs from the tab. Reproduce the issue while the network requests are being recorded. You will see session output in the frame. Once you have reproduced the issue, click the Export HAR button (). You are prompted to save the file on your computer. Save the HAR file. Attach the HAR file to your ticket. Edge Open Microsoft Edge and navigate to the page you want to record. Right-click on the page, and then click Inspect. Click the Network tab. Look for the Record button () in the upper left corner of the frame. It should be red. If the Record button is grey, click it once to start recording. Check the Preserve log box. Click the Clear button (). This removes any existing logs from the tab. Reproduce the issue while the network requests are being recorded. You will see session output in the frame. Once you have reproduced the issue, click the Export HAR button (). You are prompted to save the file on your computer. Save the HAR file. Attach the HAR file to your ticket. Firefox Open Firefox and navigate to the page you want to record. Look for the the Firefox menu in the top-right. Click More Tools. Click Web Developer Tools. The Developer Network Tools panel opens. Click the Network tab. Start performing actions in the browser. Recording starts automatically. Once you have reproduced the issue, right-click on the gear (⚙). Click Save all as HAR. You are prompted to save the file on your computer. Save the HAR file. Attach the HAR file to your ticket. Safari You need to enable the Develop Menu in Safari before you can access the developer console. Click Safari in the menu bar Click Preferences. Click the Advanced tab. Select Show Develop menu in menu bar. Click Develop in the menu bar. Click Show Web Inspector. Click the Network tab. Start performing actions in the browser. Recording starts automatically. Once you have reproduced the issue, click Export. You are prompted to save the file on your computer. Save the HAR file. Attach the HAR file to your ticket. Was this article helpful? (5) (11) Additional Informations Related Articles Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... How to send email or SMS messages from Databricks notebooks You may need to send a notification to a set of recipients from a Databricks note... Failure when accessing or mounting storage Problem You are trying to access an existing mount point, or create a new mount p... Notebook autosave fails due to file size limits Problem Notebook autosaving fails with the following error message: Failed to sav... Related Articles Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... How to send email or SMS messages from Databricks notebooks You may need to send a notification to a set of recipients from a Databricks note... Failure when accessing or mounting storage Problem You are trying to access an existing mount point, or create a new mount p... Notebook autosave fails due to file size limits Problem Notebook autosaving fails with the following error message: Failed to sav... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Generate browser HAR files",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/generate-browser-har-files"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/service-principal-cannot-create-access-token",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks Service principal cannot create access token Service principal cannot create access token You cannot create a token on behalf of a service principal with the API when token usage is disabled. Written by rakesh.parija Last published at: July 1st, 2022 Problem You are trying to create a token on behalf of a service principal, using /2.0/token-management/on-behalf-of/tokens in the REST API but are getting a PERMISSION_DENIED error. {\r\n\"error_code\": \"PERMISSION_DENIED\",\r\n\"message\": \"User xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx does not have permission to use tokens.\"\r\n} Cause This happens when the service principals are assigned to a user group that has token usage disabled. Solution Your workspace admin should enable token usage for the user group that contains the service principals. Delete Info You should create separate user groups for service principals and users who need token access and those who don't. This limits access only to those who need it and doesn't provide token access to all users in your workspace. Enable token usage via the UI Click Settings in the left hand menu. Click Admin Console. Click the Workspace settings tab. Click Permission Settings in the Personal Access Tokens field. Add the groups that need token access in the Token Usage window. Remove any groups that should not have token access. Click Save to apply the changes and close the window. Enable token usage via the REST API Review the token permissions API settings. Use this sample code to update the token permissions. Replace the following values in the sample code before running it on your local machine: <admin-access-token> - Admin personal access token. <user-group-name> - The name of the user group to grant token access permission. You can add multiple group entries if needed. <workspace-url> - Replace this value with your Workspace URL. curl --location --request PATCH 'https://<workspace-url>/api/2.0/preview/permissions/authorization/tokens'; \\\r\n--header 'Authorization: Bearer <admin-access-token>' \\\r\n--header 'Content-Type: application/json' \\\r\n--data-raw '{\r\n  \"access_control_list\": [\r\n    {\r\n      \"group_name\": \"<user-group-name>\",\r\n      \"permission_level\": \"CAN_USE\"\r\n    },\r\n    {\r\n      \"group_name\": \"<user-group-name>\",\r\n      \"permission_level\": \"CAN_USE\"\r\n    }\r\n  ]\r\n}' Delete Info This sample code only allows you to add token permissions via the API. It does not allow you to delete them. To delete token permissions via the API you must replace the token permissions for the entire workspace API using PUT instead of PATCH. Review the token permissions API settings for more details. Was this article helpful? (2) (6) Additional Informations Related Articles Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... display() does not show microseconds correctly Problem You want to display a timestamp value with microsecond precision, but whe... Notebook autosave fails due to file size limits Problem Notebook autosaving fails with the following error message: Failed to sav... Related Articles Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... display() does not show microseconds correctly Problem You want to display a timestamp value with microsecond precision, but whe... Notebook autosave fails due to file size limits Problem Notebook autosaving fails with the following error message: Failed to sav... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Service principal cannot create access token",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/service-principal-cannot-create-access-token"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/python-2-eol",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Python 2 sunset status Python 2 sunset status Learn about the sunset status of Python 2 in Databricks. Written by Adam Pavlacka Last published at: May 19th, 2022 Python.org officially moved Python 2 into EoL (end-of-life) status on January 1, 2020. What does this mean for you? Databricks Runtime 6.0 and above Databricks Runtime 6.0 and above support only Python 3. You cannot create a cluster with Python 2 using these runtimes. Any clusters created with these runtimes use Python 3 by definition. Databricks Runtime 5.5 LTS When you create a Databricks Runtime 5.5 LTS cluster by using the workspace UI, the default is Python 3. You have the option to specify Python 2. If you use the Databricks REST API (AWS | Azure) to create a cluster using Databricks Runtime 5.5 LTS, the default is Python 2. If you have a Databricks Runtime 5.5 LTS cluster running Python 2, you are not required to upgrade to Python 3. You can use the following call to specify Python 3 when you create a cluster using the Databricks REST API. \"spark_env_vars\": {\r\n  \"PYSPARK_PYTHON\": \"/databricks/python3/bin/python3\"\r\n}, Should I upgrade to Python 3? The decision to upgrade depends on your specific circumstances, including reliance on other systems and dependencies. This is a decision that should be made in conjunction with your engineering organization. The official Python.org statement is as follows: As of January 1st, 2020 no new bug reports, fixes, or changes will be made to Python 2, and Python 2 is no longer supported. We have not yet released the few changes made between when we released Python 2.7.17 (on October 19th, 2019) and January 1st. As a service to the community, we will bundle those fixes (and only those fixes) and release a 2.7.18. We plan on doing that in April 2020, because that’s convenient for the release managers, not because it implies anything about when support ends. Support Databricks does not offer official support for discontinued third-party software. Support requests related to Python 2 are not eligible for engineering support. Was this article helpful? (4) (15) Additional Informations Related Articles How to run SQL queries from Python scripts You may want to access your tables outside of Databricks notebooks. Besides conne... Job remains idle before starting Problem You have an Apache Spark job that is triggered correctly, but remains idl... Cluster cancels Python command execution after installing Bokeh Problem The cluster returns Cancelled in a Python notebook. Inspect the driver lo... AttributeError: ‘function’ object has no attribute Problem You are selecting columns from a DataFrame and you get an error message. ... Related Articles How to run SQL queries from Python scripts You may want to access your tables outside of Databricks notebooks. Besides conne... Job remains idle before starting Problem You have an Apache Spark job that is triggered correctly, but remains idl... Cluster cancels Python command execution after installing Bokeh Problem The cluster returns Cancelled in a Python notebook. Inspect the driver lo... AttributeError: ‘function’ object has no attribute Problem You are selecting columns from a DataFrame and you get an error message. ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Python 2 sunset status",
          "view_href" : "https://kb.databricks.com/en_US/python/python-2-eol"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/ebs-leaked-volumes",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters EBS leaked volumes EBS leaked volumes Learn how to handle leaked EBS volumes that are not automatically deleted. Written by Adam Pavlacka Last published at: March 4th, 2022 Problem After a cluster is terminated on AWS, some EBS volumes are not deleted automatically. These stray, unattached EBS volumes are often referred to as “leaked” volumes. Cause Databricks always sets DeletionOnTermination=true for the EBS volumes it creates when it launches clusters. Therefore, whenever a cluster instance is terminated, AWS should automatically delete all EBS volumes associated with the cluster. However, there are two circumstances in which you might see unattached EBS volumes: There is a delay of 5 minutes between a cluster termination and the termination of its instances. Hence you might see unused volumes for a few minutes. These require no action on your part. Databricks depends on AWS to clean up EBS volumes when a cluster instance is terminated. Unfortunately, if an instance dies during start-up, the EBS volumes never get attached to the instance, and AWS never cleans up the volumes. The AWS support team has confirmed that they are working on fix for this bug, but they have not provided an ETA. Solution Databricks automatically logs and deletes leaked EBS volumes. You can contact Databricks customer support to request a list of leaked EBS volumes that have been deleted from your account. Delete Note Databricks needs your permission to describe and delete volumes. If you created your Databricks account prior to version 2.44 (that is, before Apr 27, 2017) and want leaked EBS volumes to be deleted automatically, add volume permissions to the IAM role or keys used to create your account. In particular, you must add the permissions ec2:DeleteVolume and ec2:DescribeVolumes. For the complete list of permissions and instructions on how to update your existing IAM role or keys, see AWS Account. Was this article helpful? (13) (13) Additional Informations Related Articles Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Cluster failed to launch This article describes several scenarios in which a cluster fails to launch, and ... How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Related Articles Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Cluster failed to launch This article describes several scenarios in which a cluster fails to launch, and ... How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "EBS leaked volumes",
          "view_href" : "https://kb.databricks.com/en_US/clusters/ebs-leaked-volumes"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback Welcome to the Databricks Knowledge Base All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Start Here Search & Browse the Databricks Knowledge Base. Amazon Topics on Amazon Azure Topics on Azure Google Cloud Platform Topics on Google Cloud Platform Help Topics Whether it's a very specific question, or a vague one, we have it covered in our company help center. Amazon Azure Google Cloud Platform All articles Popular Articles Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Prevent duplicated columns when joining two DataFrames If you perform a join in Spark and don’t specify your join correctly you’ll end u... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... Get and set Apache Spark configuration properties in a notebook In most cases, you set the Spark config (AWS | Azure ) at the cluster level. Howe... Unexpected cluster termination Sometimes a cluster is terminated unexpectedly, not as a result of a manual termi... Forbidden error while accessing S3 data Problem While trying to access S3 data using DBFS mount or directly in Spark APIs... Generate unique increasing numeric values This article shows you how to use Apache Spark functions to generate unique incre... How to list and delete files faster in Databricks Scenario Suppose you need to delete a table that is partitioned by year, month, d... Create a DataFrame from a JSON string or Python dictionary In this article we are going to review how you can create an Apache Spark DataFra... Can't Find What you're Looking for? Feel free to suggest an article topic by clicking on the button below. Offer feedback © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Databricks Knowledge Base Welcome to the Databricks Knowledge Base  Contact Us",
          "view_href" : "https://kb.databricks.com/"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/slowdown-from-root-disk-fill",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Cluster slowdown due to Ganglia metrics filling root partition Cluster slowdown due to Ganglia metrics filling root partition Resolve cluster slowdowns due to a Ganglia metric data explosion filling the root partition. Written by arjun.kaimaparambilrajan Last published at: March 4th, 2022 Delete Note This article applies to Databricks Runtime 7.3 LTS and below. Problem Clusters start slowing down and may show a combination of the following symptoms: Unhealthy cluster events are reported: Request timed out. Driver is temporarily unavailable. Metastore is down. DBFS is down. You do not see any high GC events or memory utilization associated with the driver process. When you use top on the driver node you see an intermittent high average load. The Ganglia related gmetad process shows intermittent high CPU utilization. The root disk shows high disk usage with df -h /. Specifically, /var/lib/ganglia/rrds shows high disk usage. The Ganglia UI is unable to show the load distribution. You can verify the issue by looking for files with local in the prefix in /var/lib/ganglia/rrds. Generally, this directory should only have files prefixed with application-<applicationId>. For example: %sh ls -ltrhR /var/lib/ganglia/rrds/  | grep -i local\r\n\r\nrw-rw-rw- 1 ganglia ganglia 616K Jun 29 18:00 local-1593453624916.driver.Databricks.directoryCommit.markerReadErrors.count.rrd -rw-rw-rw- 1 ganglia ganglia 616K Jun 29 18:00 local-1593453614595.driver.Databricks.directoryCommit.deletedFilesFiltered.count.rrd -rw-rw-rw- 1 ganglia ganglia 616K Jun 29 18:00 local-1593453614595.driver.Databricks.directoryCommit.autoVacuumCount.count.rrd -rw-rw-rw- 1 ganglia ganglia 616K Jun 29 18:00 local-1593453605184.driver.CodeGenerator.generatedMethodSize.min.rrd Cause Ganglia metrics typically use less than 10GB of disk space. However, under certain circumstances, a “data explosion” can occur, which causes the root partition to fill with Ganglia metrics. Data explosions also create a dirty cache. When this happens, the Ganglia metrics can consume more than 100GB of disk space on root. This “data explosion” can happen if you define the spark session variable as global in your Python file and then call functions defined in the same file to perform Apache Spark transformation on data. When this happens, the Spark session logic can be serialized, along with the required function definition, resulting in a Spark session being created on the worker node. For example, take the following Spark session definition: %python\r\n\r\nfrom pyspark.sql import SparkSession\r\n\r\ndef get_spark():\r\n    \"\"\"Returns a spark session.\"\"\"\r\n    return SparkSession.builder.getOrCreate()\r\n\r\nif \"spark\" not in globals():\r\n  spark = get_spark()\r\n\r\ndef generator(partition):\r\n    print(globals()['spark'])\r\n    for row in partition:\r\n        yield [word.lower() for word in row[\"value\"]] If you use the following example commands, local prefixed files are created: %python\r\n\r\nfrom repro import ganglia_test\r\ndf = spark.createDataFrame([([\"Hello\"], ), ([\"Spark\"], )], [\"value\"])\r\ndf.rdd.mapPartitions(ganglia_test.generator).toDF([\"value\"]).show() The print(globals()['spark']) statement in the generator() function doesn’t result in an error, because it is available as a global variable in the worker nodes. It may fail with an invalid key error in some cases, as that value is not available as a global variable. Streaming jobs that execute on short batch intervals are susceptible to this issue. Solution Ensure that you are not using SparkSession.builder.getOrCreate() to define a Spark session as a global variable. When you troubleshoot, you can use the timestamps on files with the local prefix to help determine when a problematic change was first introduced. Was this article helpful? (8) (14) Additional Informations Related Articles How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... Cluster failed to launch This article describes several scenarios in which a cluster fails to launch, and ... Cluster fails to start with dummy does not exist error Problem You try to start a cluster, but it fails to start. You get an Apache Spar... Related Articles How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... Cluster failed to launch This article describes several scenarios in which a cluster fails to launch, and ... Cluster fails to start with dummy does not exist error Problem You try to start a cluster, but it fails to start. You get an Apache Spar... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cluster slowdown due to Ganglia metrics filling root partition",
          "view_href" : "https://kb.databricks.com/en_US/clusters/slowdown-from-root-disk-fill"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/list-all-workspace-objects",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark List all workspace objects List all workspace objects List all Databricks workspace objects under a given path. Written by Adam Pavlacka Last published at: May 19th, 2022 You can use the Databricks Workspace API (AWS | Azure | GCP) to recursively list all workspace objects under a given path. Common use cases for this include: Indexing all notebook names and types for all users in your workspace. Use the output, in conjunction with other API calls, to delete unused workspaces or to manage notebooks. Dynamically get the absolute path of a notebook under a given user, and submit that to the Databricks Jobs API to trigger notebook-based jobs (AWS | Azure | GCP). Define function This example code defines the function and the logic needed to run it. You should place this code at the beginning of your notebook. You need to replace <token> with your personal access token (AWS | Azure | GCP). %python\r\n\r\nimport requests\r\nimport json\r\nfrom ast import literal_eval\r\n\r\n# Authorization\r\nheaders = {\r\n  'Authorization': 'Bearer <token>',\r\n}\r\n\r\n# Define rec_req as a function.\r\n# Note: Default path is \"/\" which scans all users and folders.\r\n\r\ndef rec_req(instanceName,loc=\"/\"):\r\n data_path = '{{\"path\": \"{0}\"}}'.format(loc)\r\n instance = instanceName\r\n url = '{}/api/2.0/workspace/list'.format(instance)\r\n response = requests.get(url, headers=headers, data=data_path)\r\n # Raise exception if a directory or URL does not exist.\r\n response.raise_for_status()\r\n jsonResponse = response.json()\r\n for i,result in jsonResponse.items():\r\n   for value in result:\r\n    dump = json.dumps(value)\r\n    data = literal_eval(dump)\r\n    if data['object_type'] == 'DIRECTORY':\r\n     # Iterate through all folders.\r\n     rec_req(instanceName,data['path'])\r\n    elif data['object_type'] == 'NOTEBOOK':\r\n     # Return the notebook path.\r\n     print(data)\r\n    else:\r\n     # Skip imported libraries.\r\n     pass Run function Once you have defined the function in your notebook, you can call it at any time. You need to replace <instance-name> with the instance name (AWS | Azure | GCP) of your Databricks deployment. This is typically the URL, without any workspace ID. You need to replace <path> with the full path you want to search. This is typically /. %python\r\n\r\n\r\nrec_req(\"https://<instance-name>\", \"<path>\") Delete Info You should NOT include a trailing / as the last character of the instance name. The function generates an error if a trailing / is included. Was this article helpful? (7) (12) Additional Informations Related Articles Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... Reading large DBFS-mounted files using Python APIs This article explains how to resolve an error that occurs when you read large DBF... Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Job remains idle before starting Problem You have an Apache Spark job that is triggered correctly, but remains idl... Related Articles Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... Reading large DBFS-mounted files using Python APIs This article explains how to resolve an error that occurs when you read large DBF... Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Job remains idle before starting Problem You have an Apache Spark job that is triggered correctly, but remains idl... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "List all workspace objects",
          "view_href" : "https://kb.databricks.com/en_US/python/list-all-workspace-objects"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/pypmml-fail-find-py4j-jar",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries PyPMML fails with Could not find py4j jar error PyPMML fails with Could not find py4j jar error Written by arjun.kaimaparambilrajan Last published at: May 16th, 2022 Problem PyPMML is a Python PMML scoring library. After installing PyPMML in a Databricks cluster, it fails with a Py4JError: Could not find py4j jar error. %python\r\n\r\nfrom pypmml import Model\r\nmodelb = Model.fromFile('/dbfs/shyam/DecisionTreeIris.pmml')\r\n\r\nError : Py4JError: Could not find py4j jar at Cause This error occurs due to a dependency on the default Py4J library. Databricks Runtime 5.0-6.6 uses Py4J 0.10.7. Databricks Runtime 7.0 and above uses Py4J 0.10.9. The default Py4J library is installed to a different location than a standard Py4J package. As a result, when PyPMML attempts to invoke Py4J from the default path, it fails. Solution Setup a cluster-scoped init script that copies the required Py4J jar file into the expected location. Use pip to install the version of Py4J that corresponds to your Databricks Runtime version. For example, in Databricks Runtime 6.5 run pip install py4j==<0.10.7> in a notebook in install Py4J 0.10.7 on the cluster. Run find /databricks/ -name \"py4j*jar\" in a notebook to confirm the full path to the Py4J jar file. It is usually located in a path similar to /databricks/python3/share/py4j/. Manually copy the Py4J jar file from the install path to the DBFS path /dbfs/py4j/. Run the following code snippet in a Python notebook to create the install-py4j-jar.sh init script. Make sure the version number of Py4J listed in the snippet corresponds to your Databricks Runtime version. %python\r\n\r\ndbutils.fs.put(\"/databricks/init-scripts/install-py4j-jar.sh\", \"\"\"\r\n\r\n#!/bin/bash\r\necho \"Copying at `date`\"\r\nmkdir -p /share/py4j/ /current-release/\r\ncp /dbfs/py4j/py4j<version number>.jar /share/py4j/\r\ncp /dbfs/py4j/py4j<version number>.jar /current-release/\r\necho \"Copying completed at `date`\"\r\n\r\n\"\"\", True) Attach the install-py4j-jar.sh init script to your cluster, following the instructions in configure a cluster-scoped init script (AWS | Azure | GCP). Restart the cluster. Verify that PyPMML works as expected. Was this article helpful? (8) (11) Additional Informations Related Articles How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Install Turbodbc via init script Turbodbc is a Python module that uses the ODBC interface to access relational dat... Replace a default library jar Databricks includes a number of default Java and Scala libraries. You can replace... Python command fails with AssertionError: wrong color format Problem You run a Python notebook and it fails with an AssertionError: wrong colo... Related Articles How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Install Turbodbc via init script Turbodbc is a Python module that uses the ODBC interface to access relational dat... Replace a default library jar Databricks includes a number of default Java and Scala libraries. You can replace... Python command fails with AssertionError: wrong color format Problem You run a Python notebook and it fails with an AssertionError: wrong colo... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "PyPMML fails with Could not find py4j jar error",
          "view_href" : "https://kb.databricks.com/en_US/libraries/pypmml-fail-find-py4j-jar"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/bi/configure-simba-proxy-windows",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Business intelligence tools Configure Simba ODBC driver with a proxy in Windows Configure Simba ODBC driver with a proxy in Windows How to configure the Simba ODBC driver to connect through a proxy server when using Windows. Written by jordan.hicks Last published at: March 2nd, 2022 In this article you learn how to configure the Databricks ODBC Driver when your local Windows machine is behind a proxy server. Download the Simba driver for Windows Download and install the latest version of the Databricks ODBC Driver for Windows. Add proxy settings to the Windows registry Open the Windows registry and add the proxy settings to the Simba Spark ODBC Driver key. Open the Windows Registry Editor. Navigate to the HKEY_LOCAL_MACHINE\\SOFTWARE\\Simba\\Simba Spark ODBC Driver\\Driver key. Click Edit. Select New. Click String Value. Enter UseProxy as the Name and 1 as the Data value. Repeat this until you have added the following string value pairs: Name ProxyHost Data <proxy-host-address> Name ProxyPort Data <proxy-port-number> Name ProxyUID Data <proxy-username> Name ProxyPWD Data <proxy-password> Close the registry editor. Configure settings in ODBC Data Source Administrator Open the ODBC Data Sources application. Click the System DSN tab. Select the Simba Spark ODBC Driver and click Configure. Enter the connection information of your Apache Spark server. Click Advanced Options. Enable the Driver Config Take Precedence check box. Click OK. Click OK. Click OK. Was this article helpful? (13) (49) Additional Informations Related Articles Troubleshooting JDBC and ODBC connections This article provides information to help you troubleshoot the connection between... Related Articles Troubleshooting JDBC and ODBC connections This article provides information to help you troubleshoot the connection between... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Configure Simba ODBC driver with a proxy in Windows",
          "view_href" : "https://kb.databricks.com/en_US/bi/configure-simba-proxy-windows"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/cloud/redshift-connection-fails",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Cloud infrastructure Troubleshooting Amazon Redshift connection problems Troubleshooting Amazon Redshift connection problems Learn how to troubleshoot Amazon Redshift connection problems. Written by Adam Pavlacka Last published at: February 25th, 2022 Problem You created a VPC peering connection and configured an Amazon Redshift cluster in the peer network. When you attempt to access the Redshift cluster, you get the following error: Error message: OperationalError: could not connect to server: Connection timed out Cause This problem can occur if: VPC peering is misconfigured. The corresponding port is blocked at the network component level, due to Security Groups (SG), Network Access Control Lists (NACL), or other routing issues. Troubleshooting Step 1. Test the connection Check the AWS console and make sure the Redshift cluster is online in the target VPC. Run the following Bash commands to see if the connection to the cluster can be established: %sh nc -zv <hostname> <port>\r\n%sh lft <hostname>:<port>\r\n%sh telnet <hostname> <port> The connection should succeed and show the port as open. If not, go to step 2. xx.c1prsbaxxxxx.us-west-2.redshift.amazonaws.com [192.168.50.209] 5439 (?) open Step 2. Check for VPC peering or DNS error If the connection fails with either of the following errors, then the issue is a VPC peering or DNS error. The following error code indicates a VPC peering issue. Go to Step 3. xx.c1prsbaxxxxx.us-west-2.redshift.amazonaws.com [192.168.50.108] 5439 (?) : Connection timed out The following error code indicates a DNS lookup error: redshift.c1prsbaxxxx.us-west-2.redshift.amazonaws.com: forward host lookup failed: Unknown hos In this case, check that: The Redshift cluster is up. The hostname is typed correctly. The redshift cluster IP address works, in place of the hostname. Check the DNS resolution using nslookup: nslookup <hostname> If these checks appear normal, the error may lie somewhere else. Go to Step 4. Step 3. Check the VPC peering and DNS settings If Step 2 revealed a VPC peering or DNS issue: 1. Validate the peering configuration. Make sure the peering connections from requestor and acceptor VPC IDs are correct. Note the peering connection id, CIDR of Requestor and CIDR of the acceptor. Confirm that the peering connection is active from the target VPC. Make sure DNS resolution check is turned on for the Redshift VPC. When you're done, go to Step 4. 2. Check the following components from the Databricks Deployment VPC. Verify that the correct CIDR of the target VPC (Redshift) is added to the route table of the deployment VPC and routed to the correct target, which is the peering connection id. Check the NACL attached to the subnets and allow all traffic to Redshift, for both inbound and outbound rules. Check the Security Group of the deployment VPC. It should be an unmanaged security group. Make sure that port 5439 ( redshift ) is open to the target security group that is attached to Redshift. 3. Check the following components from the Redshift VPC. Verify the correct CIDR of the target VPC (Databricks deployment) is added to the route table of the deployment VPC and routed to correct target -peering connection id. Check NACL and allow all traffic from redshift (inbound rules and outbound rules). Check the Security Group of the Redshift security group. Make sure that port 5439 ( redshift ) is open to the target security group (the unmanaged security group inside the Databricks VPC). Step 4. Validate and verify the connectivity between the peered VPCs Perform the connection test again: nc -zv <hostname> <port> The connection test should succeed. If not, contact Databricks Support. xx.c1prsbaxxxxx.us-west-2.redshift.amazonaws.com [192.168.50.209] 5439 (?) open Was this article helpful? (7) (16) Additional Informations Related Articles AWS services fail with No region provided error Problem Your code snippets that use AWS services fail with a java.lang.IllegalArg... Related Articles AWS services fail with No region provided error Problem Your code snippets that use AWS services fail with a java.lang.IllegalArg... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Troubleshooting Amazon Redshift connection problems",
          "view_href" : "https://kb.databricks.com/en_US/cloud/redshift-connection-fails"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data-sources/access-blobstore-odbc",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data sources Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2 Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2 Learn how to troubleshoot JDBC and ODBC access to Azure Data Lake Storage Gen2 from Databricks. Written by Adam Pavlacka Last published at: June 1st, 2022 Problem Delete Info In general, you should use Databricks Runtime 5.2 and above, which include a built-in Azure Blob File System (ABFS) driver, when you want to access Azure Data Lake Storage Gen2 (ADLS Gen2). This article applies to users who are accessing ADLS Gen2 storage using JDBC/ODBC instead. When you run a SQL query from a JDBC or ODBC client to access ADLS Gen2, the following error occurs: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalArgumentException: No value for dfs.adls.oauth2.access.token.provider found in conf file.\r\n\r\n18/10/23 21:03:28 ERROR SparkExecuteStatementOperation: Error executing query, currentState RUNNING,\r\njava.util.concurrent.ExecutionException: java.io.IOException: There is no primary group for UGI (Basic token)chris.stevens+dbadmin (auth:SIMPLE)\r\n  at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)\r\n  at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)\r\n  at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\r\n  at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\r\n  at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2344)\r\n  at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2316)\r\n  at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\r\n  at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\r\n  at com.google.common.cache.LocalCache.get(LocalCache.java:3932)\r\n  at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\r\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:158)\r\n  at org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:257)\r\n  at org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:313)\r\n  at\r\n  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\r\n  at scala.collection.immutable.List.foldLeft(List.scala:84)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:87)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:79) When you run the query from the SQL client, you get the following error: An error occurred when executing the SQL command:\r\nselect * from test_databricks limit 50\r\n\r\n[Simba][SparkJDBCDriver](500051) ERROR processing query/statement. Error Code: 0, SQL state: com.google.common.util.concurrent.UncheckedExecutionException: com.databricks.backend.daemon.data.common.InvalidMountException: Error while using path /mnt/crm_gen2/phonecalls for resolving path '/phonecalls' within mount at '/mnt/crm_gen2'., Query: SELECT * FROM `default`.`test_databricks` `default_test_databricks` LIMIT 50. [SQL State=HY000, DB Errorcode=500051]\r\n\r\nWarnings:\r\n[Simba][SparkJDBCDriver](500100) Error getting table information from database. Cause The root cause is incorrect configuration settings to create a JDBC or ODBC connection to ABFS via ADLS Gen2, which cause queries to fail. Solution Set spark.hadoop.hive.server2.enable.doAs to false in the cluster configuration settings. Was this article helpful? (14) (43) Additional Informations Related Articles Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... Unable to read files and list directories in a WASB filesystem Problem When you try reading a file on WASB with Spark, you get the following exc... Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... Optimize read performance from JDBC data sources Problem Reading data from an external JDBC database is slow. How can I improve re... Related Articles Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... Unable to read files and list directories in a WASB filesystem Problem When you try reading a file on WASB with Spark, you get the following exc... Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... Optimize read performance from JDBC data sources Problem Reading data from an external JDBC database is slow. How can I improve re... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2",
          "view_href" : "https://kb.databricks.com/en_US/data-sources/access-blobstore-odbc"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/bchashjoin-exceeds-bcjointhreshold-oom",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark Broadcast join exceeds threshold, returns out of memory error Broadcast join exceeds threshold, returns out of memory error Resolve an Apache Spark OutOfMemorySparkException error that occurs when a table using BroadcastHashJoin exceeds the BroadcastJoinThreshold. Written by sandeep.chandran Last published at: May 23rd, 2022 Problem You are attempting to join two large tables, projecting selected columns from the first table and all columns from the second table. Despite the total size exceeding the limit set by spark.sql.autoBroadcastJoinThreshold, BroadcastHashJoin is used and Apache Spark returns an OutOfMemorySparkException error. org.apache.spark.sql.execution.OutOfMemorySparkException: Size of broadcasted table far exceeds estimates and exceeds limit of spark.driver.maxResultSize=1073741824. You can disable broadcasts for this query using set spark.sql.autoBroadcastJoinThreshold=-1 Cause This is due to a limitation with Spark’s size estimator. If the estimated size of one of the DataFrames is less than the autoBroadcastJoinThreshold, Spark may use BroadcastHashJoin to perform the join. If the available nodes do not have enough resources to accommodate the broadcast DataFrame, your job fails due to an out of memory error. Solution There are three different ways to mitigate this issue. Use ANALYZE TABLE (AWS | Azure) to collect details and compute statistics about the DataFrames before attempting a join. Cache the table (AWS | Azure) you are broadcasting. Run explain on your join command to return the physical plan. %sql\r\nexplain(<join command>) Review the physical plan. If the broadcast join returns BuildLeft, cache the left side table. If the broadcast join returns BuildRight, cache the right side table. In Databricks Runtime 7.0 and above, set the join type to SortMergeJoin with join hints enabled. Was this article helpful? (17) (35) Additional Informations Related Articles Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Date functions only accept int values in Apache Spark 3.0 Problem You are attempting to use the date_add() or date_sub() functions in Spark... Disable broadcast when query plan has BroadcastNestedLoopJoin This article explains how to disable broadcast when the query plan has BroadcastN... Duplicate columns in the metadata error Problem Your Apache Spark job is processing a Delta table when the job fails with... Generate unique increasing numeric values This article shows you how to use Apache Spark functions to generate unique incre... Related Articles Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Date functions only accept int values in Apache Spark 3.0 Problem You are attempting to use the date_add() or date_sub() functions in Spark... Disable broadcast when query plan has BroadcastNestedLoopJoin This article explains how to disable broadcast when the query plan has BroadcastN... Duplicate columns in the metadata error Problem Your Apache Spark job is processing a Delta table when the job fails with... Generate unique increasing numeric values This article shows you how to use Apache Spark functions to generate unique incre... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Broadcast join exceeds threshold, returns out of memory error",
          "view_href" : "https://kb.databricks.com/en_US/sql/bchashjoin-exceeds-bcjointhreshold-oom"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/r/rmarkdown-sparklyr-code",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles R with Apache Spark Rendering an R markdown file containing sparklyr code fails Rendering an R markdown file containing sparklyr code fails Learn how to resolve failures when rendering an R markdown file containing sparklyr. Written by Adam Pavlacka Last published at: May 20th, 2022 Problem After you install and configure RStudio in the Databricks environment, when you launch RStudio and click the Knit button to knit a Markdown file that contains code to initialize a sparklyr context, rendering fails with the following error: failed to start sparklyr backend:object 'DATABRICKS_GUID' not found Calls: <Anonymous>… tryCatch -> tryCatchList-> tryCatchOne -> <Anonymous> Execution halted Cause If you try to initialize a sparklyr context in a Markdown notebook with code similar to the following, the Markdown page fails to render because the knitr process spawns a new namespace that is missing the 'DATABRICKS_GUID' global variable. %r\r\n\r\ncase library(sparklyr)\r\nsc <- spark_connect(method = \"databricks\") Solution Instead of rendering the Markdown page by clicking the Knit button in the R Markdown console, use the following script: %r\r\n\r\nrmarkdown::render(\"your_doc.Rmd\") Render the Markdown file using the R console, and then you can access the file in the RStudio Files tab. Was this article helpful? (9) (11) Additional Informations Related Articles Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... Install rJava and RJDBC libraries This article explains how to install rJava and RJBDC libraries. Problem When you ... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... Fix the version of R packages When you use the install.packages() function to install CRAN packages, you cannot... Related Articles Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... Install rJava and RJDBC libraries This article explains how to install rJava and RJBDC libraries. Problem When you ... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... Fix the version of R packages When you use the install.packages() function to install CRAN packages, you cannot... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Rendering an R markdown file containing sparklyr code fails",
          "view_href" : "https://kb.databricks.com/en_US/r/rmarkdown-sparklyr-code"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/parquet-to-delta-fails",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Converting from Parquet to Delta Lake fails Converting from Parquet to Delta Lake fails Converting a file from Parquet to Delta Lake fails with a partition error when you have a subdirectory. Expecting 0 partition column(s), but found 1 partition column(s) Written by Jose Gonzalez Last published at: May 10th, 2022 Problem You are attempting to convert a Parquet file to a Delta Lake file. The directory containing the Parquet file contains one or more subdirectories. The conversion fails with the error message: Expecting 0 partition column(s): [], but found 1 partition column(s): [<column_name>] from parsing the file name: <path_to_the_file_location>;. Cause The conversion process is attempting to process the subdirectory as a partition. This causes the error message. Solution If you are using Databricks Runtime 7.5 or below, ensure that directories containing Parquet files do not have subdirectories. This issue is resolved in Databricks Runtime 8.0 and above. Was this article helpful? (7) (16) Additional Informations Related Articles A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... Related Articles A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Converting from Parquet to Delta Lake fails",
          "view_href" : "https://kb.databricks.com/en_US/delta/parquet-to-delta-fails"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/calculate-number-of-cores",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters How to calculate the number of cores in a cluster How to calculate the number of cores in a cluster Learn how to calculate the number of cores in a Databricks cluster. Written by Adam Pavlacka Last published at: March 2nd, 2022 You can view the number of cores in a Databricks cluster in the Workspace UI using the Metrics tab on the cluster details page. Delete Note Azure Databricks cluster nodes must have a metrics service installed. If the driver and executors are of the same node type, you can also determine the number of cores available in a cluster programmatically, using Scala utility code: Use sc.statusTracker.getExecutorInfos.length to get the total number of nodes. The result includes the driver node, so subtract 1. Use java.lang.Runtime.getRuntime.availableProcessors to get the number of cores per node. Multiply both results (subtracting 1 from the total number of nodes) to get the total number of cores available. Scala example code: java.lang.Runtime.getRuntime.availableProcessors * (sc.statusTracker.getExecutorInfos.length -1) Was this article helpful? (13) (43) Additional Informations Related Articles Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Related Articles Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to calculate the number of cores in a cluster",
          "view_href" : "https://kb.databricks.com/en_US/clusters/calculate-number-of-cores"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/python-exec-display-cancelled",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Python command execution fails with AttributeError Python command execution fails with AttributeError Learn what to do when a Python command in your Databricks notebook fails with AttributeError. Written by Adam Pavlacka Last published at: May 19th, 2022 This article can help you resolve scenarios in which Python command execution fails with an AttributeError. Problem: 'tuple' object has no attribute 'type' When you run a notebook, Python command execution fails with the following error and stack trace: AttributeError: 'tuple' object has no attribute 'type' Traceback (most recent call last):\r\nFile \"/local_disk0/tmp/1547561952809-0/PythonShell.py\", line 23, in <module>\r\n  import matplotlib as mpl\r\nFile \"/databricks/python/local/lib/python2.7/site-packages/matplotlib/__init__.py\", line 122, in <module>\r\n  from matplotlib.cbook import is_string_like, mplDeprecation, dedent, get_label\r\nFile \"/databricks/python/local/lib/python2.7/site-packages/matplotlib/cbook.py\", line 33, in <module>\r\n  import numpy as np\r\nFile \"/databricks/python/local/lib/python2.7/site-packages/numpy/__init__.py\", line 142, in <module>\r\n  from . import core\r\nFile \"/databricks/python/local/lib/python2.7/site-packages/numpy/core/__init__.py\", line 57, in <module>\r\n  from . import numerictypes as nt\r\nFile \"/databricks/python/local/lib/python2.7/site-packages/numpy/core/numerictypes.py\", line 111, in <module>\r\n  from ._type_aliases import (\r\nFile \"/databricks/python/local/lib/python2.7/site-packages/numpy/core/_type_aliases.py\", line 63, in <module>\r\n  _concrete_types = {v.type for k, v in _concrete_typeinfo.items()}\r\nFile \"/databricks/python/local/lib/python2.7/site-packages/numpy/core/_type_aliases.py\", line 63, in <setcomp>\r\n  _concrete_types = {v.type for k, v in _concrete_typeinfo.items()}\r\nAttributeError: 'tuple' object has no attribute 'type'\r\n\r\n\r\n19/01/15 11:29:26 WARN PythonDriverWrapper: setupRepl:ReplId-7d8d1-8cc01-2d329-9: at the end, the status is\r\nError(ReplId-7d8d1-8cc01-2d329-,com.databricks.backend.daemon.driver.PythonDriverLocal$PythonException: Python shell failed to start in 30 seconds) Cause A newer version of numpy (1.16.1), which is installed by default by some PyPI clients, is incompatible with other libraries. Solution Follow the steps below to create a cluster-scoped init script (AWS | Azure | GCP) that removes the current version and installs version 1.15.0 of numpy. If the init script does not already exist, create a base directory to store it: %python\r\n\r\ndbutils.fs.mkdirs(\"dbfs:/databricks/<directory>/\") Create the following script: %python\r\n\r\ndbutils.fs.put(\"dbfs:/databricks/<directory>/numpy.sh\",\"\"\"\r\n#!/bin/bash\r\npip uninstall --yes numpy\r\nrm -rf /home/ubuntu/databricks/python/lib/python3.5/site-packages/numpy*\r\nrm -rf /databricks/python/lib/python3.5/site-packages/numpy*\r\n/usr/bin/yes | /home/ubuntu/databricks/python/bin/pip install numpy==1.15.0\r\n\"\"\",True) Confirm that the script exists: %python\r\n\r\ndisplay(dbutils.fs.ls(\"dbfs:/databricks/<directory>/numpy.sh\")) Go to the cluster configuration page (AWS | Azure | GCP) and click the Advanced Options toggle. At the bottom of the page, click the Init Scripts tab: In the Destination drop-down, select DBFS, provide the file path to the script, and click Add. Restart the cluster. In your PyPI client, pin the numpy installation to version 1.15.1, the latest working version. Problem: module 'lib' has no attribute 'SSL_ST_INIT' When you run a notebook, library installation fails and all Python commands executed on the notebook are cancelled with the following error and stack trace: AttributeError: module 'lib' has no attribute 'SSL_ST_INIT' Traceback (most recent call last): File \"/databricks/python3/bin/pip\", line 7, in <module>\r\n from pip._internal import main\r\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_internal/__init__.py\", line 40, in <module>\r\n from pip._internal.cli.autocompletion import autocomplete\r\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_internal/cli/autocompletion.py\", line 8, in <module>\r\n from pip._internal.cli.main_parser import create_main_parser\r\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_internal/cli/main_parser.py\", line 12, in <module>\r\n from pip._internal.commands import (\r\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_internal/commands/__init__.py\", line 6, in <module>\r\n from pip._internal.commands.completion import CompletionCommand\r\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_internal/commands/completion.py\", line 6, in <module>\r\n from pip._internal.cli.base_command import Command\r\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_internal/cli/base_command.py\", line 20, in <module>\r\n from pip._internal.download import PipSession\r\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_internal/download.py\", line 15, in <module>\r\n from pip._vendor import requests, six, urllib3\r\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_vendor/requests/__init__.py\", line 97, in <module>\r\n from pip._vendor.urllib3.contrib import pyopenssl\r\nFile \"/databricks/python3/lib/python3.5/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 46, in <module>\r\n import OpenSSL.SSL\r\nFile \"/databricks/python3/lib/python3.5/site-packages/OpenSSL/__init__.py\", line 8, in <module>\r\n from OpenSSL import rand, crypto, SSL\r\nFile \"/databricks/python3/lib/python3.5/site-packages/OpenSSL/SSL.py\", line 124, in <module>\r\n SSL_ST_INIT = _lib.SSL_ST_INIT AttributeError: module 'lib' has no attribute 'SSL_ST_INIT' Cause A newer version of the cryptography package (in this case, 2.7) was installed by default along with another PyPI library, and this cryptography version is incompatible with the version of pyOpenSSL included in Databricks Runtimes. Solution To resolve and prevent this issue, upgrade pyOpenSSL to the most recent version before you install any library. Use a cluster-scoped init script (AWS | Azure | GCP) to install the most recent version of pyOpenSSL: If the init script does not already exist, create a base directory to store it: %python\r\n\r\ndbutils.fs.mkdirs(\"dbfs:/databricks/<directory>/\") Create the following script: %python\r\n\r\ndbutils.fs.put(\"dbfs:/databricks/<directory>/openssl_fix.sh\",\"\"\"\r\n#!/bin/bash\r\necho \"Removing pyOpenSSL package\"\r\nrm -rf /databricks/python2/lib/python2.7/site-packages/OpenSSL\r\nrm -rf /databricks/python2/lib/python2.7/site-packages/pyOpenSSL-16.0.0-*.egg-info\r\nrm -rf /databricks/python3/lib/python3.5/site-packages/OpenSSL\r\nrm -rf /databricks/python3/lib/python3.5/site-packages/pyOpenSSL-16.0.0*.egg-info\r\n/databricks/python2/bin/pip install pyOpenSSL==19.0.0\r\n/databricks/python3/bin/pip3 install pyOpenSSL==19.0.0\r\n\"\"\", True) Confirm that the script exists: %python\r\n\r\ndisplay(dbutils.fs.ls(\"dbfs:/databricks/<directory>/openssl_fix.sh\")) Go to the cluster configuration page (AWS | Azure | GCP) and click the Advanced Options toggle. At the bottom of the page, click the Init Scripts tab: In the Destination drop-down, select DBFS, provide the file path to the script, and click Add. Restart the cluster. Was this article helpful? (14) (10) Additional Informations Related Articles Install and compile Cython This document explains how to run Spark code with compiled Cython code. The steps... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... List all workspace objects You can use the Databricks Workspace API (AWS | Azure | GCP) to recursively list ... Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Related Articles Install and compile Cython This document explains how to run Spark code with compiled Cython code. The steps... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... List all workspace objects You can use the Databricks Workspace API (AWS | Azure | GCP) to recursively list ... Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Python command execution fails with AttributeError",
          "view_href" : "https://kb.databricks.com/en_US/python/python-exec-display-cancelled"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/cloud/adls-gen1-mount-problem",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Cloud infrastructure Unable to mount Azure Data Lake Storage Gen1 account Unable to mount Azure Data Lake Storage Gen1 account Learn how to resolve errors that occur when mounting Azure Data Lake Storage Gen1 to Databricks. Written by Adam Pavlacka Last published at: February 25th, 2022 Problem When you try to mount an Azure Data Lake Storage (ADLS) Gen1 account on Databricks, it fails with the error: com.microsoft.azure.datalake.store.ADLException: Error creating directory /\r\nError fetching access token\r\nOperation null failed with exception java.io.IOException : Server returned HTTP response code: 401 for URL: https://login.windows.net/18b0b5d6-b6eb-4f5d-964b-c03a6dfdeb22/oauth2/token\r\nLast encountered exception thrown after 5 tries. [java.io.IOException,java.io.IOException,java.io.IOException,java.io.IOException,java.io.IOException]\r\n [ServerRequestId:null]\r\nat com.microsoft.azure.datalake.store.ADLStoreClient.getExceptionFromResponse(ADLStoreClient.java:1169)\r\nat com.microsoft.azure.datalake.store.ADLStoreClient.createDirectory(ADLStoreClient.java:589)\r\nat com.databricks.adl.AdlFileSystem.mkdirs(AdlFileSystem.java:533)\r\nAt com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$mkdirs$1$$anonfun$apply$mcZ$sp$7$$anonfun$apply$mcZ$sp$8.apply$mcZ$sp(DatabricksFileSystemV2.scala:638) Cause This error can occur if the ADLS Gen1 account was previously mounted in the workspace, but not unmounted, and the credential used for that mount subsequently expired. When you try to mount the same account with a new credential, there is a conflict between the expired and new credentials. Solution You need to unmount all existing mounts, and then create a new mount with a new, unexpired credential. For more information, see Mount Azure Data Lake Storage Gen1 with DBFS (AWS) and Mount Azure Data Lake Storage Gen1 with DBFS (Azure). Was this article helpful? (16) (47) Additional Informations Related Articles AWS services fail with No region provided error Problem Your code snippets that use AWS services fail with a java.lang.IllegalArg... S3 part number must be between 1 and 10000 inclusive Problem When you copy a large file from the local file system to DBFS on S3, the ... Access denied when writing logs to an S3 bucket Problem When you try to write log files to an S3 bucket, you get the error: com.a... Unable to load AWS credentials Problem When you try to access AWS resources like S3, SQS or Redshift, the operat... Related Articles AWS services fail with No region provided error Problem Your code snippets that use AWS services fail with a java.lang.IllegalArg... S3 part number must be between 1 and 10000 inclusive Problem When you copy a large file from the local file system to DBFS on S3, the ... Access denied when writing logs to an S3 bucket Problem When you try to write log files to an S3 bucket, you get the error: com.a... Unable to load AWS credentials Problem When you try to access AWS resources like S3, SQS or Redshift, the operat... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Unable to mount Azure Data Lake Storage Gen1 account",
          "view_href" : "https://kb.databricks.com/en_US/cloud/adls-gen1-mount-problem"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/custom-docker-requires-root",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Custom Docker image requires root Custom Docker image requires root Custom Docker containers must be configured to start as the root user when used with Databricks. Written by dayanand.devarapalli Last published at: March 4th, 2022 Problem You are trying to launch a Databricks cluster with a custom Docker container, but cluster creation fails with an error. {\r\n\"reason\": {\r\n\"code\": \"CONTAINER_LAUNCH_FAILURE\",\r\n\"type\": \"SERVICE_FAULT\",\r\n\"parameters\": {\r\n\"instance_id\": \"i-xxxxxxx\",\r\n\"databricks_error_message\": \"Failed to launch spark container on instance i-xxxx. Exception: Could not add container for xxxx with address xxxx. Could not mkdir in container\"\r\n              }\r\n          }\r\n} Cause Databricks clusters require a root user and sudo. Custom container images that are configured to start as a non-root user are not supported. For more information, review the custom container documentation. Solution You must configure your Docker container to start as the root user. Example This container configuration starts as the standard user ubuntu. It fails to launch. FROM databricksruntime/standard:8.x\r\nRUN apt-get update -y && apt-get install -y git && \\\r\nln -s /databricks/conda/envs/dcs-minimal/bin/pip /usr/local/bin/pip && \\\r\nln -s /databricks/conda/envs/dcs-minimal/bin/python /usr/local/bin/python\r\nCOPY . /app\r\nWORKDIR /app\r\nRUN pip install -r requirements.txt .\r\nRUN chown -R ubuntu /app\r\nUSER ubuntu This container configuration starts as the root user. It launches successfully. FROM databricksruntime/standard:8.x\r\nRUN apt-get update -y && apt-get install -y git && \\\r\nln -s /databricks/conda/envs/dcs-minimal/bin/pip /usr/local/bin/pip && \\\r\nln -s /databricks/conda/envs/dcs-minimal/bin/python /usr/local/bin/python\r\nCOPY . /app\r\nWORKDIR /app\r\nRUN pip install -r requirements.txt . Was this article helpful? (10) (15) Additional Informations Related Articles Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Cluster Apache Spark configuration not applied Problem Your cluster’s Spark configuration values are not applied. Cause This hap... Related Articles Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Cluster Apache Spark configuration not applied Problem Your cluster’s Spark configuration values are not applied. Cause This hap... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Custom Docker image requires root",
          "view_href" : "https://kb.databricks.com/en_US/clusters/custom-docker-requires-root"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/create-df-from-json-string-python-dictionary",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Create a DataFrame from a JSON string or Python dictionary Create a DataFrame from a JSON string or Python dictionary Create an Apache Spark DataFrame from a variable containing a JSON string or a Python dictionary. Written by ram.sankarasubramanian Last published at: July 1st, 2022 In this article we are going to review how you can create an Apache Spark DataFrame from a variable containing a JSON string or a Python dictionary. Create a Spark DataFrame from a JSON string Add the JSON content from the variable to a list. %scala\r\n\r\nimport scala.collection.mutable.ListBuffer\r\nval json_content1 = \"{'json_col1': 'hello', 'json_col2': 32}\"\r\nval json_content2 = \"{'json_col1': 'hello', 'json_col2': 'world'}\"\r\n\r\nvar json_seq = new ListBuffer[String]()\r\njson_seq += json_content1\r\njson_seq += json_content2 Create a Spark dataset from the list. %scala\r\n\r\nval json_ds = json_seq.toDS() Use spark.read.json to parse the Spark dataset. %scala\r\n\r\nval df= spark.read.json(json_ds)\r\ndisplay(df) Combined sample code These sample code blocks combine the previous steps into individual examples. The Python and Scala samples perform the same tasks. %python\r\n\r\njson_content1 = \"{'json_col1': 'hello', 'json_col2': 32}\"\r\njson_content2 = \"{'json_col1': 'hello', 'json_col2': 'world'}\"\r\n\r\njson_list = []\r\njson_list.append(json_content1)\r\njson_list.append(json_content2)\r\n\r\ndf = spark.read.json(sc.parallelize(json_list))\r\ndisplay(df) %scala\r\n\r\nimport scala.collection.mutable.ListBuffer\r\nval json_content1 = \"{'json_col1': 'hello', 'json_col2': 32}\"\r\nval json_content2 = \"{'json_col1': 'hello', 'json_col2': 'world'}\"\r\n\r\nvar json_seq = new ListBuffer[String]()\r\njson_seq += json_content1\r\njson_seq += json_content2\r\n\r\nval json_ds = json_seq.toDS()\r\nval df= spark.read.json(json_ds)\r\ndisplay(df) Extract a string column with JSON data from a DataFrame and parse it Select the JSON column from a DataFrame and convert it to an RDD of type RDD[Row]. %scala\r\n\r\n\r\nimport org.apache.spark.sql.functions._\r\n\r\n\r\nval test_df = Seq((\"1\", \"{'json_col1': 'hello', 'json_col2': 32}\", \"1.0\"),(\"1\", \"{'json_col1': 'hello', 'json_col2': 'world'}\", \"1.0\")).toDF(\"row_number\", \"json\", \"token\")\r\n\r\n\r\nval row_rdd = test_df.select(col(\"json\")).rdd  // Selecting just the JSON column and converting it to RDD. Convert RDD[Row] to RDD[String]. %scala\r\n\r\nval string_rdd = row_rdd.map(_.mkString(\",\")) Use spark.read.json to parse the RDD[String]. %scala\r\n\r\n\r\nval df1= spark.read.json(string_rdd)\r\n display(df1) Combined sample code This sample code block combines the previous steps into a single example. %scala\r\n\r\nimport org.apache.spark.sql.functions._\r\n\r\nval test_df = Seq((\"1\", \"{'json_col1': 'hello', 'json_col2': 32}\", \"1.0\"),(\"1\", \"{'json_col1': 'hello', 'json_col2': 'world'}\", \"1.0\")).toDF(\"row_number\", \"json\", \"token\")\r\n\r\nval row_rdd = test_df.select(col(\"json\")).rdd\r\nval string_rdd = row_rdd.map(_.mkString(\",\"))\r\n\r\nval df1= spark.read.json(string_rdd)\r\ndisplay(df1) Create a Spark DataFrame from a Python dictionary Check the data type and confirm that it is of dictionary type. %python\r\n\r\njsonDataDict = {\"job_id\":33100,\"run_id\":1048560,\"number_in_job\":1,\"state\":{\"life_cycle_state\":\"PENDING\",\"state_message\":\"Waiting for cluster\"},\"task\":{\"notebook_task\":{\"notebook_path\":\"/Users/user@databricks.com/path/test_notebook\"}},\"cluster_spec\":{\"new_cluster\":{\"spark_version\":\"4.3.x-scala2.11\",\"attributes\":{\"type\":\"fixed_node\",\"memory\":\"8g\"},\"enable_elastic_disk\":\"false\",\"num_workers\":1}},\"cluster_instance\":{\"cluster_id\":\"0000-000000-wares10\"},\"start_time\":1584689872601,\"setup_duration\":0,\"execution_duration\":0,\"cleanup_duration\":0,\"creator_user_name\":\"user@databricks.com\",\"run_name\":\"my test job\",\"run_page_url\":\"https://testurl.databricks.com#job/33100/run/1\",\"run_type\":\"SUBMIT_RUN\"}\r\n\r\ntype(jsonDataDict) Use json.dumpsto convert the Python dictionary into a JSON string. %python\r\n\r\nimport json\r\njsonData = json.dumps(jsonDataDict) Add the JSON content to a list. %python\r\n\r\njsonDataList = []\r\njsonDataList.append(jsonData) Convert the list to a RDD and parse it using spark.read.json. %python\r\n\r\njsonRDD = sc.parallelize(jsonDataList)\r\ndf = spark.read.json(jsonRDD)\r\ndisplay(df) Combined sample code These sample code block combines the previous steps into a single example. %python\r\n\r\njsonDataDict = {\"job_id\":33100,\"run_id\":1048560,\"number_in_job\":1,\"state\":{\"life_cycle_state\":\"PENDING\",\"state_message\":\"Waiting for cluster\"},\"task\":{\"notebook_task\":{\"notebook_path\":\"/Users/user@databricks.com/path/test_notebook\"}},\"cluster_spec\":{\"new_cluster\":{\"spark_version\":\"4.3.x-scala2.11\",\"attributes\":{\"type\":\"fixed_node\",\"memory\":\"8g\"},\"enable_elastic_disk\":\"false\",\"num_workers\":1}},\"cluster_instance\":{\"cluster_id\":\"0000-000000-wares10\"},\"start_time\":1584689872601,\"setup_duration\":0,\"execution_duration\":0,\"cleanup_duration\":0,\"creator_user_name\":\"user@databricks.com\",\"run_name\":\"my test job\",\"run_page_url\":\"https://testurl.databricks.com#job/33100/run/1\",\"run_type\":\"SUBMIT_RUN\"}\r\n\r\ntype(jsonDataDict)\r\n\r\nimport json\r\njsonData = json.dumps(jsonDataDict)\r\n\r\njsonDataList = []\r\njsonDataList.append(jsonData)\r\n\r\njsonRDD = sc.parallelize(jsonDataList)\r\ndf = spark.read.json(jsonRDD)\r\ndisplay(df) Example notebook Review the Parse a JSON string or Python dictionary example notebook. Was this article helpful? (14) (44) Additional Informations Related Articles Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... Related Articles Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Create a DataFrame from a JSON string or Python dictionary",
          "view_href" : "https://kb.databricks.com/en_US/scala/create-df-from-json-string-python-dictionary"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/performance-degradation",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Streaming job has degraded performance Streaming job has degraded performance Streaming job has poor performance after stopping and restarting from same checkpoint. Written by ashish Last published at: May 11th, 2022 Problem You have a streaming job which has its performance degrade over time. You start a new streaming job with the same configuration and same source, and it performs better than the existing job. Cause Issues with old checkpoints can result in performance degradation in long running streaming jobs. This can happen if the job was intermittently halted and restarted from the same checkpoint. You can validate the issue by reviewing the latest micro batch offset sequence number. Solution Change the checkpoint directory. Avoid restarting old streaming jobs with the same checkpoint directories. If you cannot change the checkpoint directory, increase the cluster capacity. Was this article helpful? (8) (12) Additional Informations Related Articles Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... Apache Spark job fails with a Connection pool shut down error Problem A Spark job fails with the error message java.lang.IllegalStateException:... How to ensure idempotency for jobs When you submit jobs through the Databricks Jobs REST API, idempotency is not gua... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... Related Articles Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... Apache Spark job fails with a Connection pool shut down error Problem A Spark job fails with the error message java.lang.IllegalStateException:... How to ensure idempotency for jobs When you submit jobs through the Databricks Jobs REST API, idempotency is not gua... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Streaming job has degraded performance",
          "view_href" : "https://kb.databricks.com/en_US/jobs/performance-degradation"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/inner-join-drops-records-in-result",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark Inner join drops records in result Inner join drops records in result Avoid dropped records when performing an inner join. Written by siddharth.panchal Last published at: May 23rd, 2022 Problem You perform an inner join, but the resulting joined table is missing data. For example, assume you have two tables, orders and models. %python\r\n\r\ndf_orders = spark.createDataFrame([('Nissan','Altima','2-door 2.5 S Coupe'), ('Nissan','Altima','4-door 3.5 SE Sedan'), ('Nissan','Altima',''), ('Nissan','Altima', None)], [\"Company\", \"Model\", \"Info\"]) %python\r\n\r\ndf_models = spark.createDataFrame([('Nissan','Altima',''), ('Nissan','Altima','2-door 2.5 S Coupe'), ('Nissan','Altima','2-door 3.5 SE Coupe'), ('Nissan','Altima','4-door 2.5 S Sedan'), ('Nissan','Altima','4-door 3.5 SE Sedan'), ('Nissan','Altima','4-door 3.5 SL Sedan'), ('Nissan','Altima','4-door HYBRID Sedan'), ('Nissan','Altima',None)], [\"Company\", \"Model\", \"Info\"]) You attempt a straight join of the two tables. %python\r\ndf_orders.createOrReplaceTempView(\"Orders\")\r\ndf_models.createOrReplaceTempView(\"Models\")\r\nSQL\r\nCopy to clipboardCopy\r\nSELECT *\r\nMAGIC FROM Orders a\r\nMAGIC INNER JOIN Models b\r\nMAGIC ON a.Company = b.Company\r\nMAGIC AND a.Model = b.Model\r\nMAGIC AND a.Info = b.Info The resulting joined table only includes three of the four records from the orders table. The record with a null value in a column does not appear in the results. Cause Apache Spark does not consider null values when performing a join operation. If you attempt to join tables, and some of the columns contain null values, the null records will not be included in the resulting joined table. Solution If your source tables contain null values, you should use the Spark null safe operator (<=>). When you use <=> Spark processes null values (instead of dropping them) when performing a join. For example, if we modify the sample code with <=>, the resulting table does not drop the null values. %sql\r\n\r\nSELECT *\r\nMAGIC FROM Orders a\r\nMAGIC INNER JOIN Models b\r\nMAGIC ON a.Company = b.Company\r\nMAGIC AND a.Model = b.Model\r\nMAGIC AND a.Info <=> b.Info Example notebook Review the Inner join drops null values example notebook. Was this article helpful? (8) (13) Additional Informations Related Articles Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Generate unique increasing numeric values This article shows you how to use Apache Spark functions to generate unique incre... Error when running MSCK REPAIR TABLE in parallel Problem You are trying to run MSCK REPAIR TABLE <table-name> commands for t... Error in SQL statement: AnalysisException: Table or view not found Problem When you try to query a table or view, you get this error: AnalysisExcept... Related Articles Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Generate unique increasing numeric values This article shows you how to use Apache Spark functions to generate unique incre... Error when running MSCK REPAIR TABLE in parallel Problem You are trying to run MSCK REPAIR TABLE <table-name> commands for t... Error in SQL statement: AnalysisException: Table or view not found Problem When you try to query a table or view, you get this error: AnalysisExcept... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Inner join drops records in result",
          "view_href" : "https://kb.databricks.com/en_US/sql/inner-join-drops-records-in-result"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/active-vs-dead-jobs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Distinguish active and dead jobs Distinguish active and dead jobs Learn how to distinguish between active and dead Databricks jobs. Written by Adam Pavlacka Last published at: May 10th, 2022 Problem On clusters where there are too many concurrent jobs, you often see some jobs stuck in the Spark UI without any progress. This complicates identifying which are the active jobs/stages versus the dead jobs/stages. Cause Whenever there are too many concurrent jobs running on a cluster, there is a chance that the Spark internal eventListenerBus drops events. These events are used to track job progress in the Spark UI. Whenever the event listener drops events you start seeing dead jobs/stages in Spark UI, which never finish. The jobs are actually finished but not shown as completed in the Spark UI. You observe the following traces in driver logs: 18/01/25 06:37:32 WARN LiveListenerBus: Dropped 5044 SparkListenerEvents since Thu Jan 25 06:36:32 UTC 2018 Solution There is no way to remove dead jobs from the Spark UI without restarting the cluster. However, you can identify the active jobs and stages by running the following commands: %scala\r\n\r\nsc.statusTracker.getActiveJobIds()  // Returns an array containing the IDs of all active jobs.\r\nsc.statusTracker.getActiveStageIds() // Returns an array containing the IDs of all active stages. Was this article helpful? (10) (43) Additional Informations Related Articles Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... Job cluster limits on notebook output Problem You are running a notebook on a job cluster and you get an error message ... Job fails, but Apache Spark tasks finish Problem Your Databricks job reports a failed status, but all Spark jobs and tasks... Related Articles Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... Job cluster limits on notebook output Problem You are running a notebook on a job cluster and you get an error message ... Job fails, but Apache Spark tasks finish Problem Your Databricks job reports a failed status, but all Spark jobs and tasks... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Distinguish active and dead jobs",
          "view_href" : "https://kb.databricks.com/en_US/jobs/active-vs-dead-jobs"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/conflicting-directory-structures-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming Conflicting directory structures error Conflicting directory structures error You should use distinct paths in the storage location, otherwise conflicting directory structures may result in an error. Written by ashish Last published at: May 19th, 2022 Problem You have an Apache Spark job that is failing with a Java assertion error java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Example stack trace Caused by: org.apache.spark.sql.streaming.StreamingQueryException: There was an error when trying to infer the partition schema of the current batch of files. Please provide your partition columns explicitly by using: .option('cloudFiles.partitionColumns', 'comma-separated-list')\r\n=== Streaming Query ===\r\nIdentifier: [id = aabc5549-cb4b-4e4e-9403-4e793f4824a0, runId = 4e743dda-909f-4932-9489-3dd0b364d811]\r\nCurrent Committed Offsets: {}\r\nCurrent Available Offsets: {CloudFilesSource[<file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt]: {'seqNum':423,'sourceVersion':1}}\r\n\r\nCurrent State: ACTIVE\r\nThread State: RUNNABLE\r\n\r\nLogical Plan:\r\nCloudFilesSource[<file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt]\r\nat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:385)\r\nat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:268)\r\nCaused by: java.lang.RuntimeException: There was an error when trying to infer the partition schema of the current batch of files. Please provide your partition columns explicitly by using: .option('cloudFiles.partitionColumns', 'comma-separated-list')\r\nat com.databricks.sql.fileNotification.autoIngest.CloudFilesErrors$.partitionInferenceError(CloudFilesErrors.scala:115)\r\nat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceFileIndex.liftedTree1$1(CloudFilesSourceFileIndex.scala:65)\r\nat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceFileIndex.partitionSpec(CloudFilesSourceFileIndex.scala:63)\r\nat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:50)\r\nat com.databricks.sql.fileNotification.autoIngest.CloudFilesSource.getBatch(CloudFilesSource.scala:361)\r\n... 1 more\r\nCaused by: java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Suspicious paths:\r\n<file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt\r\n<file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt/clfy_x_clfy_evt\r\n\r\nIf provided paths are partition directories, please set 'basePath' in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.\r\nat scala.Predef$.assert(Predef.scala:223)\r\nat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:204)\r\nat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parseP Cause You have conflicting directory paths in the storage location. In the example stack trace, we see two conflicting directory paths. <file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt <file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt/clfy_x_clfy_evt Because these directories appear in the same hierarchy, an update in root or in a branch level can result in a conflict. Solution Avoid multiple concurrent updates in a hierarchical directory structure or updates happening in the same partition. You should make multiple distinct paths for updates once a conflict is detected. Alternatively, you can add more partitions. These example directories do not conflict. <file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt/evt=clfy_x_clfy_evt1 <file-system>://domain.com/km/gold/cfy_gold/clfy_x_clfy_evt/evt=clfy_x_clfy_evt2 Was this article helpful? (8) (13) Additional Informations Related Articles How to switch a SNS streaming job to a new SQS queue Problem You have a Structured Streaming job running via the S3-SQS connector. Sup... Checkpoint files not being deleted when using display() Problem You have a streaming job using display() to display DataFrames. %scala va... Kafka error: No resolvable bootstrap urls Problem You are trying to read or write data to a Kafka stream when you get an er... Get the path of files consumed by Auto Loader When you process streaming files with Auto Loader (AWS | Azure | GCP), events are... Related Articles How to switch a SNS streaming job to a new SQS queue Problem You have a Structured Streaming job running via the S3-SQS connector. Sup... Checkpoint files not being deleted when using display() Problem You have a streaming job using display() to display DataFrames. %scala va... Kafka error: No resolvable bootstrap urls Problem You are trying to read or write data to a Kafka stream when you get an er... Get the path of files consumed by Auto Loader When you process streaming files with Auto Loader (AWS | Azure | GCP), events are... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Conflicting directory structures error",
          "view_href" : "https://kb.databricks.com/en_US/streaming/conflicting-directory-structures-error"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/check-spark-property-modifiable",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks How to check if a spark property is modifiable in a notebook How to check if a spark property is modifiable in a notebook Learn how to modify Spark properties in a Databricks notebook. Written by Adam Pavlacka Last published at: May 16th, 2022 Problem You can tune applications by setting various configurations. Some configurations must be set at the cluster level, whereas some are set inside notebooks or applications. Solution To check if a particular Spark configuration can be set in a notebook, run the following command in a notebook cell: %scala\r\n\r\nspark.conf.isModifiable(\"spark.databricks.preemption.enabled\") If true is returned, then the property can be set in the notebook. Otherwise, it must be set at the cluster level. Was this article helpful? (12) (20) Additional Informations Related Articles Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Enable s3cmd for notebooks s3cmd is a client library that allows you to perform all AWS S3 operations from a... Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... Related Articles Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Enable s3cmd for notebooks s3cmd is a client library that allows you to perform all AWS S3 operations from a... Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to check if a spark property is modifiable in a notebook",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/check-spark-property-modifiable"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/running-c-plus-plus-code-scala",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Running C++ code in Scala Running C++ code in Scala Learn how to run C++ code in Scala with this example notebook. Written by Adam Pavlacka Last published at: May 23rd, 2022 Run C++ from Scala notebook Review the Run C++ from Scala notebook. Was this article helpful? (7) (13) Additional Informations Related Articles Intermittent NullPointerException when AQE is enabled Problem You get an intermittent NullPointerException error when saving your data.... Convert nested JSON to a flattened DataFrame This article shows you how to flatten nested JSON, using only $\"column.*\" and exp... Decimal$DecimalIsFractional assertion error Problem You are running a job on Databricks Runtime 7.x or above when you get a j... Related Articles Intermittent NullPointerException when AQE is enabled Problem You get an intermittent NullPointerException error when saving your data.... Convert nested JSON to a flattened DataFrame This article shows you how to flatten nested JSON, using only $\"column.*\" and exp... Decimal$DecimalIsFractional assertion error Problem You are running a job on Databricks Runtime 7.x or above when you get a j... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Running C++ code in Scala",
          "view_href" : "https://kb.databricks.com/en_US/scala/running-c-plus-plus-code-scala"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/install-turbodbc",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Install Turbodbc via init script Install Turbodbc via init script Install Turbodbc and its dependencies, libboost-all-dev, unixodbc-dev, and python-dev, with an init script. Written by John.Lourdu Last published at: May 11th, 2022 Turbodbc is a Python module that uses the ODBC interface to access relational databases. It has dependencies on libboost-all-dev, unixodbc-dev, and python-dev packages, which need to be installed in order. You can install these manually, or you can use an init script to automate the install. Create the init script Run this sample script in a notebook to create the init script on your cluster. %python\r\n\r\ndbutils.fs.mkdirs(\"dbfs:/<path-to-init-script>\")\r\ndbutils.fs.put(\"dbfs:/<path-to-init-script>/turbodbc_install.sh\", \"\"\"\r\n#!/bin/bash\r\n#install dependent packages\r\nsudo apt-get -y install libboost-all-dev unixodbc-dev python-dev\r\npip install turbodbc==4.1.1\r\n\"\"\",True) Remember the path to the init script. You will need it when configuring your cluster. Configure the init script Follow the documentation to configure a cluster-scoped init script (AWS | Azure | GCP). Specify the path to the init script. Use the same path that you used in the sample script. After configuring the init script, restart the cluster. Was this article helpful? (13) (47) Additional Informations Related Articles How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Init script fails to download Maven JAR Problem You have an init script that is attempting to install a library via Maven... Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... Related Articles How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Init script fails to download Maven JAR Problem You have an init script that is attempting to install a library via Maven... Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Install Turbodbc via init script",
          "view_href" : "https://kb.databricks.com/en_US/libraries/install-turbodbc"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/stream-xml-auto-loader",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming Stream XML files using an auto-loader Stream XML files using an auto-loader Stream XML files on Databricks by combining the auto-loading features of the Spark batch API with the OSS library Spark-XML. Written by Adam Pavlacka Last published at: May 19th, 2022 Apache Spark does not include a streaming API for XML files. However, you can combine the auto-loader features of the Spark batch API with the OSS library, Spark-XML, to stream XML files. In this article, we present a Scala based solution that parses XML data using an auto-loader. Install Spark-XML library You must install the Spark-XML OSS library on your Databricks cluster. Review the install a library on a cluster (AWS | Azure) documentation for more details. Delete Info You must ensure that the version of Spark-XML you are installing matches the version of Spark on your cluster. Create the XML file Create the XML file and use DBUtils (AWS | Azure) to save it to your cluster. %scala\r\n\r\nval xml2=\"\"\"<people>\r\n  <person>\r\n    <age born=\"1990-02-24\">25</age>\r\n  </person>\r\n  <person>\r\n    <age born=\"1985-01-01\">30</age>\r\n  </person>\r\n  <person>\r\n    <age born=\"1980-01-01\">30</age>\r\n  </person>\r\n</people>\"\"\"\r\n\r\ndbutils.fs.put(\"/<path-to-save-xml-file>/<name-of-file>.xml\",xml2) Define imports Import the required functions. %scala\r\n\r\nimport com.databricks.spark.xml.functions.from_xml\r\nimport com.databricks.spark.xml.schema_of_xml\r\nimport spark.implicits._\r\nimport com.databricks.spark.xml._\r\nimport org.apache.spark.sql.functions.{<input_file_name>} Define a UDF to convert binary to string The streaming DataFrame requires data to be in string format. You should define a user defined function to convert binary data to string data. %scala\r\n\r\n\r\nval toStrUDF = udf((bytes: Array[Byte]) => new String(bytes, \"UTF-8\")) Extract XML schema You must extract the XML schema before you can implement the streaming DataFrame. This can be inferred from the file using the schema_of_xml method from Spark-XML. The XML string is passed as input, from the binary Spark data. %scala\r\n\r\nval df_schema = spark.read.format(\"binaryFile\").load(\"/FileStore/tables/test/xml/data/age/\").select(toStrUDF($\"content\").alias(\"text\"))\r\n\r\nval payloadSchema = schema_of_xml(df_schema.select(\"text\").as[String]) Implement the stream reader At this point, all of the required dependencies have been met, so you can implement the stream reader. Use readStream with binary and autoLoader listing mode options enabled. Delete Info Listing mode is used when working with small amounts of data. You can leverage fileNotificationMode if you need to scale up your application. toStrUDF is used to convert binary data to string format (text). from_xml is used to convert the string to a complex struct type, with the user-defined schema. %scala\r\n\r\nval df = spark.readStream.format(\"cloudFiles\")\r\n  .option(\"cloudFiles.useNotifications\", \"false\") // Using listing mode, hence false is used\r\n  .option(\"cloudFiles.format\", \"binaryFile\")\r\n  .load(\"/FileStore/tables/test/xml/data/age/\")\r\n  .select(toStrUDF($\"content\").alias(\"text\")) // UDF to convert the binary to string\r\n  .select(from_xml($\"text\", payloadSchema).alias(\"parsed\")) // Function to convert string to complex types\r\n  .withColumn(\"path\",input_file_name) // input_file_name is used to extract the paths of input files View output Once everything is setup, view the output of display(df) in a notebook. Example notebook This example notebook combines all of the steps into a single, functioning example. Import it into your cluster to run the examples. Streaming XML example notebook Review the Streaming XML example notebook. Was this article helpful? (6) (14) Additional Informations Related Articles readStream() is not whitelisted error when running a query Problem You have table access control (AWS | Azure | GCP) enabled on your cluster... How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... Checkpoint files not being deleted when using display() Problem You have a streaming job using display() to display DataFrames. %scala va... Related Articles readStream() is not whitelisted error when running a query Problem You have table access control (AWS | Azure | GCP) enabled on your cluster... How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... Checkpoint files not being deleted when using display() Problem You have a streaming job using display() to display DataFrames. %scala va... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Stream XML files using an auto-loader",
          "view_href" : "https://kb.databricks.com/en_US/streaming/stream-xml-auto-loader"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/metastore/drop-table-corruptedmetadata",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Metastore Drop tables with corrupted metadata from the metastore Drop tables with corrupted metadata from the metastore Learn how to drop tables that contain corrupted metadata from a metastore. Written by Adam Pavlacka Last published at: May 16th, 2022 Problem Sometimes you cannot drop a table from the Databricks UI. Using %sql or spark.sql to drop table doesn’t work either. Cause The metadata (table schema) stored in the metastore is corrupted. When you run Drop table command, Spark checks whether table exists or not before dropping the table. Since the metadata is corrupted for the table Spark can’t drop the table and fails with following exception. %scala\r\n\r\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: The metadata is corrupted Solution Use a Hive client to drop the table since the Hive client doesn’t check for the table existence as Spark does. To drop a table: Create a function inside Hive package. %scala\r\n\r\npackage org.apache.spark.sql.hive {\r\nimport org.apache.spark.sql.hive.HiveUtils\r\nimport org.apache.spark.SparkContext\r\n\r\nobject utils {\r\n    def dropTable(sc: SparkContext, dbName: String, tableName: String, ignoreIfNotExists: Boolean, purge: Boolean): Unit = {\r\n      HiveUtils\r\n          .newClientForMetadata(sc.getConf, sc.hadoopConfiguration)\r\n          .dropTable(dbName, tableName, ignoreIfNotExists, false)\r\n    }\r\n  }\r\n} Drop corrupted tables. %scala\r\n\r\nimport org.apache.spark.sql.hive.utils\r\nutils.dropTable(sc, \"default\", \"my_table\", true, true) Was this article helpful? (10) (10) Additional Informations Related Articles Autoscaling is slow with an external metastore Problem You have an external metastore configured on your cluster and autoscaling... Drop database without deletion By default, the DROP DATABASE (AWS | Azure | GCP) command drops the database and ... How to create table DDLs to import into an external metastore Databricks supports using external metastores instead of the default Hive metasto... Data too long for column error Problem You are trying to insert a struct into a table, but you get a java.sql.SQ... Related Articles Autoscaling is slow with an external metastore Problem You have an external metastore configured on your cluster and autoscaling... Drop database without deletion By default, the DROP DATABASE (AWS | Azure | GCP) command drops the database and ... How to create table DDLs to import into an external metastore Databricks supports using external metastores instead of the default Hive metasto... Data too long for column error Problem You are trying to insert a struct into a table, but you get a java.sql.SQ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Drop tables with corrupted metadata from the metastore",
          "view_href" : "https://kb.databricks.com/en_US/metastore/drop-table-corruptedmetadata"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/redshift-loading-timestamp-problem",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management Invalid timestamp when loading data into Amazon Redshift Invalid timestamp when loading data into Amazon Redshift Learn how to resolve an invalid timestamp error when loading data into AWS Redshift. Written by Adam Pavlacka Last published at: May 31st, 2022 Problem When you use a spark-redshift write operation to save timestamp data to Amazon Redshift, the following error can occur if that timestamp data includes timezone information. Error (code 1206) while loading data into Redshift: \"Invalid timestamp format or value [YYYY-MM-DD HH24:MI:SSOF]\" Cause The Redshift table is using the Timestamp data type that doesn’t store timezone information. Solution Include the option .options(\"extracopyoptions\", \"TIMEFORMAT 'auto'\") as shown in the following Scala code: %scala\r\n\r\ndf.write\r\n.format(\"com.databricks.spark.redshift\")\r\n.options(...)\r\n.options(\"extracopyoptions\", \"TIMEFORMAT 'auto'\")\r\n.mode(\"append\")\r\n.save() If you specify auto as the argument for the DATEFORMAT or TIMEFORMAT parameter, Amazon Redshift automatically recognizes and converts the date format or time format in your source data. Was this article helpful? (7) (16) Additional Informations Related Articles Behavior of the randomSplit method When using randomSplit on a DataFrame, you could potentially observe inconsistent... How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... Prevent duplicated columns when joining two DataFrames If you perform a join in Spark and don’t specify your join correctly you’ll end u... Get and set Apache Spark configuration properties in a notebook In most cases, you set the Spark config (AWS | Azure ) at the cluster level. Howe... Related Articles Behavior of the randomSplit method When using randomSplit on a DataFrame, you could potentially observe inconsistent... How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... Prevent duplicated columns when joining two DataFrames If you perform a join in Spark and don’t specify your join correctly you’ll end u... Get and set Apache Spark configuration properties in a notebook In most cases, you set the Spark config (AWS | Azure ) at the cluster level. Howe... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Invalid timestamp when loading data into Amazon Redshift",
          "view_href" : "https://kb.databricks.com/en_US/data/redshift-loading-timestamp-problem"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dbfs/dbfs-root-permissions",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Databricks File System (DBFS) Cannot read Databricks objects stored in the DBFS root directory Cannot read Databricks objects stored in the DBFS root directory Learn what to do when you cannot read Databricks objects stored in the DBFS root directory. Written by Adam Pavlacka Last published at: March 8th, 2022 Problem An Access Denied error returns when you attempt to read Databricks objects stored in the DBFS root directory in blob storage from outside a Databricks cluster. Cause This is normal behavior for the DBFS root directory. Databricks stores objects like libraries and other temporary system files in the DBFS root directory. Databricks is the only user that can read these objects. Solution Databricks does not recommend using the root directory for storing any user files or objects. Instead, create a different blob storage directory and mount it to DBFS. Was this article helpful? (11) (16) Additional Informations Related Articles Cannot access objects written by Databricks from outside Databricks Problem When you attempt to access an object in an S3 location written by Databri... Related Articles Cannot access objects written by Databricks from outside Databricks Problem When you attempt to access an object in an S3 location written by Databri... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cannot read Databricks objects stored in the DBFS root directory",
          "view_href" : "https://kb.databricks.com/en_US/dbfs/dbfs-root-permissions"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/spark-submit-fail-parse-byte-string",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Apache Spark job fails with Failed to parse byte string Apache Spark job fails with Failed to parse byte string Apache Spark job fails with a Failed to parse byte string error. Written by noopur.nigam Last published at: May 10th, 2022 Problem Spark-submit jobs fail with a Failed to parse byte string: -1 error message. java.util.concurrent.ExecutionException: java.lang.NumberFormatException: Size must be specified as bytes (b), kibibytes (k), mebibytes (m), gibibytes (g), tebibytes (t), or pebibytes(p). E.g. 50b, 100k, or 250m.\r\nFailed to parse byte string: -1\r\nat java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\nat java.util.concurrent.FutureTask.get(FutureTask.java:206)\r\nat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:182)\r\n... 108 more\r\nCaused by: java.lang.NumberFormatException: Size must be specified as bytes (b), kibibytes (k), mebibytes (m), gibibytes (g), tebibytes (t), or pebibytes(p). E.g. 50b, 100k, or 250m.\r\nFailed to parse byte string: -1 Cause The value of the spark.driver.maxResultSize application property is negative. Solution The value assigned to spark.driver.maxResultSize defines the maximum size (in bytes) of the serialized results for each Spark action. You can assign a positive value to the spark.driver.maxResultSize property to define a specific size. You can also assign a value of 0 to define an unlimited maximum size. You cannot assign a negative value to this property. If the total size of a job is above the spark.driver.maxResultSize value, the job is aborted. You should be careful when setting an excessively high (or unlimited) value for spark.driver.maxResultSize. A high limit can cause out-of-memory errors in the driver if the spark.driver.memory property is not set high enough. See Spark Configuration Application Properties for more details. Was this article helpful? (8) (12) Additional Informations Related Articles Apache Spark Jobs hang due to non-deterministic custom UDF Problem Sometimes Apache Spark jobs hang indefinitely due to the non-deterministi... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... Create table in overwrite mode fails when interrupted Problem When you attempt to rerun an Apache Spark write operation by cancelling t... Job fails, but Apache Spark tasks finish Problem Your Databricks job reports a failed status, but all Spark jobs and tasks... Related Articles Apache Spark Jobs hang due to non-deterministic custom UDF Problem Sometimes Apache Spark jobs hang indefinitely due to the non-deterministi... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... Create table in overwrite mode fails when interrupted Problem When you attempt to rerun an Apache Spark write operation by cancelling t... Job fails, but Apache Spark tasks finish Problem Your Databricks job reports a failed status, but all Spark jobs and tasks... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Apache Spark job fails with Failed to parse byte string",
          "view_href" : "https://kb.databricks.com/en_US/jobs/spark-submit-fail-parse-byte-string"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/onehotencoderestimator-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning Error when importing OneHotEncoderEstimator Error when importing OneHotEncoderEstimator You get an error message when trying to import OneHotEncoderEstimator. Written by shyamprasad.miryala Last published at: May 16th, 2022 Problem You have migrated a notebook from Databricks Runtime 6.4 for Machine Learning or below to Databricks Runtime 7.3 for Machine Learning or above. You are attempting to import OneHotEncoderEstimator and you get an import error. ImportError: cannot import name 'OneHotEncoderEstimator' from 'pyspark.ml.feature' (/databricks/spark/python/pyspark/ml/feature.py) Cause OneHotEncoderEstimator was renamed to OneHotEncoder in Apache Spark 3.0. Solution You must replace OneHotEncoderEstimator references in your notebook with OneHotEncoder. For example, the following sample code returns an import error in Databricks Runtime 7.3 for Machine Learning or above: %python\r\n\r\nfrom pyspark.ml.feature import OneHotEncoderEstimator The following sample code functions correctly in Databricks Runtime 7.3 for Machine Learning or above: %python\r\n\r\nfrom pyspark.ml.feature import OneHotEncoder Was this article helpful? (14) (10) Additional Informations Related Articles Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... H2O.ai Sparkling Water cluster not reachable Problem You are trying to initialize H2O.ai’s Sparkling Water on Databricks Runti... Fitting an Apache SparkML model throws error Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.... How to perform group K-fold cross validation with Apache Spark Cross validation randomly splits the training data into a specified number of fol... Related Articles Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... H2O.ai Sparkling Water cluster not reachable Problem You are trying to initialize H2O.ai’s Sparkling Water on Databricks Runti... Fitting an Apache SparkML model throws error Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.... How to perform group K-fold cross validation with Apache Spark Cross validation randomly splits the training data into a specified number of fol... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Error when importing OneHotEncoderEstimator",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/onehotencoderestimator-error"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/r/rstudio-server-backend-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles R with Apache Spark RStudio server backend connection error RStudio server backend connection error RStudio server backend connection error occurs if you exceed the maximum number of RBackends on your cluster. Written by arvind.ravish Last published at: May 20th, 2022 Problem You get a backend connection error when using RStudio server. Error in Sys.setenv(EXISTING_SPARKR_BACKEND_PORT = system(paste0(\"wget -qO - 'http://localhost:6061/?type=\\\"com.databricks.backend.common.rpc.DriverMessages$StartRStudioSparkRBackend\\\"' --post-data='{\\\"@class\\\":\\\"com.databricks.backend.common.rpc.DriverMessages$StartRStudioSparkRBackend\\\", \\\"guid\\\": \\\"\", :\r\nwrong length for argument If you view the cluster driver and worker logs (AWS | Azure | GCP), you see a message about exceeding the maximum number of RBackends. 21/08/09 15:02:26 INFO RDriverLocal: 312. RDriverLocal.3f6d80d6-70c4-4101-b50f-2530df112ea2: Exceeded maximum number of RBackends limit: 200\r\n21/08/09 15:03:55 INFO RDriverLocal: 313. RDriverLocal.3f6d80d6-70c4-4101-b50f-2530df112ea2: Exceeded maximum number of RBackends limit: 200\r\n21/08/09 15:04:06 INFO RDriverLocal: 314. RDriverLocal.3f6d80d6-70c4-4101-b50f-2530df112ea2: Exceeded maximum number of RBackends limit: 200\r\n21/08/09 15:13:42 INFO RDriverLocal: 315. RDriverLocal.3f6d80d6-70c4-4101-b50f-2530df112ea2: Exceeded maximum number of RBackends limit: 200 Cause Databricks clusters are configured for 200 RBackends by default. If you exceed this limit, you get an error. Solution You can use an init script to increase the soft limit of RBackends available for use. This sample code creates an init script that sets a limit of 400 RBackends on the cluster. %scala\r\n\r\nval initScriptContent = s\"\"\"\r\n |#!/bin/bash\r\n |cat > /databricks/common/conf/rbackend_limit.conf << EOL\r\n |{\r\n | databricks.daemon.driver.maxNumRBackendsPerDriver = 400\r\n |}\r\n |EOL\r\n\"\"\".stripMargin\r\n\r\n\r\ndbutils.fs.put(\"dbfs:/databricks/<init-script-folder>/set_rbackend.sh\",initScriptContent, true) Delete Info The sample code sets the RBackends limit to 400. You can adjust this number as needed. You should not exceed 500 RBackends. Install the newly created init script as a cluster-scoped init script (AWS | Azure | GCP). You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/set_rbackend.sh). Restart the cluster after you have installed the init script. Validate solution You can confirm that the changes were successful by running this sample code in a notebook. %r\r\n\r\nlibrary(magrittr)\r\nSparkR:::callJStatic(\r\n  \"com.databricks.backend.daemon.driver.RDriverLocal\",\r\n  \"getDriver\",\r\n  get(DB_GUID_, envir = .GlobalEnv)) %>% SparkR:::callJMethod(\"conf\") %>% SparkR:::callJMethod(\"maxNumRBackendsPerDriver\") When run, this code returns the current RBackends limit on the cluster. Best practices Ensure that you log out of RStudio when you are finished using it. This terminates the R session and cleans the RBackend. If the RStudio server is killed, or the RSession terminates unexpectedly, the cleanup step may not happen. Databricks Runtime 9.0 and above automatically cleans up idle RBackend sessions. Was this article helpful? (7) (12) Additional Informations Related Articles Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... Install rJava and RJDBC libraries This article explains how to install rJava and RJBDC libraries. Problem When you ... How to parallelize R code with spark.lapply Parallelization of R code is difficult, because R code runs on the driver and R d... Related Articles Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... Install rJava and RJDBC libraries This article explains how to install rJava and RJBDC libraries. Problem When you ... How to parallelize R code with spark.lapply Parallelization of R code is difficult, because R code runs on the driver and R d... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "RStudio server backend connection error",
          "view_href" : "https://kb.databricks.com/en_US/r/rstudio-server-backend-error"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/multiple-executors-single-worker",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters How to configure single-core executors to run JNI libraries How to configure single-core executors to run JNI libraries Learn how to configure single-core executors to run JNI libraries on Databricks. Written by Adam Pavlacka Last published at: March 4th, 2022 When you create a cluster, Databricks launches one Apache Spark executor instance per worker node, and the executor uses all of the cores on the node. In certain situations, such as if you want to run non-thread-safe JNI libraries, you might need an executor that has only one core or task slot, and does not attempt to run concurrent tasks. In this case, multiple executor instances run on a single worker node, and each executor has only one core. If you run multiple executors, you increase the JVM overhead and decrease the overall memory available for processing. To start single-core executors on a worker node, configure two properties in the Spark Config: spark.executor.cores spark.executor.memory The property spark.executor.cores specifies the number of cores per executor. Set this property to 1. The property spark.executor.memory specifies the amount of memory to allot to each executor. This must be set high enough for the executors to properly function, but low enough to allow for all cores to be used. Delete Note If you set a total memory value (memory per executor x number of total cores) that is greater than the memory available on the worker node, some cores will remain unused. AWS For example, an i3.xlarge node, which has 30.5 GB of memory, shows available memory at 24.9 GB. Choose a value that fits the available memory when multiplied by the number of executors. You may need to set a value that allows for some overhead. For example, set spark.executor.cores to 1 and spark.executor.memory to 6g: The i3.xlarge instance type has 4 cores, and so 4 executors are created on the node, each with 6 GB of memory. Delete GCP For example, the n1-highmem-4 worker node has 26 GB of total memory, but only has 15.3 GB of available memory once the cluster is running. Using an example Spark Config value, we set the core value to 1 and assign 5 GB of memory to each executor. spark.executor.cores 1\r\nspark.executor.memory 5g Once the cluster starts, the worker nodes each have 4 cores, but only 3 are used. There are 3 executors, each with 5 GB of memory on each worker node. This is a total of 15 GB of memory used. The fourth core never spins up, as there is not enough memory to allocate to it. You must balance your choice of instance type with the memory required by each executor in order to maximize the use of every core on your worker nodes. Delete Was this article helpful? (6) (18) Additional Informations Related Articles Admin user cannot restart cluster to run job Problem When a user who has permission to start a cluster, such as a Databricks A... Failed to create cluster with invalid tag value Problem You are trying to create a cluster, but it is failing with an invalid tag... Cluster slowdown due to Ganglia metrics filling root partition Note This article applies to Databricks Runtime 7.3 LTS and below. Problem Cluste... Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Related Articles Admin user cannot restart cluster to run job Problem When a user who has permission to start a cluster, such as a Databricks A... Failed to create cluster with invalid tag value Problem You are trying to create a cluster, but it is failing with an invalid tag... Cluster slowdown due to Ganglia metrics filling root partition Note This article applies to Databricks Runtime 7.3 LTS and below. Problem Cluste... Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to configure single-core executors to run JNI libraries",
          "view_href" : "https://kb.databricks.com/en_US/clusters/multiple-executors-single-worker"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/hive-cursor-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake HIVE_CURSOR_ERROR when reading a table in Athena HIVE_CURSOR_ERROR when reading a table in Athena When you try to read a table in Athena, the select query returns a HIVE_CURSOR_ERROR message. Written by annapurna.hiriyur Last published at: May 10th, 2022 Problem You create an external table in Athena and integrate it with Delta Lake using the instructions in the Presto and Athena to Delta Lake integration documentation. The external table in Athena is defined in the Apache Hive metastore. You run a select query on external table from the Athena Query Editor and it returns a HIVE_CURSOR_ERROR. HIVE_CURSOR_ERROR: Can not read value at 0 in block 0 in file s3://<parquet-file-path> Cause The root cause of the issue is the different Parquet conventions used in Hive and Apache Spark. In Spark 1.4 or above, the current Parquet format is used, and decimal values are written as integers. In earlier versions of the Parquet format, decimal values are written in Apache’s fixed-length byte array format. This Parquet format is used by other Apache systems such as Hive and Apache Impala. As a result, you receive an error when the internal representation of the datatype is different due to using two Parquet formats. Solution If you are using Athena or Presto to access Delta Lake managed tables, the Parquet files must be created in a format that is compatible with Hive. You cannot choose the Parquet convention in Hive, but you can do so with Spark. Set the spark.conf.set(\"spark.sql.parquet.writeLegacyFormat\",True) property at either the cluster level or at notebook level to resolve the issue. To set the properties at cluster level: Edit the cluster properties. Click Advanced Options. Select Spark. Enter the property setting in the Spark Config field. Confirm the change. Restart the cluster. Delete Info Changing the DDL of the table to match the Spark datatype does not return any query results. spark.conf.set(\"spark.sql.parquet.writeLegacyFormat\",True) must be set. Please review the Parquet configuration documentation for more information. Was this article helpful? (7) (14) Additional Informations Related Articles Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... Delete your streaming query checkpoint and restart Problem Your job fails with a Delta table <value> doesn't exist. Please del... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... Related Articles Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... Delete your streaming query checkpoint and restart Problem Your job fails with a Delta table <value> doesn't exist. Please del... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "HIVE_CURSOR_ERROR when reading a table in Athena",
          "view_href" : "https://kb.databricks.com/en_US/delta/hive-cursor-error"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/fit-spark-model-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning Fitting an Apache SparkML model throws error Fitting an Apache SparkML model throws error Learn how to resolve errors thrown by Databricks when fitting a SparkML model or pipeline. Written by Adam Pavlacka Last published at: May 16th, 2022 Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 162.0 failed 4 times, most recent failure: Lost task 0.3 in stage 162.0 (TID 168, 10.205.250.130, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) =&gt; double) Cause Often, an error when fitting a SparkML model or Pipeline is a result of issues with the training data. Solution Check for the following issues: Identify and address NULL values in a dataset. Spark needs to know how to address missing values in the dataset. Discard rows with missing values with dropna(). Impute some value like zero or the average value of the column. This solution depends on what is meaningful for the data set. Ensure that all training data is appropriately transformed to a numeric format. Spark needs to know how to handle categorical and string variables. A variety of feature transformers are available to address data specific cases. Check for collinearity. Highly correlated or even duplicate features may cause issues with model fitting. This occurs on rare occasions, but you should make sure to rule it out. Was this article helpful? (7) (15) Additional Informations Related Articles Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... How to extract feature information for tree-based Apache SparkML pipeline models When you are fitting a tree-based model, such as a decision tree, random forest, ... Related Articles Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... How to extract feature information for tree-based Apache SparkML pipeline models When you are fitting a tree-based model, such as a decision tree, random forest, ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Fitting an Apache SparkML model throws error",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/fit-spark-model-error"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/azure-throttling",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Job failure due to Azure Data Lake Storage (ADLS) CREATE limits Job failure due to Azure Data Lake Storage (ADLS) CREATE limits Learn what to do when your Databricks job fails due to Azure Data Lake Storage CREATE limits. Written by Adam Pavlacka Last published at: May 11th, 2022 Problem When you run a job that involves creating files in Azure Data Lake Storage (ADLS), either Gen1 or Gen2, the following exception occurs: Caused by: java.io.IOException: CREATE failed with error 0x83090c25 (Files and folders are being created at too high a rate). [745c5836-264e-470c-9c90-c605f1c100f5] failed with error 0x83090c25 (Files and folders are being created at too high a rate). [2019-04-12T10:06:43.1117197-07:00] [ServerRequestId:745c5836-264e-470c-9c90-c605f1c100f5]\r\nat com.microsoft.azure.datalake.store.ADLStoreClient.getRemoteException(ADLStoreClient.java:1191)\r\nat com.microsoft.azure.datalake.store.ADLStoreClient.getExceptionFromResponse(ADLStoreClient.java:1154)\r\nat com.microsoft.azure.datalake.store.ADLStoreClient.createFile(ADLStoreClient.java:281)\r\nat com.databricks.adl.AdlFileSystem.create(AdlFileSystem.java:348)\r\nat com.databricks.spark.metrics.FileSystemWithMetrics.create(FileSystemWithMetrics.scala:280)\r\nat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$create$1$$anonfun$apply$10$$anonfun$apply$11.apply(DatabricksFileSystemV2.scala:483) Cause Each ADLS subscription level has a limit on the number of files that can be created per unit of time, although the limits may differ depending on whether you are using ADLS Gen1 or Gen2. When the limit is exceeded, file creation is throttled, and the job fails. Potential causes for this error include: Your application creates a large number of small files. External applications create a large number of files. The current limit for the subscription is too low. Solution If your application or an external application is generating a large number of files, then you need to optimize the application. If the limit on your current subscription is not appropriate for your use case, then contact the Microsoft Azure Team for assistance. Was this article helpful? (14) (45) Additional Informations Related Articles Job fails with atypical errors message Problem Your job run fails with a throttled due to observing atypical errors erro... Apache Spark job fails with Failed to parse byte string Problem Spark-submit jobs fail with a Failed to parse byte string: -1 error messa... Job cluster limits on notebook output Problem You are running a notebook on a job cluster and you get an error message ... Create table in overwrite mode fails when interrupted Problem When you attempt to rerun an Apache Spark write operation by cancelling t... Related Articles Job fails with atypical errors message Problem Your job run fails with a throttled due to observing atypical errors erro... Apache Spark job fails with Failed to parse byte string Problem Spark-submit jobs fail with a Failed to parse byte string: -1 error messa... Job cluster limits on notebook output Problem You are running a notebook on a job cluster and you get an error message ... Create table in overwrite mode fails when interrupted Problem When you attempt to rerun an Apache Spark write operation by cancelling t... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Job failure due to Azure Data Lake Storage (ADLS) CREATE limits",
          "view_href" : "https://kb.databricks.com/en_US/jobs/azure-throttling"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/ebs-failed-to-expand",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Failed to expand the EBS volume Failed to expand the EBS volume Learn how to set the correct permissions on AWS EC2 to enable EBS volume expansion. Written by Adam Pavlacka Last published at: March 4th, 2022 Problem Databricks jobs fail, due to a lack of space on the disk, even though storage auto-scaling is enabled. When you review the cluster event log, you see a message stating that the instance failed to expand disk due to an authorization error. Instance i-xxxxxxxxx failed to expand disk because: You are not authorized to perform this operation. Encoded authorization failure message: xxxxx(Service: AmazonEC2; Status Code: 403; Error Code: UnauthorizedOperation; Request ID: xxxxxx). Current size: 98.30 GB Current free space: 8.89 GB. Cause The underlying AWS account for the workspace does not have the correct permissions to enable Databricks to attach an EBS volume to the instance and then remove it after the cluster is terminated. Delete Note Cluster creation uses a different set of permissions for EBS volumes. As a result, cluster creation succeeds, but attempts to expand the existing volume fails. Solution You must add the following permissions to the Databricks workspace deployment IAM role. ec2:AttachVolume\r\nec2:CreateVolume\r\nec2:DeleteVolume\r\nec2:DescribeVolumes You can find the Databricks workspace deployment IAM role by logging in to the Account Console and navigating to the AWS account tab. Was this article helpful? (7) (18) Additional Informations Related Articles How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Cluster fails to start with dummy does not exist error Problem You try to start a cluster, but it fails to start. You get an Apache Spar... Cluster Apache Spark configuration not applied Problem Your cluster’s Spark configuration values are not applied. Cause This hap... Failed to create cluster with invalid tag value Problem You are trying to create a cluster, but it is failing with an invalid tag... Related Articles How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Cluster fails to start with dummy does not exist error Problem You try to start a cluster, but it fails to start. You get an Apache Spar... Cluster Apache Spark configuration not applied Problem Your cluster’s Spark configuration values are not applied. Cause This hap... Failed to create cluster with invalid tag value Problem You are trying to create a cluster, but it is failing with an invalid tag... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Failed to expand the EBS volume",
          "view_href" : "https://kb.databricks.com/en_US/clusters/ebs-failed-to-expand"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/use-internal-ntp",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Configure a cluster to use a custom NTP server Configure a cluster to use a custom NTP server Configure your clusters to use a custom NTP server (public or private) instead of using the default server. Written by Adam Pavlacka Last published at: March 4th, 2022 By default Databricks clusters use public NTP servers. This is sufficient for most use cases, however you can configure a cluster to use a custom NTP server. This does not have to be a public NTP server. It can be a private NTP server under your control. A common use case is to minimize the amount of Internet traffic from your cluster. Update the NTP configuration on a cluster Create a ntp.conffile with the following information: # NTP configuration\r\nserver <ntp-server-hostname> iburstwhere <ntp-server-hostname> is a NTP server hostname or a NTP server IP address. If you have multiple NTP servers to list, add them all to the file. Each server should be listed on its own line. Upload the ntp.conf file to /dbfs/databricks/init_scripts/ on your cluster. Create the script ntp.shon your cluster: %python\r\n\r\ndbutils.fs.put(\"/databricks/init_scripts/ntp.sh\",\"\"\"\r\n#!/bin/bash\r\necho \"<ntp-server-ip> <ntp-server-hostname>\" >> /etc/hosts\r\ncp /dbfs/databricks/init_scripts/ntp.conf /etc/\r\nsudo service ntp restart\"\"\",True) Confirm that the script exists: %python\r\n\r\ndisplay(dbutils.fs.ls(\"dbfs:/databricks/init_scripts/ntp.sh\")) Click Clusters, click your cluster name, click Edit, click Advanced Options, click Init Scripts. Select DBFS under Destination. Enter the full path to ntp.sh and click Add. Click Confirm and Restart. A confirmation dialog box appears. Click Confirm and wait for the cluster to restart. Verify the cluster is using the updated NTP configuration Run the following code in a notebook: %sh ntpq -p The output displays the NTP servers that are in use. Was this article helpful? (12) (55) Additional Informations Related Articles Log delivery fails with AssumeRole Problem You are using AssumeRole to send cluster logs to a S3 bucket in another a... Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Set executor log level Warning This article describes steps related to customer use of Log4j 1.x within ... How to configure single-core executors to run JNI libraries When you create a cluster, Databricks launches one Apache Spark executor instance... Related Articles Log delivery fails with AssumeRole Problem You are using AssumeRole to send cluster logs to a S3 bucket in another a... Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Set executor log level Warning This article describes steps related to customer use of Log4j 1.x within ... How to configure single-core executors to run JNI libraries When you create a cluster, Databricks launches one Apache Spark executor instance... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Configure a cluster to use a custom NTP server",
          "view_href" : "https://kb.databricks.com/en_US/clusters/use-internal-ntp"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/delta-write-fails",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Delta Lake write job fails with java.lang.UnsupportedOperationException Delta Lake write job fails with java.lang.UnsupportedOperationException Learn how to prevent java.lang.UnsupportedOperationException in Delta Lake write jobs. Written by Adam Pavlacka Last published at: May 10th, 2022 Problem Delta Lake write jobs sometimes fail with the following exception: java.lang.UnsupportedOperationException: com.databricks.backend.daemon.data.client.DBFSV1.putIfAbsent(path: Path, content: InputStream).\r\nDBFS v1 doesn't support transactional writes from multiple clusters. Please upgrade to DBFS v2.\r\nOr you can disable multi-cluster writes by setting 'spark.databricks.delta.multiClusterWrites.enabled' to 'false'.\r\nIf this is disabled, writes to a single table must originate from a single cluster. Cause Delta Lake multi-cluster writes are only supported with DBFS v2. Databricks clusters use DBFS v2 by default. All sparkSession objects use DBFS v2. However, if the application uses the FileSystem API and calls FileSystem.close(), the file system client falls back to the default value, which is v1. In this case, Delta Lake multi-cluster write operations fail. The following log trace shows that the file system object fell back to the default v1 version. <date> <time> INFO DBFS: Initialized DBFS with DBFSV1 as the delegate. Solution There are two approaches to prevent this: Never call FileSystem.close() inside the application code. If it is necessary to call the close() API, then first instantiate a new FileSystemclient object with a configuration object from the current Apache Spark session, instead of an empty configuration object: %scala\r\n\r\nval fileSystem = FileSystem.get(new java.net.URI(path), sparkSession.sessionState.newHadoopConf()) Alternatively, this code sample achieves the same goal: %scala\r\n\r\nval fileSystem = FileSystem.get(new java.net.URI(path), sc.hadoopConfiguration()) Was this article helpful? (8) (14) Additional Informations Related Articles Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... HIVE_CURSOR_ERROR when reading a table in Athena Problem You create an external table in Athena and integrate it with Delta Lake u... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Related Articles Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... HIVE_CURSOR_ERROR when reading a table in Athena Problem You create an external table in Athena and integrate it with Delta Lake u... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Delta Lake write job fails with java.lang.UnsupportedOperationException",
          "view_href" : "https://kb.databricks.com/en_US/delta/delta-write-fails"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dev-tools/sort-s3-by-mod-time",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Developer tools How to Sort S3 files By Modification Time in Databricks Notebooks How to Sort S3 files By Modification Time in Databricks Notebooks Written by Adam Pavlacka Last published at: May 9th, 2022 Problem When you use the dbutils utility to list the files in a S3 location, the S3 files list in random order. However, dbutils doesn’t provide any method to sort the files based on their modification time. dbutils doesn’t list a modification time either. Solution Use the Hadoop filesystem API to sort the S3 files, as shown here: %scala\r\n\r\nimport org.apache.hadoop.fs._\r\nval path = new Path(\"/mnt/abc\")\r\nval fs = path.getFileSystem(spark.sessionState.newHadoopConf)\r\nval inodes = fs.listStatus(path).sortBy(_.getModificationTime)\r\ninodes.filter(_.getModificationTime > 0).map(t => (t.getPath, t.getModificationTime, t.getLen)).foreach(println) This code uses the Hadoop filesystem’s listStatus method to sort the S3 files based on the modification time. Was this article helpful? (9) (15) Additional Informations Related Articles Databricks Connect reports version error with Databricks Runtime 6.4 Problem You are using the Databricks Connect client with Databricks Runtime 6.4 a... Get Apache Spark config in DBConnect You can always view the Spark configuration (AWS | Azure | GCP) for your cluster ... GeoSpark undefined function error with DBConnect Problem You are trying to use the GeoSpark function st_geofromwkt with DBConnect ... Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... Related Articles Databricks Connect reports version error with Databricks Runtime 6.4 Problem You are using the Databricks Connect client with Databricks Runtime 6.4 a... Get Apache Spark config in DBConnect You can always view the Spark configuration (AWS | Azure | GCP) for your cluster ... GeoSpark undefined function error with DBConnect Problem You are trying to use the GeoSpark function st_geofromwkt with DBConnect ... Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to Sort S3 files By Modification Time in Databricks Notebooks",
          "view_href" : "https://kb.databricks.com/en_US/dev-tools/sort-s3-by-mod-time"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/metastore/set-up-embedded-metastore",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Metastore How to set up an embedded Apache Hive metastore How to set up an embedded Apache Hive metastore Learn how to set up an embedded Apache Hive metastore with Databricks. Written by Adam Pavlacka Last published at: May 16th, 2022 You can set up a Databricks cluster to use an embedded metastore. You can use an embedded metastore when you only need to retain table metadata during the life of the cluster. If the cluster is restarted, the metadata is lost. If you need to persist the table metadata or other data after a cluster restart, then you should use the default metastore or set up an external metastore. This example uses the Apache Derby embedded metastore, which is an in-memory lightweight database. Follow the instructions in the notebook to install the metastore. You should always perform this procedure on a test cluster before applying it to other clusters. Set up an embedded Hive metastore notebook Review the embedded Hive metastore notebook. Was this article helpful? (6) (14) Additional Informations Related Articles Drop tables with corrupted metadata from the metastore Problem Sometimes you cannot drop a table from the Databricks UI. Using %sql or s... AnalysisException when dropping table on Azure-backed metastore Problem When you try to drop a table in an external Hive version 2.0 or 2.1 metas... How to create table DDLs to import into an external metastore Databricks supports using external metastores instead of the default Hive metasto... Listing table names Problem To fetch all the table names from metastore you can use either spark.cata... Related Articles Drop tables with corrupted metadata from the metastore Problem Sometimes you cannot drop a table from the Databricks UI. Using %sql or s... AnalysisException when dropping table on Azure-backed metastore Problem When you try to drop a table in an external Hive version 2.0 or 2.1 metas... How to create table DDLs to import into an external metastore Databricks supports using external metastores instead of the default Hive metasto... Listing table names Problem To fetch all the table names from metastore you can use either spark.cata... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to set up an embedded Apache Hive metastore",
          "view_href" : "https://kb.databricks.com/en_US/metastore/set-up-embedded-metastore"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/manage-size-delta-table",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Manage the size of Delta tables Manage the size of Delta tables Recommendations that can help you manage the size of your Delta tables. Written by Jose Gonzalez Last published at: May 23rd, 2022 Delta tables are different than traditional tables. Delta tables include ACID transactions and time travel features, which means they maintain transaction logs and stale data files. These additional features require storage space. In this article we discuss recommendations that can help you manage the size of your Delta tables. Enable file system versioning When you enable file system versioning, you keep multiple variants of your data in the same storage bucket. The file system creates versions of your data, instead of deleting items, which increases the storage space available for your Delta table. Enable bloom filters A Bloom filter index (AWS | Azure | GCP) is a space-efficient data structure that enables data skipping on chosen columns, particularly for fields containing arbitrary text. Databricks supports file level Bloom filters; each data file can have a single Bloom filter index file associated with it. Before reading a file Databricks checks the index file and the file is read only if the index indicates that the file might match a data filter. The size of a Bloom filter depends on the number elements in the set for which the Bloom filter has been created and the required false positive probability (FPP). The lower the FPP, the higher the number of used bits per element and the more accurate it will be, at the cost of more storage space. Review your Delta logRetentionDuration policy Log files are retained for 30 days by default. This value is configurable through the delta.logRetentionDuration property. You can set a value for this property with the ALTER TABLE SET TBLPROPERTIES SQL method. The more days you retain, the more storage space you consume. For example if you set delta.logRetentionDuration = '365 days' it keeps the log files for 365 days instead of the default of 30 days. VACUUM your Delta table VACUUM (AWS | Azure | GCP) removes data files that are no longer in the latest state of the transaction log for the table and are older than a retention threshold. Files are deleted according to the time they have been logically removed from Delta’s transaction log + retention hours, not their modification timestamps on the storage system. The default threshold is 7 days. Databricks does not automatically trigger VACUUM operations on Delta tables. You must run this command manually. VACUUM helps you delete obsolete files that are no longer needed. OPTIMIZE your Delta table The OPTIMIZE (AWS | Azure | GCP) command compacts multiple Delta files into large single files. This improves the overall query speed and performance of your Delta table by helping you avoid having too many small files around. By default, OPTIMIZE creates 1GB files. Was this article helpful? (7) (12) Additional Informations Related Articles Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Best practice for cache(), count(), and take() cache() is an Apache Spark transformation that can be used on a DataFrame, Datase... Cannot modify the value of an Apache Spark config Problem You are trying to SET the value of a Spark config in a notebook and get a... Convert nested JSON to a flattened DataFrame This article shows you how to flatten nested JSON, using only $\"column.*\" and exp... Related Articles Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Best practice for cache(), count(), and take() cache() is an Apache Spark transformation that can be used on a DataFrame, Datase... Cannot modify the value of an Apache Spark config Problem You are trying to SET the value of a Spark config in a notebook and get a... Convert nested JSON to a flattened DataFrame This article shows you how to flatten nested JSON, using only $\"column.*\" and exp... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Manage the size of Delta tables",
          "view_href" : "https://kb.databricks.com/en_US/scala/manage-size-delta-table"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/cluster-spark-config-not-applied",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Cluster Apache Spark configuration not applied Cluster Apache Spark configuration not applied Values set in your cluster's Spark configuration are not applying correctly. Written by Gobinath.Viswanathan Last published at: March 4th, 2022 Problem Your cluster’s Spark configuration values are not applied. Cause This happens when the Spark config values are declared in the cluster configuration as well as in an init script. When Spark config values are located in more than one place, the configuration in the init script takes precedence and the cluster ignores the configuration settings in the UI. Solution You should define your Spark configuration values in one place. Choose to define the Spark configuration in the cluster configuration or include the Spark configuration in an init script. Do not do both. Was this article helpful? (9) (16) Additional Informations Related Articles Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... Related Articles Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cluster Apache Spark configuration not applied",
          "view_href" : "https://kb.databricks.com/en_US/clusters/cluster-spark-config-not-applied"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/mlflow-artifacts-outdated-client",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning OSError when accessing MLflow experiment artifacts OSError when accessing MLflow experiment artifacts Resolve an `OSError` when trying to access, download, or log MLflow experiment artifacts. Written by Adam Pavlacka Last published at: May 16th, 2022 Problem You get an OSError: No such file or directory error message when trying to download or log artifacts using one of the following: MlflowClient.download_artifacts() mlflow.[flavor].log_model() mlflow.[flavor].load_model() mlflow.log_artifacts() OSError: No such file or directory: '/dbfs/databricks/mlflow-tracking/<experiment-id>/<run-id>/artifacts/...' Cause Your MLflow client is out of date. Older versions of MLflow do not provide support for artifacts stored in dbfs:/databricks/mlflow-tracking/. Solution Upgrade to MLflow version 1.9.1 or higher and try again. %sh\r\n\r\npip install --upgrade mlflow Was this article helpful? (8) (13) Additional Informations Related Articles How to speed up cross-validation Hyperparameter tuning of Apache SparkML models takes a very long time, depending ... How to perform group K-fold cross validation with Apache Spark Cross validation randomly splits the training data into a specified number of fol... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... Experiment warning when legacy artifact storage location is used Problem A new icon appears on the MLflow Experiments page with the following open... Related Articles How to speed up cross-validation Hyperparameter tuning of Apache SparkML models takes a very long time, depending ... How to perform group K-fold cross validation with Apache Spark Cross validation randomly splits the training data into a specified number of fol... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... Experiment warning when legacy artifact storage location is used Problem A new icon appears on the MLflow Experiments page with the following open... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "OSError when accessing MLflow experiment artifacts",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/mlflow-artifacts-outdated-client"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/set-core-site-xml",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Set Apache Hadoop core-site.xml properties Set Apache Hadoop core-site.xml properties Set Apache Hadoop core-site.xml properties in a Databricks cluster. Written by arjun.kaimaparambilrajan Last published at: March 4th, 2022 You have a scenario that requires Apache Hadoop properties to be set. You would normally do this in the core-site.xml file. In this article, we explain how you can set core-site.xml in a cluster. Create the core-site.xml file in DBFS You need to create a core-site.xml file and save it to DBFS on your cluster. An easy way to create this file is via a bash script in a notebook. This example code creates a hadoop-configs folder on your cluster and then writes a single property core-site.xml file to that folder. %sh\r\n\r\nmkdir -p /dbfs/hadoop-configs/\r\ncat << 'EOF' > /dbfs/hadoop-configs/core-site.xml\r\n <property>\r\n    <name><property-name-here></name>\r\n    <value><property-value-here></value>\r\n </property>\r\nEOF You can add multiple properties to the file by adding additional name/value pairs to the script. You can also create this file locally, and then upload it to your cluster. Create an init script that loads core-site.xml This example code creates an init script called set-core-site-configs.sh that uses the core-site.xml file you just created. If you manually uploaded a core-site.xml file and stored it elsewhere, you should update the config_xml value in the example code. %python\r\n\r\ndbutils.fs.put(\"/databricks/scripts/set-core-site-configs.sh\", \"\"\"\r\n#!/bin/bash\r\n   \r\necho \"Setting core-site.xml configs at `date`\"\r\n \r\nSTART_DRIVER_SCRIPT=/databricks/spark/scripts/start_driver.sh\r\nSTART_WORKER_SCRIPT=/databricks/spark/scripts/start_spark_slave.sh\r\n \r\nTMP_DRIVER_SCRIPT=/tmp/start_driver_temp.sh\r\nTMP_WORKER_SCRIPT=/tmp/start_spark_slave_temp.sh\r\n \r\nTMP_SCRIPT=/tmp/set_core-site_configs.sh\r\n \r\nconfig_xml=\"/dbfs/hadoop-configs/core-site.xml\"\r\n\r\ncat >\"$TMP_SCRIPT\" <<EOL\r\n#!/bin/bash\r\n## Setting core-site.xml configs\r\n\r\nsed -i '/<\\/configuration>/{\r\n    r $config_xml\r\n    a \\</configuration>\r\n    d\r\n}' /databricks/spark/dbconf/hadoop/core-site.xml\r\n \r\nEOL\r\ncat \"$TMP_SCRIPT\" > \"$TMP_DRIVER_SCRIPT\"\r\ncat \"$TMP_SCRIPT\" > \"$TMP_WORKER_SCRIPT\"\r\n \r\ncat \"$START_DRIVER_SCRIPT\" >> \"$TMP_DRIVER_SCRIPT\"\r\nmv \"$TMP_DRIVER_SCRIPT\" \"$START_DRIVER_SCRIPT\"\r\n \r\ncat \"$START_WORKER_SCRIPT\" >> \"$TMP_WORKER_SCRIPT\"\r\nmv \"$TMP_WORKER_SCRIPT\" \"$START_WORKER_SCRIPT\"\r\n \r\necho \"Completed core-site.xml config changes `date`\" \r\n \r\n\"\"\", True) Attach the init script to your cluster You need to configure the newly created init script as a cluster-scoped init script. If you used the example code, your Destination is DBFS and the Init Script Path is dbfs:/databricks/scripts/set-core-site-configs.sh. If you customized the example code, ensure that you enter the correct path and name of the init script when you attach it to the cluster. Was this article helpful? (8) (16) Additional Informations Related Articles Multi-part upload failure Problem You observe a job failure with the exception: com.amazonaws.SdkClientExce... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Cluster slowdown due to Ganglia metrics filling root partition Note This article applies to Databricks Runtime 7.3 LTS and below. Problem Cluste... Failed to create cluster with invalid tag value Problem You are trying to create a cluster, but it is failing with an invalid tag... Related Articles Multi-part upload failure Problem You observe a job failure with the exception: com.amazonaws.SdkClientExce... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Cluster slowdown due to Ganglia metrics filling root partition Note This article applies to Databricks Runtime 7.3 LTS and below. Problem Cluste... Failed to create cluster with invalid tag value Problem You are trying to create a cluster, but it is failing with an invalid tag... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Set Apache Hadoop core-site.xml properties",
          "view_href" : "https://kb.databricks.com/en_US/clusters/set-core-site-xml"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/maven-library-version-mgmt",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries How to correctly update a Maven library in Databricks How to correctly update a Maven library in Databricks Learn how to correctly update a Maven library in Databricks. Written by Adam Pavlacka Last published at: May 11th, 2022 Problem You make a minor update to a library in the repository, but you don’t want to change the version number because it is a small change for testing purposes. When you attach the library to your cluster again, your code changes are not included in the library. Cause One strength of Databricks is the ability to install third-party or custom libraries, such as from a Maven repository. However, when a library is updated in the repository, there is no automated way to update the corresponding library in the cluster. When you request Databricks to download a library in order to attach it to a cluster, the following process occurs: In Databricks, you request a library from a Maven repository. Databricks checks the local cache for the library, and if it is not present, downloads the library from the Maven repository to a local cache. Databricks then copies the library to DBFS (/FileStore/jars/maven/). Upon subsequent requests for the library, Databricks uses the file that has already been copied to DBFS, and does not download a new copy. Solution To ensure that an updated version of a library (or a library that you have customized) is downloaded to a cluster, make sure to increment the build number or version number of the artifact in some way. For example, you can change libA_v1.0.0-SNAPSHOT to libA_v1.0.1-SNAPSHOT, and then the new library will download. You can then attach it to your cluster. Was this article helpful? (6) (14) Additional Informations Related Articles Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... Library unavailability causing job failures Problem You are launching jobs that import external libraries and get an Import E... Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Related Articles Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... Library unavailability causing job failures Problem You are launching jobs that import external libraries and get an Import E... Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to correctly update a Maven library in Databricks",
          "view_href" : "https://kb.databricks.com/en_US/libraries/maven-library-version-mgmt"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/mlflow-fail-access-hive",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning MLflow project fails to access an Apache Hive table MLflow project fails to access an Apache Hive table Resolve \"Table or view not found\" error when an MLflow project fails to access an Apache Hive table. Written by vikas.yadav Last published at: May 16th, 2022 Problem You have an MLflow project that fails to access a Hive table and returns a Table or view not found error. pyspark.sql.utils.AnalysisException: \"Table or view not found: `default`.`tab1`; line 1 pos 21;\\n'Aggregate [unresolvedalias(count(1), None)]\\n+- 'UnresolvedRelation `default`.`tab1`\\n\"\r\nxxxxx ERROR mlflow.cli: === Run (ID 'xxxxx') failed === Cause This happens when the SparkSession object is created inside the MLflow project without Hive support. Solution Configure SparkSession with the .enableHiveSupport() option in the session builder. Do this as part of your MLflow project. %scala\r\n\r\nval spark = SparkSession.builder.enableHiveSupport().getOrCreate() Was this article helpful? (7) (12) Additional Informations Related Articles Fitting an Apache SparkML model throws error Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.... Error when importing OneHotEncoderEstimator Problem You have migrated a notebook from Databricks Runtime 6.4 for Machine Lear... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... Related Articles Fitting an Apache SparkML model throws error Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.... Error when importing OneHotEncoderEstimator Problem You have migrated a notebook from Databricks Runtime 6.4 for Machine Lear... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "MLflow project fails to access an Apache Hive table",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/mlflow-fail-access-hive"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/streaming-notebook-stuck",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks Cannot run notebook commands after canceling streaming cell Cannot run notebook commands after canceling streaming cell Learn what to do if you can't run Databricks notebook commands after you cancel a streaming cell. Written by Adam Pavlacka Last published at: May 17th, 2022 Problem After you cancel a running streaming cell in a notebook attached to a Databricks Runtime 5.0 cluster, you cannot run any subsequent commands in the notebook. The commands are left in the “waiting to run” state, and you must clear the notebook’s state or detach and reattach the cluster before you can successfully run commands on the notebook. Note that this issue occurs only when you cancel a single cell; it does not apply when you run all and cancel all cells. Version This problem affects Databricks Runtime 5.0 clusters. It also affects Databricks Runtime 4.3 clusters whose Spark Configuration spark.databricks.chauffeur.enableIdleContextTracking has been set to true. Cause Databricks Runtime 4.3 introduced an optional idle execution context feature, which is enabled by default in Databricks Runtime 5.0, that allows the execution context to track streaming execution sequences to determine if they are idle. Unfortunately, this introduced an issue that causes the underlying execution context to be left in an invalid state when you cancel a streaming cell. This prevents additional commands from being run until the notebook state is reset. This behavior is specific to interactive notebooks and does not affect jobs. For more information about idle execution contexts, see Execution contexts (AWS | Azure). Solution Databricks is working to resolve this issue and release a maintenance update for Databricks Runtime 5.0. In the meantime, you can do either of the following: To remediate an affected notebook without restarting the cluster, go to the notebook’s Clear menu and select Clear State: If restarting the cluster is acceptable, you can solve the issue by turning off idle context tracking. Set the following Spark config (AWS | Azure | GCP) value on the cluster: spark.databricks.chauffeur.enableIdleContextTracking falseThen restart the cluster. Was this article helpful? (8) (12) Additional Informations Related Articles Common errors in notebooks There are some common issues that occur when using notebooks. This section outlin... display() does not show microseconds correctly Problem You want to display a timestamp value with microsecond precision, but whe... Item was too large to export Problem You are trying to export notebooks using the workspace UI and are getting... Notebook autosave fails due to file size limits Problem Notebook autosaving fails with the following error message: Failed to sav... Related Articles Common errors in notebooks There are some common issues that occur when using notebooks. This section outlin... display() does not show microseconds correctly Problem You want to display a timestamp value with microsecond precision, but whe... Item was too large to export Problem You are trying to export notebooks using the workspace UI and are getting... Notebook autosave fails due to file size limits Problem Notebook autosaving fails with the following error message: Failed to sav... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cannot run notebook commands after canceling streaming cell",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/streaming-notebook-stuck"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/log-delivery-fail-assume-role",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Log delivery fails with AssumeRole Log delivery fails with AssumeRole You get an access denied error when you try to use `AssumeRole` to send cluster logs to a S3 bucket in another account. Written by dayanand.devarapalli Last published at: March 4th, 2022 Problem You are using AssumeRole to send cluster logs to a S3 bucket in another account and you get an access denied error. Cause AssumeRole does not allow you to send cluster logs to a S3 bucket in another account. This is because the log daemon runs on the host machine. It does not run inside the container. Only items that run inside the container have access to the Apache Spark configuration. This is required for AssumeRole to work correctly. Solution You can achieve a similar result in one of two ways. You can use a cross account bucket policy and send the logs to a S3 bucket in another account. You can mount the target S3 bucket on DBFS with a role that has write permissions. Specify this path as the log delivery destination. Was this article helpful? (6) (16) Additional Informations Related Articles Admin user cannot restart cluster to run job Problem When a user who has permission to start a cluster, such as a Databricks A... Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... Cluster failed to launch This article describes several scenarios in which a cluster fails to launch, and ... Related Articles Admin user cannot restart cluster to run job Problem When a user who has permission to start a cluster, such as a Databricks A... Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... Cluster failed to launch This article describes several scenarios in which a cluster fails to launch, and ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Log delivery fails with AssumeRole",
          "view_href" : "https://kb.databricks.com/en_US/clusters/log-delivery-fail-assume-role"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/read-fail-jdbc-dbr6x",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Trouble reading external JDBC tables after upgrading from Databricks Runtime 5.5 Trouble reading external JDBC tables after upgrading from Databricks Runtime 5.5 Fail to read external JDBC tables after upgrading from Databricks Runtime 5.5 to 6.0 and above. Written by Mohammed.Haseeb Last published at: May 23rd, 2022 Problem Attempting to read external tables via JDBC works fine on Databricks Runtime 5.5, but the same table reads fail on Databricks Runtime 6.0 and above. You see an error similar to the following: com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: java.util.concurrent.ExecutionException: org.apache.spark.sql.AnalysisException: org.apache.spark.sql.jdbc does not allow user-specified schemas.\r\nat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)\r\nat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)\r\nat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\r\nat java.lang.Thread.run(Thread.java:748)\r\n.\r\nCaused by: org.apache.spark.sql.AnalysisException: org.apache.spark.sql.jdbc does not allow user-specified schemas.;\r\n\r\nat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350) Cause Databricks Runtime 5.5 and below infers the session_id attribute as a smallint. Databricks Runtime 6.0 and above infers the session_id attribute as an int. This change to the session_id attribute causes queries to fail with a schema issue. Solution If you are using external tables that were created in Databricks Runtime 5.5 and below in Databricks Runtime 6.0 and above, you must set the Apache Spark configuration spark.sql.legacy.mssqlserver.numericMapping.enabled to true. This ensures that Databricks Runtime 6.0 and above infers the session_id attribute as a smallint. Open the Clusters page. Select a cluster. Click Edit. Click Advanced Options. Click Spark. In the Spark config field, enter spark.sql.legacy.mssqlserver.numericMapping.enabled true. Save the change and start, or restart, the cluster. Was this article helpful? (6) (14) Additional Informations Related Articles Manage the size of Delta tables Delta tables are different than traditional tables. Delta tables include ACID tra... Intermittent NullPointerException when AQE is enabled Problem You get an intermittent NullPointerException error when saving your data.... Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... Create a DataFrame from a JSON string or Python dictionary In this article we are going to review how you can create an Apache Spark DataFra... Related Articles Manage the size of Delta tables Delta tables are different than traditional tables. Delta tables include ACID tra... Intermittent NullPointerException when AQE is enabled Problem You get an intermittent NullPointerException error when saving your data.... Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... Create a DataFrame from a JSON string or Python dictionary In this article we are going to review how you can create an Apache Spark DataFra... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Trouble reading external JDBC tables after upgrading from Databricks Runtime 5.5",
          "view_href" : "https://kb.databricks.com/en_US/scala/read-fail-jdbc-dbr6x"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/spark-shows-less-memory",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Apache Spark UI shows less than total node memory Apache Spark UI shows less than total node memory Learn what to do when the Spark UI shows less memory than is actually available on the node. Written by Adam Pavlacka Last published at: July 11th, 2022 Problem The Executors tab in the Spark UI shows less memory than is actually available on the node: AWS An m4.xlarge instance (16 GB ram, 4 core) for the driver node, shows 4.5 GB memory on the Executors tab. An m4.large instance (8 GB ram, 2 core) for the driver node, shows 710 GB memory on the Executors tab: Delete Azure An F8s instance (16 GB, 4 core) for the driver node, shows 4.5 GB of memory on the Executors tab. An F4s instance (8 GB, 4 core) for the driver node, shows 710 GB of memory on the Executors tab: Delete Cause The total amount of memory shown is less than the memory on the cluster because some memory is occupied by the kernel and node-level services. Solution To calculate the available amount of memory, you can use the formula used for executor memory allocation (all_memory_size * 0.97 - 4800MB) * 0.8, where: 0.97 accounts for kernel overhead. 4800 MB accounts for internal node-level services (node daemon, log daemon, and so on). 0.8 is a heuristic to ensure the LXC container running the Spark process doesn’t crash due to out-of-memory errors. Total available memory for storage on an instance is (8192MB * 0.97 - 4800MB) * 0.8 - 1024 = 1.2 GB. Because the parameter spark.memory.fraction is by default 0.6, approximately (1.2 * 0.6) = ~710 MB is available for storage. You can change the spark.memory.fraction Spark configuration to adjust this parameter. Calculate the available memory for a new parameter as follows: If you use an instance, which has 8192 MB memory, it has available memory 1.2 GB. If you specify a spark.memory.fraction of 0.8, the Executors tab in the Spark UI should show: (1.2 * 0.8) GB = ~960 MB. Was this article helpful? (7) (16) Additional Informations Related Articles Cluster Apache Spark configuration not applied Problem Your cluster’s Spark configuration values are not applied. Cause This hap... Set executor log level Warning This article describes steps related to customer use of Log4j 1.x within ... S3 connection fails with \"No role specified and no roles available\" Problem You are using Databricks Utilities (dbutils) to access a S3 bucket, but i... EBS leaked volumes Problem After a cluster is terminated on AWS, some EBS volumes are not deleted au... Related Articles Cluster Apache Spark configuration not applied Problem Your cluster’s Spark configuration values are not applied. Cause This hap... Set executor log level Warning This article describes steps related to customer use of Log4j 1.x within ... S3 connection fails with \"No role specified and no roles available\" Problem You are using Databricks Utilities (dbutils) to access a S3 bucket, but i... EBS leaked volumes Problem After a cluster is terminated on AWS, some EBS volumes are not deleted au... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Apache Spark UI shows less than total node memory",
          "view_href" : "https://kb.databricks.com/en_US/clusters/spark-shows-less-memory"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/get-and-set-spark-config",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management Get and set Apache Spark configuration properties in a notebook Get and set Apache Spark configuration properties in a notebook Written by mathan.pillai Last published at: May 26th, 2022 In most cases, you set the Spark config (AWS | Azure) at the cluster level. However, there may be instances when you need to check (or set) the values of specific Spark configuration properties in a notebook. This article shows you how to display the current value of a Spark configuration property in a notebook. It also shows you how to set a new value for a Spark configuration property in a notebook. Get Spark configuration properties To get the current value of a Spark config property, evaluate the property without including a value. Python %python\r\n\r\nspark.conf.get(\"spark.<name-of-property>\") R %r\r\n\r\nlibrary(SparkR)\r\nsparkR.conf(\"spark.<name-of-property>\") Scala %scala\r\n\r\nspark.conf.get(\"spark.<name-of-property>\") SQL %sql\r\n\r\nGET spark.<name-of-property>; Set Spark configuration properties To set the value of a Spark configuration property, evaluate the property and assign a value. Delete Info You can only set Spark configuration properties that start with the spark.sql prefix. Python %python\r\n\r\nspark.conf.set(\"spark.sql.<name-of-property>\", <value>) R %r\r\n\r\nlibrary(SparkR)\r\nsparkR.session()\r\nsparkR.session(sparkConfig = list(spark.sql.<name-of-property> = \"<value>\")) Scala %scala\r\n\r\nspark.conf.set(\"spark.sql.<name-of-property>\", <value>) SQL %sql\r\n\r\nSET spark.sql.<name-of-property> = <value>; Examples Get the current value of spark.rpc.message.maxSize. %sql\r\n\r\nSET spark.rpc.message.maxSize; Set the value of spark.sql.autoBroadcastJoinThreshold to -1. %python\r\n\r\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) Was this article helpful? (14) (49) Additional Informations Related Articles How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... Related Articles How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Get and set Apache Spark configuration properties in a notebook",
          "view_href" : "https://kb.databricks.com/en_US/data/get-and-set-spark-config"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data-sources/inconsistent-timestamp-results-jdbc",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data sources Inconsistent timestamp results with JDBC applications Inconsistent timestamp results with JDBC applications Timestamp records are inconsistent with JDBC applications when daylight saving time adjustments are made. Written by manjunath.swamy Last published at: June 1st, 2022 Problem When using JDBC applications with Databricks clusters you see inconsistent java.sql.Timestamp results when switching between standard time and daylight saving time. Cause Databricks clusters use UTC by default. java.sql.Timestamp uses the JVM’s local time zone. If a Databricks cluster returns 2021-07-12 21:43:08 as a string, the JVM parses it as 2021-07-12 21:43:08 and assumes the time zone is local. This works normally for most of the year, but when the local time zone has a DST adjustment, it causes an issue as UTC does not change. For example, on March 14, 2021, the US switched from standard time to daylight saving time. This means that local time went from 1:59 am to 3:00 am. If a Databricks cluster returns 2021-03-14 02:10:55, the JVM automatically converts it to 2021-03-14 03:10:55 because 02:10:55 does not exist in local time on that date. Solution Option 1: Configure the JVM time zone to UTC. Set the user.timezone property to GMT. Review the Java time zone settings documentation for more information. Option 2: Use ODBC instead of JDBC. ODBC interprets timestamps as UTC. Install the Databricks ODBC Driver. Connect pyodbc (AWS | Azure | GCP) to Databricks. You can also use turbodbc. Option 3: Set the local time zone to UTC in your JDBC application. Review the documentation for your JDBC application to learn how to configure the local time zone settings. Was this article helpful? (8) (15) Additional Informations Related Articles CosmosDB-Spark connector library conflict This article explains how to resolve an issue running applications that use the C... Delete table when underlying S3 bucket is deleted Problem You are trying to drop or alter a table when you get an error. Error in S... Failure to detect encoding in JSON Problem Spark job fails with an exception containing the message: Invalid UTF-32 ... Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2 Problem Info In general, you should use Databricks Runtime 5.2 and above, which i... Related Articles CosmosDB-Spark connector library conflict This article explains how to resolve an issue running applications that use the C... Delete table when underlying S3 bucket is deleted Problem You are trying to drop or alter a table when you get an error. Error in S... Failure to detect encoding in JSON Problem Spark job fails with an exception containing the message: Invalid UTF-32 ... Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2 Problem Info In general, you should use Databricks Runtime 5.2 and above, which i... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Inconsistent timestamp results with JDBC applications",
          "view_href" : "https://kb.databricks.com/en_US/data-sources/inconsistent-timestamp-results-jdbc"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/library-fail-transient-maven",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Libraries failing due to transient Maven issue Libraries failing due to transient Maven issue Library resolution failed. Cannot download some libraries due to transient Maven issue. Written by dayanand.devarapalli Last published at: May 11th, 2022 Problem Job fails because libraries cannot be installed. Library resolution failed. Cause: java.lang.RuntimeException: Cannot download some libraries due to transient Maven issue. Please try again later Cause After a Databricks upgrade, your cluster attempts to download any required libraries from Maven. After downloading, the libraries are stored as Workspace libraries. The next time you run a cluster that requires the libraries, they are loaded as Workspace libraries. This behavior is due to legacy code. Solution There is no workaround for this issue. This only happens after a maintenance window, and only if Maven is unavailable at the time. It is a rare corner case. Was this article helpful? (8) (13) Additional Informations Related Articles Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Install PyGraphViz PyGraphViz Python libraries are used to plot causal inference networks. If you tr... Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Cannot uninstall library from UI Problem Usually, libraries can be uninstalled in the Clusters UI. If the checkbox... Related Articles Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Install PyGraphViz PyGraphViz Python libraries are used to plot causal inference networks. If you tr... Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Cannot uninstall library from UI Problem Usually, libraries can be uninstalled in the Clusters UI. If the checkbox... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Libraries failing due to transient Maven issue",
          "view_href" : "https://kb.databricks.com/en_US/libraries/library-fail-transient-maven"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/append-a-row-to-rdd-or-dataframe",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management Append to a DataFrame Append to a DataFrame Learn how to append to a DataFrame in Databricks. Written by Adam Pavlacka Last published at: March 4th, 2022 To append to a DataFrame, use the union method. %scala\r\n\r\nval firstDF = spark.range(3).toDF(\"myCol\")\r\nval newRow = Seq(20)\r\nval appended = firstDF.union(newRow.toDF())\r\ndisplay(appended) %python\r\n\r\nfirstDF = spark.range(3).toDF(\"myCol\")\r\nnewRow = spark.createDataFrame([[20]])\r\nappended = firstDF.union(newRow)\r\ndisplay(appended) Was this article helpful? (13) (60) Additional Informations Related Articles How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... Get and set Apache Spark configuration properties in a notebook In most cases, you set the Spark config (AWS | Azure ) at the cluster level. Howe... Related Articles How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... Get and set Apache Spark configuration properties in a notebook In most cases, you set the Spark config (AWS | Azure ) at the cluster level. Howe... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Append to a DataFrame",
          "view_href" : "https://kb.databricks.com/en_US/data/append-a-row-to-rdd-or-dataframe"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/unable-to-cast-string-to-varchar",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Unable to cast string to varchar Unable to cast string to varchar Use varchar type in Databricks Runtime 8.0 and above. It can only be used in table schema. It cannot be used in functions or operators. Written by DD Sharma Last published at: May 10th, 2022 Problem You are trying to cast a string type column to varchar but it isn’t working. Delete Info The varchar data type (AWS | Azure | GCP) is available in Databricks Runtime 8.0 and above. Create a simple Delta table, with one column as type string. %sql\r\n\r\nCREATE OR REPLACE TABLE delta_table1 (`col1` string)\r\nUSING DELTA; Use SHOW TABLE on the newly created table and it reports a string type. %sql\r\n\r\nSHOW CREATE TABLE delta_table1; Create a second Delta table, based on the first, and convert the string type column into varchar. %sql\r\n\r\nCREATE OR REPLACE TABLE delta_varchar_table1\r\nUSING DELTA\r\nAS\r\nSELECT cast(col1 AS VARCHAR(1000)) FROM delta_table1; Use SHOW TABLE on the newly created table and it reports that the table got created, but the column is string type. %sql\r\n\r\nSHOW CREATE TABLE delta_varchar_table1; Cause The varchar type can only be used in table schema. It cannot be used in functions or operators. Please review the Spark supported data types documentation for more information. Solution You cannot cast string to varchar, but you can create a varchar Delta table. %sql\r\n\r\nCREATE OR REPLACE TABLE delta_varchar_table2 (`col1` VARCHAR(1000))\r\nUSING DELTA; Use SHOW TABLE on the newly created table and it reports a varchartype. %sql\r\n\r\nSHOW CREATE TABLE delta_varchar_table2; You can now create another varchar Delta table, based on the first, and it keeps the varchartype. %sql\r\n\r\nCREATE OR REPLACE TABLE delta_varchar_table3\r\nUSING DELTA\r\nAS\r\nSELECT * FROM delta_varchar_table2; Use SHOW TABLE on the newly created table and it reports a varchartype. %sql\r\n\r\nSHOW CREATE TABLE delta_varchar_table3; Was this article helpful? (6) (14) Additional Informations Related Articles Best practices for dropping a managed Delta Lake table Regardless of how you drop a managed table, it can take a significant amount of t... How to populate or update columns in an existing Delta table Problem You have an existing Delta table, with a few empty columns. You need to p... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... How Delta cache behaves on an autoscaling cluster This article is about how Delta cache (AWS | Azure | GCP) behaves on an auto-scal... Related Articles Best practices for dropping a managed Delta Lake table Regardless of how you drop a managed table, it can take a significant amount of t... How to populate or update columns in an existing Delta table Problem You have an existing Delta table, with a few empty columns. You need to p... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... How Delta cache behaves on an autoscaling cluster This article is about how Delta cache (AWS | Azure | GCP) behaves on an auto-scal... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Unable to cast string to varchar",
          "view_href" : "https://kb.databricks.com/en_US/delta/unable-to-cast-string-to-varchar"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/append-output-not-supported-no-watermark",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming Append output is not supported without a watermark Append output is not supported without a watermark Append output mode is not supported on aggregated DataFrames without a watermark. Written by Adam Pavlacka Last published at: May 17th, 2022 Problem You are performing an aggregation using append mode and an exception error message is returned. Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark Cause You cannot use append mode on an aggregated DataFrame without a watermark. This is by design. Solution You must apply a watermark to the DataFrame if you want to use append mode on an aggregated DataFrame. The aggregation must have an event-time column, or a window on the event-time column. Group the data by window and word and compute the count of each group. .withWatermark() must be called on the same column as the timestamp column used in the aggregation. The example code shows how this can be done. Replace the value <type> with the type of element you are processing. For example, you would use Row if you are processing by row. Replace the value <words> with the streaming DataFrame of schema { timestamp: Timestamp, word: String }. %java\r\n\r\nDataset<type> windowedCounts = <words>\r\n    .withWatermark(\"timestamp\", \"10 minutes\")\r\n    .groupBy(\r\n        functions.window(words.col(\"timestamp\"), \"10 minutes\", \"5 minutes\"),\r\n        words.col(\"word\"))\r\n    .count(); %python\r\n\r\nwindowedCounts = <words> \\\r\n    .withWatermark(\"timestamp\", \"10 minutes\") \\\r\n    .groupBy(\r\n        window(words.timestamp, \"10 minutes\", \"5 minutes\"),\r\n        words.word) \\\r\n    .count() %scala\r\n\r\nimport spark.implicits._\r\n\r\nval windowedCounts = <words>\r\n    .withWatermark(\"timestamp\", \"10 minutes\")\r\n    .groupBy(\r\n        window($\"timestamp\", \"10 minutes\", \"5 minutes\"),\r\n        $\"word\")\r\n    .count() You must call .withWatermark() before you perform the aggregation. Attempting otherwise fails with an error message. For example, df.groupBy(\"time\").count().withWatermark(\"time\", \"1 min\") returns an exception. Please refer to the Apache Spark documentation on conditions for watermarking to clean the aggregation slate for more information. Was this article helpful? (9) (42) Additional Informations Related Articles Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... Get the path of files consumed by Auto Loader When you process streaming files with Auto Loader (AWS | Azure | GCP), events are... How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Handling partition column values while using an SQS queue as a streaming source Problem If data in S3 is stored by partition, the partition column values are use... Related Articles Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... Get the path of files consumed by Auto Loader When you process streaming files with Auto Loader (AWS | Azure | GCP), events are... How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Handling partition column values while using an SQS queue as a streaming source Problem If data in S3 is stored by partition, the partition column values are use... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Append output is not supported without a watermark",
          "view_href" : "https://kb.databricks.com/en_US/streaming/append-output-not-supported-no-watermark"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/query-not-skip-header-ext-table",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark Query does not skip header row on external table Query does not skip header row on external table External Hive tables do not skip the header row when queried from Spark SQL. Written by manisha.jena Last published at: May 24th, 2022 Problem You are attempting to query an external Hive table, but it keeps failing to skip the header row, even though TBLPROPERTIES ('skip.header.line.count'='1') is set in the HiveContext. You can reproduce the issue by creating a table with this sample code. %sql\r\n\r\nCREATE EXTERNAL TABLE school_test_score (\r\n  `school` varchar(254),\r\n  `student_id` varchar(254),\r\n  `gender` varchar(254),\r\n  `pretest` varchar(254),\r\n  `posttest` varchar(254))\r\nROW FORMAT DELIMITED\r\n  FIELDS TERMINATED BY ','\r\n  LINES TERMINATED BY '\\n'\r\nSTORED AS INPUTFORMAT\r\n  'org.apache.hadoop.mapred.TextInputFormat'\r\nOUTPUTFORMAT\r\n  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\r\nLOCATION\r\n  'dbfs:/FileStore/table_header/'\r\nTBLPROPERTIES (\r\n   'skip.header.line.count'='1'\r\n) If you try to select the first five rows from the table, the first row is the header row. %sql\r\n\r\nSELECT * FROM school_test_score LIMIT 5 Cause If you query directly from Hive, the header row is correctly skipped. Apache Spark does not recognize the skip.header.line.count property in HiveContext, so it does not skip the header row. Spark is behaving as designed. Solution You need to use Spark options to create the table with a header option. %sql\r\n\r\nCREATE TABLE student_test_score (school String, student_id String, gender String, pretest String, posttest String) USING CSV\r\nOPTIONS (path \"dbfs:/FileStore/table_header/\",\r\n        delimiter \",\",\r\n        header \"true\")\r\n        ; Select the first five rows from the table and the header row is not included. %sql\r\n\r\nSELECT * FROM school_test_score LIMIT 5 Was this article helpful? (7) (12) Additional Informations Related Articles Error in SQL statement: AnalysisException: Table or view not found Problem When you try to query a table or view, you get this error: AnalysisExcept... Generate unique increasing numeric values This article shows you how to use Apache Spark functions to generate unique incre... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Duplicate columns in the metadata error Problem Your Apache Spark job is processing a Delta table when the job fails with... Related Articles Error in SQL statement: AnalysisException: Table or view not found Problem When you try to query a table or view, you get this error: AnalysisExcept... Generate unique increasing numeric values This article shows you how to use Apache Spark functions to generate unique incre... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Duplicate columns in the metadata error Problem Your Apache Spark job is processing a Delta table when the job fails with... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Query does not skip header row on external table",
          "view_href" : "https://kb.databricks.com/en_US/sql/query-not-skip-header-ext-table"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/get-file-path-auto-loader",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming Get the path of files consumed by Auto Loader Get the path of files consumed by Auto Loader Get the path and filename of all files consumed by Auto Loader and write them out as a new column. Written by Adam Pavlacka Last published at: May 18th, 2022 When you process streaming files with Auto Loader (AWS | Azure | GCP), events are logged based on the files created in the underlying storage. This article shows you how to add the file path for every filename to a new column in the output DataFrame. One use case for this is auditing. When files are ingested to a partitioned folder structure there is often useful metadata, such as the timestamp, which can be extracted from the path for auditing purposes. For example, assume a file path and filename of 2020/2021-01-01/file1_T191634.csv. From this path you can apply custom UDFs and use regular expressions to extract details like the date (2021-01-01) and the timestamp (T191634). The following example code uses input_file_name() get the path and filename for every row and write it to a new column named filePath. %scala\r\n\r\nval df = spark.readStream.format(\"cloudFiles\")\r\n  .schema(schema)\r\n  .option(\"cloudFiles.format\", \"csv\")\r\n  .option(\"cloudFiles.region\",\"ap-south-1\")\r\n  .load(\"path\")\r\n  .withColumn(\"filePath\",input_file_name()) Was this article helpful? (11) (13) Additional Informations Related Articles Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... Related Articles Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Get the path of files consumed by Auto Loader",
          "view_href" : "https://kb.databricks.com/en_US/streaming/get-file-path-auto-loader"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/spark-udf-performance",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Apache Spark Jobs hang due to non-deterministic custom UDF Apache Spark Jobs hang due to non-deterministic custom UDF Learn what to do when your Apache Spark job hangs due to a non-deterministic custom UDF. Written by Adam Pavlacka Last published at: May 10th, 2022 Problem Sometimes Apache Spark jobs hang indefinitely due to the non-deterministic behavior of a Spark User-Defined Function (UDF). Here is an example of such a function: %scala\r\n\r\nval convertorUDF = (commentCol: String) =>\r\n    {\r\n              #UDF definition\r\n    }\r\nval translateColumn = udf(convertorUDF) If you call this UDF using the withColumn() API and then apply some filter transformation on the resulting DataFrame, the UDF could potentially execute multiple times for each record, affecting application performance. %scala\r\n\r\nval translatedDF = df.withColumn(\"translatedColumn\", translateColumn( df(\"columnToTranslate\")))\r\nval filteredDF = translatedDF.filter(!translatedDF(\"translatedColumn\").contains(\"Invalid URL Provided\")) && !translatedDF(\"translatedColumn\").contains(\"Unable to connect to Microsoft API\")) Cause Sometimes a deterministic UDF can behave nondeterministically, performing duplicate invocations depending on the definition of the UDF. You often see this behavior when you use a UDF on a DataFrame to add an additional column using the withColumn() API, and then apply a transformation (filter) to the resulting DataFrame. Solution UDFs must be deterministic. Due to optimization, duplicate invocations might be eliminated or the function can be invoked more times than it is present in the query. The better option is to cache the DataFrame where you are using the UDF. If the DataFrame contains a large amount of data, then writing it to a Parquet format file is optimal. You can use the following code to cache the result: %scala\r\n\r\nval translatedDF = df.withColumn(\"translatedColumn\", translateColumn( df(\"columnToTranslate\"))).cache() Was this article helpful? (10) (10) Additional Informations Related Articles How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... Job fails, but Apache Spark tasks finish Problem Your Databricks job reports a failed status, but all Spark jobs and tasks... Job fails due to job rate limit Problem A Databricks notebook or Jobs API request returns the following error: Er... Related Articles How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... Job fails, but Apache Spark tasks finish Problem Your Databricks job reports a failed status, but all Spark jobs and tasks... Job fails due to job rate limit Problem A Databricks notebook or Jobs API request returns the following error: Er... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Apache Spark Jobs hang due to non-deterministic custom UDF",
          "view_href" : "https://kb.databricks.com/en_US/jobs/spark-udf-performance"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/python-cmd-fail-conda-cluster",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning Python commands fail on Machine Learning clusters Python commands fail on Machine Learning clusters Python commands are failing on Databricks Runtime for Machine Learning clusters. Conda. Written by arjun.kaimaparambilrajan Last published at: May 16th, 2022 Problem You are using a Databricks Runtime for Machine Learning cluster and Python notebooks are failing. You find an invalid syntax error in the logs. SyntaxError: invalid syntax\r\n  File \"/local_disk0/tmp/1593092990800-0/PythonShell.py\", line 363\r\n    def __init__(self, *args, condaMagicHandler=None, **kwargs): Cause Key values in the /etc/environment/ file are being overwritten by user environment variables. There are several default environment variables that should not be overwritten. For example, MLFLOW_CONDA_HOME=/databricks/conda is set by default. If you overwrite this value it can result in the invalid syntax error. This sample init script can cause the issue, because it is replacing, rather than appending a value. %python\r\n\r\ndbutils.fs.put(\"/databricks/init-scripts/set-env.sh\", \"\"\"#!/bin/bash\r\nsudo echo VAR1=\"VAL1\" > /etc/environment\r\nsudo echo VAR2=\"VAL2\" > /etc/environment\r\nsudo echo VAR3=\"VAL3\" > /etc/environment\r\n\"\"\", true) Solution You should not overwrite any values in the /etc/environment/ file. You should always append variables to the /etc/environment/ file. This sample init script avoids the issue by appending every to value to the /etc/environment/ file. %python\r\n\r\ndbutils.fs.put(\"/databricks/init-scripts/set-env.sh\", \"\"\"#!/bin/bash\r\nsudo echo VAR1=\"VAL1\" >> /etc/environment\r\nsudo echo VAR2=\"VAL2\" >> /etc/environment\r\nsudo echo VAR3=\"VAL3\" >> /etc/environment\r\n\"\"\", true) Was this article helpful? (9) (10) Additional Informations Related Articles MLflow project fails to access an Apache Hive table Problem You have an MLflow project that fails to access a Hive table and returns ... H2O.ai Sparkling Water cluster not reachable Problem You are trying to initialize H2O.ai’s Sparkling Water on Databricks Runti... Experiment warning when custom artifact storage location is used Problem When you create an MLflow experiment with a custom artifact location, you... Incorrect results when using documents as inputs Problem You have a ML model that takes documents as inputs, specifically, an arra... Related Articles MLflow project fails to access an Apache Hive table Problem You have an MLflow project that fails to access a Hive table and returns ... H2O.ai Sparkling Water cluster not reachable Problem You are trying to initialize H2O.ai’s Sparkling Water on Databricks Runti... Experiment warning when custom artifact storage location is used Problem When you create an MLflow experiment with a custom artifact location, you... Incorrect results when using documents as inputs Problem You have a ML model that takes documents as inputs, specifically, an arra... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Python commands fail on Machine Learning clusters",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/python-cmd-fail-conda-cluster"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dbfs/how-to-specify-the-dbfs-path",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Databricks File System (DBFS) How to specify the DBFS path How to specify the DBFS path Learn how to specify the DBFS path in Apache Spark, Bash, DBUtils, Python, and Scala Written by ram.sankarasubramanian Last published at: March 8th, 2022 When working with Databricks you will sometimes have to access the Databricks File System (DBFS). Accessing files on DBFS is done with standard filesystem commands, however the syntax varies depending on the language or tool used. For example, take the following DBFS path: dbfs:/mnt/test_folder/test_folder1/ Apache Spark Under Spark, you should specify the full path inside the Spark read command. spark.read.parquet(“dbfs:/mnt/test_folder/test_folder1/file.parquet”) DBUtils When you are using DBUtils, the full DBFS path should be used, just like it is in Spark commands. The language specific formatting around the DBFS path differs depending on the language used. Bash %fs\r\nls dbfs:/mnt/test_folder/test_folder1/ Python %python\r\n\r\ndbutils.fs.ls(‘dbfs:/mnt/test_folder/test_folder1/’) Scala %scala\r\n\r\ndbutils.fs.ls(“dbfs:/mnt/test_folder/test_folder1/”) Delete Note Specifying dbfs: is not required when using DBUtils or Spark commands. The path dbfs:/mnt/test_folder/test_folder1/ is equivalent to /mnt/test_folder/test_folder1/. Shell commands Shell commands do not recognize the DFBS path. Instead, DBFS and the files within, are accessed with the same syntax as any other folder on the file system. Bash ls /dbfs/mnt/test_folder/test_folder1/\r\ncat /dbfs/mnt/test_folder/test_folder1/file_name.txt Python import os\r\nos.listdir('/dbfs/mnt/test_folder/test_folder1/’) Scala import java.io.File\r\nval directory = new File(\"/dbfs/mnt/test_folder/test_folder1/\")\r\ndirectory.listFiles Was this article helpful? (11) (12) Additional Informations Related Articles Cannot read Databricks objects stored in the DBFS root directory Problem An Access Denied error returns when you attempt to read Databricks object... How to calculate the Databricks file system (DBFS) S3 API call cost The cost of a DBFS S3 bucket is primarily driven by the number of API calls, and ... Cannot access objects written by Databricks from outside Databricks Problem When you attempt to access an object in an S3 location written by Databri... Related Articles Cannot read Databricks objects stored in the DBFS root directory Problem An Access Denied error returns when you attempt to read Databricks object... How to calculate the Databricks file system (DBFS) S3 API call cost The cost of a DBFS S3 bucket is primarily driven by the number of API calls, and ... Cannot access objects written by Databricks from outside Databricks Problem When you attempt to access an object in an S3 location written by Databri... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to specify the DBFS path",
          "view_href" : "https://kb.databricks.com/en_US/dbfs/how-to-specify-the-dbfs-path"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/administration/who-deleted-workspace",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Databricks administration How to discover who deleted a workspace in Azure portal How to discover who deleted a workspace in Azure portal Learn how to discover who deleted an Azure Databricks workspace. Written by Adam Pavlacka Last published at: February 25th, 2022 If your workspace has disappeared or been deleted, you can identify which user deleted it by checking the Activity log in the Azure portal. Go to the Activity log in the Azure portal. Expand the timeline to focus on when the workspace was deleted. Filter the log for a record of the specific event. Click on the event to display information about the event, including the user who initiated the event. The screenshot shows how you can click the Remove Databricks Workspace event in the Operation Name column, and then view detailed information about the event. If you are still unable to find who deleted the workspace, create a support case with Microsoft Support. Provide details such as the workspace id and the time range of the event (including your time zone). Microsoft Support will review the corresponding backend activity logs. Was this article helpful? (7) (15) Additional Informations Related Articles How to discover who deleted a cluster in Azure portal If a cluster in your workspace has disappeared or been deleted, you can identify ... SSO server redirects to original URL, not to vanity Databricks URL Problem When you log into Databricks using a vanity URL (such as mycompany.cloud.... SSO SAML authentication error with PingFederate Problem When using PingFederate to authenticate over a SSO connection with Databr... Related Articles How to discover who deleted a cluster in Azure portal If a cluster in your workspace has disappeared or been deleted, you can identify ... SSO server redirects to original URL, not to vanity Databricks URL Problem When you log into Databricks using a vanity URL (such as mycompany.cloud.... SSO SAML authentication error with PingFederate Problem When using PingFederate to authenticate over a SSO connection with Databr... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to discover who deleted a workspace in Azure portal",
          "view_href" : "https://kb.databricks.com/en_US/administration/who-deleted-workspace"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/install-private-pypi-repo",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Install a private PyPI repo Install a private PyPI repo How to install libraries from private PyPI repositories. Written by darshan.bargal Last published at: March 4th, 2022 Certain use cases may require you to install libraries from private PyPI repositories. If you are installing from a public repository, you should review the library documentation. This article shows you how to configure an example init script that authenticates and downloads a PyPI library from a private repository. Create init script Create (or verify) a directory to store the init script.<init-script-folder>is the name of the folder where you store your init scripts. dbutils.fs.mkdirs(\"dbfs:/databricks/<init-script-folder>/\") Create the init script. dbutils.fs.put(\"/databricks/<init-script-folder>/private-pypi-install.sh\",\"\"\"\r\n#!/bin/bash\r\n/databricks/python/bin/pip install --index-url=https://${<repo-username>}:${<repo-password>}@<private-pypi-repo-domain-name> private-package==<version>\r\n\"\"\", True) Verify that your init script exists. display(dbutils.fs.ls(\"dbfs:/databricks/<init-script-folder>/private-pypi-install.sh\")) Install as a cluster-scoped init script Install the init script that you just created as a cluster-scoped init script. You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/private-pypi-install.sh). Restart the cluster Restart your cluster after you have installed the init script. Once the cluster starts up, verify that it successfully installed the custom library from the private PyPI repository. If the custom library is not installed, double check the username and password that you set for the private PyPI repository in the init script. Use the init script with a job cluster Once you have the init script created, and verified working, you can include it in a create-job.json file when using the Jobs API to start a job cluster. {\r\n  \"cluster_id\": \"1202-211320-brick1\",\r\n  \"num_workers\": 1,\r\n  \"spark_version\": \"<spark-version>\",\r\n  \"node_type_id\": \"<node-type>\",\r\n  \"cluster_log_conf\": {\r\n    \"dbfs\" : {\r\n      \"destination\": \"dbfs:/cluster-logs\"\r\n    }\r\n  },\r\n  \"init_scripts\": [ {\r\n    \"dbfs\": {\r\n      \"destination\": \"dbfs:/databricks/<init-script-folder>/private-pypi-install.sh\"\r\n    }\r\n  } ]\r\n} Was this article helpful? (8) (14) Additional Informations Related Articles Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Related Articles Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Install a private PyPI repo",
          "view_href" : "https://kb.databricks.com/en_US/clusters/install-private-pypi-repo"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/job-fails-connection-pool",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Apache Spark job fails with a Connection pool shut down error Apache Spark job fails with a Connection pool shut down error Apache Spark job fails with a java.lang.IllegalStateException: Connection pool shut down error. Written by noopur.nigam Last published at: May 11th, 2022 Problem A Spark job fails with the error message java.lang.IllegalStateException: Connection pool shut down when attempting to write data into a Delta table on S3. Cause Spark jobs writing to S3 are limited to a maximum number of simultaneous connections. The java.lang.IllegalStateException: Connection pool shut down occurs when this connection pool is exhausted. Solution The client connection pool is configured by the fs.s3a.connection.maximum value. This defines the maximum number of simultaneous connections to S3. It defaults to a value of 200. You can increase the size of the client connection pool by setting a higher value in the Spark configuration properties. Databricks recommends that you set the maximum number of connections to a multiple of the total number of cores in your cluster. For example, if you are using a 32 core cluster, you should try setting fs.s3a.connection.maximum to a value of 320 or 352. Once the maximum number of connections is set high enough, the java.lang.IllegalStateException: Connection pool shut down will no longer occur. Was this article helpful? (6) (14) Additional Informations Related Articles Apache Spark Jobs hang due to non-deterministic custom UDF Problem Sometimes Apache Spark jobs hang indefinitely due to the non-deterministi... Job fails, but Apache Spark tasks finish Problem Your Databricks job reports a failed status, but all Spark jobs and tasks... Apache Spark job fails with Failed to parse byte string Problem Spark-submit jobs fail with a Failed to parse byte string: -1 error messa... Apache Spark UI shows wrong number of jobs Problem You are reviewing the number of active Apache Spark jobs on a cluster in ... Related Articles Apache Spark Jobs hang due to non-deterministic custom UDF Problem Sometimes Apache Spark jobs hang indefinitely due to the non-deterministi... Job fails, but Apache Spark tasks finish Problem Your Databricks job reports a failed status, but all Spark jobs and tasks... Apache Spark job fails with Failed to parse byte string Problem Spark-submit jobs fail with a Failed to parse byte string: -1 error messa... Apache Spark UI shows wrong number of jobs Problem You are reviewing the number of active Apache Spark jobs on a cluster in ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Apache Spark job fails with a Connection pool shut down error",
          "view_href" : "https://kb.databricks.com/en_US/jobs/job-fails-connection-pool"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/metastore/jpn-char-external-metastore",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Metastore Japanese character support in external metastore Japanese character support in external metastore Use Japanese characters in tables in an external metastore. Written by Adam Pavlacka Last published at: May 16th, 2022 Problem You are trying to use Japanese characters in your tables, but keep getting errors. Create a table with the OPTIONS keyword OPTIONS provides extra metadata to the table. You try creating a table with OPTIONS and specify the charset as utf8mb4. %sql\r\n\r\nCREATE TABLE default.JPN_COLUMN_NAMES('作成年月' string\r\n,'計上年月' string\r\n,'所属コード' string\r\n,'生保代理店コード＿８桁' string\r\n,'所属名' string\r\n)\r\nusing csv  OPTIONS (path \"/mnt/tabledata/testdata/\", header \"true\", delimiter \",\", inferSchema \"false\", ignoreLeadingWhiteSpace \"false\", ignoreTrailingWhiteSpace \"false\", multiLine \"true\", escape \"\\\"\" , charset \"utf8mb4\"); The result is an error. Error in SQL statement: AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Put request failed : INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (?,?,?) Create a table without the OPTIONS keyword You try to create a table without using OPTIONS. %sql\r\n\r\nCREATE TABLE test.JPN_COLUMN_NAMES (`作成年月` string ,`計上年月` string) USING csv\r\ndescribe extended test.JPN_COLUMN_NAMES; The table appears to be created, but the column names are shown as ???? instead of using the specified Japanese characters. Create a table with Hive table expression You try creating a Hive format table and specify the charset as utf8mb4. %sql\r\n\r\nCREATE TABLE test.JPN_COLUMN_NAMES (`作成年月` string ,`計上年月` string)\r\n   ROW FORMAT SERDE \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\"\r\n   WITH SERDEPROPERTIES ( \"separatorChar\" = \",\",\r\n    \"quoteChar\" = \"\\\"\",\r\n    \"escapeChar\" = \"\\\\\",\r\n    \"serialization.encoding\"='utf8mb4')\r\n    TBLPROPERTIES ( 'store.charset'='utf8mb4',\r\n    'retrieve.charset'='utf8mb4'); The result is an error. Caused by: java.sql.SQLException: Incorrect string value: '\\xE4\\xBD\\x9C\\xE6\\x88\\x90...' for column 'COLUMN_NAME' at row 1\r\nQuery is: INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,`COLUMN_NAME`,TYPE_NAME,INTEGER_IDX) VALUES (6544,<null>,'作成年月','string',0) Cause When a table is created, an entry is updated in the Hive metastore. The Hive metastore is typically a MySQL database. When a new table is created, the names of the columns are inserted into the TABLE_PARAMS of the metastore. The charset collation of PARAM_VALUE from TABLE_PARAMS is latin1_bin as collation and the charset is latin1. %scala\r\n\r\nexecuteQuery(\"\"\"SELECT TABLE_SCHEMA , TABLE_NAME , COLUMN_NAME , COLLATION_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'TABLE_PARAMS' \"\"\") Solution latin1 does not have support for Japanese characters, but UTF-8 does. You need to use an external metastore with UTF-8_bin as collation and the charset as UTF-8. Any MySQL database 5.6 or above can be used as a Hive metastore. For this example, we are using MySQL 8.0.13-4. Create an external Apache Hive metastore (AWS | Azure | GCP). Create a database to instantiate the new metastore with default tables. %sql\r\n\r\ncreate database <database_name> The newly created tables can be explored in the external database objects browser or by using the show tables command. %sql\r\n\r\n-- Run in the metastore database.\r\nshow tables in <database_name> Check the collation information in MySQL at the table level. %sql\r\n\r\nSELECT TABLE_COLLATION,TABLE_NAME,TABLE_TYPE,TABLE_COLLATION FROM INFORMATION_SCHEMA.TABLES where TABLE_TYPE like 'BASE%' Check the collation information in MySQL at the column level. %sql\r\n\r\nSELECT TABLE_SCHEMA , TABLE_NAME , COLUMN_NAME , COLLATION_NAME FROM INFORMATION_SCHEMA.COLUMNS Change the charset from latin1 to UTF-8. %sql\r\n\r\n-- Run in the metastore database. All queries are compatible with MySQL.\r\n-- Change collation and charset across the database.\r\nALTER DATABASE <database_name> CHARACTER SET utf8 COLLATE utf8_bin;\r\n-- Change collation and charset per table.\r\nALTER TABLE <table_name> CONVERT TO CHARACTER SET utf8 COLLATE utf8_bin;\r\n-- Change collation and charset at the column level.\r\nALTER TABLE <table_name> MODIFY <column_name> <datatype> CHARACTER SET utf8 COLLATE utf8_bin; You can now correctly view Japanese characters when you display the table. Was this article helpful? (7) (13) Additional Informations Related Articles Listing table names Problem To fetch all the table names from metastore you can use either spark.cata... How to set up an embedded Apache Hive metastore You can set up a Databricks cluster to use an embedded metastore. You can use an ... How to create table DDLs to import into an external metastore Databricks supports using external metastores instead of the default Hive metasto... AnalysisException when dropping table on Azure-backed metastore Problem When you try to drop a table in an external Hive version 2.0 or 2.1 metas... Related Articles Listing table names Problem To fetch all the table names from metastore you can use either spark.cata... How to set up an embedded Apache Hive metastore You can set up a Databricks cluster to use an embedded metastore. You can use an ... How to create table DDLs to import into an external metastore Databricks supports using external metastores instead of the default Hive metasto... AnalysisException when dropping table on Azure-backed metastore Problem When you try to drop a table in an external Hive version 2.0 or 2.1 metas... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Japanese character support in external metastore",
          "view_href" : "https://kb.databricks.com/en_US/metastore/jpn-char-external-metastore"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/s3-failed-no-roles-available",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters S3 connection fails with \"No role specified and no roles available\" S3 connection fails with \"No role specified and no roles available\" Databricks Utilities (dbutils) fails with a role error when IAM Role Passthrough and an Instance Profile are both enabled. Written by pavan.kumarchalamcharla Last published at: March 4th, 2022 Problem You are using Databricks Utilities (dbutils) to access a S3 bucket, but it fails with a No role specified and no roles available error. You have confirmed that the instance profile associated with the cluster has the permissions needed to access the S3 bucket. Unable to load AWS credentials from any provider in the chain: [com.databricks.backend.daemon.driver.aws.AwsLocalCredentialContextTokenProvider@ff3090c: No role specified and no roles available., com.databricks.backend.daemon.driver.aws.ProxiedIAMCredentialProvider@31f1245d: User does not have any IAM roles] Cause This can occur when both of the following items are true: The cluster has IAM Role Passthrough enabled. The cluster has an Instance Profile enabled. When IAM Role Passthrough is enabled, every other authentication mechanism set at the cluster or notebook level is overwritten by IAM passthrough authentication. Solution When using a regular instance profile, make sure that IAM Role Passthrough is disabled. Review the documentation on secure access to S3 buckets using instance profiles to ensure that the correct IAM role is configured. When using the IAM Role Passthrough feature, make sure you have correctly configured the meta instance profile. For more information, review the documentation on accessing S3 buckets using IAM credential passthrough with Databricks SCIM. Was this article helpful? (7) (15) Additional Informations Related Articles Admin user cannot restart cluster to run job Problem When a user who has permission to start a cluster, such as a Databricks A... Persist Apache Spark CSV metrics to a DBFS location Spark has a configurable metrics system that supports a number of sinks, includin... EBS leaked volumes Problem After a cluster is terminated on AWS, some EBS volumes are not deleted au... Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Related Articles Admin user cannot restart cluster to run job Problem When a user who has permission to start a cluster, such as a Databricks A... Persist Apache Spark CSV metrics to a DBFS location Spark has a configurable metrics system that supports a number of sinks, includin... EBS leaked volumes Problem After a cluster is terminated on AWS, some EBS volumes are not deleted au... Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "S3 connection fails with \"No role specified and no roles available\"",
          "view_href" : "https://kb.databricks.com/en_US/clusters/s3-failed-no-roles-available"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/send-email",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks How to send email or SMS messages from Databricks notebooks How to send email or SMS messages from Databricks notebooks Learn how to send an email from your Databricks notebook. Written by Adam Pavlacka Last published at: May 17th, 2022 You may need to send a notification to a set of recipients from a Databricks notebook. For example, you may want to send email based on matching business rules or based on a command’s success or failure. This article describes two approaches to sending email or SMS messages from a notebook. Both examples use Python notebooks: Send email or SMS messages from a notebook using Amazon SNS This approach requires that you have an Amazon Simple Notification Service (SNS) article to send the notification to. Recipients who are subscribed to the SNS article will receive the email or SMS notification. Delete Info You may need to install boto3, a Python library for working with Amazon AWS: %python\r\n\r\npip install boto3 Add the following Python commands to your notebook, replacing <sample values> with your own: %python\r\n\r\n# Import the boto3 client\r\nimport boto3\r\n\r\n# Set the AWS region name, retrieve the access key & secret key from dbutils secrets.\r\n# For information about how to store the credentials in a secret, see\r\n# https://docs.databricks.com/user-guide/secrets/secrets.html\r\nAWS_REGION = \"<region-name>\"\r\nACCESS_KEY = dbutils.secrets.get('<scope-name>','<access-key>')\r\nSECRET_KEY = dbutils.secrets.get('<scope-name>','<secret-key>')\r\nsender='<sender@email-domain.com>'\r\n\r\n#Create the boto3 client with the region name, access key and secret keys.\r\nclient = boto3.client('sns',region_name=AWS_REGION,\r\naws_access_key_id=ACCESS_KEY,\r\naws_secret_access_key=SECRET_KEY)\r\n\r\n# Add email subscribers\r\nfor email in list_of_emails:\r\n    client.subscribe(\r\n        articleArn=article_arn,\r\n        Protocol='email',\r\n        Endpoint=email  # <-- email address who'll receive an email.\r\n    )\r\n\r\n# Add phone subscribers\r\nfor number in list_of_phone_numbers:\r\n    client.subscribe(\r\n        articleArn=article_arn,\r\n        Protocol='sms',\r\n        Endpoint=number  # <-- phone numbers who'll receive SMS.\r\n    )\r\n\r\n# Send message to the SNS article using publish method.\r\nresponse = client.publish(\r\narticleArn='<article-arn-name>',\r\nMessage=\"Hello From Databricks..\",\r\nSubject='Email Notification from Databricks..',\r\nMessageStructure='string'\r\n) Send email from a notebook using Amazon SES This example notebook demonstrates how to send an email message that includes HTML content and a file attachment, using Amazon Simple Email Service (SES). Example Send email notebook Review the Send email notebook. Was this article helpful? (10) (18) Additional Informations Related Articles Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... Error: Received command c on object id p0 Problem You have imported Python libraries, but when you try to execute Python co... Failure when accessing or mounting storage Problem You are trying to access an existing mount point, or create a new mount p... Notebook autosave fails due to file size limits Problem Notebook autosaving fails with the following error message: Failed to sav... Related Articles Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... Error: Received command c on object id p0 Problem You have imported Python libraries, but when you try to execute Python co... Failure when accessing or mounting storage Problem You are trying to access an existing mount point, or create a new mount p... Notebook autosave fails due to file size limits Problem Notebook autosaving fails with the following error message: Failed to sav... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to send email or SMS messages from Databricks notebooks",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/send-email"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/job-kinesis-connector-fail-oom",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming Streaming job using Kinesis connector fails Streaming job using Kinesis connector fails A streaming job writing to a Kinesis sink fails with out of memory error because HTTP clients are not getting terminated. Written by ashish Last published at: May 19th, 2022 Problem You have a streaming job writing to a Kinesis sink, and it is failing with out of memory error messages. java.lang.OutOfMemoryError: GC Overhead limit exceeded java.lang.OutOfMemoryError: Java heap space. Symptoms include: Ganglia shows a gradual increase in JVM memory usage. Microbatch analysis shows input and processing rates are consistent, which means there are no issues with the source or processing. A heapdump shows Java hashmaps occupying large spaces on the JVM heap and increasing over time. Cause The most common reason for this error is that you are closing the Kinesis client, but you are not closing the HTTP client. For example, a new Kinesis client is usually created for every partition. %scala\r\n\r\nclass KinesisSink extends ForeachWriter[SinkInput] {   private var kinesisClient: KinesisClient = _   override def open(partitionId: Long, version: Long): Boolean = {\r\n    val httpClient = ApacheHttpClient\r\n        .builder()\r\n        .build()\r\n    kinesisClient = KinesisClient\r\n      .builder()\r\n      .region(Region.of(region))\r\n      .httpClient(httpClient)\r\n      .build()\r\n    true\r\n  }\r\n override def process(value: KinesisSinkInput): Unit = {\r\n    // Main process is here.\r\n  }\r\n override def close(errorOrNull: Throwable): Unit = {\r\n    kinesisClient.close()\r\n  }\r\n} This sample code is calling kinesisClient.close() but it is not calling httpClient.close(). This means HTTP clients are being created, and using resources to open TCP connections, but are not getting terminated. Solution Ensure that you are closing HTTP clients when they are no longer required. %scala\r\n\r\noverride def close(errorOrNull: Throwable): Unit = {\r\n  client.close()\r\n  httpClient.close()\r\n} Was this article helpful? (8) (11) Additional Informations Related Articles Get the path of files consumed by Auto Loader When you process streaming files with Auto Loader (AWS | Azure | GCP), events are... Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... Conflicting directory structures error Problem You have an Apache Spark job that is failing with a Java assertion error ... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... Related Articles Get the path of files consumed by Auto Loader When you process streaming files with Auto Loader (AWS | Azure | GCP), events are... Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... Conflicting directory structures error Problem You have an Apache Spark job that is failing with a Java assertion error ... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Streaming job using Kinesis connector fails",
          "view_href" : "https://kb.databricks.com/en_US/streaming/job-kinesis-connector-fail-oom"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/install-pygraphviz",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Install PyGraphViz Install PyGraphViz Install PyGraphViz with all required dependencies. Written by pavan.kumarchalamcharla Last published at: May 11th, 2022 PyGraphViz Python libraries are used to plot causal inference networks. If you try to install PyGraphViz as a standard library, it fails due to dependency errors. PyGraphViz has the following dependencies: python3-dev graphviz libgraphviz-dev pkg-config Install via notebook Install the dependencies with apt-get. %sh\r\n\r\nsudo apt-get install -y python3-dev graphviz libgraphviz-dev pkg-config After the dependencies are installed, use pip to install PyGraphViz. %sh\r\n\r\npip install pygraphviz Install via init script Create the init script. %python\r\n\r\ndbutils.fs.put(\"dbfs:/databricks/<init-script-folder>/install-pygraphviz.sh\",\r\n\"\"\"\r\n#!/bin/bash\r\n#install dependent packages\r\nsudo apt-get install -y python3-dev graphviz libgraphviz-dev pkg-config\r\npip install pygraphviz\"\"\", True) Install the init script that you just created as a cluster-scoped init script (AWS | Azure | GCP). You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/install-pygraphviz.sh). Restart the cluster after you have installed the init script. Was this article helpful? (7) (14) Additional Informations Related Articles Install package using previous CRAN snapshot Problem You are trying to install a library package via CRAN, and are getting a L... How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Init script fails to download Maven JAR Problem You have an init script that is attempting to install a library via Maven... Related Articles Install package using previous CRAN snapshot Problem You are trying to install a library package via CRAN, and are getting a L... How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Init script fails to download Maven JAR Problem You have an init script that is attempting to install a library via Maven... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Install PyGraphViz",
          "view_href" : "https://kb.databricks.com/en_US/libraries/install-pygraphviz"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/pystan-fails-dbr64es",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Latest PyStan fails to install on Databricks Runtime 6.4 Latest PyStan fails to install on Databricks Runtime 6.4 PyStan 3 doesn't install on Databricks Runtime 6.4 ES. Written by rakesh.parija Last published at: May 11th, 2022 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime 6.4 Extended Support cluster and get a ManagedLibraryInstallFailed error message. java.lang.RuntimeException: ManagedLibraryInstallFailed: org.apache.spark.SparkException: Process List(/databricks/python/bin/pip, install, pystan, --disable-pip-version-check) exited with code 1.   Could not find a version that satisfies the requirement httpstan<4.5,>=4.4 (from pystan) (from versions: 0.1.0, 0.1.1, 0.2.3, 0.2.5, 0.3.0, 0.3.1, 0.4.0, 0.5.0, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.7.5, 0.7.6, 0.8.0, 0.9.0, 0.10.1, 1.0.0)\r\nNo matching distribution found for httpstan<4.5,>=4.4 (from pystan)\r\n for library:PythonPyPiPkgId(pystan,None,None,List()),isSharedLibrary=false Cause When you install PyStan via PyPi, it attempts to install the latest version. PyStan 3.0.0 and above are not compatible with Databricks Runtime 6.4 Extended Support. Solution You should use pystan version 2.19.1.1 on Databricks Runtime 6.4 Extended Support. Specify pystan==2.19.1.1 when you install the library on your cluster (AWS | Azure). This is the most recent version that is compatible with Databricks Runtime 6.4 Extended Support. If you require pystan version 3.0.0 or above, you should upgrade to Databricks Runtime 7.3 LTS or above. Was this article helpful? (8) (13) Additional Informations Related Articles Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... Related Articles Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Latest PyStan fails to install on Databricks Runtime 6.4",
          "view_href" : "https://kb.databricks.com/en_US/libraries/pystan-fails-dbr64es"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/streaming-job-stuck-writing-checkpoint",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming Streaming job gets stuck writing to checkpoint Streaming job gets stuck writing to checkpoint Streaming job appears to be stuck even though no error is thrown. You are using DBFS for checkpoint storage, but it has filled up. Written by Jose Gonzalez Last published at: May 19th, 2022 Problem You are monitoring a streaming job, and notice that it appears to get stuck when processing data. When you review the logs, you discover the job gets stuck when writing data to a checkpoint. INFO HDFSBackedStateStoreProvider: Deleted files older than 381160 for HDFSStateStoreProvider[id = (op=0,part=89),dir = dbfs:/FileStore/R_CHECKPOINT5/state/0/89]:\r\nINFO StateStore: Retrieved reference to StateStoreCoordinator: org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef@56a4cb80\r\nINFO HDFSBackedStateStoreProvider: Deleted files older than 381160 for HDFSStateStoreProvider[id = (op=0,part=37),dir = dbfs:/FileStore/R_CHECKPOINT5/state/0/37]:\r\nINFO StateStore: Retrieved reference to StateStoreCoordinator: org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef@56a4cb80\r\nINFO HDFSBackedStateStoreProvider: Deleted files older than 313920 for HDFSStateStoreProvider[id = (op=0,part=25),dir = dbfs:/FileStore/PYTHON_CHECKPOINT5/state/0/25]: Cause You are trying to use a checkpoint location in your local DBFS path. %scala\r\n\r\nquery = streamingInput.writeStream.option(\"checkpointLocation\", \"/FileStore/checkpoint\").start() Solution You should use persistent storage for streaming checkpoints. You should not use DBFS for streaming checkpoint storage. Was this article helpful? (7) (11) Additional Informations Related Articles How to restart a structured streaming query from last written offset Scenario You have a stream, running a windowed aggregation query, that reads from... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... RocksDB fails to acquire a lock Problem You are trying to use RocksDB as a state store for your structured stream... Kafka error: No resolvable bootstrap urls Problem You are trying to read or write data to a Kafka stream when you get an er... Related Articles How to restart a structured streaming query from last written offset Scenario You have a stream, running a windowed aggregation query, that reads from... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... RocksDB fails to acquire a lock Problem You are trying to use RocksDB as a state store for your structured stream... Kafka error: No resolvable bootstrap urls Problem You are trying to read or write data to a Kafka stream when you get an er... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Streaming job gets stuck writing to checkpoint",
          "view_href" : "https://kb.databricks.com/en_US/streaming/streaming-job-stuck-writing-checkpoint"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/job-fails-throttled-atypical",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Job fails with atypical errors message Job fails with atypical errors message Job run is throttled and fails due to observing atypical errors message. Written by Adam Pavlacka Last published at: May 11th, 2022 Problem Your job run fails with a throttled due to observing atypical errors error message. Cluster became unreachable during run Cause: xxx-xxxxxx-xxxxxxx is throttled due to observing atypical errors Cause The jobs on this cluster have returned too many large results to the Apache Spark driver node. As a result, the chauffeur service runs out of memory, and the cluster becomes unreachable. This can happen after calling the .collect or .show API. Solution You can either reduce the workload on the cluster or increase the value of spark.memory.chauffeur.size. The chauffeur service runs on the same host as the Spark driver. When you allocate more memory to the chauffeur service, less overall memory will be available for the Spark driver. Set the value of spark.memory.chauffeur.size: Open the cluster configuration page in your workspace. Click Edit. Expand Advanced Options. Enter the value of spark.memory.chauffeur.size in mb in the Spark config field. Click Confirm and Restart. Delete Info The default value for spark.memory.chauffeur.size is 1024 megabytes. This is written as spark.memory.chauffeur.size 1024mb in the Spark configuration. The maximum value is the lesser of 16 GB or 20% of the driver node’s total memory. Was this article helpful? (7) (14) Additional Informations Related Articles Create table in overwrite mode fails when interrupted Problem When you attempt to rerun an Apache Spark write operation by cancelling t... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... Apache Spark UI shows wrong number of jobs Problem You are reviewing the number of active Apache Spark jobs on a cluster in ... Job cluster limits on notebook output Problem You are running a notebook on a job cluster and you get an error message ... Related Articles Create table in overwrite mode fails when interrupted Problem When you attempt to rerun an Apache Spark write operation by cancelling t... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... Apache Spark UI shows wrong number of jobs Problem You are reviewing the number of active Apache Spark jobs on a cluster in ... Job cluster limits on notebook output Problem You are running a notebook on a job cluster and you get an error message ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Job fails with atypical errors message",
          "view_href" : "https://kb.databricks.com/en_US/jobs/job-fails-throttled-atypical"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dbfs/s3-connection-reset-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Databricks File System (DBFS) S3 connection reset error S3 connection reset error Apache Spark job fails with S3 connection reset error. Written by arjun.kaimaparambilrajan Last published at: March 15th, 2022 Problem Your Apache Spark job fails when attempting an S3 operation. The error message Caused by: java.net.SocketException: Connection reset appears in the stack trace. Example stack trace from an S3 read operation: Caused by: javax.net.ssl.SSLException: Connection reset; Request ID: XXXXX, Extended Request ID: XXXXX, Cloud Provider: AWS, Instance ID: i-XXXXXXXX\r\nat sun.security.ssl.Alert.createSSLException(Alert.java:127)\r\nat sun.security.ssl.TransportContext.fatal(TransportContext.java:324)\r\n...\r\nat sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:833)\r\nat org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137)\r\n    ...\r\nat org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)\r\nat com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:90)\r\nat com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)\r\nat com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:90)\r\n...\r\nCaused by: java.net.SocketException: Connection reset\r\nat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\nat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\nat sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:467)\r\nat sun.security.ssl.SSLSocketInputRecord.readFully(SSLSocketInputRecord.java:450)\r\nat sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:243) Cause The old version of the Hadoop S3 connector does not retry on SocketTimeoutException or SSLException errors. These exceptions can occur when there is a client side timeout or server side timeout, respectively. Solution This issue has been resolved in a new version of the Hadoop S3 connector. Databricks Runtime 7.3 LTS and above use the new connector. If you are using Databricks Runtime 7.3 LTS or above, ensure that these settings DO NOT exist in the cluster’s Spark configuration: spark.hadoop.fs.s3.impl com.databricks.s3a.S3AFileSystem\r\nspark.hadoop.fs.s3n.impl com.databricks.s3a.S3AFileSystem\r\nspark.hadoop.fs.s3a.impl com.databricks.s3a.S3AFileSystem If you are using Databricks Runtime 7.0 - 7.2, upgrade to Databricks Runtime 7.3 LTS or above. If you are using Databricks Runtime 6.4 or below, contact support for assistance. Was this article helpful? (9) (12) Additional Informations Related Articles Cannot access objects written by Databricks from outside Databricks Problem When you attempt to access an object in an S3 location written by Databri... Cannot read Databricks objects stored in the DBFS root directory Problem An Access Denied error returns when you attempt to read Databricks object... Parallelize filesystem operations When you need to speed up copy and move operations, parallelizing them is usually... How to specify the DBFS path When working with Databricks you will sometimes have to access the Databricks Fil... Related Articles Cannot access objects written by Databricks from outside Databricks Problem When you attempt to access an object in an S3 location written by Databri... Cannot read Databricks objects stored in the DBFS root directory Problem An Access Denied error returns when you attempt to read Databricks object... Parallelize filesystem operations When you need to speed up copy and move operations, parallelizing them is usually... How to specify the DBFS path When working with Databricks you will sometimes have to access the Databricks Fil... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "S3 connection reset error",
          "view_href" : "https://kb.databricks.com/en_US/dbfs/s3-connection-reset-error"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/special-characters-in-xml",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Load special characters with Spark-XML Load special characters with Spark-XML Special characters are not rendering correctly. Use charset with Spark-XML. Written by annapurna.hiriyur Last published at: May 19th, 2022 Problem You have special characters in your source files and are using the OSS library Spark-XML. The special characters do not render correctly. For example, “CLU®” is rendered as “CLU�”. Cause Spark-XML supports the UTF-8 character set by default. You are using a different character set in your XML files. Solution You must specify the character set you are using in your XML files when reading the data. Use the charset option to define the character set when reading an XML file with Spark-XML. For example, if your source file is using ISO-8859-1: %python\r\n\r\ndfResult = spark.read.format('xml').schema(customSchema) \\\r\n.options(rowTag='Entity') \\\r\n.options(charset='ISO-8859-1')\\\r\n.load('/<path-to-xml>/<sample-file>.xml') Review the Spark-XML README file for more information on supported options. Was this article helpful? (9) (10) Additional Informations Related Articles Use the HDFS API to read files in Python There may be times when you want to read files directly without using third party... Install and compile Cython This document explains how to run Spark code with compiled Cython code. The steps... Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... Related Articles Use the HDFS API to read files in Python There may be times when you want to read files directly without using third party... Install and compile Cython This document explains how to run Spark code with compiled Cython code. The steps... Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Load special characters with Spark-XML",
          "view_href" : "https://kb.databricks.com/en_US/python/special-characters-in-xml"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data-sources/redshift-npe",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data sources Accessing Redshift fails with NullPointerException Accessing Redshift fails with NullPointerException Learn how to resolve a `NullPointerException` error that occurs when you read a Redshift table. Written by Adam Pavlacka Last published at: June 1st, 2022 Problem Sometimes when you read a Redshift table: %scala\r\n\r\nval original_df = spark.read.\r\n      format(\"com.databricks.spark.redshift\").\r\n      option(\"url\", url).\r\n      option(\"user\", user).\r\n      option(\"password\", password).\r\n      option(\"query\", query).\r\n      option(\"forward_spark_s3_credentials\", true).\r\n      option(\"tempdir\", \"path\").\r\n      load() The Spark job will throw a NullPointerException: Caused by: java.lang.NullPointerException\r\n  at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:194) Cause The problem comes from the way Spark reads data from Redshift. The Amazon Redshift data source uses Redshift’s unload format to read data from Redshift: Spark first issues an unload command to Redshift to make it dump the contents of the table in the unload format to temporary files, and then Spark scans those temporary files. This text-based unload format does not differentiate between an empty string and a null string by default – both are encoded as an empty string in the resulting file. When spark-redshift reads the data in the unload format, there’s not enough information for it to tell whether the input was an empty string or a null, and currently it simply deems it’s a null. Solution In Scala, set the nullable to true for all the String columns: %scala\r\n\r\nimport org.apache.spark.sql.types.{StructField, StructType, StringType}\r\nimport org.apache.spark.sql.{DataFrame, SQLContext}\r\n\r\n\r\ndef setNullableStateForAllStringColumns(df: DataFrame, nullable: Boolean) = {\r\n  StructType(df.schema.map {\r\n    case StructField( c, StringType, _, m) => StructField( c, StringType, nullable = nullable, m)\r\n    case StructField( c, t, n, m) => StructField( c, t, n, m)\r\n  })\r\n} In Python: %python\r\n\r\ndef set_nullable_for_all_string_columns(df, nullable):\r\n    from pyspark.sql.types import StructType, StructField, StringType\r\n    new_schema = StructType([StructField(f.name, f.dataType, nullable, f.metadata)\r\n                             if (isinstance(f.dataType, StringType))\r\n                             else StructField(f.name, f.dataType, f.nullable, f.metadata)\r\n                             for f in df.schema.fields])\r\n    return new_schema To use this function, get the schema of original_df, then modify the schema to make all String columns to nullable, then re-read from Redshift: %scala\r\n\r\nval df = spark.read.schema(setNullableStateForAllStringColumns(original_df, true)).\r\n      format(\"com.databricks.spark.redshift\").\r\n      option(\"url\", url).\r\n      option(\"user\", user).\r\n      option(\"password\", password).\r\n      option(\"query\", query).\r\n      option(\"forward_spark_s3_credentials\", true).\r\n      option(\"tempdir\", \"path\").\r\n      load() Was this article helpful? (9) (14) Additional Informations Related Articles CosmosDB-Spark connector library conflict This article explains how to resolve an issue running applications that use the C... Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2 Problem Info In general, you should use Databricks Runtime 5.2 and above, which i... Apache Spark JDBC datasource query option doesn’t work for Oracle database Problem When you use the query option with the Apache Spark JDBC datasource to co... Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... Related Articles CosmosDB-Spark connector library conflict This article explains how to resolve an issue running applications that use the C... Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2 Problem Info In general, you should use Databricks Runtime 5.2 and above, which i... Apache Spark JDBC datasource query option doesn’t work for Oracle database Problem When you use the query option with the Apache Spark JDBC datasource to co... Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Accessing Redshift fails with NullPointerException",
          "view_href" : "https://kb.databricks.com/en_US/data-sources/redshift-npe"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dbfs/parallelize-fs-operations",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Databricks File System (DBFS) Parallelize filesystem operations Parallelize filesystem operations Parallelize Apache Spark filesystem operations with DBUtils and Hadoop FileUtil; emulate DistCp. Written by sandeep.chandran Last published at: March 10th, 2022 When you need to speed up copy and move operations, parallelizing them is usually a good option. You can use Apache Spark to parallelize operations on executors. On Databricks you can use DBUtils APIs, however these API calls are meant for use on driver nodes, and shouldn’t be used on Spark jobs running on executors. In this article, we are going to show you how to use the Apache Hadoop FileUtil function along with DBUtils to parallelize a Spark copy operation. You can use this example as a basis for other filesystem operations. Delete Note The example copy operation may look familiar as we are using DBUtils and Hadoop FileUtil to emulate the functions of the Hadoop DistCp tool. Import required libraries Import the Hadoop functions and define your source and destination locations. %scala\r\n\r\nimport org.apache.hadoop.fs._\r\n\r\nval source = \"<source dir>\"\r\nval dest = \"<destination dir>\"\r\n\r\ndbutils.fs.mkdirs(dest) Broadcast information from the driver to executors %scala\r\n\r\nval conf = new org.apache.spark.util.SerializableConfiguration(sc.hadoopConfiguration)\r\nval broadcastConf = sc.broadcast(conf)\r\nval broadcastDest = sc.broadcast(dest) Copy paths to a sequence %scala\r\n\r\nval filesToCopy = dbutils.fs.ls(source).map(_.path) Parallelize the sequence and divide the workload Here we first get the Hadoop configuration and destination path. Then we create the path objects, before finally executing the FileUtil.copy command. %scala\r\n\r\nspark.sparkContext.parallelize(filesToCopy).foreachPartition { rows =>\r\n  rows.foreach { file =>\r\n\r\n\r\n    val conf = broadcastConf.value.value\r\n    val destPathBroadcasted = broadcastDest.value\r\n\r\n\r\n    val fromPath = new Path(file)\r\n    val toPath = new Path(destPathBroadcasted)\r\n    val fromFs = fromPath.getFileSystem(conf)\r\n    val toFs = toPath.getFileSystem(conf)\r\n\r\n\r\n    FileUtil.copy(fromFs, fromPath, toFs, toPath, false, conf)\r\n  }\r\n} Was this article helpful? (12) (13) Additional Informations Related Articles Cannot read Databricks objects stored in the DBFS root directory Problem An Access Denied error returns when you attempt to read Databricks object... How to calculate the Databricks file system (DBFS) S3 API call cost The cost of a DBFS S3 bucket is primarily driven by the number of API calls, and ... Cannot access objects written by Databricks from outside Databricks Problem When you attempt to access an object in an S3 location written by Databri... How to specify the DBFS path When working with Databricks you will sometimes have to access the Databricks Fil... Related Articles Cannot read Databricks objects stored in the DBFS root directory Problem An Access Denied error returns when you attempt to read Databricks object... How to calculate the Databricks file system (DBFS) S3 API call cost The cost of a DBFS S3 bucket is primarily driven by the number of API calls, and ... Cannot access objects written by Databricks from outside Databricks Problem When you attempt to access an object in an S3 location written by Databri... How to specify the DBFS path When working with Databricks you will sometimes have to access the Databricks Fil... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Parallelize filesystem operations",
          "view_href" : "https://kb.databricks.com/en_US/dbfs/parallelize-fs-operations"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/id-duplicate-on-append",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Identify duplicate data on append operations Identify duplicate data on append operations Written by chetan.kardekar Last published at: May 10th, 2022 A common issue when performing append operations on Delta tables is duplicate data. For example, assume user 1 performs a write operation on Delta table A. At the same time, user 2 performs an append operation on Delta table A. This can lead to duplicate records in the table. In this article, we review basic troubleshooting steps that you can use to identify duplicate records, as well as the user name, and notebooks or jobs that resulted in the duplicate data. Identify columns with duplicate records %sql\r\n\r\nselect count(*) as count, <column-name> from <table-name> group by <column-name> order by <column-name> The output identifies all columns with duplicate data. Identify input files with duplicate data Select a data point from the previous query and use it to determine which files provided duplicate data. %sql\r\n\r\nselect *, input_file_name() as path from <table-name> where <column-name>=<any-duplicated-value> The output includes a column called path, which identifies the full path to each input file. Identify the location table %sql\r\n\r\ndescribe table extended <table-name> Use the location table results to search for parquet paths %sh\r\n\r\ngrep -r 'part-<filename-01>.snappy.parquet' /dbfs/user/hive/warehouse/<path-to-log>/_delta_log %sh\r\n\r\ngrep -r 'part-<filename-02.snappy.parquet' /dbfs/user/hive/warehouse/<path-to-log>/_delta_log The results allow you to identify the impacted Delta versions. Check the Delta history for the impacted versions %sql\r\n\r\nselect * from (describe history <table-name> ) t where t.version In(0,1) The Delta history results provide the user name, as well as the notebook or job id that caused the duplicate to appear in the Delta table. Now that you have identified the source of the duplicate data, you can modify the notebook or job to prevent it from happening. Example notebook Review the Identify duplicate data on append example notebook. Was this article helpful? (8) (14) Additional Informations Related Articles How Delta cache behaves on an autoscaling cluster This article is about how Delta cache (AWS | Azure | GCP) behaves on an auto-scal... How to improve performance of Delta Lake MERGE INTO queries using partition pruning This article explains how to trigger partition pruning in Delta Lake MERGE INTO (... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... Delete your streaming query checkpoint and restart Problem Your job fails with a Delta table <value> doesn't exist. Please del... Related Articles How Delta cache behaves on an autoscaling cluster This article is about how Delta cache (AWS | Azure | GCP) behaves on an auto-scal... How to improve performance of Delta Lake MERGE INTO queries using partition pruning This article explains how to trigger partition pruning in Delta Lake MERGE INTO (... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... Delete your streaming query checkpoint and restart Problem Your job fails with a Delta table <value> doesn't exist. Please del... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Identify duplicate data on append operations",
          "view_href" : "https://kb.databricks.com/en_US/delta/id-duplicate-on-append"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/cannot-import-egg-module",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Cannot import module in egg library Cannot import module in egg library The module in the egg library cannot be imported. Easy install, Python. Written by xin.wang Last published at: May 11th, 2022 Problem You try to install an egg library to your cluster and it fails with a message that the a module in the library cannot be imported. Even a simple import fails. import sys\r\negg_path='/dbfs/<path-to-egg-file>/<egg-file>.egg'\r\nsys.path.append(egg_path)\r\nimport shap_master Cause This error message occurs due to the way the library is packed. Solution If the standard library import options do not work, you should use easy_install to import the library. %python\r\n\r\ndbutils.fs.put(\"/<path>/<library-name>.sh\",\"\"\"\r\n#!/bin/bash\r\neasy_install-3.7 /dbfs/<path-to-egg-file>/<egg-file>.egg\"\"\")\r\n\"\"\") Delete Warning The version of easy_install must match the version of Python on the cluster. You can determine the version of Python on your cluster by reviewing the release notes (AWS | Azure). Was this article helpful? (9) (38) Additional Informations Related Articles Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Library unavailability causing job failures Problem You are launching jobs that import external libraries and get an Import E... How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Init script fails to download Maven JAR Problem You have an init script that is attempting to install a library via Maven... Related Articles Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Library unavailability causing job failures Problem You are launching jobs that import external libraries and get an Import E... How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Init script fails to download Maven JAR Problem You have an init script that is attempting to install a library via Maven... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cannot import module in egg library",
          "view_href" : "https://kb.databricks.com/en_US/libraries/cannot-import-egg-module"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/item-too-large-to-export",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks Item was too large to export Item was too large to export Export notebooks and folders larger than 10 MB in size. Written by pavan.kumarchalamcharla Last published at: May 16th, 2022 Problem You are trying to export notebooks using the workspace UI and are getting an error message. This item was too large to export. Try exporting smaller or fewer items. Cause The notebook files are larger than 10 MB in size. Solution The simplest solution is to limit the size of the notebook or folder that you are trying to download to 10 MB or less. If it is smaller than 10 MB in size, you can download it via the workspace UI. If the notebook or folder is larger than 10 MB in size, you should use the Databricks CLI (AWS | Azure | GCP) to export the contents. Example code This example code exports all notebooks and folders in a workspace to a folder on your local machine. Make sure you create a workspace profile in the CLI before attempting to run this sample code. Copy the example code to your local machine as a Python file (ex. export-notebook.py). import sys\r\nimport os\r\nimport subprocess\r\nfrom subprocess import call, check_output\r\n\r\nEXPORT_PROFILE = \"primary\"\r\n\r\n# Get a list of all users.\r\nuser_list_out = check_output([\"databricks\", \"workspace\", \"ls\", \"/Users\", \"--profile\", EXPORT_PROFILE])\r\nuser_list = (user_list_out.decode(encoding=\"utf-8\")).splitlines()\r\n\r\nprint (user_list)\r\n\r\n# Export folders and notebooks for each user.\r\n# Note: This does not include libraries.\r\n\r\nfor user in user_list:\r\n  print ((\"Trying to migrate workspace for user \") + user)\r\n\r\n  subprocess.call(str(\"mkdir -p \") + str(user), shell = True)\r\n  export_exit_status = call(\"databricks workspace export_dir /Users/\" + str(user) + \" ./\" + str(user) + \" --profile \" + EXPORT_PROFILE, shell = True)\r\n\r\n  if export_exit_status==0:\r\n    print (\"Export Success\")\r\n  else:\r\n    print (\"Export Failure\")\r\nprint (\"All done\") Run the sample code under Python 3. python3 export-notebook.py The notebooks and folders from your workspace are downloaded to the local folder where you ran the example code. Was this article helpful? (7) (21) Additional Informations Related Articles Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... Enable s3cmd for notebooks s3cmd is a client library that allows you to perform all AWS S3 operations from a... Error: Received command c on object id p0 Problem You have imported Python libraries, but when you try to execute Python co... Related Articles Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... Enable s3cmd for notebooks s3cmd is a client library that allows you to perform all AWS S3 operations from a... Error: Received command c on object id p0 Problem You have imported Python libraries, but when you try to execute Python co... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Item was too large to export",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/item-too-large-to-export"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/cloud/har-log-analysis",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Cloud infrastructure How to analyze user interface performance issues How to analyze user interface performance issues Learn how to troubleshoot Databricks user interface performance issues. Written by Adam Pavlacka Last published at: February 25th, 2022 Problem The Databricks user interface seems to be running slowly. Cause User interface performance issues typically occur due to network latency or a database query taking more time than expected. In order to troubleshoot this type of problem, you need to collect network logs and analyze them to see which network traffic is affected. In most cases, you will need the assistance of Databricks Support to identify and resolve issues with Databricks user interface performance, but you can also analyze the logs yourself with a tool such as G Suite Toolbox HAR Analyzer. This tool helps you analyze the logs and identify the exact API and the time taken for each request. Solution This is the procedure for Google Chrome. For other browsers, see G Suite Toolbox HAR Analyzer. Open Google Chrome and go to the page where the issue occurs. In the Chrome menu bar, select View > Developer > Developer Tools. In the panel at the bottom of your screen, select the Network tab. Look for a round Record button in the upper left corner of the Network tab, and make sure it is red. If it is grey, click it once to start recording. Check the box next to Preserve log. Click Clear to clear out any existing logs from the Network tab. Reproduce the issue while the network requests are being recorded. After you reproduce and record the issue, right-click anywhere on the grid of network requests to open a context menu, select Save all as HAR with Content, and save the file to your computer. Analyze the file using the HAR Analyzer tool. If this analysis does not resolve the problem, open a support ticket and upload the HAR file or attach it to your email so that Databricks can analyze it. Example output from HAR Analyzer Was this article helpful? (13) (48) Additional Informations Related Articles Configure custom DNS settings using dnsmasq dnsmasq is a tool for installing and configuring DNS routing rules for cluster no... Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... AWS services fail with No region provided error Problem Your code snippets that use AWS services fail with a java.lang.IllegalArg... Access denied when writing logs to an S3 bucket Problem When you try to write log files to an S3 bucket, you get the error: com.a... Related Articles Configure custom DNS settings using dnsmasq dnsmasq is a tool for installing and configuring DNS routing rules for cluster no... Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... AWS services fail with No region provided error Problem Your code snippets that use AWS services fail with a java.lang.IllegalArg... Access denied when writing logs to an S3 bucket Problem When you try to write log files to an S3 bucket, you get the error: com.a... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to analyze user interface performance issues",
          "view_href" : "https://kb.databricks.com/en_US/cloud/har-log-analysis"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/metastore/create-table-ddl-for-metastore",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Metastore How to create table DDLs to import into an external metastore How to create table DDLs to import into an external metastore Learn how to export all table metadata from Hive to an external metastore from Databricks. Written by Adam Pavlacka Last published at: May 16th, 2022 Databricks supports using external metastores instead of the default Hive metastore. You can export all table metadata from Hive to the external metastore. Use the Apache Spark Catalog API to list the tables in the databases contained in the metastore. Use the SHOW CREATE TABLE statement to generate the DDLs and store them in a file. Use the file to import the table DDLs into the external metastore. The following code accomplishes the first two steps. %python\r\n\r\ndbs = spark.catalog.listDatabases()\r\nfor db in dbs:\r\n  f = open(\"your_file_name_{}.ddl\".format(db.name), \"w\")\r\n  tables = spark.catalog.listTables(db.name)\r\n  for t in tables:\r\n    DDL = spark.sql(\"SHOW CREATE TABLE {}.{}\".format(db.name, t.name))\r\n    f.write(DDL.first()[0])\r\n    f.write(\"\\n\")\r\nf.close() You can use the resulting file to import the table DDLs into the external metastore. Was this article helpful? (13) (9) Additional Informations Related Articles Data too long for column error Problem You are trying to insert a struct into a table, but you get a java.sql.SQ... Autoscaling is slow with an external metastore Problem You have an external metastore configured on your cluster and autoscaling... Drop database without deletion By default, the DROP DATABASE (AWS | Azure | GCP) command drops the database and ... Related Articles Data too long for column error Problem You are trying to insert a struct into a table, but you get a java.sql.SQ... Autoscaling is slow with an external metastore Problem You have an external metastore configured on your cluster and autoscaling... Drop database without deletion By default, the DROP DATABASE (AWS | Azure | GCP) command drops the database and ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to create table DDLs to import into an external metastore",
          "view_href" : "https://kb.databricks.com/en_US/metastore/create-table-ddl-for-metastore"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/spark-ui-not-in-sync-with-job",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Apache Spark UI is not in sync with job Apache Spark UI is not in sync with job Status of Spark jobs gets out of sync with the Spark UI when events drop from the event queue before being processed. Written by chetan.kardekar Last published at: July 8th, 2022 Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS | Azure | GCP). Some of the jobs that are confirmed to be in the Completed state are shown as Active/Running in the Spark UI. In some cases the Spark UI may appear blank. When you review the driver logs, you see an AsyncEventQueue warning. Logs\r\n=====\r\n20/12/23 21:20:26 WARN AsyncEventQueue: Dropped 93909 events from shared since Wed Dec 23 21:19:26 UTC 2020. \r\n20/12/23 21:21:26 WARN AsyncEventQueue: Dropped 52354 events from shared since Wed Dec 23 21:20:26 UTC 2020. \r\n20/12/23 21:22:26 WARN AsyncEventQueue: Dropped 94137 events from shared since Wed Dec 23 21:21:26 UTC 2020. \r\n20/12/23 21:23:26 WARN AsyncEventQueue: Dropped 44245 events from shared since Wed Dec 23 21:22:26 UTC 2020. \r\n20/12/23 21:24:26 WARN AsyncEventQueue: Dropped 126763 events from shared since Wed Dec 23 21:23:26 UTC 2020.\r\n20/12/23 21:25:26 WARN AsyncEventQueue: Dropped 94156 events from shared since Wed Dec 23 21:24:26 UTC 2020. Info This is related to the Apache Spark UI shows wrong number of jobs KB article. Cause All Spark jobs, stages, and tasks are pushed to the event queue. The backend listener reads the Spark UI events from this queue and renders the Spark UI. The default capacity of the event queue (spark.scheduler.listenerbus.eventqueue.capacity) is 20000. If more events are pushed to the event queue than the backend listener can consume, the oldest events get dropped from the queue and the listener never consumes them. These events are lost and do not get rendered in the Spark UI. Solution Set the value of spark.scheduler.listenerbus.eventqueue.capacity in your cluster’s Spark config (AWS | Azure | GCP) at cluster level to a value greater than 20000. This value sets the capacity for the app status event queue, which holds events for internal application status listeners. Increasing this value allows the event queue to hold a larger number of events, but may result in the driver using more memory. Was this article helpful? (11) (10) Additional Informations Related Articles Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Best practice for cache(), count(), and take() cache() is an Apache Spark transformation that can be used on a DataFrame, Datase... Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... Cannot modify the value of an Apache Spark config Problem You are trying to SET the value of a Spark config in a notebook and get a... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Related Articles Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Best practice for cache(), count(), and take() cache() is an Apache Spark transformation that can be used on a DataFrame, Datase... Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... Cannot modify the value of an Apache Spark config Problem You are trying to SET the value of a Spark config in a notebook and get a... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Apache Spark UI is not in sync with job",
          "view_href" : "https://kb.databricks.com/en_US/scala/spark-ui-not-in-sync-with-job"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/join-two-dataframes-duplicated-columns",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management Prevent duplicated columns when joining two DataFrames Prevent duplicated columns when joining two DataFrames Learn how to prevent duplicated columns when joining two DataFrames in \\Databricks. Written by Adam Pavlacka Last published at: May 31st, 2022 If you perform a join in Spark and don’t specify your join correctly you’ll end up with duplicate column names. This makes it harder to select those columns. This article and notebook demonstrate how to perform a join so that you don’t have duplicated columns. Join on columns If you join on columns, you get duplicated columns. Scala %scala\r\n\r\nval llist = Seq((\"bob\", \"2015-01-13\", 4), (\"alice\", \"2015-04-23\",10))\r\nval left = llist.toDF(\"name\",\"date\",\"duration\")\r\nval right = Seq((\"alice\", 100),(\"bob\", 23)).toDF(\"name\",\"upload\")\r\n\r\nval df = left.join(right, left.col(\"name\") === right.col(\"name\")) Python %python\r\n\r\nllist = [('bob', '2015-01-13', 4), ('alice', '2015-04-23',10)]\r\nleft = spark.createDataFrame(llist, ['name','date','duration'])\r\nright = spark.createDataFrame([('alice', 100),('bob', 23)],['name','upload'])\r\n\r\ndf = left.join(right, left.name == right.name) Solution Specify the join column as an array type or string. Scala %scala\r\n\r\nval df = left.join(right, Seq(\"name\")) %scala\r\n\r\nval df = left.join(right, \"name\") Python %python\r\n\r\ndf = left.join(right, [\"name\"]) %python\r\n\r\ndf = left.join(right, \"name\") R First register the DataFrames as tables. %python\r\n\r\nleft.createOrReplaceTempView(\"left_test_table\")\r\nright.createOrReplaceTempView(\"right_test_table\") %r\r\n\r\nlibrary(SparkR)\r\nsparkR.session()\r\nleft <- sql(\"SELECT * FROM left_test_table\")\r\nright <- sql(\"SELECT * FROM right_test_table\") The above code results in duplicate columns. The following code does not. %r\r\n\r\nhead(drop(join(left, right, left$name == right$name), left$name)) Join DataFrames with duplicated columns notebook Review the Join DataFrames with duplicated columns example notebook. Was this article helpful? (13) (50) Additional Informations Related Articles How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... Get and set Apache Spark configuration properties in a notebook In most cases, you set the Spark config (AWS | Azure ) at the cluster level. Howe... How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... Related Articles How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... Get and set Apache Spark configuration properties in a notebook In most cases, you set the Spark config (AWS | Azure ) at the cluster level. Howe... How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Prevent duplicated columns when joining two DataFrames",
          "view_href" : "https://kb.databricks.com/en_US/data/join-two-dataframes-duplicated-columns"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/r/sparkr-lapply",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles R with Apache Spark How to parallelize R code with spark.lapply How to parallelize R code with spark.lapply Learn how to parallelize R code using spark.lapply. Written by Adam Pavlacka Last published at: May 20th, 2022 Parallelization of R code is difficult, because R code runs on the driver and R data.frames are not distributed. Often, there is existing R code that is run locally and that is converted to run on Apache Spark. In other cases, some SparkR functions used for advanced statistical analysis and machine learning techniques may not support distributed computing. In such cases, the SparkR UDF API can be used to distribute the desired workload across a cluster. Example use case: You want to train multiple machine learning models on the same data, for example for hyper parameter tuning. If the data set fits on each worker, it may be more efficient to use the SparkR UDF API to train several versions of the model at once. The spark.lapply function enables you to perform the same task on multiple workers, by running a function over a list of elements. For each element in a list: Send the function to a worker. Execute the function. Return the result of all workers as a list to the driver. In the following example, a support vector machine model is fit on the iris dataset with 3-fold cross validation while the cost is varied from 0.5 to 1 by increments of 0.1. The output is a list with the summary of the models for the various cost parameters. %r\r\n\r\nlibrary(SparkR)\r\n\r\nspark.lapply(seq(0.5, 1, by = 0.1), function(x) {\r\n  library(e1071)\r\n  model <- svm(Species ~ ., iris, cost = x, cross = 3)\r\n  summary(model)\r\n}) Delete Info You must install packages on all workers. Was this article helpful? (9) (10) Additional Informations Related Articles Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... Fix the version of R packages When you use the install.packages() function to install CRAN packages, you cannot... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... Related Articles Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... Fix the version of R packages When you use the install.packages() function to install CRAN packages, you cannot... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to parallelize R code with spark.lapply",
          "view_href" : "https://kb.databricks.com/en_US/r/sparkr-lapply"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/from-json-null-spark3",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark from_json returns null in Apache Spark 3.0 from_json returns null in Apache Spark 3.0 Spark 3.0 and above cannot parse JSON arrays as structs; from_json returns null. Written by shanmugavel.chandrakasu Last published at: May 23rd, 2022 Problem The from_json function is used to parse a JSON string and return a struct of values. For example, if you have the JSON string [{\"id\":\"001\",\"name\":\"peter\"}], you can pass it to from_json with a schema and get parsed struct values in return. %python\r\n\r\nfrom pyspark.sql.functions import col, from_json\r\ndisplay(\r\n  df.select(col('value'), from_json(col('value'), json_df_schema, {\"mode\" : \"PERMISSIVE\"}))\r\n) In this example, the dataframe contains a column “value”, with the contents [{“id”:”001”,”name”:”peter”}] and the schema is StructType(List(StructField(id,StringType,true),StructField(name,StringType,true))). This works correctly on Spark 2.4 and below (Databricks Runtime 6.4 ES and below). * id:\r\n  \"001\"\r\n* name:\r\n  \"peter\" This returns null values on Spark 3.0 and above (Databricks Runtime 7.3 LTS and above). * id:\r\n  null\r\n* name:\r\n  null Cause This occurs because Spark 3.0 and above cannot parse JSON arrays as structs. You can confirm this by running from_json in FAILFAST mode. %python\r\n\r\nfrom pyspark.sql.functions import col, from_json\r\ndisplay(\r\n  df.select(col('value'), from_json(col('value'), json_df_schema, {\"mode\" : \"FAILFAST\"}))\r\n) This returns an error message that defines the root cause. Caused by: RuntimeException: Parsing JSON arrays as structs is forbidden Solution You must pass the schema as ArrayType instead of StructType in Databricks Runtime 7.3 LTS and above. %python\r\n\r\nfrom pyspark.sql.types import StringType, ArrayType, StructType, StructField\r\nschema_spark_3 = ArrayType(StructType([StructField(\"id\",StringType(),True),StructField(\"name\",StringType(),True)]))\r\n\r\n\r\nfrom pyspark.sql.functions import col, from_json\r\ndisplay(\r\n  df.select(col('value'), from_json(col('value'), schema_spark_3, {\"mode\" : \"PERMISSIVE\"}))\r\n) In this example code, the previous StructType schema is enclosed in ArrayType and the new schema is used with from_json. This parses the JSON string correctly and returns the expected values. Was this article helpful? (10) (11) Additional Informations Related Articles Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Cannot modify the value of an Apache Spark config Problem You are trying to SET the value of a Spark config in a notebook and get a... Related Articles Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Cannot modify the value of an Apache Spark config Problem You are trying to SET the value of a Spark config in a notebook and get a... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "from_json returns null in Apache Spark 3.0",
          "view_href" : "https://kb.databricks.com/en_US/scala/from-json-null-spark3"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/cloud/unable-load-aws-credentials",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Cloud infrastructure Unable to load AWS credentials Unable to load AWS credentials Written by Adam Pavlacka Last published at: February 25th, 2022 Problem When you try to access AWS resources like S3, SQS or Redshift, the operation fails with the error: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [BasicAWSCredentialsProvider: Access key or secret key is null, com.amazonaws.auth.InstanceProfileCredentialsProvider@a590007a: The requested metadata is not found at https://<ip-address>/latest/meta-data/iam/security-credentials/] Cause Scenario 1: To access AWS resources such as S3, SQS, or Redshift, the access permissions have to be provided either through an IAM role or through AWS credentials. If these credentials are not provided, then the above error can occur. Scenario 2: The IAM role is provided while launching the cluster, but due to some misconfiguration, the role is not attached correctly. To debug this, run the following Bash command from a notebook that is attached to the cluster: curl https://<ip-address>/latest/meta-data/iam/security-credentials/<role_name>You should get a result like this: \"Code\" : \"AssumeRoleUnauthorizedAccess\",\"Message\" : \"EC2 cannot assume the role <role_name>. Please see documentation at https://docs.amazonwebservices.com/IAM/latest/UserGuide/RolesTroubleshooting.html.\",\r\n\"LastUpdated\" : \"2019-05-03T15:36:26Z\" Solution Attach the correct IAM role to the cluster. The trust relationship of the IAM role should have the following policy: {\r\n  \"Version\": \"2012-10-17\",\r\n  \"Statement\": [\r\n    {\r\n      \"Effect\": \"Allow\",\r\n      \"Principal\": {\r\n        \"Service\": \"ec2.amazonaws.com\"\r\n      },\r\n      \"Action\": \"sts:AssumeRole\"\r\n    }\r\n  ]\r\n} Was this article helpful? (11) (13) Additional Informations Related Articles AWS services fail with No region provided error Problem Your code snippets that use AWS services fail with a java.lang.IllegalArg... Vulnerability scan shows vulnerabilities in Databricks EC2 instances Problem The Corporate Information Security (CIS) Vulnerability Management team id... Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... Configure custom DNS settings using dnsmasq dnsmasq is a tool for installing and configuring DNS routing rules for cluster no... Related Articles AWS services fail with No region provided error Problem Your code snippets that use AWS services fail with a java.lang.IllegalArg... Vulnerability scan shows vulnerabilities in Databricks EC2 instances Problem The Corporate Information Security (CIS) Vulnerability Management team id... Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... Configure custom DNS settings using dnsmasq dnsmasq is a tool for installing and configuring DNS routing rules for cluster no... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Unable to load AWS credentials",
          "view_href" : "https://kb.databricks.com/en_US/cloud/unable-load-aws-credentials"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/edited-policy-not-applied",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Cannot apply updated cluster policy Cannot apply updated cluster policy When performing an update to an existing cluster policy, the update does not apply unless you remove and re-add the policy. Written by jordan.hicks Last published at: March 4th, 2022 Problem You are attempting to update an existing cluster policy, however the update does not apply to the cluster associated with the policy. If you attempt to edit a cluster that is managed by a policy, the changes are not applied or saved. Cause This is a known issue that is being addressed. Solution You can use a workaround until a permanent fix is available. Edit the cluster policy. Re-attribute the policy to Free form. Add the edited policy back to the cluster. If you want to edit a cluster that is associated with a policy: Terminate the cluster. Associate a different policy to the cluster. Edit the cluster. Re-associate the original policy to the cluster. Was this article helpful? (9) (16) Additional Informations Related Articles How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Related Articles How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cannot apply updated cluster policy",
          "view_href" : "https://kb.databricks.com/en_US/clusters/edited-policy-not-applied"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/cannot-grow-bufferholder-exceeds-size",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark Cannot grow BufferHolder; exceeds size limitation Cannot grow BufferHolder; exceeds size limitation Cannot grow BufferHolder by size because the size after growing exceeds limitation; java.lang.IllegalArgumentException error. Written by Adam Pavlacka Last published at: May 23rd, 2022 Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow BufferHolder error. java.lang.IllegalArgumentException: Cannot grow BufferHolder by size XXXXXXXXX because the size after growing exceeds size limitation 2147483632 Cause BufferHolder has a maximum size of 2147483632 bytes (approximately 2 GB). If a column value exceeds this size, Spark returns the exception. This can happen when using aggregates like collect_list. This example code generates duplicates in the column values which exceed the maximum size of BufferHolder. As a result, it returns an IllegalArgumentException: Cannot grow BufferHolder error when run in a notebook. %sql\r\n\r\nimport org.apache.spark.sql.functions._\r\nspark.range(10000000).withColumn(\"id1\",lit(\"jkdhdbjasdshdjkqgdkdkasldksashjckabacbaskcbakshckjasbc$%^^&&&&&*jxcfdkwbfkjwdqndlkjqslkndskbndkjqbdjkbqwjkdbxnsa xckqjwbdxsabvnxbaskxqbhwdhqjskdjxbqsjdhqkjsdbkqsjdkjqdhkjqsabcxns ckqjdkqsbcxnsab ckjqwbdjckqscx ns csjhdjkqsdhjkqshdjsdhqksjdhxqkjshjkshdjkqsdhkjqsdhjqskxb kqscbxkjqsc\")).groupBy(\"id1\").\r\nagg(collect_list(\"id1\").alias(\"days\")).\r\nshow() Solution You must ensure that column values do not exceed 2147483632 bytes. This may require you to adjust how you process data in your notebook. Looking at our example code, using collect_set instead of collect_list, resolves the issue and allows the example to run to completion. This single change works because the example data set contains a large number of duplicate entries. %sql\r\n\r\nimport org.apache.spark.sql.functions._\r\nspark.range(10000000).withColumn(\"id1\",lit(\"jkdhdbjasdshdjkqgdkdkasldksashjckabacbaskcbakshckjasbc$%^^&&&&&*jxcfdkwbfkjwdqndlkjqslkndskbndkjqbdjkbqwjkdbxnsa xckqjwbdxsabvnxbaskxqbhwdhqjskdjxbqsjdhqkjsdbkqsjdkjqdhkjqsabcxns ckqjdkqsbcxnsab ckjqwbdjckqscx ns csjhdjkqsdhjkqshdjsdhqksjdhxqkjshjkshdjkqsdhkjqsdhjqskxb kqscbxkjqsc\")).groupBy(\"id1\").\r\nagg(collect_set(\"id1\").alias(\"days\")).\r\nshow() If using collect_set does not keep the size of the column below the BufferHolder limit of 2147483632 bytes, the IllegalArgumentException: Cannot grow BufferHolder error still occurs. In this case, we would have to split the list into multiple DataFrames and write it out as separate files. Was this article helpful? (12) (37) Additional Informations Related Articles Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Related Articles Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cannot grow BufferHolder; exceeds size limitation",
          "view_href" : "https://kb.databricks.com/en_US/sql/cannot-grow-bufferholder-exceeds-size"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/jobs-idempotency",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs How to ensure idempotency for jobs How to ensure idempotency for jobs Learn how to ensure that jobs submitted through the Databricks REST API aren't duplicated if there is a retry after a request times out. Written by Adam Pavlacka Last published at: May 11th, 2022 When you submit jobs through the Databricks Jobs REST API, idempotency is not guaranteed. If the client request is timed out and the client resubmits the same request, you may end up with duplicate jobs running. To ensure job idempotency when you submit jobs through the Jobs API, you can use an idempotency token to define a unique value for a specific job run. If the same job has to be retried because the client did not receive a response due to a network error, the client can retry the job using the same idempotency token, ensuring that a duplicate job run is not triggered. Here is an example of a REST API JSON payload for the Runs Submit API using an idempotency_token with a value of 123: {\r\n  \"run_name\":\"my spark task\",\r\n  \"new_cluster\": {\r\n    \"spark_version\":\"5.5.x-scala2.11\",\r\n    \"node_type_id\":\"r5.xlarge\",\r\n    \"aws_attributes\": {\r\n      \"availability\":\"ON_DEMAND\"\r\n    },\r\n    \"num_workers\":10\r\n  },\r\n  \"libraries\": [\r\n    {\r\n      \"jar\":\"dbfs:/my-jar.jar\"\r\n    },\r\n    {\r\n      \"maven\": {\r\n        \"coordinates\":\"org.jsoup:jsoup:1.7.2\"\r\n      }\r\n    }\r\n  ],\r\n  \"spark_jar_task\": {\r\n    \"main_class_name\":\"com.databricks.ComputeModels\"\r\n  },\r\n  \"idempotency_token\":\"123\"\r\n} All requests with the same idempotency token should return 200 with the same run ID. Was this article helpful? (8) (14) Additional Informations Related Articles Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... Apache Spark Jobs hang due to non-deterministic custom UDF Problem Sometimes Apache Spark jobs hang indefinitely due to the non-deterministi... Databricks job fails because library is not installed Problem A Databricks job fails because the job requires a library that is not yet... Job fails with invalid access token Problem Long running jobs, such as streaming jobs, fail after 48 hours when using... Related Articles Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... Apache Spark Jobs hang due to non-deterministic custom UDF Problem Sometimes Apache Spark jobs hang indefinitely due to the non-deterministi... Databricks job fails because library is not installed Problem A Databricks job fails because the job requires a library that is not yet... Job fails with invalid access token Problem Long running jobs, such as streaming jobs, fail after 48 hours when using... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to ensure idempotency for jobs",
          "view_href" : "https://kb.databricks.com/en_US/jobs/jobs-idempotency"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/r/install-rjava-rjdbc-libraries",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles R with Apache Spark Install rJava and RJDBC libraries Install rJava and RJDBC libraries Learn how to install rJava and RJDBC libraries on your Databricks cluster. Written by Adam Pavlacka Last published at: May 20th, 2022 This article explains how to install rJava and RJBDC libraries. Problem When you install rJava and RJDBC libraries with the following command in a notebook cell: %r\r\n\r\ninstall.packages(c(\"rJava\", \"RJDBC\")) You observe the following error: ERROR: configuration failed for package 'rJava' Cause The rJava and RJDBC packages check for Java dependencies and file paths that are not present in the Databricks R directory. Solution Follow the steps below to install these libraries on running clusters. Run following commands in a %shcell. %sh\r\n\r\nls -l /usr/bin/java\r\nls -l /etc/alternatives/java\r\nln -s /usr/lib/jvm/java-8-openjdk-amd64 /usr/lib/jvm/default-java\r\nR CMD javareconf Install the rJava and RJDBC packages. %r\r\n\r\ninstall.packages(c(\"rJava\", \"RJDBC\")) Verify that the rJava package is installed. %r\r\n\r\ndyn.load('/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so')\r\nlibrary(rJava) Was this article helpful? (7) (15) Additional Informations Related Articles Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... How to parallelize R code with spark.lapply Parallelization of R code is difficult, because R code runs on the driver and R d... Fix the version of R packages When you use the install.packages() function to install CRAN packages, you cannot... Related Articles Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... How to parallelize R code with spark.lapply Parallelization of R code is difficult, because R code runs on the driver and R d... Fix the version of R packages When you use the install.packages() function to install CRAN packages, you cannot... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Install rJava and RJDBC libraries",
          "view_href" : "https://kb.databricks.com/en_US/r/install-rjava-rjdbc-libraries"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/termination-reasons",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Unexpected cluster termination Unexpected cluster termination Learn how to troubleshoot a Databricks cluster that stopped unexpectedly. Written by Adam Pavlacka Last published at: March 4th, 2022 Sometimes a cluster is terminated unexpectedly, not as a result of a manual termination or a configured automatic termination. A cluster can be terminated for many reasons. Some terminations are initiated by Databricks and others are initiated by the cloud provider. This article describes termination reasons and steps for remediation. Databricks initiated request limit exceeded To defend against API abuses, ensure quality of service, and prevent you from accidentally creating too many large clusters, Databricks throttles all cluster up-sizing requests, including cluster creation, starting, and resizing. The throttling uses the token bucket algorithm to limit the total number of nodes that anyone can launch over a defined interval across your Databricks deployment, while allowing burst requests of certain sizes. Requests coming from both the web UI and the APIs are subject to rate limiting. When cluster requests exceed rate limits, the limit-exceeding request fails with a REQUEST_LIMIT_EXCEEDED error. Solution If you hit the limit for your legitimate workflow, Databricks recommends that you do the following: Retry your request a few minutes later. Spread out your recurring workflow evenly in the planned time frame. For example, instead of scheduling all of your jobs to run at an hourly boundary, try distributing them at different intervals within the hour. Consider using clusters with a larger node type and smaller number of nodes. Use autoscaling clusters. If these options don’t work for you, contact Databricks Support to request a limit increase for the core instance. For other Databricks initiated termination reasons, see Termination Code. Cloud provider initiated terminations This article lists common cloud provider related termination reasons and remediation steps. AWS Provider limit Databricks launches a cluster by requesting resources on behalf of your cloud account. Sometimes, these requests fail because they would exceed your cloud account’s resource limits. In AWS, common error codes include: InstanceLimitExceeded AWS limits the number of running instances for each node type. Possible solutions include: Request a cluster with fewer nodes. Request a cluster with a different node type. Ask AWS support to increase instance limits. Client.VolumeLimitExceeded The cluster creation request exceeded the EBS volume limit. AWS has two types of volume limits: a limit on the total number of EBS volumes, and a limit on the total storage size of EBS volumes. Potential remediation steps: Request a cluster with fewer nodes. Check which of the two limits was exceeded. (AWS trusted advisor shows service limits for free). If the request exceeded the total number of EBS volumes, try reducing the requested number of volumes per node. If the request exceeded the total EBS storage size, try reducing the requested storage size and/or the number of EBS volumes. Ask AWS support to increase EBS volume limits. RequestLimitExceeded AWS limits the rate of API requests made for an AWS account. Wait a while before retrying the request. Provider shutdown The Spark driver is a single point of failure because it holds all cluster state. If the instance hosting the driver node is shut down, Databricks terminates the cluster. In AWS, common error codes include: Client.UserInitiatedShutdown Instance was terminated by a direct request to AWS which did not originate from Databricks. Contact your AWS administrator for more details. Server.InsufficientInstanceCapacity AWS could not satisfy the instance request. Wait a while and retry the request. Contact AWS support if the problem persists. Server.SpotInstanceTermination Instance was terminated by AWS because the current spot price has exceeded the maximum bid made for this instance. Use an on-demand instance for the driver, choose a different availability zone, or specify a higher spot bid price. For other shutdown-related error codes, refer to AWS docs. Delete Launch failure AWS In AWS, common error codes include: UnauthorizedOperation Databricks was not authorized to launch the requested instances. Possible reasons include: Your AWS administrator invalidated the AWS access key or IAM role used to launch instances. You are trying to launch a cluster using an IAM role that Databricks does not have permission to use. Contact the AWS administrator who set up the IAM role. For more information, see Secure Access to S3 Buckets Using IAM Roles. Unsupported with message “EBS-optimized instances are not supported for your requested configuration” The selected instance type is not available in the selected availability zone (AZ). It does not actually have anything to do with EBS-optimization being enabled. To remediate, you can choose a different instance type or AZ. AuthFailure.ServiceLinkedRoleCreationNotPermitted The provided credentials do not have permission to create the service-linked role for EC2 spot instances. The Databricks administrator needs to update the credentials used to launch instances in your account. Instructions and the updated policy can be found AWS Account. See Error Codes for a complete list of AWS error codes. Delete Azure This termination reason occurs when Azure Databricks fails to acquire virtual machines. The error code and message from the API are propagated to help you troubleshoot the issue. OperationNotAllowed You have reached a quota limit, usually number of cores, that your subscription can launch. Request a limit increase in Azure portal. See Azure subscription and service limits, quotas, and constraints. PublicIPCountLimitReached You have reached the limit of the public IPs that you can have running. Request a limit increase in Azure Portal. SkuNotAvailable The resource SKU you have selected (such as VM size) is not available for the location you have selected. To resolve, see Resolve errors for SKU not available. ReadOnlyDisabledSubscription Your subscription was disabled. Follow the steps in Why is my Azure subscription disabled and how do I reactivate it? to reactivate your subscription. ResourceGroupBeingDeleted Can occur if someone cancels your Azure Databricks workspace in the Azure portal and you try to create a cluster at the same time. The cluster fails because the resource group is being deleted. SubscriptionRequestsThrottled Your subscription is hitting the Azure Resource Manager request limit (see Throttling Resource Manager requests). Typical cause is that another system outside Azure Databricks) making a lot of API calls to Azure. Contact Azure support to identify this system and then reduce the number of API calls. Delete Communication lost Databricks was able to launch the cluster, but lost the connection to the instance hosting the Spark driver. AWS Caused by an incorrect networking configuration (for example, changing security group settings for Databricks workers) or a transient AWS networking issue. Delete Azure Caused by the driver virtual machine going down or a networking issue. Delete Was this article helpful? (14) (49) Additional Informations Related Articles Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Set instance_profile_arn as optional with a cluster policy In this article, we review the steps to create a cluster policy for the AWS attri... Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... Replay Apache Spark events in a cluster The Spark UI is commonly used as a debugging tool for Spark jobs. If the Spark UI... Related Articles Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Set instance_profile_arn as optional with a cluster policy In this article, we review the steps to create a cluster policy for the AWS attri... Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... Replay Apache Spark events in a cluster The Spark UI is commonly used as a debugging tool for Spark jobs. If the Spark UI... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Unexpected cluster termination",
          "view_href" : "https://kb.databricks.com/en_US/clusters/termination-reasons"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dev-tools/use-tcpdump-create-pcap-files",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Developer tools Use tcpdump to create pcap files Use tcpdump to create pcap files Analyze network traffic between nodes on a specific cluster by using tcpdump to create pcap files. Written by pavan.kumarchalamcharla Last published at: May 9th, 2022 If you want to analyze the network traffic between nodes on a specific cluster, you can install tcpdump on the cluster and use it to dump the network packet details to pcap files. The pcap files can then be downloaded to a local machine for analysis. Create the tcpdump init script Run this sample script in a notebook on the cluster to create the init script. %python\r\ndbutils.fs.put(\"dbfs://databricks/<path-to-init-script>/tcp_dump.sh\",\"\"\"\r\n#!/bin/bash\r\nDB_CLUSTER_ID=$(echo $HOSTNAME | awk -F '-' '{print$1\"-\"$2\"-\"$3}')\r\n\r\nif [[ ! -d /dbfs/databricks/tcpdump/${DB_CLUSTER_ID} ]] ; then\r\nsudo mkdir -p /dbfs/databricks/tcpdump/${DB_CLUSTER_ID}\r\nfi\r\n\r\nBASEDIR=\"/dbfs/databricks/tcpdump/${DB_CLUSTER_ID}\"\r\n\r\nmkdir -p ${BASEDIR}\r\n\r\nMYIP=$(ip route get 10 | awk '{print $NF;exit}')\r\necho \"initiating tcpdump\"\r\nsudo tcpdump -w ${BASEDIR}/trace_%Y_%m_%d_%H_%M_%S_${MYIP}.pcap -W 1000 -G 1800 -C 200 &\r\necho \"initiated tcpdump\"\"\"\", True) Remember the path to the init script. You will need it when configuring your cluster. Configure the init script Follow the documentation to configure a cluster-scoped init script (AWS | Azure | GCP). Specify the path to the init script. Use the same path that you used in the sample script (dbfs://databricks/<path-to-init-script>/tcp_dump.sh) After configuring the init script, restart the cluster. Locate pcap files Once the cluster has started, it automatically starts creating pcap files which contain the recorded network information. The pcap files are located in the folder dbfs://databricks/tcpdump/${<cluster-id>}. Download pcap files Download the pcap files to your local host for analysis. There are multiple ways to download files to your local machine. One option is the Databricks CLI (AWS | Azure). Was this article helpful? (10) (11) Additional Informations Related Articles Invalid Access Token error when running jobs with Airflow Problem When you run scheduled Airflow Databricks jobs, you get this error: Inval... Get Apache Spark config in DBConnect You can always view the Spark configuration (AWS | Azure | GCP) for your cluster ... Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... Failed to create process error with Databricks CLI in Windows Problem While trying to access the Databricks CLI (AWS | Azure | GCP) in Windows,... Related Articles Invalid Access Token error when running jobs with Airflow Problem When you run scheduled Airflow Databricks jobs, you get this error: Inval... Get Apache Spark config in DBConnect You can always view the Spark configuration (AWS | Azure | GCP) for your cluster ... Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... Failed to create process error with Databricks CLI in Windows Problem While trying to access the Databricks CLI (AWS | Azure | GCP) in Windows,... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Use tcpdump to create pcap files",
          "view_href" : "https://kb.databricks.com/en_US/dev-tools/use-tcpdump-create-pcap-files"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/library-fail-dependency-exception",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Libraries fail with dependency exception Libraries fail with dependency exception Learn why notebook-scoped libraries trigger an Apache Spark dependency exception; return a requirement cannot be satisfied error. Written by jordan.hicks Last published at: May 11th, 2022 Problem You have a Python function that is defined in a custom egg or wheel file and also has dependencies that are satisfied by another customer package installed on the cluster. When you call this function, it returns an error that says the requirement cannot be satisfied. org.apache.spark.SparkException: Process List(/local_disk0/pythonVirtualEnvDirs/virtualEnv-d82b31df-1da3-4ee9-864d-8d1fce09c09b/bin/python, /local_disk0/pythonVirtualEnvDirs/virtualEnv-d82b31df-1da3-4ee9-864d-8d1fce09c09b/bin/pip, install, fractal==0.1.0, --disable-pip-version-check) exited with code 1. Could not find a version that satisfies the requirement fractal==0.1.0 (from versions: 0.1.1, 0.1.2, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.2.7, 0.2.8, 0.2.9, 0.3.0) As an example, imagine that you have both wheel A and wheel B installed, either to the cluster via the UI or via notebook-scoped libraries. Assume that wheel A has a dependency on wheel B. dbutils.library.install(/path_to_wheel/A.whl) dbutils.library.install(/path_to_wheel/B.whl) When you try to make a call using one of these libraries, you get a requirement cannot be satisfied error. Cause Even though the requirements have been met by installing the required dependencies via the cluster UI or via a notebook-scoped library installation, Databricks cannot guarantee the order in which specific libraries are installed on the cluster. If a library is being referenced and it has not been distributed to the executor nodes, it will fallback to PyPI and use it locally to satisfy the requirement. Solution You should use one egg or wheel file that contains all required code and dependencies. This ensures that your code has the correct libraries loaded and available at run time. Was this article helpful? (7) (16) Additional Informations Related Articles Cannot uninstall library from UI Problem Usually, libraries can be uninstalled in the Clusters UI. If the checkbox... Error when installing pyodbc on a cluster Problem One of the following errors occurs when you use pip to install the pyodbc... Error when installing Cartopy on a cluster Problem You are trying to install Cartopy on a cluster and you receive a ManagedL... How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... Related Articles Cannot uninstall library from UI Problem Usually, libraries can be uninstalled in the Clusters UI. If the checkbox... Error when installing pyodbc on a cluster Problem One of the following errors occurs when you use pip to install the pyodbc... Error when installing Cartopy on a cluster Problem You are trying to install Cartopy on a cluster and you receive a ManagedL... How to correctly update a Maven library in Databricks Problem You make a minor update to a library in the repository, but you don’t wan... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Libraries fail with dependency exception",
          "view_href" : "https://kb.databricks.com/en_US/libraries/library-fail-dependency-exception"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/file-sink-streaming",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming Streaming with File Sink: Problems with recovery if you change checkpoint or output directories Streaming with File Sink: Problems with recovery if you change checkpoint or output directories Learn how to resolve issues that occur with recovery if you change checkpoint or output directories when streaming with File Sink. Written by Adam Pavlacka Last published at: May 17th, 2022 When you stream data into a file sink, you should always change both checkpoint and output directories together. Otherwise, you can get failures or unexpected outputs. Apache Spark creates a folder inside the output directory named _spark_metadata. This folder contains write-ahead logs for every batch run. This is how Spark gets exactly-once guarantees when writing to a file system. This folder contains save files for each batch (named 0,1,2,3 etc + 19.compact, n.compact etc). These files include JSON that gives details about the output for the particular batch. With the help of this data, once a batch has succeeded, any duplicate batch output is discarded. If you change the checkpoint directory but not the output directory: When you change the checkpoint directory, the stream job will start batches again from 0. Since 0 is already present in the _spark_metadata folder, the output file will be discarded even if it has new data. That is, if you stop the previous run on the 500th batch, the next run with same output directory and different checkpoint directory will give output only on the 501st batch. All of the previous batches will be silently discarded. If you change the output directory but not the checkpoint directory: When you change only the output directory, it loses all of the batch data from the _spark_metadata folder. But Spark starts writing from the next batch according to the checkpoint directory. For example, if the previous run was stopped at 500, the first write of the new stream job will be at file 501 on _spark_metadata and you lose all of the old batches. When you read the files back, you get the error metadata for batch 0(or first compact file (19.compact)) is not found. Was this article helpful? (5) (18) Additional Informations Related Articles Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... Related Articles Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Streaming with File Sink: Problems with recovery if you change checkpoint or output directories",
          "view_href" : "https://kb.databricks.com/en_US/streaming/file-sink-streaming"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/r/persist-share-code-rstudio",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles R with Apache Spark How to persist and share code in RStudio How to persist and share code in RStudio Learn how to share notebooks between Databricks and RStudio. Written by Adam Pavlacka Last published at: May 20th, 2022 Problem Unlike a Databricks notebook that has version control built in, code developed in RStudio is lost when the high concurrency cluster hosting Rstudio is shut down. Solution To persist and share code in RStudio, do one of the following: From RStudio, save the code to a folder on DBFS which is accessible from both Databricks notebooks and RStudio. Use the integrated support for version control like Git in RStudio. Save the R notebook to your local file system by exporting it as Rmarkdown, then import the file into the RStudio instance. The blog Sharing R Notebooks using RMarkdown describes the steps in more detail. This process allows you to persist code developed in RStudio and share notebooks between the Databricks notebook environment and RStudio. Was this article helpful? (9) (12) Additional Informations Related Articles How to parallelize R code with spark.lapply Parallelization of R code is difficult, because R code runs on the driver and R d... Fix the version of R packages When you use the install.packages() function to install CRAN packages, you cannot... Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... Related Articles How to parallelize R code with spark.lapply Parallelization of R code is difficult, because R code runs on the driver and R d... Fix the version of R packages When you use the install.packages() function to install CRAN packages, you cannot... Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... How to parallelize R code with gapply Parallelization of R code is difficult, because R code runs on the driver and R d... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to persist and share code in RStudio",
          "view_href" : "https://kb.databricks.com/en_US/r/persist-share-code-rstudio"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/chained-transformations",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management Simplify chained transformations Simplify chained transformations Learn how to simplify chained transformations on your DataFrame in Databricks. Written by Adam Pavlacka Last published at: May 25th, 2022 Sometimes you may need to perform multiple transformations on your DataFrame: %scala\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.DataFrame\r\n\r\nval testDf = (1 to 10).toDF(\"col\")\r\n\r\ndef func0(x: Int => Int, y: Int)(in: DataFrame): DataFrame = {\r\n  in.filter('col > x(y))\r\n}\r\ndef func1(x: Int)(in: DataFrame): DataFrame = {\r\n  in.selectExpr(\"col\", s\"col + $x as col1\")\r\n}\r\ndef func2(add: Int)(in: DataFrame): DataFrame = {\r\n  in.withColumn(\"col2\", expr(s\"col1 + $add\"))\r\n} When you apply these transformations, you may end up with spaghetti code like this: %scala\r\n\r\ndef inc(i: Int) = i + 1\r\n\r\nval tmp0 = func0(inc, 3)(testDf)\r\nval tmp1 = func1(1)(tmp0)\r\nval tmp2 = func2(2)(tmp1)\r\nval res = tmp2.withColumn(\"col3\", expr(\"col2 + 3\")) This article describes several methods to simplify chained transformations. DataFrame transform API To benefit from the functional programming style in Spark, you can leverage the DataFrame transform API, for example: %scala\r\n\r\nval res = testDf.transform(func0(inc, 4))\r\n                .transform(func1(1))\r\n                .transform(func2(2))\r\n                .withColumn(\"col3\", expr(\"col2 + 3\")) Function.chain API To go even further, you can leverage the Scala Function library, to chain the transformations, for example: %scala\r\n\r\nval chained = Function.chain(List(func0(inc, 4)(_), func1(1)(_), func2(2)(_)))\r\nval res = testDf.transform(chained)\r\n                .withColumn(\"col3\", expr(\"col2 + 3\")) implicit class Another alternative is to define a Scala implicit class, which allows you to eliminate the DataFrame transform API: %scala\r\n\r\nimplicit class MyTransforms(df: DataFrame) {\r\n    def func0(x: Int => Int, y: Int): DataFrame = {\r\n        df.filter('col > x(y))\r\n    }\r\n    def func1(x: Int): DataFrame = {\r\n        df.selectExpr(\"col\", s\"col + $x as col1\")\r\n    }\r\n    def func2(add: Int): DataFrame = {\r\n        df.withColumn(\"col2\", expr(s\"col1 + $add\"))\r\n    }\r\n} Then you can call the functions directly: %scala\r\n\r\nval res = testDf.func0(inc, 1)\r\n            .func1(2)\r\n            .func2(3)\r\n            .withColumn(\"col3\", expr(\"col2 + 3\")) Was this article helpful? (11) (14) Additional Informations Related Articles Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... Related Articles Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Simplify chained transformations",
          "view_href" : "https://kb.databricks.com/en_US/data/chained-transformations"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/dstream-not-supported",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming Apache Spark DStream is not supported Apache Spark DStream is not supported DStreams are not supported in Databricks. Migrate from DStream API to Structured Streaming. Written by Adam Pavlacka Last published at: May 17th, 2022 Problem You are attempting to use a Spark Discretized Stream (DStream) in a Databricks streaming job, but the job is failing. Cause DStreams and the DStream API are not supported by Databricks. Solution Instead of using Spark DStream, you should migrate to Structured Streaming. Review the Databricks Structured Streaming in production (AWS | Azure | GCP) documentation for more information. Was this article helpful? (8) (14) Additional Informations Related Articles Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... Related Articles Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Apache Spark DStream is not supported",
          "view_href" : "https://kb.databricks.com/en_US/streaming/dstream-not-supported"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/dupe-column-in-metadata",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark Duplicate columns in the metadata error Duplicate columns in the metadata error Spark job fails while processing a Delta table with org.apache.spark.sql.AnalysisException Found duplicate column(s) in the metadata error. Written by vikas.yadav Last published at: May 23rd, 2022 Problem Your Apache Spark job is processing a Delta table when the job fails with an error message. org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the metadata update: col1, col2... Cause There are duplicate column names in the Delta table. Column names that differ only by case are considered duplicate. Delta Lake is case preserving, but case insensitive, when storing a schema. Parquet is case sensitive when storing and returning column information. Spark can be case sensitive, but it is case insensitive by default. In order to avoid potential data corruption or data loss, duplicate column names are not allowed. Solution Delta tables must not contain duplicate column names. Ensure that all column names are unique. Was this article helpful? (6) (16) Additional Informations Related Articles Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Date functions only accept int values in Apache Spark 3.0 Problem You are attempting to use the date_add() or date_sub() functions in Spark... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Disable broadcast when query plan has BroadcastNestedLoopJoin This article explains how to disable broadcast when the query plan has BroadcastN... Related Articles Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Date functions only accept int values in Apache Spark 3.0 Problem You are attempting to use the date_add() or date_sub() functions in Spark... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Disable broadcast when query plan has BroadcastNestedLoopJoin This article explains how to disable broadcast when the query plan has BroadcastN... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Duplicate columns in the metadata error",
          "view_href" : "https://kb.databricks.com/en_US/sql/dupe-column-in-metadata"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/kafka-no-resolvable-bootstrap-urls",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming Kafka error: No resolvable bootstrap urls Kafka error: No resolvable bootstrap urls A 'No resolvable bootstrap urls' error occurs when you try to read or write data to a Kafka stream. Written by Adam Pavlacka Last published at: May 18th, 2022 Problem You are trying to read or write data to a Kafka stream when you get an error message. kafkashaded.org.apache.kafka.common.KafkaException: Failed to construct kafka consumer\r\n\r\nCaused by: kafkashaded.org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers If you are running a notebook, the error message appears in a notebook cell. If you are running a JAR job, the error message appears in the cluster driver and worker logs (AWS | Azure | GCP). Cause This error message occurs when an invalid hostname or IP address is passed to the kafka.bootstrap.servers configuration in readStream. This means a Kafka bootstrap server is not running at the given hostname or IP address. Solution Contact your Kafka admin to determine the correct hostname or IP address for the Kafka bootstrap servers in your environment. Make sure you use the correct hostname or IP address when you establish the connection between Kafka and your Apache Spark structured streaming application. Was this article helpful? (12) (11) Additional Informations Related Articles Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... Handling partition column values while using an SQS queue as a streaming source Problem If data in S3 is stored by partition, the partition column values are use... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... How to switch a SNS streaming job to a new SQS queue Problem You have a Structured Streaming job running via the S3-SQS connector. Sup... Related Articles Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... Handling partition column values while using an SQS queue as a streaming source Problem If data in S3 is stored by partition, the partition column values are use... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... How to switch a SNS streaming job to a new SQS queue Problem You have a Structured Streaming job running via the S3-SQS connector. Sup... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Kafka error: No resolvable bootstrap urls",
          "view_href" : "https://kb.databricks.com/en_US/streaming/kafka-no-resolvable-bootstrap-urls"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/ip-access-list-update-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters IP access list update returns INVALID_STATE IP access list update returns INVALID_STATE Cannot update IP access list. INVALID_STATE error message. Written by Gobinath.Viswanathan Last published at: March 4th, 2022 Problem You are trying to update an IP access list and you get an INVALID_STATE error message. {\"error_code\":\"INVALID_STATE\",\"message\":\"Your current IP 3.3.3.3 will not be allowed to access the workspace under current configuration\"} Cause The IP access list update that you are trying to commit does not include your current public IP address. If your current IP address is not included in the access list, you are blocked from the environment. If you assume that your current IP is 3.3.3.3, this example API call results in an INVALID_STATE error message. %sh\r\ncurl -X POST -n \\\r\n  https://<databricks-instance>/api/2.0/ip-access-lists\r\n  -d '{\r\n    \"label\": \"office\",\r\n    \"list_type\": \"ALLOW\",\r\n    \"ip_addresses\": [\r\n        \"1.1.1.1\",\r\n        \"2.2.2.2/21\"\r\n      ]\r\n    }' Solution You must always include your current public IP address in the JSON file that is used to update the IP access list. If you assume that your current IP is 3.3.3.3, this example API call results in a successful IP access list update. %sh\r\ncurl -X POST -n \\\r\n  https://<databricks-instance>/api/2.0/ip-access-lists\r\n  -d '{\r\n    \"label\": \"office\",\r\n    \"list_type\": \"ALLOW\",\r\n    \"ip_addresses\": [\r\n        \"1.1.1.1\",\r\n        \"2.2.2.2/21\",\r\n        \"3.3.3.3\"\r\n      ]\r\n    }' Was this article helpful? (5) (14) Additional Informations Related Articles Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Related Articles Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "IP access list update returns INVALID_STATE",
          "view_href" : "https://kb.databricks.com/en_US/clusters/ip-access-list-update-error"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/spark-ui-wrong-number-jobs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Apache Spark UI shows wrong number of jobs Apache Spark UI shows wrong number of jobs Apache Spark UI shows the wrong number of active jobs. Written by ashish Last published at: May 11th, 2022 Problem You are reviewing the number of active Apache Spark jobs on a cluster in the Spark UI, but the number is too high to be accurate. If you restart the cluster, the number of jobs shown in the Spark UI is correct at first, but over time it grows abnormally high. Cause The Spark UI is not always accurate for large, or long-running, clusters due to event drops. The Spark UI requires termination entries to know when an active job has completed. If a job misses this entry, due to errors or unexpected failure, the job may stop running while incorrectly showing as active in the Spark UI. Delete Info For more information review the Apache Spark UI is not in sync with job KB article. Solution You should not use the Spark UI as a source of truth for active jobs on a cluster. The method sc.statusTracker().getActiveJobIds() in the Spark API is a reliable way to track the number of active jobs. Please review the Spark Status Tracker documentation for more information. Was this article helpful? (11) (8) Additional Informations Related Articles Create table in overwrite mode fails when interrupted Problem When you attempt to rerun an Apache Spark write operation by cancelling t... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... Job fails due to job rate limit Problem A Databricks notebook or Jobs API request returns the following error: Er... Apache Spark job fails with Failed to parse byte string Problem Spark-submit jobs fail with a Failed to parse byte string: -1 error messa... Related Articles Create table in overwrite mode fails when interrupted Problem When you attempt to rerun an Apache Spark write operation by cancelling t... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... Job fails due to job rate limit Problem A Databricks notebook or Jobs API request returns the following error: Er... Apache Spark job fails with Failed to parse byte string Problem Spark-submit jobs fail with a Failed to parse byte string: -1 error messa... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Apache Spark UI shows wrong number of jobs",
          "view_href" : "https://kb.databricks.com/en_US/jobs/spark-ui-wrong-number-jobs"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/remove-log4j1x-jmsappender-socketserver-classes",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Remove Log4j 1.x JMSAppender and SocketServer classes from classpath Remove Log4j 1.x JMSAppender and SocketServer classes from classpath Remove Log4j 1.x JMSAppender and SocketServer classes from classpath. Written by Adam Pavlacka Last published at: May 16th, 2022 Databricks recently published a blog on Log4j 2 Vulnerability (CVE-2021-44228) Research and Assessment. Databricks does not directly use a version of Log4j known to be affected by this vulnerability within the Databricks platform in a way we understand may be vulnerable. Databricks also does not use the affected classes from Log4j 1.x with known vulnerabilities (CVE-2021-4104, CVE-2020-9488, and CVE-2019-17571). However, if your code uses one of these classes (JMSAppender or SocketServer), your use may potentially be impacted by these vulnerabilities. If your code uses Log4j, you should upgrade to Log4j 2.17 or above. If you cannot upgrade for technical reasons, you can use a global init script (AWS | Azure | GCP) to strip the affected classes from Log4j on cluster start. Delete Warning Because we do not control the code you run, we cannot guarantee that this solution will prevent Log4j from loading the affected classes in all cases. Configure the global init script Delete Info Running this script is a breaking change for any code that relies on the affected classes. AWS Go to the Admin Console and click the Global Init Scripts tab. Click the + Add button. Enter the name of the script. Copy the following script into the Script field. %sh\r\n\r\n#!/bin/bash\r\n\r\necho 'Init script to remove certain Log4J 1.x classes, version 1.0 (2021-12-17)'\r\n\r\nFILES_TO_DELETE=(\r\n  org/apache/log4j/net/JMSAppender.class\r\n  org/apache/log4j/net/SocketServer.class\r\n)\r\n\r\nfind \"/databricks\" \\\r\n    -name '*log4j*.jar' \\\r\n    -exec echo -e \"\\nProcessing {}\" \\; -exec zip -d {} \"${FILES_TO_DELETE[@]}\" \\;\r\n\r\nexit 0 If you have more than one global init script configured for your workspace, you should configure this script to run after your other scripts. Ensure the Enabled switch is toggled on. Click Add. Restart ALL running clusters. Delete Azure Go to the Admin Console and click the Global Init Scripts tab. Click the + Add button. Enter the name of the script. Copy the following script into the Script field. %sh\r\n\r\n#!/bin/bash\r\n\r\necho 'Init script to remove certain Log4J 1.x classes, version 1.0 (2021-12-17)'\r\n\r\nFILES_TO_DELETE=(\r\n  org/apache/log4j/net/JMSAppender.class\r\n  org/apache/log4j/net/SocketServer.class\r\n)\r\n\r\nfind \"/databricks\" \\\r\n    -name '*log4j*.jar' \\\r\n    -exec echo -e \"\\nProcessing {}\" \\; -exec zip -d {} \"${FILES_TO_DELETE[@]}\" \\;\r\n\r\nexit 0 If you have more than one global init script configured for your workspace, you should configure this script to run after your other scripts. Ensure the Enabled switch is toggled on. Click Add. Restart ALL running clusters. Delete GCP Use the Global Init Scripts API 2.0 to apply the following init script to every cluster in your workspace. %sh\r\n\r\n#!/bin/bash\r\n\r\necho 'Init script to remove certain Log4J 1.x classes, version 1.0 (2021-12-17)'\r\n\r\nFILES_TO_DELETE=(\r\n  org/apache/log4j/net/JMSAppender.class\r\n  org/apache/log4j/net/SocketServer.class\r\n)\r\n\r\nfind \"/databricks\" \\\r\n    -name '*log4j*.jar' \\\r\n    -exec echo -e \"\\nProcessing {}\" \\; -exec zip -d {} \"${FILES_TO_DELETE[@]}\" \\;\r\n\r\nexit 0 Restart ALL running clusters after applying the global init script. Delete Verify the affected classes are not available You should run a test on each cluster to ensure the affected classes are not available. Test 1 You can run an assert check on the affected classes in a notebook. %scala\r\n\r\nassert(this.getClass.getClassLoader().getResource(\"org/apache/log4j/net/JMSAppender.class\") == null)\r\nassert(this.getClass.getClassLoader().getResource(\"org/apache/log4j/net/SocketServer.class\") == null) This sample code runs successfully if you have disabled the affected classes. This sample code should return an error if you have NOT disabled the affected classes. Test 2 You can attempt to import the affected classes into a notebook. %scala\r\n\r\nimport org.apache.log4j.net.JMSAppender\r\nimport org.apache.log4j.net.SocketServer This sample code runs successfully if you have NOT disabled the affected classes. This sample code should return an error if you have disabled the affected classes. Caveats There are some corner cases where you can re-introduce the Log4j 1.x versions of JMSAppender or SocketServer. Problem If you install a Maven library with a transitive dependency on Log4j 1.x, all of its classes are re-added to the classpath. Solution You can work around this issue by adding Log4j to the Exclusions field when installing Maven libraries. Problem If you configure an external Apache Hive metastore, Apache Spark uses Ivy to resolve and download the correct metastore client library, and all of its transitive dependencies, possibly including Log4j 1.x. To speed up cluster launch, you can cache the downloaded jars on DBFS and use an init script to install from the cache. If you cache jars like this, it is possible that Log4j 1.x may be included. Solution You can configure the init script for your external metastore to delete the affected classes. Was this article helpful? (5) (14) Additional Informations Related Articles Libraries fail with dependency exception Problem You have a Python function that is defined in a custom egg or wheel file ... New job fails when adding a library from DBFS or S3 Problem You create a new job and attempt to add a library from DBFS or S3 storage... Error when installing pyodbc on a cluster Problem One of the following errors occurs when you use pip to install the pyodbc... Install Turbodbc via init script Turbodbc is a Python module that uses the ODBC interface to access relational dat... Related Articles Libraries fail with dependency exception Problem You have a Python function that is defined in a custom egg or wheel file ... New job fails when adding a library from DBFS or S3 Problem You create a new job and attempt to add a library from DBFS or S3 storage... Error when installing pyodbc on a cluster Problem One of the following errors occurs when you use pip to install the pyodbc... Install Turbodbc via init script Turbodbc is a Python module that uses the ODBC interface to access relational dat... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Remove Log4j 1.x JMSAppender and SocketServer classes from classpath",
          "view_href" : "https://kb.databricks.com/en_US/libraries/remove-log4j1x-jmsappender-socketserver-classes"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/mlflow-artifacts-permission-denied",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning PERMISSION_DENIED error when accessing MLflow experiment artifact PERMISSION_DENIED error when accessing MLflow experiment artifact Resolve a PERMISSION_DENIED error when trying to access MLflow experiment artifacts. Written by Adam Pavlacka Last published at: May 16th, 2022 Problem You get a PERMISSION_DENIED error when trying to access an MLflow artifact using the MLflow client. RestException: PERMISSION_DENIED: User <user> does not have permission to 'View' experiment with id <experiment-id> or RestException: PERMISSION_DENIED: User <user> does not have permission to 'Edit' experiment with id <experiment-id> Cause With the extension of MLflow experiment permissions to artifacts, you must have explicit permission to access artifacts of an MLflow experiment. The error suggests that you do not have permission to access artifacts of the experiment. Solution Ask the experiment owner to give you the appropriate level of permissions to access the experiment. Experiment permissions (AWS | Azure | GCP) automatically apply to artifacts of an experiment. Was this article helpful? (12) (8) Additional Informations Related Articles Experiment warning when legacy artifact storage location is used Problem A new icon appears on the MLflow Experiments page with the following open... How to speed up cross-validation Hyperparameter tuning of Apache SparkML models takes a very long time, depending ... Incorrect results when using documents as inputs Problem You have a ML model that takes documents as inputs, specifically, an arra... Error when importing OneHotEncoderEstimator Problem You have migrated a notebook from Databricks Runtime 6.4 for Machine Lear... Related Articles Experiment warning when legacy artifact storage location is used Problem A new icon appears on the MLflow Experiments page with the following open... How to speed up cross-validation Hyperparameter tuning of Apache SparkML models takes a very long time, depending ... Incorrect results when using documents as inputs Problem You have a ML model that takes documents as inputs, specifically, an arra... Error when importing OneHotEncoderEstimator Problem You have migrated a notebook from Databricks Runtime 6.4 for Machine Lear... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "PERMISSION_DENIED error when accessing MLflow experiment artifact",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/mlflow-artifacts-permission-denied"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/cannot-modify-spark-serializer",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Cannot modify the value of an Apache Spark config Cannot modify the value of an Apache Spark config You cannot modify the value of a Spark config setting within a notebook. It must be set at the cluster level. Written by Adam Pavlacka Last published at: May 20th, 2022 Problem You are trying to SET the value of a Spark config in a notebook and get a Cannot modify the value of a Spark config error. For example: %sql\r\n\r\nSET spark.serializer=org.apache.spark.serializer.KryoSerializer Error in SQL statement: AnalysisException: Cannot modify the value of a Spark config: spark.serializer; Cause The SET command does not work on SparkConf entries. This is by design in Spark 3.0 and above. Solution You should remove SET commands for SparkConf entries from your notebook. You can enter SparkConf values at the cluster level by entering them in the cluster’s Spark config (AWS | Azure | GCP) and restarting the cluster. Was this article helpful? (8) (13) Additional Informations Related Articles Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... Related Articles Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cannot modify the value of an Apache Spark config",
          "view_href" : "https://kb.databricks.com/en_US/scala/cannot-modify-spark-serializer"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/new-job-library-fail",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries New job fails when adding a library from DBFS or S3 New job fails when adding a library from DBFS or S3 New jobs fail when adding a library from DBFS or S3 storage. Error Uncaught TypeError Cannot read property 'concat' of undefined Written by jordan.hicks Last published at: May 12th, 2022 Problem You create a new job and attempt to add a library from DBFS or S3 storage. The workspace UI returns an error. Error: Uncaught TypeError: Cannot read property 'concat' of undefined\r\nReload the page and try again. If the error persists, contact support. Clicking Reload page does not resolve the error message. Cause This is a known issue. A fix is expected in early December. Solution Until the permanent fix is available, you can use a workaround to add the library. Add a workspace library. (This can be any library. It is a temporary addition.) Add your selected library from DBFS or S3. Remove the workspace library. You can now finish creating the job as normal. Was this article helpful? (7) (14) Additional Informations Related Articles Install Turbodbc via init script Turbodbc is a Python module that uses the ODBC interface to access relational dat... Cannot uninstall library from UI Problem Usually, libraries can be uninstalled in the Clusters UI. If the checkbox... Error when installing Cartopy on a cluster Problem You are trying to install Cartopy on a cluster and you receive a ManagedL... Library unavailability causing job failures Problem You are launching jobs that import external libraries and get an Import E... Related Articles Install Turbodbc via init script Turbodbc is a Python module that uses the ODBC interface to access relational dat... Cannot uninstall library from UI Problem Usually, libraries can be uninstalled in the Clusters UI. If the checkbox... Error when installing Cartopy on a cluster Problem You are trying to install Cartopy on a cluster and you receive a ManagedL... Library unavailability causing job failures Problem You are launching jobs that import external libraries and get an Import E... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "New job fails when adding a library from DBFS or S3",
          "view_href" : "https://kb.databricks.com/en_US/libraries/new-job-library-fail"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/checkpoint-no-cleanup-foreachbatch",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming Checkpoint files not being deleted when using foreachBatch() Checkpoint files not being deleted when using foreachBatch() Learn how to prevent foreachBatch() checkpoint files from using a large amount of storage. Written by Adam Pavlacka Last published at: May 19th, 2022 Problem You have a streaming job using foreachBatch() to process DataFrames. %scala\r\n\r\nstreamingDF.writeStream.outputMode(\"append\").foreachBatch { (batchDF: DataFrame, batchId: Long) =>\r\n  batchDF.write.format(\"parquet\").mode(\"overwrite\").save(output_directory)\r\n}.start() Checkpoint files are being created, but are not being deleted. You can verify the problem by navigating to the root directory and looking in the /local_disk0/tmp/ folder. Checkpoint files remain in the folder. Cause The command foreachBatch() is used to support DataFrame operations that are not normally supported on streaming DataFrames. By using foreachBatch() you can apply these operations to every micro-batch. This requires a checkpoint directory to track the streaming updates. If you have not specified a custom checkpoint location, a default checkpoint directory is created at /local_disk0/tmp/. Databricks uses the checkpoint directory to ensure correct and consistent progress information. When a stream is shut down, either purposely or accidentally, the checkpoint directory allows Databricks to restart and pick up exactly where it left off. If a stream is shut down by cancelling the stream from the notebook, the Databricks job attempts to clean up the checkpoint directory on a best-effort basis. If the stream is terminated in any other way, or if the job is terminated, the checkpoint directory is not cleaned up. This is as designed. Solution You should manually specify the checkpoint directory with the checkpointLocation option. %scala\r\n\r\nstreamingDF.writeStream.option(\"checkpointLocation\",\"<checkpoint-path>\").outputMode(\"append\").foreachBatch { (batchDF: DataFrame, batchId: Long) =>\r\nbatchDF.write.format(\"parquet\").mode(\"overwrite\").save(output_directory)\r\n}.start() Was this article helpful? (7) (14) Additional Informations Related Articles How to restart a structured streaming query from last written offset Scenario You have a stream, running a windowed aggregation query, that reads from... Checkpoint files not being deleted when using display() Problem You have a streaming job using display() to display DataFrames. %scala va... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Related Articles How to restart a structured streaming query from last written offset Scenario You have a stream, running a windowed aggregation query, that reads from... Checkpoint files not being deleted when using display() Problem You have a streaming job using display() to display DataFrames. %scala va... Apache Spark DStream is not supported Problem You are attempting to use a Spark Discretized Stream (DStream) in a Datab... How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Checkpoint files not being deleted when using foreachBatch()",
          "view_href" : "https://kb.databricks.com/en_US/streaming/checkpoint-no-cleanup-foreachbatch"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/administration/who-deleted-cluster",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Databricks administration How to discover who deleted a cluster in Azure portal How to discover who deleted a cluster in Azure portal Learn how to discover who deleted an Azure Databricks cluster. Written by Adam Pavlacka Last published at: February 25th, 2022 If a cluster in your workspace has disappeared or been deleted, you can identify which user deleted it by running a query in the Log Analytics workspaces service in the Azure portal. Delete Note If you do not have an analytics workspace set up, you must configure Diagnostic Logging in Azure Databricks before you continue. Load the Log Analytics workspaces service in the Azure portal. Click the name of your workspace. Click Logs. Look for the following text: Type your query here or click one of the example queries to start. Enter the following query: DatabricksClusters\r\n| where ActionName == \"permanentDelete\"\r\n     and Response contains \"\\\"statusCode\\\":200\"\r\n     and RequestParams contains \"\\\"cluster_id\\\":\\\"0210-024915-bore731\\\"\"  // Add cluster_id filter if cluster id is known\r\n     and TimeGenerated between(datetime(\"2020-01-25 00:00:00\") .. datetime(\"2020-01-28 00:00:00\"))  // Add timestamp (in UTC) filter to narrow down the result.\r\n| extend id = parse_json(Identity)\r\n| extend requestParams = parse_json(RequestParams)\r\n| project UserEmail=id.email,clusterId = requestParams.cluster_id, SourceIPAddress, EventTime=TimeGenerated Edit the cluster_id as required. Edit the datetime values to filter on a specific time range. Click Run to execute the query. The results (if any) display below the query box. If you are still unable to find who deleted the cluster, create a support case with Microsoft Support. Provide details such as the workspace id and the time range of the event (including your time zone). Microsoft Support will review the corresponding backend activity logs. Was this article helpful? (9) (20) Additional Informations Related Articles SSO SAML authentication error with PingFederate Problem When using PingFederate to authenticate over a SSO connection with Databr... SSO server redirects to original URL, not to vanity Databricks URL Problem When you log into Databricks using a vanity URL (such as mycompany.cloud.... How to discover who deleted a workspace in Azure portal If your workspace has disappeared or been deleted, you can identify which user de... Related Articles SSO SAML authentication error with PingFederate Problem When using PingFederate to authenticate over a SSO connection with Databr... SSO server redirects to original URL, not to vanity Databricks URL Problem When you log into Databricks using a vanity URL (such as mycompany.cloud.... How to discover who deleted a workspace in Azure portal If your workspace has disappeared or been deleted, you can identify which user de... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to discover who deleted a cluster in Azure portal",
          "view_href" : "https://kb.databricks.com/en_US/administration/who-deleted-cluster"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/drop-delta-table",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Best practices for dropping a managed Delta Lake table Best practices for dropping a managed Delta Lake table Learn the best practices for dropping a managed Delta Lake table. Written by Adam Pavlacka Last published at: May 10th, 2022 Regardless of how you drop a managed table, it can take a significant amount of time, depending on the data size. Delta Lake managed tables in particular contain a lot of metadata in the form of transaction logs, and they can contain duplicate data files. If a Delta table has been in use for a long time, it can accumulate a very large amount of data. In the Databricks environment, there are two ways to drop tables (AWS | Azure | GCP): Run DROP TABLE in a notebook cell. Click Delete in the UI. Even though you can delete tables in the background without affecting workloads, it is always good to make sure that you run DELETE FROM (AWS | Azure | GCP) and VACUUM (AWS | Azure | GCP) before you start a drop command on any table. This ensures that the metadata and file sizes are cleaned up before you initiate the actual data deletion. For example, if you are trying to delete the Delta table events, run the following commands before you start the DROP TABLE command: Run DELETE FROM: DELETE FROM events Run VACUUM with an interval of zero: VACUUM events RETAIN 0 HOURS These two steps reduce the amount of metadata and number of uncommitted files that would otherwise increase the data deletion time. Was this article helpful? (16) (49) Additional Informations Related Articles A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... How Delta cache behaves on an autoscaling cluster This article is about how Delta cache (AWS | Azure | GCP) behaves on an auto-scal... How to improve performance of Delta Lake MERGE INTO queries using partition pruning This article explains how to trigger partition pruning in Delta Lake MERGE INTO (... Related Articles A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... How Delta cache behaves on an autoscaling cluster This article is about how Delta cache (AWS | Azure | GCP) behaves on an auto-scal... How to improve performance of Delta Lake MERGE INTO queries using partition pruning This article explains how to trigger partition pruning in Delta Lake MERGE INTO (... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Best practices for dropping a managed Delta Lake table",
          "view_href" : "https://kb.databricks.com/en_US/delta/drop-delta-table"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/optimize-delta-sink-structured-streaming",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Optimize a Delta sink in a structured streaming application Optimize a Delta sink in a structured streaming application Optimize your Delta sink by using a mod value on the batchId to optimize when foreachBatch runs. Written by mathan.pillai Last published at: May 10th, 2022 You are using a Delta table as the sink for your structured streaming application and you want to optimize the Delta table so that queries are faster. If your structured streaming application has a very frequent trigger interval, it may not create sufficient files that are eligible for compaction in each microbatch. The autoOptimize operation compacts to 128 MB files. An explicit optimize operation compacts Delta Lake files to 1 GB files. If you do not have a sufficient number of eligible files in each microbatch, you should optimize the Delta table files periodically. Use foreachBatch with a mod value One of the easiest ways to periodically optimize the Delta table sink in a structured streaming application is by using foreachBatch with a mod value on the microbatch batchId. Assume that you have a streaming DataFrame that was created from a Delta table. You use foreachBatch when writing the streaming DataFrame to the Delta sink. Within foreachBatch, the mod value of batchId is used so the optimize operation is run after every 10 microbatches, and the zorder operation is run after every 101 microbatches. %scala\r\n\r\nval df = spark.readStream.format(\"delta\").table(\"<table-name>\")\r\ndf.writeStream.format(\"delta\")\r\n  .foreachBatch{ (batchDF: DataFrame, batchId: Long) =>\r\n    batchDF.persist()\r\n    if(batchId % 10 == 0){spark.sql(\"optimize <table-name>\")}\r\n    if(batchId % 101 == 0){spark.sql(\"optimize <table-name> zorder by (<zorder-column-name>)\")}\r\n    batchDF.write.format(\"delta\").mode(\"append\").saveAsTable(\"<table-name>\")\r\n  }.outputMode(\"update\")\r\n  .start() You can modify the mod value as appropriate for your structured streaming application. Was this article helpful? (9) (12) Additional Informations Related Articles Object lock error when writing Delta Lake tables to S3 Problem You are trying to perform a Delta write operation to a S3 bucket and get ... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... HIVE_CURSOR_ERROR when reading a table in Athena Problem You create an external table in Athena and integrate it with Delta Lake u... Delta Lake write job fails with java.lang.UnsupportedOperationException Problem Delta Lake write jobs sometimes fail with the following exception: java.l... Related Articles Object lock error when writing Delta Lake tables to S3 Problem You are trying to perform a Delta write operation to a S3 bucket and get ... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... HIVE_CURSOR_ERROR when reading a table in Athena Problem You create an external table in Athena and integrate it with Delta Lake u... Delta Lake write job fails with java.lang.UnsupportedOperationException Problem Delta Lake write jobs sometimes fail with the following exception: java.l... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Optimize a Delta sink in a structured streaming application",
          "view_href" : "https://kb.databricks.com/en_US/delta/optimize-delta-sink-structured-streaming"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dev-tools/dbconnect-spark-session-null",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Developer tools Apache Spark session is null in DBConnect Apache Spark session is null in DBConnect A `sparkSession is null while trying to executeCollectResult` error message occurs when using DBConnect. Written by Jose Gonzalez Last published at: April 1st, 2022 Problem You are trying to run your code using Databricks Connect (AWS | Azure | GCP) when you get a sparkSession is null error message. java.lang.AssertionError: assertion failed: sparkSession is null while trying to executeCollectResult\r\nat scala.Predef$.assert(Predef.scala:170)\r\nat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:323)\r\nat org.apache.spark.sql.Dataset$$anonfun$50.apply(Dataset.scala:3351)\r\nat org.apache.spark.sql.Dataset$$anonfun$50.apply(Dataset.scala:3350)\r\nat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3485)\r\nat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3480)\r\nat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:111)\r\nat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:240)\r\nat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:97)\r\nat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:170)\r\nat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3480)\r\nat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3350)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\r\nat py4j.Gateway.invoke(Gateway.java:295)\r\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\nat py4j.GatewayConnection.run(GatewayConnection.java:251)\r\nat java.lang.Thread.run(Thread.java:748) Cause You get the sparkSession is null error message if a Spark session is not active on your cluster when you try to run your code using DBConnect. Solution You must ensure that a Spark session is active on your cluster before you attempt to run your code locally using DBConnect. You can use the following Python example code to check for a Spark session and create one if it does not exist. %python\r\n\r\nfrom pyspark.sql import SparkSession\r\nspark = SparkSession.builder.getOrCreate() Delete Warning DBConnect only works with supported Databricks Runtime versions. Ensure that you are using a supported runtime on your cluster before using DBConnect. Was this article helpful? (12) (11) Additional Informations Related Articles Databricks Connect reports version error with Databricks Runtime 6.4 Problem You are using the Databricks Connect client with Databricks Runtime 6.4 a... Failed to create process error with Databricks CLI in Windows Problem While trying to access the Databricks CLI (AWS | Azure | GCP) in Windows,... GeoSpark undefined function error with DBConnect Problem You are trying to use the GeoSpark function st_geofromwkt with DBConnect ... Get Apache Spark config in DBConnect You can always view the Spark configuration (AWS | Azure | GCP) for your cluster ... How to Sort S3 files By Modification Time in Databricks Notebooks Problem When you use the dbutils utility to list the files in a S3 location, the ... Related Articles Databricks Connect reports version error with Databricks Runtime 6.4 Problem You are using the Databricks Connect client with Databricks Runtime 6.4 a... Failed to create process error with Databricks CLI in Windows Problem While trying to access the Databricks CLI (AWS | Azure | GCP) in Windows,... GeoSpark undefined function error with DBConnect Problem You are trying to use the GeoSpark function st_geofromwkt with DBConnect ... Get Apache Spark config in DBConnect You can always view the Spark configuration (AWS | Azure | GCP) for your cluster ... How to Sort S3 files By Modification Time in Databricks Notebooks Problem When you use the dbutils utility to list the files in a S3 location, the ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Apache Spark session is null in DBConnect",
          "view_href" : "https://kb.databricks.com/en_US/dev-tools/dbconnect-spark-session-null"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/switch-s3-new-queue",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming How to switch a SNS streaming job to a new SQS queue How to switch a SNS streaming job to a new SQS queue Written by Adam Pavlacka Last published at: May 18th, 2022 Problem You have a Structured Streaming job running via the S3-SQS connector. Suppose you want to recreate the source SQS, backed by SNS data, and you want to proceed with a new queue to be processed in the same job and in the same output directory. Solution Use the following procedure: Create new SQS queues and subscribe to s3-events (from SNS). At this point, the same messages are in both the old and new queues. Set the option allowOverwrites to false in the new streaming job and start running it. Take an overlap of a short time interval greater than the trigger time and shut down the old job. Why does this work? With SQS Stream, Apache Spark maintains the file paths in the checkpoint directory. And if you set allowOverwrites to false (the default is true), one of the fetches will be discarded while running both queues simultaneously. In this event, files are not reprocessed and there aren’t any duplicates or loss of data. Was this article helpful? (6) (15) Additional Informations Related Articles How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Get the path of files consumed by Auto Loader When you process streaming files with Auto Loader (AWS | Azure | GCP), events are... How to restart a structured streaming query from last written offset Scenario You have a stream, running a windowed aggregation query, that reads from... Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... Related Articles How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Get the path of files consumed by Auto Loader When you process streaming files with Auto Loader (AWS | Azure | GCP), events are... How to restart a structured streaming query from last written offset Scenario You have a stream, running a windowed aggregation query, that reads from... Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to switch a SNS streaming job to a new SQS queue",
          "view_href" : "https://kb.databricks.com/en_US/streaming/switch-s3-new-queue"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/pattern-match-files-in-path",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Select files using a pattern match Select files using a pattern match Use a glob pattern match to select specific files in a folder. Written by mathan.pillai Last published at: May 23rd, 2022 When selecting files, a common requirement is to only read specific files from a folder. For example, if you are processing logs, you may want to read files from a specific month. Instead of enumerating each file and folder to find the desired files, you can use a glob pattern to match multiple files with a single expression. This article uses example patterns to show you how to read specific files from a sample list. Sample files Assume that the following files are located in the root folder. //root/1999.txt\r\n//root/2000.txt\r\n//root/2001.txt\r\n//root/2002.txt\r\n//root/2003.txt\r\n//root/2004.txt\r\n//root/2005.txt\r\n//root/2020/04.txt\r\n//root/2020/05.txt Glob patterns Asterisk * - The asterisk matches one or more characters. It is a wild card for multiple characters. This example matches all files with a .txt extension %scala\r\n\r\ndisplay(spark.read.format(\"text\").load(\"//root/*.txt\")) Question mark ? - The question mark matches a single character. It is a wild card that is limited to replacing a single character. This example matches all files from the root folder, except 1999.txt. It does not search the contents of the 2020 folder. %scala\r\n\r\ndisplay(spark.read.format(\"text\").load(\"//root/200?.txt\")) Character class [ab] - The character class matches a single character from the set. It is represented by the characters you want to match inside a set of brackets. This example matches all files with a 2 or 3 in place of the matched character. It returns 2002.txt and 2003.txt from the sample files. %scala\r\n\r\ndisplay(spark.read.format(\"text\").load(\"//root/200[23].txt\")) Negated character class [^ab] - The negated character class matches a single character that is not in the set. It is represented by the characters you want to exclude inside a set of brackets. This example matches all files except those with a 2 or 3 in place of the matched character. It returns 2000.txt, 2001.txt, 2004.txt, and 2005.txt from the sample files. %scala\r\n\r\ndisplay(spark.read.format(\"text\").load(\"//root/200[^23].txt\")) Character range [a-b] - The character class matches a single character in the range of values. It is represented by the range of characters you want to match inside a set of brackets. This example matches all files with a character within the search range in place of the matched character. It returns 2002.txt, 2003.txt, 2004.txt, and 2005.txt from the sample files. %scala\r\n\r\ndisplay(spark.read.format(\"text\").load(\"//root/200[2-5].txt\")) Negated character range [^a-b] - The negated character class matches a single character that is not in the range of values. It is represented by the range of characters you want to exclude inside a set of brackets. This example matches all files with a character outside the search range in place of the matched character. It returns 2000.txt and 2001.txt from the sample files. %scala\r\n\r\ndisplay(spark.read.format(\"text\").load(\"//root/200[^2-5].txt\")) Alternation {a,b} - Alternation matches either expression. It is represented by the expressions you want to match inside a set of curly brackets. This example matches all files with an expression that matches one of the two selected expressions. It returns 2004.txt and 2005.txt from the sample files. %scala\r\n\r\ndisplay(spark.read.format(\"text\").load(\"//root/20{04, 05}.txt\")) Was this article helpful? (6) (13) Additional Informations Related Articles Trouble reading external JDBC tables after upgrading from Databricks Runtime 5.5 Problem Attempting to read external tables via JDBC works fine on Databricks Runt... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Running C++ code in Scala Run C++ from Scala notebook Review the Run C++ from Scala notebook.... Cannot modify the value of an Apache Spark config Problem You are trying to SET the value of a Spark config in a notebook and get a... Related Articles Trouble reading external JDBC tables after upgrading from Databricks Runtime 5.5 Problem Attempting to read external tables via JDBC works fine on Databricks Runt... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Running C++ code in Scala Run C++ from Scala notebook Review the Run C++ from Scala notebook.... Cannot modify the value of an Apache Spark config Problem You are trying to SET the value of a Spark config in a notebook and get a... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Select files using a pattern match",
          "view_href" : "https://kb.databricks.com/en_US/scala/pattern-match-files-in-path"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/show-databases-unexpected-name",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark SHOW DATABASES command returns unexpected column name SHOW DATABASES command returns unexpected column name Running the `SHOW DATABASES` command returns an unexpected column name. Written by Jose Gonzalez Last published at: May 24th, 2022 Problem You are using the SHOW DATABASES command and it returns an unexpected column name. Cause The column name returned by the SHOW DATABASES command changed in Databricks Runtime 7.0. Databricks Runtime 6.4 Extended Support and below: SHOW DATABASES returns namespace as the column name. Databricks Runtime 7.0 and above: SHOW DATABASES returns databaseName as the column name. Solution You can enable legacy column naming by setting the property spark.sql.legacy.keepCommandOutputSchema to false in the cluster’s Spark config (AWS | Azure | GCP). Was this article helpful? (7) (15) Additional Informations Related Articles JDBC write fails with a PrimaryKeyViolation error Problem You are using JDBC to write to a SQL table that has primary key constrain... Duplicate columns in the metadata error Problem Your Apache Spark job is processing a Delta table when the job fails with... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Error when running MSCK REPAIR TABLE in parallel Problem You are trying to run MSCK REPAIR TABLE <table-name> commands for t... Related Articles JDBC write fails with a PrimaryKeyViolation error Problem You are using JDBC to write to a SQL table that has primary key constrain... Duplicate columns in the metadata error Problem Your Apache Spark job is processing a Delta table when the job fails with... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Error when running MSCK REPAIR TABLE in parallel Problem You are trying to run MSCK REPAIR TABLE <table-name> commands for t... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "SHOW DATABASES command returns unexpected column name",
          "view_href" : "https://kb.databricks.com/en_US/sql/show-databases-unexpected-name"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/identify-less-used-jobs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Identify less used jobs Identify less used jobs Identify rarely used and less used jobs in a workspace and delete them in order to stay under the job limit. Written by Adam Pavlacka Last published at: May 10th, 2022 The workspace has a limit on the number of jobs that can be shown in the UI. The current job limit is 1000. If you exceed the job limit, you receive a QUOTA_EXCEEDED error message. 'error_code':'QUOTA_EXCEEDED','message':'The quota for the number of jobs has been reached. The current quota is 1000. This quota is only applied to jobs created through the UI or through the /jobs/create endpoint, which are displayed in the Jobs UI To resolve this issue, you must delete jobs from the workspace UI. You can manually select jobs for removal, or you can use the Jobs Quota notebook to identify jobs which are not used very often and delete them. The Jobs Quota notebook identifies jobs for deletion based on: Jobs which have never run. Jobs that have not run in the past 14 days. Scheduled jobs that have been paused for 14 days or more. Jobs Quota notebook Review the Jobs Quota notebook. Import the Jobs Quota notebook You can import the Jobs Quota notebook directly into your workspace and attach the notebook to a cluster (AWS | Azure). Attach it to an all-purpose cluster. Run the Jobs Quota notebook Open the Jobs Quota notebook and click Run All to run the notebook. Review results After you run the Jobs Quota notebook, a data table is generated. This table displays information about all the running jobs in your workspace, such as the URL of a job, date_of_creation, created_by, last run (days from current time), and other job details. Delete Info If no other jobs are available in your workspace, the Jobs Quota notebook displays the message No Jobs To Display. Delete jobs You can delete the unneeded jobs manually. Pass the job_id of the job you want to delete to the function deleteByJobId() in the Jobs Quota notebook. A confirmation message is shown when a job is successfully deleted. An error message is shown if you attempt to delete a job that does not exist. Keep calling the deleteByJobId() function in the Jobs Quota notebook until all unneeded jobs are deleted. Was this article helpful? (7) (13) Additional Informations Related Articles How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... Related Articles How to delete all jobs using the REST API Run the following commands to delete all jobs in a Databricks workspace. Identify... Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Identify less used jobs",
          "view_href" : "https://kb.databricks.com/en_US/jobs/identify-less-used-jobs"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/cython",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Install and compile Cython Install and compile Cython Learn how to install and compile Cython with Databricks. Written by Adam Pavlacka Last published at: May 19th, 2022 This document explains how to run Spark code with compiled Cython code. The steps are as follows: Creates an example Cython module on DBFS (AWS | Azure). Adds the file to the Spark session. Creates a wrapper method to load the module on the executors. Runs the mapper on a sample dataset. Generate a larger dataset and compare the performance with native Python example. Delete Info By default, paths use dbfs:/ if no protocol is referenced. %python\r\n\r\n# Write an example cython module to /example/cython/fib.pyx in DBFS.\r\ndbutils.fs.put(\"/example/cython/fib.pyx\", \"\"\"\r\ndef fib_mapper_cython(n):\r\n    '''\r\n    Return the first fibonnaci number > n.\r\n    '''\r\n    cdef int a = 0\r\n    cdef int b = 1\r\n    cdef int j = int(n)\r\n    while b<j:\r\n        a, b  = b, a+b\r\n    return b, 1\r\n\"\"\", True)\r\n\r\n# Write an example input file to /example/cython/input.txt in DBFS.\r\n# Every line of this file is an integer.\r\ndbutils.fs.put(\"/example/cython_input/input.txt\", \"\"\"\r\n1\r\n10\r\n100\r\n\"\"\", True)\r\n\r\n# Take a look at the example input.\r\ndbutils.fs.head(\"/example/cython_input/input.txt\") Add Cython Source Files to Spark To make the Cython source files available across the cluster, we will use sc.addPyFile to add these files to Spark. For example, %python\r\n\r\nsc.addPyFile(\"dbfs:/example/cython/fib.pyx\") Test Cython compilation on the driver node This code will test compilation on the driver node first. %python\r\n\r\nimport pyximport\r\nimport os\r\n\r\npyximport.install()\r\nimport fib Define the wapper function to compile and import the module The print statements will get executed on the executor nodes. You can view the stdout log messages to track the progress of your module. %python\r\n\r\nimport sys, os, shutil, cython\r\n\r\ndef spark_cython(module, method):\r\n  def wrapped(*args, **kwargs):\r\n    print 'Entered function with: %s' % args\r\n    global cython_function_\r\n    try:\r\n      return cython_function_(*args, **kwargs)\r\n    except:\r\n      import pyximport\r\n      pyximport.install()\r\n      print 'Cython compilation complete'\r\n      cython_function_ = getattr(__import__(module), method)\r\n    print 'Defined function: %s' % cython_function_\r\n    return cython_function_(*args, **kwargs)\r\n  return wrapped Run the Cython example The below snippet runs the fibonacci example on a few data points. %python\r\n\r\n# use the CSV reader to generate a Spark DataFrame. Roll back to RDDs from DataFrames and grab the single element from the GenericRowObject\r\nlines = spark.read.csv(\"/example/cython_input/\").rdd.map(lambda y: y.__getitem__(0))\r\n\r\nmapper = spark_cython('fib', 'fib_mapper_cython')\r\nfib_frequency = lines.map(mapper).reduceByKey(lambda a, b: a+b).collect()\r\nprint fib_frequency Performance comparison Below we’ll test out the speed difference between the 2 implementations. We will use the spark.range() api to generate data points from 10,000 to 100,000,000 with 50 Spark partitions. We will write this output to DBFS as a CSV. For this test, disable autoscaling (AWS | Azure) in order to make sure the cluster has the fixed number of Spark executors. %python\r\n\r\ndbutils.fs.rm(\"/tmp/cython_input/\", True)\r\nspark.range(10000, 100000000, 1, 50).write.csv(\"/tmp/cython_input/\") Normal PySpark code %python\r\n\r\ndef fib_mapper_python(n):\r\n  a = 0\r\n  b = 1\r\n  print \"Trying: %s\" % n\r\n  while b < int(n):\r\n    a, b = b, a+b\r\n  return (b, 1)\r\n\r\nprint fib_mapper_python(2000)\r\n\r\nlines = spark.read.csv(\"/tmp/cython_input/\").rdd.map(lambda y: y.__getitem__(0))\r\nfib_frequency = lines.map(lambda x: fib_mapper_python(x)).reduceByKey(lambda a, b: a+b).collect()\r\nprint fib_frequency Test Cython code Now test the compiled Cython code. %python\r\n\r\nlines = spark.read.csv(\"/tmp/cython_input/\").rdd.map(lambda y: y.__getitem__(0))\r\nmapper = spark_cython('fib', 'fib_mapper_cython')\r\nfib_frequency = lines.map(mapper).reduceByKey(lambda a, b: a+b).collect()\r\nprint fib_frequency The test dataset we generated has 50 Spark partitions, which creates 50 csv files seen below. You can view the dataset with dbutils.fs.ls(\"/tmp/cython_input/\"). Was this article helpful? (14) (44) Additional Informations Related Articles Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... AttributeError: ‘function’ object has no attribute Problem You are selecting columns from a DataFrame and you get an error message. ... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... Related Articles Create a cluster with Conda Conda is a popular open source package management system for the Anaconda repo. D... Display file and directory timestamp details In this article we show you how to display detailed timestamps, including the dat... AttributeError: ‘function’ object has no attribute Problem You are selecting columns from a DataFrame and you get an error message. ... Convert Python datetime object to string There are multiple ways to display date and time values with Python, however not ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Install and compile Cython",
          "view_href" : "https://kb.databricks.com/en_US/python/cython"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/hyperopt-fail-maxnumconcurrenttasks",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning Hyperopt fails with maxNumConcurrentTasks error Hyperopt fails with maxNumConcurrentTasks error Do NOT install Hyperopt on a Databricks Runtime for Machine Learning cluster. Written by chetan.kardekar Last published at: May 16th, 2022 Problem You are tuning machine learning parameters using Hyperopt when your job fails with a py4j.Py4JException: Method maxNumConcurrentTasks([]) does not exist error. You are using a Databricks Runtime for Machine Learning (Databricks Runtime ML) cluster. Cause Databricks Runtime ML has a compatible version of Hyperopt pre-installed (AWS | Azure | GCP). If you manually install a second version of Hyperopt, it causes a conflict. Solution Do not install Hyperopt on Databricks Runtime ML clusters. Was this article helpful? (10) (11) Additional Informations Related Articles Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... How to speed up cross-validation Hyperparameter tuning of Apache SparkML models takes a very long time, depending ... Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... Error when importing OneHotEncoderEstimator Problem You have migrated a notebook from Databricks Runtime 6.4 for Machine Lear... Related Articles Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... How to speed up cross-validation Hyperparameter tuning of Apache SparkML models takes a very long time, depending ... Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... Error when importing OneHotEncoderEstimator Problem You have migrated a notebook from Databricks Runtime 6.4 for Machine Lear... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Hyperopt fails with maxNumConcurrentTasks error",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/hyperopt-fail-maxnumconcurrenttasks"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/delta-merge-cannot-resolve-field",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Delta Merge cannot resolve nested field Delta Merge cannot resolve nested field Delta Merge fails with a `Delta Merge cannot resolve 'field' due to data type mismatch` error message. Written by Adam Pavlacka Last published at: May 10th, 2022 Problem You are attempting a Delta Merge with automatic schema evolution, but it fails with a Delta Merge: cannot resolve 'field' due to data type mismatch error message. Cause This can happen if you have made changes to the nested column fields. For example, assume we have a column called Address with the fields streetName, houseNumber, and city nested inside. Attempting to add an additional field, or remove a field, causes any upcoming insert or update transaction on the table to fail, even if mergeSchema is true for the transaction. Solution This behavior is by design. The Delta automatic schema evolution feature only supports top level columns. Nested fields are not supported. Please review the Delta Lake Automatic schema evolution (AWS | Azure | GCP) documentation for more information. Was this article helpful? (9) (15) Additional Informations Related Articles Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... Converting from Parquet to Delta Lake fails Problem You are attempting to convert a Parquet file to a Delta Lake file. The di... Related Articles Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Cannot delete data using JDBC in Eclipse Problem You cannot delete data from a Delta table using JDBC from your local Ecli... Converting from Parquet to Delta Lake fails Problem You are attempting to convert a Parquet file to a Delta Lake file. The di... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Delta Merge cannot resolve nested field",
          "view_href" : "https://kb.databricks.com/en_US/delta/delta-merge-cannot-resolve-field"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/nbconvert-wrong-color-assert",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Python command fails with AssertionError: wrong color format Python command fails with AssertionError: wrong color format Resolve a wrong color format AssertionError caused by nbconvert when a Python command fails. Written by John.Lourdu Last published at: May 16th, 2022 Problem You run a Python notebook and it fails with an AssertionError: wrong color format message. An example stack trace:   File \"/local_disk0/tmp/1599775649524-0/PythonShell.py\", line 39, in <module>\r\n    from IPython.nbconvert.filters.ansi import ansi2html\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 963, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 902, in _find_spec\r\n  File \"<frozen importlib._bootstrap>\", line 876, in _find_spec_legacy\r\n  File \"/databricks/python/lib/python3.7/site-packages/IPython/utils/shimmodule.py\", line 36, in find_module\r\n    mod = import_item(mirror_name)\r\n  File \"/databricks/python/lib/python3.7/site-packages/IPython/utils/importstring.py\", line 31, in import_item\r\n    module = __import__(package, fromlist=[obj])\r\n  File \"/databricks/python/lib/python3.7/site-packages/nbconvert/__init__.py\", line 4, in <module>\r\n    from .exporters import *\r\n  File \"/databricks/python/lib/python3.7/site-packages/nbconvert/exporters/__init__.py\", line 4, in <module>\r\n    from .slides import SlidesExporter\r\n  File \"/databricks/python/lib/python3.7/site-packages/nbconvert/exporters/slides.py\", line 12, in <module>\r\n    from ..preprocessors.base import Preprocessor\r\n  File \"/databricks/python/lib/python3.7/site-packages/nbconvert/preprocessors/__init__.py\", line 7, in <module>\r\n    from .csshtmlheader import CSSHTMLHeaderPreprocessor\r\n  File \"/databricks/python/lib/python3.7/site-packages/nbconvert/preprocessors/csshtmlheader.py\", line 14, in <module>\r\n    from jupyterlab_pygments import JupyterStyle\r\n  File \"/databricks/python/lib/python3.7/site-packages/jupyterlab_pygments/__init__.py\", line 4, in <module>\r\n    from .style import JupyterStyle\r\n  File \"/databricks/python/lib/python3.7/site-packages/jupyterlab_pygments/style.py\", line 10, in <module>\r\n    class JupyterStyle(Style):\r\n  File \"/databricks/python/lib/python3.7/site-packages/pygments/style.py\", line 101, in __new__\r\n    ndef[0] = colorformat(styledef)\r\n  File \"/databricks/python/lib/python3.7/site-packages/pygments/style.py\", line 58, in colorformat\r\n    assert False, \"wrong color format %r\" % text\r\nAssertionError: wrong color format 'var(--jp-mirror-editor-variable-color)' Cause This is caused by an incompatible version of the nbconvert library. If you do not have nbconvert pinned to the correct version, it is possible to accidentally install an incompatible version via PyPI. Solution Manually install nbconvert version 6.0.0rc0 on the cluster. This overrides any incorrect version of the library that may have been installed. Click the clusters icon in the sidebar. Click the cluster name. Click the Libraries tab. Click Install New. In the Library Source button list, select PyPi. Enter nbconvert==6.0.0rc0 in the Package field. Click Install. Was this article helpful? (6) (13) Additional Informations Related Articles Install Turbodbc via init script Turbodbc is a Python module that uses the ODBC interface to access relational dat... Libraries fail with dependency exception Problem You have a Python function that is defined in a custom egg or wheel file ... Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Error when installing pyodbc on a cluster Problem One of the following errors occurs when you use pip to install the pyodbc... Related Articles Install Turbodbc via init script Turbodbc is a Python module that uses the ODBC interface to access relational dat... Libraries fail with dependency exception Problem You have a Python function that is defined in a custom egg or wheel file ... Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Error when installing pyodbc on a cluster Problem One of the following errors occurs when you use pip to install the pyodbc... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Python command fails with AssertionError: wrong color format",
          "view_href" : "https://kb.databricks.com/en_US/libraries/nbconvert-wrong-color-assert"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/json-reader-parses-value-as-null",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks JSON reader parses values as null JSON reader parses values as null When you read a JSON file, the Spark JSON reader returns null values instead of the actual data. Written by saritha.shivakumar Last published at: May 16th, 2022 Problem You are attempting to read a JSON file. You know the file has data in it, but the Apache Spark JSON reader is returning a null value. Example code You can use this example code to reproduce the problem. Create a test JSON file in DBFS. %python\r\n\r\ndbutils.fs.rm(\"dbfs:/tmp/json/parse_test.txt\")\r\ndbutils.fs.put(\"dbfs:/tmp/json/parse_test.txt\",\r\n\"\"\"\r\n{\"data_flow\":{\"upstream\":[{\"$\":{\"source\":\"input\"},\"cloud_type\":\"\"},{\"$\":{\"source\":\"File\"},\"cloud_type\":{\"azure\":\"cloud platform\",\"aws\":\"cloud service\"}}]}}\r\n\"\"\") Read the JSON file. %python\r\n\r\njsontest = spark.read.option(\"inferSchema\",\"true\").json(\"dbfs:/tmp/json/parse_test.txt\")\r\ndisplay(jsontest) The result is a null value. Cause In Spark 2.4 and below, the JSON parser allows empty strings. Only certain data types, such as IntegerType are treated as null when empty. In Spark 3.0 and above, the JSON parser does not allow empty strings. An exception is thrown for all data types, except BinaryType and StringType. For more information, review the Spark SQL Migration Guide. Example code The example JSON shows the error because the data has two identical classification fields. The first cloud_type entry is an empty string. The second cloud_type entry has data. \"cloud_type\":\"\"\r\n\"cloud_type\":{\"azure\":\"cloud platform\",\"aws\":\"cloud service\"} Because the JSON parser does not allow empty strings in Spark 3.0 and above, a null value is returned as output. Solution Set the Spark config (AWS | Azure | GCP) value spark.sql.legacy.json.allowEmptyString.enabled to True. This configures the Spark 3.0 JSON parser to allow empty strings. You can set this configuration at the cluster level or the notebook level. Example code %python\r\n\r\nspark.conf.set(\"spark.sql.legacy.json.allowEmptyString.enabled\", True)\r\njsontest1 = spark.read.option(\"inferSchema\",\"true\").json(\"dbfs:/tmp/json/parse_test.txt\")\r\ndisplay(jsontest1) Was this article helpful? (10) (18) Additional Informations Related Articles Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Enable s3cmd for notebooks s3cmd is a client library that allows you to perform all AWS S3 operations from a... How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... Related Articles Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Enable s3cmd for notebooks s3cmd is a client library that allows you to perform all AWS S3 operations from a... How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "JSON reader parses values as null",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/json-reader-parses-value-as-null"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/replay-cluster-spark-events",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Replay Apache Spark events in a cluster Replay Apache Spark events in a cluster Use a single node cluster to replay another cluster's event log in the Spark UI. Written by arjun.kaimaparambilrajan Last published at: March 4th, 2022 The Spark UI is commonly used as a debugging tool for Spark jobs. If the Spark UI is inaccessible, you can load the event logs in another cluster and use the Event Log Replay notebook to replay the Spark events. Delete Warning Cluster log delivery is not enabled by default. You must enable cluster log delivery before starting your cluster, otherwise there will be no logs to replay. Follow the documentation to configure Cluster log delivery on your cluster. The location of the cluster logs depends on the Cluster Log Path that you set during cluster configuration. For example, if the log path is dbfs:/cluster-logs, the log files for a specific cluster will be stored in dbfs:/cluster-logs/<cluster-name> and the individual event logs will be stored in dbfs:/cluster-logs/<cluster-name>/eventlog/<cluster-name-cluster-ip>/<log-id>/. Delete Note This example uses DBFS for cluster logs, but that is not a requirement. You can store cluster logs in DBFS or S3 storage. Confirm cluster logs exist Review the cluster log path and verify that logs are being written for your chosen cluster. Log files are written every five minutes. Launch a single node cluster Launch a single node cluster. You will replay the logs on this cluster. Select the instance type based on the size of the event logs that you want to replay. Run the Event Log Replay notebook Attach the Event Log Replay notebook to the single node cluster. Enter the path to your chosen cluster event logs in the event_log_path field in the notebook. Run the notebook. Event Log Replay notebook Open notebook in a new tab. Prevent items getting dropped from the UI If you have a long-running cluster, it is possible for some jobs and/or stages to get dropped from the Spark UI. This happens due to default UI limits that are intended to prevent the UI from using up too much memory and causing an out-of-memory error on the cluster. If you are using a single node cluster to replay the event logs, you can increase the default UI limits and devote more memory to the Spark UI. This prevents items from getting dropped. You can adjust these values during cluster creation by editing the Spark Config. This example contains the default values for these properties. spark.ui.retainedJobs 1000\r\nspark.ui.retainedStages 1000\r\nspark.ui.retainedTasks 100000\r\nspark.sql.ui.retainedExecutions 1000 Was this article helpful? (9) (14) Additional Informations Related Articles Failed to expand the EBS volume Problem Databricks jobs fail, due to a lack of space on the disk, even though sto... Multi-part upload failure Problem You observe a job failure with the exception: com.amazonaws.SdkClientExce... Failed to create cluster with invalid tag value Problem You are trying to create a cluster, but it is failing with an invalid tag... Custom Docker image requires root Problem You are trying to launch a Databricks cluster with a custom Docker contai... Related Articles Failed to expand the EBS volume Problem Databricks jobs fail, due to a lack of space on the disk, even though sto... Multi-part upload failure Problem You observe a job failure with the exception: com.amazonaws.SdkClientExce... Failed to create cluster with invalid tag value Problem You are trying to create a cluster, but it is failing with an invalid tag... Custom Docker image requires root Problem You are trying to launch a Databricks cluster with a custom Docker contai... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Replay Apache Spark events in a cluster",
          "view_href" : "https://kb.databricks.com/en_US/clusters/replay-cluster-spark-events"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/howto-jobsdeleterestapi",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs How to delete all jobs using the REST API How to delete all jobs using the REST API Learn how to delete all Databricks jobs using the REST API. Written by Adam Pavlacka Last published at: May 10th, 2022 Run the following commands to delete all jobs in a Databricks workspace. Identify the jobs to delete and list them in a text file: %sh\r\n\r\ncurl -X GET -u \"Bearer: <token>\" https://<databricks-instance>/api/2.0/jobs/list | grep -o -P 'job_id.{0,6}' | awk -F':' '{print $2}' >> job_id.txt Run the curlcommand in a loop to delete the identified jobs: %sh\r\n\r\nwhile read line\r\ndo\r\njob_id=$line\r\ncurl -X POST -u \"Bearer: <token>\" https://<databricks-instance>/api/2.0/jobs/delete -d '{\"job_id\": '\"$job_id\"'}'\r\ndone < job_id.txt Was this article helpful? (7) (14) Additional Informations Related Articles Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... Related Articles Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... Distinguish active and dead jobs Problem On clusters where there are too many concurrent jobs, you often see some ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to delete all jobs using the REST API",
          "view_href" : "https://kb.databricks.com/en_US/jobs/howto-jobsdeleterestapi"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/streaming/ss-read-from-last-offset",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Streaming How to restart a structured streaming query from last written offset How to restart a structured streaming query from last written offset Learn how to restart a structured streaming query from the last written offset. Written by Adam Pavlacka Last published at: May 18th, 2022 Scenario You have a stream, running a windowed aggregation query, that reads from Apache Kafka and writes files in Append mode. You want to upgrade the application and restart the query with the offset equal to the last written offset. You want to discard all state information that hasn’t been written to the sink, start processing from the earliest offsets that contributed to the discarded state, and modify the checkpoint directory accordingly. However, if you use existing checkpoints after upgrading the application code, old states and objects from the previous application version are re-used, which results in unexpected output such as reading from old sources or processing with old application code. Solution Apache Spark maintains state across the execution and binary objects on checkpoints. Therefore you cannot modify the checkpoint directory. As an alternative, copy and update the offset with the input records and store this in a file or a database. Read it during the initialization of the next restart and use the same value in readStream. Make sure to delete the checkpoint directory. You can get the current offsets by using asynchronous APIs: %scala\r\n\r\nspark.streams.addListener(new StreamingQueryListener() {\r\n    override def onQueryStarted(queryStarted: QueryStartedEvent): Unit = {\r\n        println(\"Query started:\" + queryStarted.id)\r\n    }\r\n    override def onQueryTerminated(queryTerminated: QueryTerminatedEvent): Unit = {\r\n        println(\"Query terminated\" + queryTerminated.id)\r\n    }\r\n    override def onQueryProgress(queryProgress: QueryProgressEvent): Unit = {\r\n     println(\"Query made progress\")\r\n        println(\"Starting offset:\" + queryProgress.progress.sources(0).startOffset)\r\n        println(\"Ending offset:\" + queryProgress.progress.sources(0).endOffset)\r\n        //Logic to save these offsets\r\n    }\r\n}) You can use readStream with the latest offset written by the process shown above: %scala\r\n\r\noption(\"startingOffsets\",  \"\"\" {\"articleA\":{\"0\":23,\"1\":-1},\"articleB\":{\"0\":-2}} \"\"\") The input schema for streaming records is: root\r\n|-- key: binary (nullable = true)\r\n|-- value: binary (nullable = true)\r\n|-- article: string (nullable = true)\r\n|-- partition: integer (nullable = true)\r\n|-- offset: long (nullable = true)\r\n|-- timestamp: timestamp (nullable = true)\r\n|-- timestampType: integer (nullable = true) Also, you can implement logic to save and update the offset to a database and read it at the next restart. Was this article helpful? (7) (16) Additional Informations Related Articles Handling partition column values while using an SQS queue as a streaming source Problem If data in S3 is stored by partition, the partition column values are use... How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... Related Articles Handling partition column values while using an SQS queue as a streaming source Problem If data in S3 is stored by partition, the partition column values are use... How to set up Apache Kafka on Databricks This article explains how to set up Apache Kafka on AWS EC2 machines and connect ... Streaming with File Sink: Problems with recovery if you change checkpoint or output directories When you stream data into a file sink, you should always change both checkpoint a... Append output is not supported without a watermark Problem You are performing an aggregation using append mode and an exception erro... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to restart a structured streaming query from last written offset",
          "view_href" : "https://kb.databricks.com/en_US/streaming/ss-read-from-last-offset"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/error-run-msck-repair-table-parallel",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark Error when running MSCK REPAIR TABLE in parallel Error when running MSCK REPAIR TABLE in parallel Do not run `MSCK REPAIR` commands in parallel. It results in a read timed out or out of memory error message. Written by ashritha.laxminarayana Last published at: May 23rd, 2022 Problem You are trying to run MSCK REPAIR TABLE <table-name> commands for the same table in parallel and are getting java.net.SocketTimeoutException: Read timed out or out of memory error messages. Cause When you try to add a large number of new partitions to a table with MSCK REPAIR in parallel, the Hive metastore becomes a limiting factor, as it can only add a few partitions per second. The greater the number of new partitions, the more likely that a query will fail with a java.net.SocketTimeoutException: Read timed out error or an out of memory error message. Solution You should not attempt to run multiple MSCK REPAIR TABLE <table-name> commands in parallel. Databricks uses multiple threads for a single MSCK REPAIR by default, which splits createPartitions() into batches. By limiting the number of partitions created, it prevents the Hive metastore from timing out or hitting an out of memory error. It also gathers the fast stats (number of files and the total size of files) in parallel, which avoids the bottleneck of listing the metastore files sequentially. This is controlled by spark.sql.gatherFastStats, which is enabled by default. Was this article helpful? (14) (8) Additional Informations Related Articles Error in SQL statement: AnalysisException: Table or view not found Problem When you try to query a table or view, you get this error: AnalysisExcept... Duplicate columns in the metadata error Problem Your Apache Spark job is processing a Delta table when the job fails with... Generate unique increasing numeric values This article shows you how to use Apache Spark functions to generate unique incre... Disable broadcast when query plan has BroadcastNestedLoopJoin This article explains how to disable broadcast when the query plan has BroadcastN... Related Articles Error in SQL statement: AnalysisException: Table or view not found Problem When you try to query a table or view, you get this error: AnalysisExcept... Duplicate columns in the metadata error Problem Your Apache Spark job is processing a Delta table when the job fails with... Generate unique increasing numeric values This article shows you how to use Apache Spark functions to generate unique incre... Disable broadcast when query plan has BroadcastNestedLoopJoin This article explains how to disable broadcast when the query plan has BroadcastN... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Error when running MSCK REPAIR TABLE in parallel",
          "view_href" : "https://kb.databricks.com/en_US/sql/error-run-msck-repair-table-parallel"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/python-command-cancelled",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Cluster cancels Python command execution due to library conflict Cluster cancels Python command execution due to library conflict Learn what to do when your Databricks cluster cancels Python command execution due to a library conflict. Written by Adam Pavlacka Last published at: May 19th, 2022 Problem The cluster returns Cancelled in a Python notebook. Notebooks in all other languages execute successfully on the same cluster. Cause When you install a conflicting version of a library, such as ipython, ipywidgets, numpy, scipy, or pandas to the PYTHONPATH, then the Python REPL can break, causing all commands to return Cancelled after 30 seconds. This also breaks %sh, the notebook macro that lets you enter shell scripts in Python notebook cells. Delete Info If you’ve recently installed a bokeh library on the cluster, the installation may have included an incompatible tornado library. See Cluster cancels Python command execution after installing Bokeh. If you’ve installed a numpy library, it may be incompatible. See Python command execution fails with AttributeError. Solution To solve this problem, do the following: Identify the conflicting library and uninstall it. Install the correct version of the library in a notebook or with a cluster-scoped init script. Identify the conflicting library Uninstall each library one at a time, and check if the Python REPL still breaks. If the REPL still breaks, reinstall the library you removed and remove the next one. When you find the library that causes the REPL to break, install the correct version of that library using one of the two methods below. You can also inspect the driver log (std.err) for the cluster (on the Cluster Configuration page) for a stack trace and error message that can help identify the library conflict. Install the correct library Do one of the following. Option 1: Install in a notebook using pip3 %sh \r\n\r\nsudo apt-get -y install python3-pip\r\n  pip3 install <library-name> Option 2: Install using a cluster-scoped init script Follow the steps below to create a cluster-scoped init script (AWS | Azure | GCP) that installs the correct version of the library. Replace <library-name> in the examples with the filename of the library to install. If the init script does not already exist, create a base directory to store it: %sh\r\n\r\ndbutils.fs.mkdirs(\"dbfs:/databricks/<directory>/\") Create the following script: %sh\r\n\r\ndbutils.fs.put(\"/databricks/init/cluster-name/<library-name>.sh\",\"\"\"\r\n #!/bin/bash\r\n sudo apt-get -y install python3-pip\r\n sudo pip3 install <library-name>\r\n \"\"\", True) Confirm that the script exists: %sh\r\n\r\ndisplay(dbutils.fs.ls(\"dbfs:/databricks/<directory>/<library-name>.sh\")) Go to the cluster configuration page (AWS | Azure | GCP) and click the Advanced Options toggle. At the bottom of the page, click the Init Scripts tab: In the Destination drop-down, select DBFS, provide the file path to the script, and click Add. Restart the cluster. Was this article helpful? (6) (12) Additional Informations Related Articles Use the HDFS API to read files in Python There may be times when you want to read files directly without using third party... Python commands fail on high concurrency clusters Problem You are attempting to run Python commands on a high concurrency cluster. ... Load special characters with Spark-XML Problem You have special characters in your source files and are using the OSS li... How to import a custom CA certificate When working with Python, you may want to import a custom CA certificate to avoid... Related Articles Use the HDFS API to read files in Python There may be times when you want to read files directly without using third party... Python commands fail on high concurrency clusters Problem You are attempting to run Python commands on a high concurrency cluster. ... Load special characters with Spark-XML Problem You have special characters in your source files and are using the OSS li... How to import a custom CA certificate When working with Python, you may want to import a custom CA certificate to avoid... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cluster cancels Python command execution due to library conflict",
          "view_href" : "https://kb.databricks.com/en_US/python/python-command-cancelled"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/cannot-import-timestamp-millis-unix-millis",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Cannot import timestamp_millis or unix_millis Cannot import timestamp_millis or unix_millis Cannot use timestamp_millis or unix_millis directly with a DataFrame. You must first use selectExpr() or use SQL commands. Written by saritha.shivakumar Last published at: May 20th, 2022 Problem You are trying to import timestamp_millis or unix_millis into a Scala notebook, but get an error message. %scala\r\n\r\nimport org.apache.spark.sql.functions.{timestamp_millis, unix_millis} error: value timestamp_millis is not a member of object org.apache.spark.sql.functions\r\nimport org.apache.spark.sql.functions.{timestamp_millis, unix_millis} Cause The functions timestamp_millis and unix_millis are not available in the Apache Spark DataFrame API. These functions are specific to SQL and are included in Spark 3.1.1 and above. Solution You need to use selectExpr() with timestamp_millis or unix_millis if you want to use either one of them with a DataFrame. selectExpr() takes a set of SQL expressions and runs them. For example, this sample code returns an error message when run. %scala\r\n\r\nimport sqlContext.implicits._\r\nval df = Seq(\r\n (1, \"First Value\"),\r\n (2, \"Second Value\")\r\n).toDF(\"int_column\", \"string_column\")\r\n\r\nimport org.apache.spark.sql.functions.{unix_millis}\r\nimport org.apache.spark.sql.functions.col\r\ndf.select(unix_millis(col(\"int_column\"))).show() error: value unix_millis is not a member of object org.apache.spark.sql.functions\r\nimport org.apache.spark.sql.functions.{unix_millis} While this sample code, using selectExpr(), successfully returns timestamp values. %scala\r\n\r\nimport org.apache.spark.sql.functions._\r\nimport sqlContext.implicits._\r\nval ndf = Seq(\r\n (1, \"First Value\"),\r\n (2, \"Second Value\")\r\n).toDF(\"int_column\", \"string_column\")\r\n\r\ndisplay(ndf.selectExpr(\"timestamp_millis(int_column)\")) Example notebook Review the Cannot import timestamp_millis or unix_millis example notebook. Was this article helpful? (9) (25) Additional Informations Related Articles Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Best practice for cache(), count(), and take() cache() is an Apache Spark transformation that can be used on a DataFrame, Datase... Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Related Articles Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Best practice for cache(), count(), and take() cache() is an Apache Spark transformation that can be used on a DataFrame, Datase... Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cannot import timestamp_millis or unix_millis",
          "view_href" : "https://kb.databricks.com/en_US/scala/cannot-import-timestamp-millis-unix-millis"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/metastore/data-too-long-for-column",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Metastore Data too long for column error Data too long for column error If a column exceeds 4000 characters it is too big for the default datatype and returns an error. Written by Adam Pavlacka Last published at: May 16th, 2022 Problem You are trying to insert a struct into a table, but you get a java.sql.SQLException: Data too long for column error. Caused by: java.sql.SQLException: Data too long for column 'TYPE_NAME' at row 1\r\nQuery is: INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,`COLUMN_NAME`,TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) , parameters [103182,<null>,'address','struct<street_address1:struct<street_number:int,street_name:string,street_type:string,country:string,postal_code:string>,street_address2:struct<street_number:int,street_name:string,street_type:string,country:string,postal_code:string>,street_address3:struct<street_number:int,street_name:string,street_type:string,country:string,postal_code:string>,street_address4:struct<street_number:int,street_name:string,street_type:string,country:string,postal_code:string>,street_address5:struct<street_number:int,street_name:string,street_type:string,country:string,postal_code:string>,street_address6:struct<street_number:int,street_name:string,street_type:string,country:string,postal_code:string>,street_address7:struct<street_number:int,street_name:string,street_type:string,country:string,postal_code:string>,street_address8:struct<street_number:int,street_name:string,street_type:string,coun...\r\n        at org.mariadb.jdbc.internal.util.LogQueryTool.exceptionWithQuery(LogQueryTool.java:153)\r\n        at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.executeQuery(AbstractQueryProtocol.java:255)\r\n        at org.mariadb.jdbc.MariaDbPreparedStatementClient.executeInternalBatch(MariaDbPreparedStatementClient.java:368)\r\n        at org.mariadb.jdbc.MariaDbPreparedStatementClient.executeBatch(MariaDbPreparedStatementClient.java:280) Cause The root cause of this issue is a default datatype of varchar(4000) for the column in your table. If you have a deeply nested struct that exceeds more than 4000 characters in length, it exceeds the size of the default datatype and results in an error message. You can validate this by describing the column you are trying to insert the data into. It will return a datatype of varchar(4000). Solution You should use an external metastore if you are going to exceed 4000 characters within a column. The default datatype for the Databricks Hive metastore is varchar(4000) and cannot be changed. When you use an external metastore, you have full control over the length of column and database names. You also have control over the collation of column names, database names, and table names. Review the external Apache Hive metastore (AWS | Azure | GCP) documentation to learn how to setup an external metastore. Once the metastore tables have been created, you can directly modify the column data types by using the ALTER TABLE command. Was this article helpful? (13) (10) Additional Informations Related Articles Autoscaling is slow with an external metastore Problem You have an external metastore configured on your cluster and autoscaling... Related Articles Autoscaling is slow with an external metastore Problem You have an external metastore configured on your cluster and autoscaling... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Data too long for column error",
          "view_href" : "https://kb.databricks.com/en_US/metastore/data-too-long-for-column"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/object-lock-error-write-delta-s3",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Object lock error when writing Delta Lake tables to S3 Object lock error when writing Delta Lake tables to S3 Delta Lake does not support S3 buckets with object lock enabled. com.amazonaws.services.s3.model.AmazonS3Exception Written by ashritha.laxminarayana Last published at: May 10th, 2022 Problem You are trying to perform a Delta write operation to a S3 bucket and get an error message. com.amazonaws.services.s3.model.AmazonS3Exception: Content-MD5 HTTP header is required for Put Part requests with Object Lock parameters Cause Delta Lake does not support S3 buckets with object lock enabled. Solution You should use an S3 bucket that does not have object lock enabled. For more information, please review the AWS documentation on S3 object lock. Was this article helpful? (7) (11) Additional Informations Related Articles Delta Lake write job fails with java.lang.UnsupportedOperationException Problem Delta Lake write jobs sometimes fail with the following exception: java.l... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... How to populate or update columns in an existing Delta table Problem You have an existing Delta table, with a few empty columns. You need to p... Best practices for dropping a managed Delta Lake table Regardless of how you drop a managed table, it can take a significant amount of t... Related Articles Delta Lake write job fails with java.lang.UnsupportedOperationException Problem Delta Lake write jobs sometimes fail with the following exception: java.l... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... How to populate or update columns in an existing Delta table Problem You have an existing Delta table, with a few empty columns. You need to p... Best practices for dropping a managed Delta Lake table Regardless of how you drop a managed table, it can take a significant amount of t... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Object lock error when writing Delta Lake tables to S3",
          "view_href" : "https://kb.databricks.com/en_US/delta/object-lock-error-write-delta-s3"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dev-tools/terraform-registry-does-not-have-a-provider-error",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Developer tools Terraform registry does not have a provider error Terraform registry does not have a provider error You cannot install the Databricks Terraform provider if the required_providers block is not defined in your modules. Written by prabakar.ammeappin Last published at: July 1st, 2022 Problem You are installing the Databricks Terraform provider (AWS | Azure | GCP) and get a Databricks provider registry error. Error while installing hashicorp/databricks: provider registry\r\nregistry.terraform.io does not have a provider named\r\nregistry.terraform.io/hashicorp/databricks Cause This error occurs when the required_providers block is not defined in every module that uses the Databricks Terraform provider. Solution Create a versions.tf file with the following contents: # versions.tf\r\nterraform {\r\n  required_providers {\r\n    databricks = {\r\n      source  = \"databricks/databricks\"\r\n      version = \"1.0.0\"\r\n    }\r\n  }\r\n} Save a copy of this version.tf file in every module in the environments level of your code base. Remove the version field from the versions.tf file and save a copy of the updated file in every module in the modules level of your code base. For example: ├── environments\r\n│   ├── sandbox\r\n│   │   ├── README.md\r\n│   │   ├── main.tf\r\n│   │   └── versions.tf   // This file contains the \"version\" field.\r\n│   └── production\r\n│       ├── README.md\r\n│       ├── main.tf\r\n│       └── versions.tf   // This file contains the \"version\" field.\r\n└── modules\r\n    ├── first-module\r\n    │   ├── ...\r\n    │   └── versions.tf   // This file does NOT contain the \"version\" field.\r\n    └── second-module\r\n        ├── ...\r\n        └── versions.tf   // This file does NOT contain the \"version\" field. Review the Requiring providers Terraform documentation for more information. Was this article helpful? (4) (6) Additional Informations Related Articles Failed credential validation checks error with Terraform Problem You are using Terraform to deploy a workspace in AWS and you get a Failed... Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... ProtoSerializer stack overflow error in DBConnect Problem You are using DBConnect (AWS | Azure | GCP) to run a PySpark transformati... Use tcpdump to create pcap files If you want to analyze the network traffic between nodes on a specific cluster, y... Related Articles Failed credential validation checks error with Terraform Problem You are using Terraform to deploy a workspace in AWS and you get a Failed... Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... ProtoSerializer stack overflow error in DBConnect Problem You are using DBConnect (AWS | Azure | GCP) to run a PySpark transformati... Use tcpdump to create pcap files If you want to analyze the network traffic between nodes on a specific cluster, y... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Terraform registry does not have a provider error",
          "view_href" : "https://kb.databricks.com/en_US/dev-tools/terraform-registry-does-not-have-a-provider-error"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/fail-create-cluster-tag-limit",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Failed to create cluster with invalid tag value Failed to create cluster with invalid tag value Cluster creation fails if optional tag values do not conform to cloud vendor requirements. Written by kavya.parag Last published at: March 4th, 2022 Problem You are trying to create a cluster, but it is failing with an invalid tag value error message. System.Exception: Content={\"error_code\":\"INVALID_PARAMETER_VALUE\",\"message\":\"\\nInvalid tag value (<<<<TAG-VALUE>>>>) - the length cannot exceed 256\\nUnicode characters in UTF-8.\\n \"} Cause Limitations on tag Key and Value are set by the cloud provider. AWS AWS tag keys must: Contain 1-127 characters Contain letters, spaces, numbers, or the characters + - = . _ : / @ Not start with aws: Not duplicate an existing key AWS tag values must: Contain 1-255 characters Contain letters, spaces, numbers, or the characters + - = . _ : / @ Not start with aws: For more information, please refer to the AWS tag naming limits and requirements documentation. Delete Azure Azure tag keys must: Contain 1-512 characters Contain letters, numbers, spaces (except < > * % & : \\ ? / + ) Not start with azure, microsoft, or windows Not duplicate an existing key Azure tag values must: Contain 1-256 characters Contain letters, numbers, spaces (except < > * % & : \\ ? / + ) Not start with azure, microsoft, or windows For more information, please refer to the Azure tag resource limitations documentation. Delete GCP Google Cloud tag keys must: Contain 1-63 characters Contain letters, numbers, or the characters - _ . Not duplicate an existing key Google Cloud tag values must: Contain 1-63 characters Contain letters, numbers, or the characters - _ . For more information, please refer to the Google Cloud requirements for labels documentation. Delete Solution Databricks can not modify these limits. Requests to update any limits on tagging must be made directly with the cloud provider support team. Was this article helpful? (10) (13) Additional Informations Related Articles Cluster Apache Spark configuration not applied Problem Your cluster’s Spark configuration values are not applied. Cause This hap... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Custom Docker image requires root Problem You are trying to launch a Databricks cluster with a custom Docker contai... Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... Related Articles Cluster Apache Spark configuration not applied Problem Your cluster’s Spark configuration values are not applied. Cause This hap... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Custom Docker image requires root Problem You are trying to launch a Databricks cluster with a custom Docker contai... Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Failed to create cluster with invalid tag value",
          "view_href" : "https://kb.databricks.com/en_US/clusters/fail-create-cluster-tag-limit"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/job-fails-invalid-access-token",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Job fails with invalid access token Job fails with invalid access token Jobs that run more than 48 hours fail with invalid access token error when the dbutils token expires. Written by manjunath.swamy Last published at: May 11th, 2022 Problem Long running jobs, such as streaming jobs, fail after 48 hours when using dbutils.secrets.get() (AWS | Azure | GCP). For example: %python\r\n\r\nstreamingInputDF1 = (\r\n     spark\r\n    .readStream                       \r\n    .format(\"delta\")               \r\n    .table(\"default.delta_sorce\")\r\n  )\r\n\r\n\r\ndef writeIntodelta(batchDF, batchId):\r\n  table_name = dbutils.secrets.get(\"secret1\",\"table_name\")\r\n  batchDF = batchDF.drop_duplicates()\r\n  batchDF.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\r\n\r\n\r\nstreamingInputDF1 \\\r\n  .writeStream \\\r\n  .format(\"delta\") \\\r\n  .option(\"checkpointLocation\", \"dbfs:/tmp/delta_to_delta\") \\\r\n  .foreachBatch(writeIntodelta) \\\r\n  .outputMode(\"append\") \\\r\n  .start() This example code returns an error after 48 hours. <head>\r\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\"/>\r\n<title>Error 403 Invalid access token.</title>\r\n</head>\r\n<body><h2>HTTP ERROR 403</h2>\r\n<p>Problem accessing /api/2.0/secrets/get. Reason:\r\n<pre>    Invalid access token.</pre></p>\r\n</body> Cause Databricks Utilities (dbutils) (AWS | Azure | GCP) tokens expire after 48 hours. This is by design. Solution You cannot extend the life of a token. Jobs that take more than 48 hours to complete should not use dbutils.secrets.get(). Was this article helpful? (9) (12) Additional Informations Related Articles Apache Spark job fails with a Connection pool shut down error Problem A Spark job fails with the error message java.lang.IllegalStateException:... Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... Job fails due to job rate limit Problem A Databricks notebook or Jobs API request returns the following error: Er... Apache Spark Jobs hang due to non-deterministic custom UDF Problem Sometimes Apache Spark jobs hang indefinitely due to the non-deterministi... Related Articles Apache Spark job fails with a Connection pool shut down error Problem A Spark job fails with the error message java.lang.IllegalStateException:... Spark job fails with Driver is temporarily unavailable Problem A Databricks notebook returns the following error: Driver is temporarily ... Job fails due to job rate limit Problem A Databricks notebook or Jobs API request returns the following error: Er... Apache Spark Jobs hang due to non-deterministic custom UDF Problem Sometimes Apache Spark jobs hang indefinitely due to the non-deterministi... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Job fails with invalid access token",
          "view_href" : "https://kb.databricks.com/en_US/jobs/job-fails-invalid-access-token"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/cluster-manager-limit",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Job fails due to cluster manager core instance request limit Job fails due to cluster manager core instance request limit Learn how to troubleshoot Databricks errors related to API rate limits. Written by Adam Pavlacka Last published at: March 4th, 2022 Problem A Databricks Notebook or Job API returns the following error: Unexpected failure while creating the cluster for the job. Cause REQUEST_LIMIT_EXCEEDED: Your request was rejected due to API rate limit. Please retry your request later, or choose a larger node type instead. Cause The error indicates the Cluster Manager Service core instance request limit was exceeded. A Cluster Manager core instance can support a maximum of 1000 requests. Solution Contact Databricks Support to increase the limit set in the core instance. Databricks can increase the job limit maxBurstyUpsizePerOrg up to 2000, and upsizeTokenRefillRatePerMin up to 120. Current running jobs are affected when the limit is increased. Increasing these values can stop the throttling issue, but can also cause high CPU utilization. The best solution for this issue is to replace the Cluster Manager core instance with a larger instance that can support maximum data transmission rates. Databricks Support can change the current Cluster Manager instance type to a larger one. Was this article helpful? (11) (13) Additional Informations Related Articles Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... Cluster failed to launch This article describes several scenarios in which a cluster fails to launch, and ... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Related Articles Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... Cluster failed to launch This article describes several scenarios in which a cluster fails to launch, and ... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Job fails due to cluster manager core instance request limit",
          "view_href" : "https://kb.databricks.com/en_US/clusters/cluster-manager-limit"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/metastore/parquet-timestamp-requires-msver12",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Metastore Parquet timestamp requires Hive metastore 1.2 or above Parquet timestamp requires Hive metastore 1.2 or above Update the Hive metastore to version 1.2 or above to use TIMESTAMP with a Parquet table. Written by rakesh.parija Last published at: May 16th, 2022 Problem You are trying to create a Parquet table using TIMESTAMP, but you get an error message. Error in SQL statement: QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.UnsupportedOperationException: Parquet does not support timestamp. See HIVE-6384 Example code %sql\r\n\r\nCREATE EXTERNAL TABLE IF NOT EXISTS testTable (\r\n  emp_name STRING,\r\n  joing_datetime TIMESTAMP,\r\n)\r\nPARTITIONED BY\r\n  (date DATE)\r\nSTORED AS\r\n  PARQUET\r\nLOCATION\r\n  \"/mnt/<path-to-data>/emp.testTable\" Cause Parquet requires a Hive metastore version of 1.2 or above in order to use TIMESTAMP. Delete Info The default Hive metastore client version used in Databricks Runtime is 0.13.0. Solution You must upgrade the Hive metastore client on the cluster. You can do this by adding the following settings to the cluster’s Spark config (AWS | Azure | GCP). Databricks Runtime 6.6 and below spark.sql.hive.metastore.version 1.2.1\r\nspark.sql.hive.metastore.jars builtin Databricks Runtime 7.0 and above spark.sql.hive.metastore.jars /dbfs <path-to-downloaded-jars>\r\nspark.sql.hive.metastore.version 1.2.1 Delete Info For Databricks Runtime 7.0 and above you must download the metastore jars and point to them (AWS | Azure | GCP) as detailed in the Databricks documentation. Was this article helpful? (5) (15) Additional Informations Related Articles How to set up an embedded Apache Hive metastore You can set up a Databricks cluster to use an embedded metastore. You can use an ... AnalysisException when dropping table on Azure-backed metastore Problem When you try to drop a table in an external Hive version 2.0 or 2.1 metas... How to troubleshoot several Apache Hive metastore problems Problem 1: External metastore tables not available When you inspect the driver lo... How to create table DDLs to import into an external metastore Databricks supports using external metastores instead of the default Hive metasto... Related Articles How to set up an embedded Apache Hive metastore You can set up a Databricks cluster to use an embedded metastore. You can use an ... AnalysisException when dropping table on Azure-backed metastore Problem When you try to drop a table in an external Hive version 2.0 or 2.1 metas... How to troubleshoot several Apache Hive metastore problems Problem 1: External metastore tables not available When you inspect the driver lo... How to create table DDLs to import into an external metastore Databricks supports using external metastores instead of the default Hive metasto... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Parquet timestamp requires Hive metastore 1.2 or above",
          "view_href" : "https://kb.databricks.com/en_US/metastore/parquet-timestamp-requires-msver12"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/r/sparkr-gapply",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles R with Apache Spark How to parallelize R code with gapply How to parallelize R code with gapply Learn how to parallelize R code using gapply. Written by Adam Pavlacka Last published at: May 20th, 2022 Parallelization of R code is difficult, because R code runs on the driver and R data.frames are not distributed. Often, there is existing R code that is run locally and that is converted to run on Apache Spark. In other cases, some SparkR functions used for advanced statistical analysis and machine learning techniques may not support distributed computing. In such cases, the SparkR UDF API can be used to distribute the desired workload across a cluster. Example use case: You want to train a machine learning model on subsets of a data set, grouped by a key. If the subsets of the data fit on the workers, it may be more efficient to use the SparkR UDF API to train multiple models at once. The gapply and gapplyCollect functions apply a function to each group in a Spark DataFrame. For each group in a Spark DataFrame: Collect each group as an R data.frame. Send the function to the worker and execute. Return the result to the driver as specified by the schema. Delete Info When you call gapply, you must specify the output schema. With gapplyCollect, the result is collected to the driver using an R data.frame for the output. In the following example, a separate support vector machine model is fit on the airquality data for each month. The output is a data.frame with the resulting MSE for each month, shown both with and without specifying the schema. %r\r\n\r\ndf <- createDataFrame(na.omit(airquality))\r\n\r\nschema <- structType(\r\n  structField(\"Month\", \"MSE\"),\r\n  structField(\"integer\", \"Number\"))\r\n\r\nresult <- gapply(df, c(\"Month\"), function(key, x) {\r\n  library(e1071)\r\n  data.frame(month = key, mse = svm(Ozone ~ ., x, cross = 3)$tot.MSE)\r\n}, schema) %r\r\n\r\ndf <- createDataFrame(na.omit(airquality))\r\n\r\ngapplyCollect(df, c(\"Month\"), function(key, x) {\r\n  library(e1071)\r\n  y <- data.frame(month = key, mse = svm(Ozone ~ ., x, cross = 3)$tot.MSE)\r\n names(y) <- c(\"Month\", \"MSE\")\r\n  y\r\n}) Delete Info Start with a Spark DataFrame and install packages on all workers. Was this article helpful? (8) (11) Additional Informations Related Articles Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... Fix the version of R packages When you use the install.packages() function to install CRAN packages, you cannot... Related Articles Change version of R (r-base) These instructions describe how to install a different version of R (r-base) on a... Fix the version of R packages When you use the install.packages() function to install CRAN packages, you cannot... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to parallelize R code with gapply",
          "view_href" : "https://kb.databricks.com/en_US/r/sparkr-gapply"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/cloud/security-vulns-db-ec2-instances",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Cloud infrastructure Vulnerability scan shows vulnerabilities in Databricks EC2 instances Vulnerability scan shows vulnerabilities in Databricks EC2 instances Learn how to resolve vulnerabilities in long-running Databricks EC2 instances. Written by Adam Pavlacka Last published at: February 25th, 2022 Problem The Corporate Information Security (CIS) Vulnerability Management team identifies vulnerabilities in AWS instances that are traced to EC2 instances created by Databricks (worker AMI). Cause The Databricks security team addresses all critical vulnerabilities and updates the core and worker AMIs on a regular basis. However, if there are long-running clusters that have not been restarted, the newest AMIs don’t get picked up by the cluster EC2 instances. Therefore, a scan might find vulnerabilities. Solution Restart long-running clusters or start a brand new cluster when you do vulnerability scans against specific Databricks worker EC2 instances. Was this article helpful? (10) (12) Additional Informations Related Articles AWS services fail with No region provided error Problem Your code snippets that use AWS services fail with a java.lang.IllegalArg... Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... Related Articles AWS services fail with No region provided error Problem Your code snippets that use AWS services fail with a java.lang.IllegalArg... Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Vulnerability scan shows vulnerabilities in Databricks EC2 instances",
          "view_href" : "https://kb.databricks.com/en_US/cloud/security-vulns-db-ec2-instances"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/set-executor-log-level",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Set executor log level Set executor log level Learn how to set the log levels on Databricks executors. Written by Adam Pavlacka Last published at: March 4th, 2022 Delete Warning This article describes steps related to customer use of Log4j 1.x within a Databricks cluster. Log4j 1.x is no longer maintained and has three known CVEs (CVE-2021-4104, CVE-2020-9488, and CVE-2019-17571). If your code uses one of the affected classes (JMSAppender or SocketServer), your use may potentially be impacted by these vulnerabilities. To set the log level on all executors, you must set it inside the JVM on each worker. For example: %scala\r\n\r\nsc.parallelize(Seq(\"\")).foreachPartition(x => {\r\n  import org.apache.log4j.{LogManager, Level}\r\n  import org.apache.commons.logging.LogFactory\r\n\r\n  LogManager.getRootLogger().setLevel(Level.DEBUG)\r\n  val log = LogFactory.getLog(\"EXECUTOR-LOG:\")\r\n  log.debug(\"START EXECUTOR DEBUG LOG LEVEL\")\r\n}) To verify that the level is set, navigate to the Spark UI, select the Executors tab, and open the stderr log for any executor: Was this article helpful? (14) (11) Additional Informations Related Articles Replay Apache Spark events in a cluster The Spark UI is commonly used as a debugging tool for Spark jobs. If the Spark UI... Cluster fails to start with dummy does not exist error Problem You try to start a cluster, but it fails to start. You get an Apache Spar... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Log delivery fails with AssumeRole Problem You are using AssumeRole to send cluster logs to a S3 bucket in another a... Related Articles Replay Apache Spark events in a cluster The Spark UI is commonly used as a debugging tool for Spark jobs. If the Spark UI... Cluster fails to start with dummy does not exist error Problem You try to start a cluster, but it fails to start. You get an Apache Spar... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Log delivery fails with AssumeRole Problem You are using AssumeRole to send cluster logs to a S3 bucket in another a... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Set executor log level",
          "view_href" : "https://kb.databricks.com/en_US/clusters/set-executor-log-level"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/writing-with-dataframe-api-aws",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management Access denied when writing to an S3 bucket using RDD Access denied when writing to an S3 bucket using RDD Learn how to resolve an access denied error when writing to an S3 bucket using RDD. Written by Adam Pavlacka Last published at: May 31st, 2022 Problem Writing to an S3 bucket using RDDs fails. The driver node can write, but the worker (executor) node returns an access denied error. Writing with the DataFrame API, however works fine. For example, let’s say you run the following code: %scala\r\n\r\nimport java.io.File\r\nimport java.io.Serializable\r\nimport org.apache.spark.{SparkConf, SparkContext}\r\nimport org.apache.hadoop.fs.{FileSystem, Path}\r\nimport org.apache.hadoop.conf.Configuration\r\nimport java.net.URI\r\nimport scala.collection.mutable\r\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\r\nimport org.apache.spark.streaming.dstream.InputDStream\r\n\r\nval ssc = new StreamingContext(sc, Seconds(10))\r\nval rdd1 = sc.parallelize(Seq(1,2))\r\nval rdd2 = sc.parallelize(Seq(3,4))\r\nval inputStream = ssc.queueStream[Int](mutable.Queue(rdd1,rdd2))\r\nval result = inputStream.map(x => x*x)\r\nval count = result.foreachRDD { rdd =>\r\nval config = new Configuration(sc.hadoopConfiguration) with Serializable\r\n rdd.mapPartitions {\r\n   _.map { entry =>\r\n       val fs = FileSystem.get(URI.create(\"s3://dx.lz.company.fldr.dev/part_0000000-123\"), config)\r\n       val path = new Path(\"s3://dx.lz.company.fldr.dev/part_0000000-123\")\r\n       val file = fs.create(path)\r\n       file.write(\"foobar\".getBytes)\r\n       file.close()\r\n   }\r\n }.count()\r\n}\r\n\r\nprintln(s\"Count is $count\")\r\n\r\nssc.start() The following error is returned: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 0.0 failed 4 times, most recent failure:\r\nLost task 3.3 in stage 0.0 (TID 7, 10.205.244.228, executor 0): java.rmi.RemoteException: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied; Request ID: F81ADFACBCDFE626,\r\nExtended Request ID: 1DNcBUHsmUFFI9a1lz0yGt4dnRjdY5V3C+J/DiEeg8Z4tMOLphZwW2U+sdxmr8fluQZ1R/3BCep, Cause When you write to the worker node using RDD, the IAM policy denies access if you use Serializable, as in val config = new Configuration(sc.hadoopConfiguration) with Serializable. Solution There are two ways to solve this problem: Option 1: Use DataFrames %scala\r\n\r\ndbutils.fs.put(\"s3a://dx.lz.company.fldr.dev/test-gopi/test0.txt\", \"foobar\")\r\nval df = spark.read.text(\"s3a://dx.lz.company.fldr.dev/test-gopi/test0.txt\")\r\ndf.write.text(\"s3a://dx.lz.company.fldr.dev/test-gopi/text1.txt\")\r\nval df1 = spark.read.text(\"s3a://dx.lz.company.fldr.dev/test-gopi/text1.txt\") Option 2: Use SerializableConfiguration If you want to use RDDs, use: %scala\r\n\r\nval config = sc.broadcast(new SerializableConfiguration(sc.hadoopConfiguration)) For example: %scala\r\n\r\nimport java.io.File\r\nimport java.io.Serializable\r\nimport org.apache.spark.{SparkConf, SparkContext}\r\nimport org.apache.hadoop.fs.{FileSystem, Path}\r\nimport org.apache.hadoop.conf.Configuration\r\nimport java.net.URI\r\nimport scala.collection.mutable\r\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\r\nimport org.apache.spark.streaming.dstream.InputDStream\r\nimport org.apache.spark.util.SerializableConfiguration\r\n\r\nval ssc = new StreamingContext(sc, Seconds(10))\r\nval rdd1 = sc.parallelize(Seq(1,2))\r\nval rdd2 = sc.parallelize(Seq(3,4))\r\nval inputStream = ssc.queueStream[Int](mutable.Queue(rdd1,rdd2))\r\nval result = inputStream.map(x => x*x)\r\nval count = result.foreachRDD { rdd =>\r\n//val config = new Configuration(sc.hadoopConfiguration) with Serializable\r\nval config = sc.broadcast(new SerializableConfiguration(sc.hadoopConfiguration))\r\nrdd.mapPartitions {\r\n   _.map { entry =>\r\n       val fs = FileSystem.get(URI.create(\"s3://pathpart_0000000-123\"), config.value.value)\r\n       val path = new Path(\"s3:///path/part_0000000-123\")\r\n       val file = fs.create(path)\r\n       file.write(\"foobar\".getBytes)\r\n       file.close()\r\n   }\r\n }.count()\r\n}\r\n\r\nprintln(s\"Count is $count\")\r\n\r\nssc.start() Was this article helpful? (7) (15) Additional Informations Related Articles How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... Behavior of the randomSplit method When using randomSplit on a DataFrame, you could potentially observe inconsistent... Generate schema from case class Spark provides an easy way to generate a schema from a Scala case class. For case... Related Articles How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... Behavior of the randomSplit method When using randomSplit on a DataFrame, you could potentially observe inconsistent... Generate schema from case class Spark provides an easy way to generate a schema from a Scala case class. For case... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Access denied when writing to an S3 bucket using RDD",
          "view_href" : "https://kb.databricks.com/en_US/data/writing-with-dataframe-api-aws"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dev-tools/dbcli-win-fail-create-process",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Developer tools Failed to create process error with Databricks CLI in Windows Failed to create process error with Databricks CLI in Windows Databricks CLI may not work correctly in Windows if your Python path has a space in it. Written by John.Lourdu Last published at: May 9th, 2022 Problem While trying to access the Databricks CLI (AWS | Azure | GCP) in Windows, you get a failed to create process error message. Cause This can happen: If multiple instances of the Databricks CLI are installed on the system. If the Python path on your Windows system includes a space. Delete Info There is a known issue in pip which causes pip installed software to fail if there is a space in your Python path. Solution Ensure that you do not have multiple instances of the Databricks CLI installed by running where databricks. If you do have multiple instances installed, delete all instances except the one in the user profile path. Ensure that Python is installed to a path without spaces, or ensure that you have enclosed the path in quotes when it is referenced on the first line of any script in the \\Scripts directory. If the first line of your script looks like this, it will fail: #!c:\\program files\\python\\python38\\python.exe If the first line of your script looks like this, it will work correctly: #!\"c:\\program files\\python\\python38\\python.exe\" Was this article helpful? (13) (20) Additional Informations Related Articles Databricks Connect reports version error with Databricks Runtime 6.4 Problem You are using the Databricks Connect client with Databricks Runtime 6.4 a... Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... Related Articles Databricks Connect reports version error with Databricks Runtime 6.4 Problem You are using the Databricks Connect client with Databricks Runtime 6.4 a... Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Failed to create process error with Databricks CLI in Windows",
          "view_href" : "https://kb.databricks.com/en_US/dev-tools/dbcli-win-fail-create-process"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/security/table-create-security-exception",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Security and permissions Table creation fails with security exception Table creation fails with security exception Learn what to do when table creation fails with a security exception. Written by Adam Pavlacka Last published at: May 17th, 2022 Problem You attempt to create a table using a cluster that has Table ACLs enabled, but the following error occurs: Error in SQL statement: SecurityException: User does not have permission SELECT on any file. Cause This error occurs on a Table ACL-enabled cluster if you are not an administrator and you do not have sufficient privileges to create a table. AWS For example, in your notebook you attempt to create a table using a Parquet data source located on S3: %sql\r\n\r\nCREATE TABLE mytable\r\n  USING PARQUET\r\n  OPTIONS (PATH='s3://my-root-bucket/subfolder/my-table') Delete Azure For example, in your notebook you attempt to create a table using a Parquet data source located on Azure Blob Storage: %sql\r\n\r\nCREATE TABLE mytable\r\n  USING PARQUET\r\n  OPTIONS (PATH='wasbs://my-container@my-storage-account.blob.core.windows.net/my-table') Delete Solution You should ask your administrator to grant you access to the blob storage filesystem, using either of the following options. If an administrator cannot grant you access to the data object, you’ll have to ask an administrator to make the table for you. If you want to use a CTAS (CREATE TABLE AS SELECT) statement to create the table, the administrator should grant you SELECT privileges on the filesystem: %sql\r\n\r\nGRANT SELECT ON ANY FILE TO `user1` Example CTAS statement: AWS %sql\r\nCREATE TABLE mytable\r\n      AS SELECT * FROM parquet.`s3://my-root-bucket/subfolder/my-table` Delete Azure %sql\r\n\r\nCREATE TABLE mytable\r\n      AS SELECT * FROM parquet.`wasbs://my-container@my-storage-account.blob.core.windows.net/my-table` Delete If you want to use a CTOP (CREATE TABLE OPTIONS PATH) statement to make the table, the administrator must elevate your privileges by granting MODIFY in addition to SELECT. %sql\r\n\r\nGRANT SELECT, MODIFY ON ANY FILE TO `user1`Example CTOP statement: AWS %sql\r\n\r\nCREATE TABLE mytable\r\n   USING PARQUET\r\n   OPTIONS (PATH='s3://my-root-bucket/subfolder/my-table') Delete Azure %sql\r\n\r\nCREATE TABLE mytable\r\n   USING PARQUET\r\n   OPTIONS (PATH='wasbs://my-container@my-storage-account.blob.core.windows.net/my-table') Delete Delete Warning It is important to understand the security implications of granting ANY FILE permissions on a filesystem. You should only grant ANY FILE to privileged users. Users with lower privileges on the cluster should never access data by referencing an actual storage location. Instead, they should access data from tables that are created by privileged users, thus ensuring that Table ACLS are enforced. In addition, if files in the Databricks root and data buckets are accessible by the cluster and users have MODIFY privileges, the admin should lock down the root. AWS Granting the data access privileges described above does not supersede any underlying IAM roles or S3 bucket policies. For example, if a grant statement like GRANT SELECT, MODIFY ON ANY FILE TO user1 is executed but an IAM role attached to the cluster explicitly denies reads to the target S3 bucket, then the GRANT statement will not make the bucket or the objects within the bucket suddenly readable. Delete Azure Granting the data access privileges described above does not supersede any underlying user permissions or Blob Storage container access control. For example, if a grant statement like GRANT SELECT, MODIFY ON ANY FILE TO user1 is executed but a user permission attached to the cluster explicitly denies reads to the target container, then the GRANT statement will not make the container or the objects within the container suddenly readable. Delete Was this article helpful? (9) (12) Additional Informations Related Articles Forbidden error while accessing S3 data Problem While trying to access S3 data using DBFS mount or directly in Spark APIs... Related Articles Forbidden error while accessing S3 data Problem While trying to access S3 data using DBFS mount or directly in Spark APIs... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Table creation fails with security exception",
          "view_href" : "https://kb.databricks.com/en_US/security/table-create-security-exception"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/autotermination-disabled-error-creating-job",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Auto termination is disabled when starting a job cluster Auto termination is disabled when starting a job cluster Auto termination policies are not supported on job clusters. Written by navya.athiraram Last published at: March 4th, 2022 Problem You are trying to start a job cluster, but the job creation fails with an error message. Error creating job\r\nCluster autotermination is currently disabled. Cause Job clusters auto terminate once the job is completed. As a result, they do not support explicit autotermination policies. If you include autotermination_minutes in your cluster policy JSON, you get the error on job creation. {\r\n \"autotermination_minutes\": {\r\n  \"type\": \"fixed\",\r\n   \"value\": 30,\r\n   \"hidden\": true\r\n  }\r\n} Solution Do not define autotermination_minutes in the cluster policy for job clusters. Auto termination should only be used for all-purpose clusters. Was this article helpful? (10) (59) Additional Informations Related Articles Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Cluster slowdown due to Ganglia metrics filling root partition Note This article applies to Databricks Runtime 7.3 LTS and below. Problem Cluste... Multi-part upload failure Problem You observe a job failure with the exception: com.amazonaws.SdkClientExce... Replay Apache Spark events in a cluster The Spark UI is commonly used as a debugging tool for Spark jobs. If the Spark UI... Related Articles Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Cluster slowdown due to Ganglia metrics filling root partition Note This article applies to Databricks Runtime 7.3 LTS and below. Problem Cluste... Multi-part upload failure Problem You observe a job failure with the exception: com.amazonaws.SdkClientExce... Replay Apache Spark events in a cluster The Spark UI is commonly used as a debugging tool for Spark jobs. If the Spark UI... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Auto termination is disabled when starting a job cluster",
          "view_href" : "https://kb.databricks.com/en_US/clusters/autotermination-disabled-error-creating-job"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data-sources/wasb-check-blob-types",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data sources Unable to read files and list directories in a WASB filesystem Unable to read files and list directories in a WASB filesystem Learn how to interpret errors that occur when accessing WASB append blob types in Databricks. Written by Adam Pavlacka Last published at: June 1st, 2022 Problem When you try reading a file on WASB with Spark, you get the following exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 19, 10.139.64.5, executor 0): shaded.databricks.org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: Incorrect Blob type, please use the correct Blob type to access a blob on the server. Expected BLOCK_BLOB, actual APPEND_BLOB. When you try listing files in WASB using dbutils.fs.ls or the Hadoop API, you get the following exception: java.io.FileNotFoundException: File/<some-directory> does not exist. Cause The WASB filesystem supports three types of blobs: block, page, and append. Block blobs are optimized for upload of large blocks of data (the default in Hadoop). Page blobs are optimized for random read and write operations. Append blobs are optimized for append operations. See Understanding block blobs, append blobs, and page blobs for details. The errors described above occur if you try to read an append blob or list a directory that contains only append blobs. The Databricks and Hadoop Azure WASB implementations do not support reading append blobs. Similarly when listing a directory, append blobs are ignored. There is no workaround to enable reading append blobs or listing a directory that contains only append blobs. However, you can use either Azure CLI or Azure Storage SDK for Python to identify if a directory contains append blobs or a file is an append blob. You can verify whether a directory contains append blobs by running the following Azure CLI command: az storage blob list \\\r\n  --auth-mode key \\\r\n  --account-name <account-name> \\\r\n  --container-name <container-name> \\\r\n  --prefix <path> The result is returned as a JSON document, in which you can easily find the blob type for each file. If directory is large, you can limit number of results with the flag --num-results <num>. You can also use Azure Storage SDK for Python to list and explore files in a WASB filesystem: %python\r\n\r\n\r\niter = service.list_blobs(\"container\")\r\nfor blob in iter:\r\n  if blob.properties.blob_type == \"AppendBlob\":\r\n    print(\"\\t Blob name: %s, %s\" % (blob.name, blob.properties.blob_type)) Databricks does support accessing append blobs using the Hadoop API, but only when appending to a file. Solution There is no workaround for this issue. Use Azure CLI or Azure Storage SDK for Python to identify if the directory contains append blobs or the object is an append blob. You can implement either a Spark SQL UDF or custom function using RDD API to load, read, or convert blobs using Azure Storage SDK for Python. Was this article helpful? (7) (11) Additional Informations Related Articles Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... Delete table when underlying S3 bucket is deleted Problem You are trying to drop or alter a table when you get an error. Error in S... Related Articles Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... Delete table when underlying S3 bucket is deleted Problem You are trying to drop or alter a table when you get an error. Error in S... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Unable to read files and list directories in a WASB filesystem",
          "view_href" : "https://kb.databricks.com/en_US/data-sources/wasb-check-blob-types"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/metastore/slow-autoscaling-external-metastore",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Metastore Autoscaling is slow with an external metastore Autoscaling is slow with an external metastore Improve autoscaling performance by only installing metastore jars to the driver. Written by Gobinath.Viswanathan Last published at: May 16th, 2022 Problem You have an external metastore configured on your cluster and autoscaling is enabled, but the cluster is not autoscaling effectively. Cause You are copying the metastore jars to every executor, when they are only needed in the driver. It takes time to initialize and run the jars every time a new executor spins up. As a result, adding more executors takes longer than it should. Solution You should configure your cluster so the metastore jars are only copied to the driver. Option 1: Use an init script to copy the metastore jars. Create a cluster with spark.sql.hive.metastore.jars set to maven and spark.sql.hive.metastore.version to match the version of your metastore. Start the cluster and search the driver logs for a line that includes Downloaded metastore jars to. 17/11/18 22:41:19 INFO IsolatedClientLoader: Downloaded metastore jars to <path> <path> is the location of the downloaded jars in the driver node of the cluster. Copy the jars to a DBFS location. %sh\r\n\r\ncp -r <path> /dbfs/ExternalMetaStore_jar_location Create the init script. %python\r\n\r\ndbutils.fs.put(\"dbfs:/databricks/<init-script-folder>/external-metastore-jars-to-driver.sh\",\r\n\"\"\"\r\n#!/bin/bash\r\nif [[ $DB_IS_DRIVER = \"TRUE\" ]]; then\r\nmkdir -p /databricks/metastorejars/\r\ncp -r /dbfs/ExternalMetaStore_jar_location/* /databricks/metastorejars/\r\nfi\"\"\", True) Install the init script that you just created as a cluster-scoped init script (AWS | Azure | GCP). You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/external-metastore-jars-to-driver.sh). Restart the cluster. Option 2: Use the Apache Spark configuration settings to copy the metastore jars to the driver. Enter the following settings into your Spark config (AWS | Azure | GCP): spark.hadoop.javax.jdo.option.ConnectionURL jdbc:mysql://<mysql-host>:<mysql-port>/<metastore-db>\r\nspark.hadoop.javax.jdo.option.ConnectionDriverName <driver>\r\nspark.hadoop.javax.jdo.option.ConnectionUserName <mysql-username>\r\nspark.hadoop.javax.jdo.option.ConnectionPassword <mysql-password>\r\nspark.sql.hive.metastore.version <hive-version>\r\nspark.sql.hive.metastore.jars /dbfs/metastore/jars/* The source path can be external mounted storage or DBFS. The metastore configuration can be applied globally within the workspace by using cluster policies (AWS | Azure | GCP). Option 3: Build a custom Databricks container with preloaded jars on AWS or Azure. Review the documentation on customizing containers with Databricks Container Services. Was this article helpful? (9) (13) Additional Informations Related Articles Data too long for column error Problem You are trying to insert a struct into a table, but you get a java.sql.SQ... Drop database without deletion By default, the DROP DATABASE (AWS | Azure | GCP) command drops the database and ... How to create table DDLs to import into an external metastore Databricks supports using external metastores instead of the default Hive metasto... Drop tables with corrupted metadata from the metastore Problem Sometimes you cannot drop a table from the Databricks UI. Using %sql or s... Error in CREATE TABLE with external Hive metastore Problem You are connecting to an external MySQL metastore and attempting to creat... Related Articles Data too long for column error Problem You are trying to insert a struct into a table, but you get a java.sql.SQ... Drop database without deletion By default, the DROP DATABASE (AWS | Azure | GCP) command drops the database and ... How to create table DDLs to import into an external metastore Databricks supports using external metastores instead of the default Hive metasto... Drop tables with corrupted metadata from the metastore Problem Sometimes you cannot drop a table from the Databricks UI. Using %sql or s... Error in CREATE TABLE with external Hive metastore Problem You are connecting to an external MySQL metastore and attempting to creat... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Autoscaling is slow with an external metastore",
          "view_href" : "https://kb.databricks.com/en_US/metastore/slow-autoscaling-external-metastore"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/cloud/aws-no-region-provided-dbr7",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Cloud infrastructure AWS services fail with No region provided error AWS services fail with No region provided error AWS services fail with a Java \"No region provided\" error in Databricks Runtime 7.0 and above. Written by arjun.kaimaparambilrajan Last published at: February 25th, 2022 Problem Your code snippets that use AWS services fail with a java.lang.IllegalArgumentException: No region provided error in Databricks Runtime 7.0 and above. The same code worked in Databricks Runtime 6.6 and below. You can verify the issue by running the example code snippet in a notebook. In Databricks Runtime 7.0 and above, it will return the exception shown in the example error message. Example code snippet (Scala): import com.amazonaws.regions.Regions\r\nimport com.amazonaws.regions.Regions\r\nimport com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder\r\nimport com.amazonaws.services.securitytoken.AWSSecurityTokenService\r\n\r\n\r\n val sts: AWSSecurityTokenService = AWSSecurityTokenServiceClientBuilder.standard()\r\n.withRegion(Regions.fromName(\"us-west-2\"))\r\n.build() Example error message: java.lang.IllegalArgumentException: No region provided\r\n        at com.amazonaws.AmazonWebServiceClient.setRegion(AmazonWebServiceClient.java:509)\r\n        at com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.setRegion(AWSSecurityTokenServiceClient.java:1436)\r\n        at com.amazonaws.client.builder.AwsClientBuilder.setRegion(AwsClientBuilder.java:456)\r\n        at com.amazonaws.client.builder.AwsClientBuilder.configureMutableProperties(AwsClientBuilder.java:424)\r\n        at com.amazonaws.client.builder.AwsSyncClientBuilder.build(AwsSyncClientBuilder.java:46) Cause There was a change in the AWS Java SDK version 1.11.655 which causes this issue in Databricks Runtime 7.0 and above. Solution You must set the sts_regional_endpoints value to regional on the cluster. This ensures that the AWS STS endpoint for the currently configured region is used. Open the Clusters page. Select a cluster. Click Edit. Click Advanced Options. Click Spark. In the Environment Variables field, enter AWS_STS_REGIONAL_ENDPOINTS=\"regional\". Save the change and start, or restart, the cluster. Was this article helpful? (10) (46) Additional Informations Related Articles Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... Vulnerability scan shows vulnerabilities in Databricks EC2 instances Problem The Corporate Information Security (CIS) Vulnerability Management team id... Configure custom DNS settings using dnsmasq dnsmasq is a tool for installing and configuring DNS routing rules for cluster no... Unable to load AWS credentials Problem When you try to access AWS resources like S3, SQS or Redshift, the operat... Access denied when writing logs to an S3 bucket Problem When you try to write log files to an S3 bucket, you get the error: com.a... Related Articles Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... Vulnerability scan shows vulnerabilities in Databricks EC2 instances Problem The Corporate Information Security (CIS) Vulnerability Management team id... Configure custom DNS settings using dnsmasq dnsmasq is a tool for installing and configuring DNS routing rules for cluster no... Unable to load AWS credentials Problem When you try to access AWS resources like S3, SQS or Redshift, the operat... Access denied when writing logs to an S3 bucket Problem When you try to write log files to an S3 bucket, you get the error: com.a... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "AWS services fail with No region provided error",
          "view_href" : "https://kb.databricks.com/en_US/cloud/aws-no-region-provided-dbr7"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/cloud/cross-account-log-delivery",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Cloud infrastructure Access denied when writing logs to an S3 bucket Access denied when writing logs to an S3 bucket Learn how to resolve an access denied error while writing logs to an S3 bucket. Written by Adam Pavlacka Last published at: February 25th, 2022 Problem When you try to write log files to an S3 bucket, you get the error: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied\r\n(Service: Amazon S3; Status Code: 403; Error Code: AccessDenied;\r\nRequest ID: 2F8D8A07CD8817EA), S3 Extended Request ID: Cause The DBFS mount is in an S3 bucket that assumes roles and uses sse-kms encryption. The assumed role has full S3 access to the location where you are trying to save the log file. The location also can access the kms key. However, access is denied because the logging daemon isn’t inside the container on the host machine. Only the container on the host machine has access to the Apache Spark configuration that assumes the role. Solution Use the cluster IAM Role to deliver the logs. Configure the logs to deliver to an S3 bucket in the AWS account for the Databricks data plane VPC (your customer Databricks account). Review the Cluster Log Delivery documentation for more information. Was this article helpful? (10) (15) Additional Informations Related Articles Vulnerability scan shows vulnerabilities in Databricks EC2 instances Problem The Corporate Information Security (CIS) Vulnerability Management team id... AWS services fail with No region provided error Problem Your code snippets that use AWS services fail with a java.lang.IllegalArg... Unable to load AWS credentials Problem When you try to access AWS resources like S3, SQS or Redshift, the operat... Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... Related Articles Vulnerability scan shows vulnerabilities in Databricks EC2 instances Problem The Corporate Information Security (CIS) Vulnerability Management team id... AWS services fail with No region provided error Problem Your code snippets that use AWS services fail with a java.lang.IllegalArg... Unable to load AWS credentials Problem When you try to access AWS resources like S3, SQS or Redshift, the operat... Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Access denied when writing logs to an S3 bucket",
          "view_href" : "https://kb.databricks.com/en_US/cloud/cross-account-log-delivery"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data-sources/jdbc-optimize-read",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data sources Optimize read performance from JDBC data sources Optimize read performance from JDBC data sources Learn how to optimize performance when reading from JDBC data sources in Databricks. Written by Adam Pavlacka Last published at: June 1st, 2022 Problem Reading data from an external JDBC database is slow. How can I improve read performance? Solution See the detailed discussion in the Databricks documentation on how to optimize performance when reading data (AWS | Azure | GCP) from an external JDBC database. Was this article helpful? (6) (16) Additional Informations Related Articles Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... Unable to read files and list directories in a WASB filesystem Problem When you try reading a file on WASB with Spark, you get the following exc... Delete table when underlying S3 bucket is deleted Problem You are trying to drop or alter a table when you get an error. Error in S... Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... Related Articles Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... Unable to read files and list directories in a WASB filesystem Problem When you try reading a file on WASB with Spark, you get the following exc... Delete table when underlying S3 bucket is deleted Problem You are trying to drop or alter a table when you get an error. Error in S... Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Optimize read performance from JDBC data sources",
          "view_href" : "https://kb.databricks.com/en_US/data-sources/jdbc-optimize-read"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/metastore/list-tables",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Metastore Listing table names Listing table names Learn how to list table names in Databricks. Written by Adam Pavlacka Last published at: May 16th, 2022 Problem To fetch all the table names from metastore you can use either spark.catalog.listTables() or %sql show tables. If you observe the duration to fetch the details you can see spark.catalog.listTables() usually takes longer than %sql show tables. Cause spark.catalog.listTables() tries to fetch every table’s metadata first and then show the requested table names. This process is slow when dealing with complex schemas and larger numbers of tables. Solution To get only the table names, use %sql show tables which internally invokes SessionCatalog.listTables which fetches only the table names. Was this article helpful? (15) (53) Additional Informations Related Articles AnalysisException when dropping table on Azure-backed metastore Problem When you try to drop a table in an external Hive version 2.0 or 2.1 metas... Data too long for column error Problem You are trying to insert a struct into a table, but you get a java.sql.SQ... How to create table DDLs to import into an external metastore Databricks supports using external metastores instead of the default Hive metasto... Autoscaling is slow with an external metastore Problem You have an external metastore configured on your cluster and autoscaling... Related Articles AnalysisException when dropping table on Azure-backed metastore Problem When you try to drop a table in an external Hive version 2.0 or 2.1 metas... Data too long for column error Problem You are trying to insert a struct into a table, but you get a java.sql.SQ... How to create table DDLs to import into an external metastore Databricks supports using external metastores instead of the default Hive metasto... Autoscaling is slow with an external metastore Problem You have an external metastore configured on your cluster and autoscaling... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Listing table names",
          "view_href" : "https://kb.databricks.com/en_US/metastore/list-tables"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/spark-job-fail-parquet-column-convert",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Apache Spark job fails with Parquet column cannot be converted error Apache Spark job fails with Parquet column cannot be converted error Parquet column cannot be converted error appears when you are reading decimal data in Parquet format and writing to a Delta table. Written by shanmugavel.chandrakasu Last published at: May 20th, 2022 Problem You are reading data in Parquet format and writing to a Delta table when you get a Parquet column cannot be converted error message. The cluster is running Databricks Runtime 7.3 LTS or above. org.apache.spark.SparkException: Task failed while writing rows.\r\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file s3://bucket-name/landing/edw/xxx/part-xxxx-tid-c00.snappy.parquet. Parquet column cannot be converted. Column: [Col1], Expected: DecimalType(10,0), Found: FIXED_LEN_BYTE_ARRAY\r\n\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException. Cause The vectorized Parquet reader is decoding the decimal type column to a binary format. The vectorized Parquet reader is enabled by default in Databricks Runtime 7.3 and above for reading datasets in Parquet files. The read schema uses atomic data types: binary, boolean, date, string, and timestamp. Delete Info This error only occurs if you have decimal type columns in the source data. Solution If you have decimal type columns in your source data, you should disable the vectorized Parquet reader. Set spark.sql.parquet.enableVectorizedReader to false in the cluster’s Spark configuration to disable the vectorized Parquet reader at the cluster level. You can also disable the vectorized Parquet reader at the notebook level by running: %scala\r\n\r\nspark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\") Delete Info The vectorized Parquet reader enables native record-level filtering using push-down filters, improving memory locality, and cache utilization. If you disable the vectorized Parquet reader, there may be a minor performance impact. You should only disable it, if you have decimal type columns in your source data. Was this article helpful? (11) (43) Additional Informations Related Articles Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Related Articles Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Apache Spark job fails with Parquet column cannot be converted error",
          "view_href" : "https://kb.databricks.com/en_US/scala/spark-job-fail-parquet-column-convert"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/launch-fails-clientinternalerror",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Launch fails with Client.InternalError Launch fails with Client.InternalError Cluster launch fails with Client.InternalError on a E2 workspace due to IAM role KMS key policy. Written by satyadeepak.bollineni Last published at: March 4th, 2022 Problem You deploy a new E2 workspace, but you get cluster launch failures with the message Client.InternalError. Cause You have encryption of the EBS volumes at the AWS account level or you are using a custom KMS key for EBS encryption. Either one of these scenarios can result in a Client.InternalError when you try to create a cluster in an E2 workspace. Solution Add the following JSON policy statement to the AWS key policy for your KMS key. This policy statement grants the Databricks cross-account IAM role the ability to use the KMS key. {\r\n    \"Sid\": \"AllowDatabricksToUseEBSEncryptionKey\",\r\n    \"Effect\": \"Allow\",\r\n    \"Principal\": {\r\n        \"AWS\": \"arn:aws:iam::<customer_aws_account_id>:role/<customer_cross_account_iam_role>\"\r\n    },\r\n    \"Action\": [\r\n        \"kms:Decrypt\",\r\n        \"kms:GenerateDataKey*\",\r\n        \"kms:CreateGrant\",\r\n        \"kms:DescribeKey\"\r\n    ],\r\n    \"Resource\": \"*\",\r\n    \"Condition\": {\r\n        \"ForAnyValue:StringLike\": {\r\n            \"kms:ViaService\": \"ec2.*.amazonaws.com\"\r\n        }\r\n    }\r\n} Was this article helpful? (6) (15) Additional Informations Related Articles How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... Related Articles How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Install a private PyPI repo Certain use cases may require you to install libraries from private PyPI reposito... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Launch fails with Client.InternalError",
          "view_href" : "https://kb.databricks.com/en_US/clusters/launch-fails-clientinternalerror"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/overwrite-log4j-logs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters How to overwrite log4j configurations on Databricks clusters How to overwrite log4j configurations on Databricks clusters Learn how to overwrite log4j configurations on Databricks clusters. Written by Adam Pavlacka Last published at: March 4th, 2022 Delete Warning This article describes steps related to customer use of Log4j 1.x within a Databricks cluster. Log4j 1.x is no longer maintained and has three known CVEs (CVE-2021-4104, CVE-2020-9488, and CVE-2019-17571). If your code uses one of the affected classes (JMSAppender or SocketServer), your use may potentially be impacted by these vulnerabilities. You should not enable either of these classes in your cluster. There is no standard way to overwrite log4j configurations on clusters with custom configurations. You must overwrite the configuration files using init scripts. The current configurations are stored in two log4j.properties files: On the driver: %sh\r\n\r\ncat /home/ubuntu/databricks/spark/dbconf/log4j/driver/log4j.properties On the worker: %sh\r\n\r\ncat /home/ubuntu/databricks/spark/dbconf/log4j/executor/log4j.properties To set class-specific logging on the driver or on workers, use the following script: %sh\r\n\r\n#!/bin/bash\r\necho \"Executing on Driver: $DB_IS_DRIVER\"\r\nif [[ $DB_IS_DRIVER = \"TRUE\" ]]; then\r\nLOG4J_PATH=\"/home/ubuntu/databricks/spark/dbconf/log4j/driver/log4j.properties\"\r\nelse\r\nLOG4J_PATH=\"/home/ubuntu/databricks/spark/dbconf/log4j/executor/log4j.properties\"\r\nfi\r\necho \"Adjusting log4j.properties here: ${LOG4J_PATH}\"\r\necho \"log4j.<custom-prop>=<value>\" >> ${LOG4J_PATH} Replace <custom-prop> with the property name, and <value> with the property value. Upload the script to DBFS and select a cluster using the cluster configuration UI. You can also set log4j.properties for the driver in the same way. Was this article helpful? (8) (14) Additional Informations Related Articles Auto termination is disabled when starting a job cluster Problem You are trying to start a job cluster, but the job creation fails with an... Log delivery fails with AssumeRole Problem You are using AssumeRole to send cluster logs to a S3 bucket in another a... Cluster fails to start with dummy does not exist error Problem You try to start a cluster, but it fails to start. You get an Apache Spar... S3 connection fails with \"No role specified and no roles available\" Problem You are using Databricks Utilities (dbutils) to access a S3 bucket, but i... Related Articles Auto termination is disabled when starting a job cluster Problem You are trying to start a job cluster, but the job creation fails with an... Log delivery fails with AssumeRole Problem You are using AssumeRole to send cluster logs to a S3 bucket in another a... Cluster fails to start with dummy does not exist error Problem You try to start a cluster, but it fails to start. You get an Apache Spar... S3 connection fails with \"No role specified and no roles available\" Problem You are using Databricks Utilities (dbutils) to access a S3 bucket, but i... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to overwrite log4j configurations on Databricks clusters",
          "view_href" : "https://kb.databricks.com/en_US/clusters/overwrite-log4j-logs"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/execution/maximum-execution-context",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Job execution Maximum execution context or notebook attachment limit reached Maximum execution context or notebook attachment limit reached Learn what to do when the maximum execution context or notebook attachment limit is reached in Databricks. Written by Adam Pavlacka Last published at: May 11th, 2022 Problem Notebook or job execution stops and returns either of the following errors: Run result unavailable: job failed with error message\r\nContext ExecutionContextId(1731742567765160237) is disconnected. Can’t attach this notebook because the cluster has reached the attached notebook limit. Detach a notebook and retry. Cause When you attach a notebook to a cluster, Databricks creates an execution context (AWS | Azure). If there are too many notebooks attached to a cluster or too many jobs are created, at some point the cluster reaches its maximum threshold limit of 145 execution contexts, and Databricks returns an error. Solution Configure context auto-eviction (AWS | Azure), which allows Databricks to remove (evict) idle execution contexts. Additionally, from the pipeline and ETL design perspective, you can avoid this issue by using: Fewer notebooks to reduce the number of execution contexts that are created. A job cluster instead of an interactive cluster. If the use case permits, submit notebooks or jars as jobs. Was this article helpful? (12) (11) Additional Informations Related Articles Increase the number of tasks per stage When using the spark-xml package, you can increase the number of tasks per stage ... Related Articles Increase the number of tasks per stage When using the spark-xml package, you can increase the number of tasks per stage ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Maximum execution context or notebook attachment limit reached",
          "view_href" : "https://kb.databricks.com/en_US/execution/maximum-execution-context"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/jobs/job-fails-no-library",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Jobs Databricks job fails because library is not installed Databricks job fails because library is not installed Learn how to prevent Databricks jobs from failing due to uninstalled libraries. Written by Adam Pavlacka Last published at: May 11th, 2022 Problem A Databricks job fails because the job requires a library that is not yet installed, causing Import errors. Cause The error occurs because the job starts running before required libraries install. If you run a job on a cluster in either of the following situations, the cluster can experience a delay in installing libraries: When you start an existing cluster with libraries in terminated state. When you start a new cluster that uses a shared library (a library installed on all clusters). Solution If a job requires certain libraries, make sure to attach the libraries as dependent libraries within job itself. Refer to the following article and steps on how to set up dependent libraries when you create a job. Add libraries as dependent libraries when you create a job (AWS | Azure). 1. Open Add Dependent Library dialog: AWS Delete Azure Delete 2. Choose library: AWS Delete Azure Delete 3. Verify library: AWS Delete Azure Delete Was this article helpful? (6) (17) Additional Informations Related Articles Job cluster limits on notebook output Problem You are running a notebook on a job cluster and you get an error message ... Apache Spark job fails with maxResultSize exception Problem A Spark job fails with a maxResultSize exception: org.apache.spark.SparkE... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... Apache Spark Jobs hang due to non-deterministic custom UDF Problem Sometimes Apache Spark jobs hang indefinitely due to the non-deterministi... Related Articles Job cluster limits on notebook output Problem You are running a notebook on a job cluster and you get an error message ... Apache Spark job fails with maxResultSize exception Problem A Spark job fails with a maxResultSize exception: org.apache.spark.SparkE... Identify less used jobs The workspace has a limit on the number of jobs that can be shown in the UI. The ... Apache Spark Jobs hang due to non-deterministic custom UDF Problem Sometimes Apache Spark jobs hang indefinitely due to the non-deterministi... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Databricks job fails because library is not installed",
          "view_href" : "https://kb.databricks.com/en_US/jobs/job-fails-no-library"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/mlflow-artifacts-download",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning Download artifacts from MLflow Download artifacts from MLflow How to download artifacts from MLflow to local storage. Written by shanmugavel.chandrakasu Last published at: May 16th, 2022 By default, the MLflow client saves artifacts to an artifact store URI during an experiment. The artifact store URI is similar to /dbfs/databricks/mlflow-tracking/<experiment-id>/<run-id>/artifacts/. This artifact store is a MLflow managed location, so you cannot download artifacts directly. You must use client.download_artifacts in the MLflow client to copy artifacts from the artifact store to another storage location. Example code This example code downloads the MLflow artifacts from a specific run and stores them in the location specified as local_dir. Replace <local-path-to-store-artifacts> with the local path where you want to store the artifacts. Replace <run-id> with the run_id of your specified MLflow run. %python\r\n\r\nimport mlflow\r\nimport os\r\nfrom mlflow.tracking import MlflowClient\r\nclient = MlflowClient()\r\nlocal_dir = \"<local-path-to-store-artifacts>\"\r\nif not os.path.exists(local_dir):\r\n  os.mkdir(local_dir)\r\n\r\n# Creating sample artifact \"features.txt\".\r\nfeatures = \"rooms, zipcode, median_price, school_rating, transport\"\r\nwith open(\"features.txt\", 'w') as f:\r\n    f.write(features)\r\n\r\n# Creating sample MLflow run & logging artifact \"features.txt\" to the MLflow run.\r\nwith mlflow.start_run() as run:\r\n    mlflow.log_artifact(\"features.txt\", artifact_path=\"features\")\r\n\r\n# Download the artifact to local storage.\r\nlocal_path = client.download_artifacts(<run-id>, \"features\", local_dir)\r\nprint(\"Artifacts downloaded in: {}\".format(local_dir))\r\nprint(\"Artifacts: {}\".format(local_dir)) After the artifacts have been downloaded to local storage, you can copy (or move) them to an external filesystem or a mount point using standard tools. Copy to an external filesystem %scala\r\n\r\ndbutils.fs.cp(local_dir, \"<filesystem://path-to-store-artifacts>\") Move to a mount point %python\r\n\r\nshutil.move(local_dir, \"/dbfs/mnt/<path-to-store-artifacts>\") Was this article helpful? (8) (12) Additional Informations Related Articles Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... Related Articles Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Download artifacts from MLflow",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/mlflow-artifacts-download"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/notebook-autosave",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks Notebook autosave fails due to file size limits Notebook autosave fails due to file size limits Learn what to do when your Databricks notebook fails to autosave due to file size limits. Written by Adam Pavlacka Last published at: May 16th, 2022 Problem Notebook autosaving fails with the following error message: Failed to save revision: Notebook size exceeds limit. This is most commonly caused by cells with large results. Remove some cells or split the notebook. Cause The maximum notebook size allowed for autosaving is 8 MB. Solution First, check the size of your notebook file using your browser’s developer tools. In Chrome, for example, click View > Developer > Developer Tools. Click the Network tab and view the Size column for the notebook file. Then, there are two possible solutions: You can manually save notebooks up to 32 MB. You can reduce the size of your notebook by hiding large results. Graphing tools like plotly and matplotlib can generate large sets of results that display as large images. You can reduce the notebook size by hiding these large results and images. Was this article helpful? (8) (23) Additional Informations Related Articles How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Error: Received command c on object id p0 Problem You have imported Python libraries, but when you try to execute Python co... Failure when accessing or mounting storage Problem You are trying to access an existing mount point, or create a new mount p... Related Articles How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Error: Received command c on object id p0 Problem You have imported Python libraries, but when you try to execute Python co... Failure when accessing or mounting storage Problem You are trying to access an existing mount point, or create a new mount p... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Notebook autosave fails due to file size limits",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/notebook-autosave"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/cannot-import-tabularprediction",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Cannot import TabularPrediction from AutoGluon Cannot import TabularPrediction from AutoGluon Cannot import TabularPrediction from AutoGluon v0.0.14 due to a namespace collision. Upgrade to AutoGluon v0.0.15. Written by kavya.parag Last published at: May 11th, 2022 Problem You are trying to import TabularPrediction from AutoGluon, but are getting an error message. ImportError: cannot import name 'TabularPrediction' from 'autogluon' (unknown location) This happens when AutoGluon is installed via a notebook or as a cluster-installed library (AWS | Azure | GCP). You can reproduce the error by running the import command in your notebook: %python\r\n\r\nimport autogluon as ag\r\nfrom autogluon import TabularPrediction as task Cause There is a namespace collision in AutoGluon v0.0.14. autogluon==0.0.14 installs gluoncv>=0.5.0,<1.0. This results in gluoncv==0.9.0 getting installed, which creates the namespace collision. Solution The namespace collision was resolved in AutoGluon v0.0.15. Upgrade to AutoGluon v0.0.15 to use TabularPrediction. Specify autogluon==0.0.15 when installing AutoGluon as a cluster-installed library from PyPI. You can also install it via a notebook. %sh\r\npip install autogluon==0.0.15 autogluon.tabular \"mxnet<2.0.0\" After you have upgraded to AutoGluon v0.0.15, you can successfully import TabularPrediction. %python\r\n\r\nimport autogluon as ag\r\nfrom autogluon import TabularPrediction as task Was this article helpful? (8) (27) Additional Informations Related Articles Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... Related Articles Cannot import module in egg library Problem You try to install an egg library to your cluster and it fails with a mes... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cannot import TabularPrediction from AutoGluon",
          "view_href" : "https://kb.databricks.com/en_US/libraries/cannot-import-tabularprediction"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/libraries/cant-uninstall-libraries",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Libraries Cannot uninstall library from UI Cannot uninstall library from UI Learn what to do when you can't uninstall a library using the Databricks user interface. Written by Adam Pavlacka Last published at: May 11th, 2022 Problem Usually, libraries can be uninstalled in the Clusters UI. If the checkbox to select the library is disabled, then it’s not possible to uninstall the library from the UI. Cause If you create a library using REST API version 1.2 and if auto-attach is enabled, the library is installed on all clusters. In this scenario, the Clusters UI checkbox to select the library to uninstall is disabled. Solution Create a workspace library pointing to the DBFS location of the library that you are unable to uninstall. Example: You can’t uninstall a JAR library that is available at this DBFS location: dbfs:/Filestore/jars/custom_elastic_spark.jar Create a new workspace library pointing to the same DBFS location. In the library UI, select the checkbox to uninstall the library from individual clusters. Was this article helpful? (11) (11) Additional Informations Related Articles Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Init script fails to download Maven JAR Problem You have an init script that is attempting to install a library via Maven... Install PyGraphViz PyGraphViz Python libraries are used to plot causal inference networks. If you tr... Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... Related Articles Latest PyStan fails to install on Databricks Runtime 6.4 Problem You are trying to install the PyStan PyPi package on a Databricks Runtime... Init script fails to download Maven JAR Problem You have an init script that is attempting to install a library via Maven... Install PyGraphViz PyGraphViz Python libraries are used to plot causal inference networks. If you tr... Cannot import TabularPrediction from AutoGluon Problem You are trying to import TabularPrediction from AutoGluon, but are gettin... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cannot uninstall library from UI",
          "view_href" : "https://kb.databricks.com/en_US/libraries/cant-uninstall-libraries"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/cmd-c-on-object-id-p0",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks Error: Received command c on object id p0 Error: Received command c on object id p0 You see the error message `INFO:py4j.java_gateway:Received command c on object id p0` after running Python code with imported libraries. Written by sandeep.chandran Last published at: May 16th, 2022 Problem You have imported Python libraries, but when you try to execute Python code in a notebook you get a repeating message as output. INFO:py4j.java_gateway:Received command c on object id p0\r\nINFO:py4j.java_gateway:Received command c on object id p0\r\nINFO:py4j.java_gateway:Received command c on object id p0\r\nINFO:py4j.java_gateway:Received command c on object id p0\r\nINFO:py4j.java_gateway:Received command c on object id p0\r\nINFO:py4j.java_gateway:Received command c on object id p0\r\nINFO:py4j.java_gateway:Received command c on object id p0 Cause The default log level for py4j.java_gateway is ERROR. If any of the imported Python libraries set the log level to INFO you will see this message. Solution You can prevent the output of the INFO messages by setting the log level back to ERROR after importing the libraries. %python\r\n\r\nimport logging\r\nlogger = spark._jvm.org.apache.log4j\r\nlogging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR) Was this article helpful? (14) (24) Additional Informations Related Articles Enable s3cmd for notebooks s3cmd is a client library that allows you to perform all AWS S3 operations from a... How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Common errors in notebooks There are some common issues that occur when using notebooks. This section outlin... Related Articles Enable s3cmd for notebooks s3cmd is a client library that allows you to perform all AWS S3 operations from a... How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Common errors in notebooks There are some common issues that occur when using notebooks. This section outlin... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Error: Received command c on object id p0",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/cmd-c-on-object-id-p0"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/metastore/drop-table-exception-azure-metastore",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Metastore AnalysisException when dropping table on Azure-backed metastore AnalysisException when dropping table on Azure-backed metastore Learn how to overcome an AnalysisException when dropping a table on an Azure-backed metastore. Written by Adam Pavlacka Last published at: May 16th, 2022 Problem When you try to drop a table in an external Hive version 2.0 or 2.1 metastore that is deployed on Azure SQL Database, Databricks throws the following exception: com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Exception thrown when executing query : SELECT 'org.apache.hadoop.hive.metastore.model.MStorageDescriptor' AS NUCLEUS_TYPE,A0.INPUT_FORMAT,A0.IS_COMPRESSED,A0.IS_STOREDASSUBDIRECTORIES,A0.LOCATION,A0.NUM_BUCKETS,A0.OUTPUT_FORMAT,A0.SD_ID FROM SDS A0 WHERE A0.CD_ID = ? OFFSET 0 ROWS FETCH NEXT ROW ONLY );\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:107)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.doDropTable(HiveExternalCatalog.scala:483)\r\n  at org.apache.spark.sql.catalyst.catalog.ExternalCatalog.dropTable(ExternalCatalog.scala:122)\r\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.dropTable(SessionCatalog.scala:638)\r\n  at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:212) Cause This is a known Hive bug (HIVE-14698), caused by another known bug with the datanucleus-rdbms module in the package. It is fixed in datanucleus-rdbms 4.1.16. However, Hive 2.0 and 2.1 metastores use version 4.1.7 and these versions are affected. Solution Do one of the following: Upgrade the Hive metastore to version 2.3.0. This also resolves problems due to any other Hive bug that is fixed in version 2.3.0. Import the following notebook to your workspace and follow the instructions to replace the datanucleus-rdbms JAR. This notebook is written to upgrade the metastore to version 2.1.1. You might want to have a similar version in your server side. External metastore upgrade notebook Review the External metastore upgrade notebook. Was this article helpful? (10) (12) Additional Informations Related Articles Drop tables with corrupted metadata from the metastore Problem Sometimes you cannot drop a table from the Databricks UI. Using %sql or s... Error in CREATE TABLE with external Hive metastore Problem You are connecting to an external MySQL metastore and attempting to creat... Drop database without deletion By default, the DROP DATABASE (AWS | Azure | GCP) command drops the database and ... Data too long for column error Problem You are trying to insert a struct into a table, but you get a java.sql.SQ... Related Articles Drop tables with corrupted metadata from the metastore Problem Sometimes you cannot drop a table from the Databricks UI. Using %sql or s... Error in CREATE TABLE with external Hive metastore Problem You are connecting to an external MySQL metastore and attempting to creat... Drop database without deletion By default, the DROP DATABASE (AWS | Azure | GCP) command drops the database and ... Data too long for column error Problem You are trying to insert a struct into a table, but you get a java.sql.SQ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "AnalysisException when dropping table on Azure-backed metastore",
          "view_href" : "https://kb.databricks.com/en_US/metastore/drop-table-exception-azure-metastore"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/df-missing-columns-to-redshift",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Write a DataFrame with missing columns to a Redshift table Write a DataFrame with missing columns to a Redshift table Use `FILLRECORD` to populate missing columns when writing a DataFrame to a Redshift table. Written by Jose Gonzalez Last published at: May 23rd, 2022 Problem When writing to Redshift tables, if the target table has more columns than the source Apache Spark DataFrame you may get a copy error. The COPY failed with error: [Amazon][Amazon Redshift] (1203) Error occurred while trying to execute a query: ERROR: Load into table table-name failed. Check the 'stl_load_errors' system table for details. “1203 - Input data had fewer columns than were defined in the DDL” Cause The source Spark DataFrame and the target Redshift table need to have the same number of columns. Solution Option 1: Update the notebook or job operation to add the missing columns in the spark DataFrame. You can populate the new columns with null values if there is no data, or with actual values if there is new data that needs to be written to the target Redshift table. This option requires manual intervention and can become time consuming if there are a large number of notebooks or jobs that need to be modified, or if new columns are added to the target on a regular basis. Option 2: Use the AWS Redshift data conversion parameter FILLRECORD. When FILLRECORD is used, it allows data files to be loaded when contiguous columns are missing at the end of some of the records. The missing columns are filled with either zero-length strings or null values, as appropriate for the data types of the columns in question. FILLRECORD can be specified using extracopyoptions while performing the df.write operation. %scala\r\n\r\ndf.write \\\r\n  .format(\"com.databricks.spark.redshift\") \\\r\n  .option(\"url\", \"jdbc:redshift://redshifthost:5439/database?user=username&password=pass\") \\\r\n  .option(\"dbtable\", \"my_table_copy\") \\\r\n  .option(\"tempdir\", \"s3n://path/for/temp/data\") \\\r\n  .option(\"extracopyoptions\", \"FILLRECORD\") \\\r\n  .save() Was this article helpful? (9) (12) Additional Informations Related Articles Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Trouble reading external JDBC tables after upgrading from Databricks Runtime 5.5 Problem Attempting to read external tables via JDBC works fine on Databricks Runt... Select files using a pattern match When selecting files, a common requirement is to only read specific files from a ... Running C++ code in Scala Run C++ from Scala notebook Review the Run C++ from Scala notebook.... Related Articles Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Trouble reading external JDBC tables after upgrading from Databricks Runtime 5.5 Problem Attempting to read external tables via JDBC works fine on Databricks Runt... Select files using a pattern match When selecting files, a common requirement is to only read specific files from a ... Running C++ code in Scala Run C++ from Scala notebook Review the Run C++ from Scala notebook.... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Write a DataFrame with missing columns to a Redshift table",
          "view_href" : "https://kb.databricks.com/en_US/scala/df-missing-columns-to-redshift"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data-sources/kafka-client-term-offsetoutofrange",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data sources Kafka client terminated with OffsetOutOfRangeException Kafka client terminated with OffsetOutOfRangeException Kafka client is terminated with `OffsetOutOfRangeException` when trying to fetch messages Written by vikas.yadav Last published at: June 1st, 2022 Problem You have an Apache Spark application that is trying to fetch messages from an Apache Kafka source when it is terminated with a kafkashaded.org.apache.kafka.clients.consumer.OffsetOutOfRangeException error message. Cause Your Spark application is trying to fetch expired data offsets from Kafka. We generally see this in these two scenarios: Scenario 1 The Spark application is terminated while processing data. When the Spark application is restarted, it tries to fetch data based on previously calculated data offsets. If any of the data offsets have expired during the time the Spark application was terminated, this issue can occur. Scenario 2 Your retention policy is set to a shorter time than the time require to process the batch. By the time the batch is done processing, some of the Kafka partition offsets have expired. The offsets are calculated for the next batch, and if there is a mismatch in the checkpoint metadata due to the expired offsets, this issue can occur. Solution Scenario 1 - Option 1 Delete the existing checkpoint before restarting the Spark application. A new checkpoint offset is created with the details of the newly fetched offset. The downside to this approach is that some of the data may be missed, because the offsets have expired in Kafka. Scenario 1 - Option 2 Increase the Kafka retention policy of the topic so that it is longer than the time the Spark application is offline. No data is missed with this solution, because no offsets have expired before the Spark application is restarted. There are two types of retention policies: Time based retention - This type of policy defines the amount of time to keep a log segment before it is automatically deleted. The default time based data retention window for all topics is seven days. You can review the Kafka documentation for log.retention.hours, log.retention.minutes, and log.retention.ms for more information. Size based retention - This type of policy defines the amount of data to retain in the log for each topic-partition. This limit is per-partition. This value is unlimited by default. You can review the Kafka documentation for log.retention.bytes for more information. Delete Info If multiple retention policies are set, the more restrictive one controls. This can be overridden on a per topic basis. Review Kafka’s Topic-level configuration for more information on how to set a per topic override. Scenario 2 - Option 1 Increase the retention policy of the partition. This is accomplished in the same way as the solution for Scenario 1 - Option 2. Scenario 2 - Option 2 Increase the number of parallel workers by configuring .option(\"minPartitions\",<X>) for readStream. The option minPartitions defines the minimum number of partitions to read from Kafka. By default, Spark uses a one-to-one mapping of Kafka topic partitions to Spark partitions when consuming data from Kafka. If you set the option minPartitions to a value greater than the number of your Kafka topic partitions, Spark separates the Kafka topic partitions into smaller pieces. This option is recommended at times of data skew, peak loads, and if your stream is falling behind. Setting this value greater than the default results in the initialization of Kafka consumers at each trigger. This can impact performance if you use SSL when connecting to Kafka. Was this article helpful? (7) (15) Additional Informations Related Articles Optimize read performance from JDBC data sources Problem Reading data from an external JDBC database is slow. How can I improve re... Failure to detect encoding in JSON Problem Spark job fails with an exception containing the message: Invalid UTF-32 ... Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... Related Articles Optimize read performance from JDBC data sources Problem Reading data from an external JDBC database is slow. How can I improve re... Failure to detect encoding in JSON Problem Spark job fails with an exception containing the message: Invalid UTF-32 ... Failure when mounting or accessing Azure Blob storage Problem When you try to access an already created mount point or create a new mou... Create tables on JSON datasets In this article we cover how to create a table on JSON datasets using SerDe. Down... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Kafka client terminated with OffsetOutOfRangeException",
          "view_href" : "https://kb.databricks.com/en_US/data-sources/kafka-client-term-offsetoutofrange"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/extract-feature-info",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning How to extract feature information for tree-based Apache SparkML pipeline models How to extract feature information for tree-based Apache SparkML pipeline models Learn how to extract feature information for tree-based ML pipeline models in Databricks. Written by Adam Pavlacka Last published at: May 16th, 2022 When you are fitting a tree-based model, such as a decision tree, random forest, or gradient boosted tree, it is helpful to be able to review the feature importance levels along with the feature names. Typically models in SparkML are fit as the last stage of the pipeline. To extract the relevant feature information from the pipeline with the tree model, you must extract the correct pipeline stage. You can extract the feature names from the VectorAssembler object: %python\r\n\r\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\r\nfrom pyspark.ml.classification import DecisionTreeClassifier\r\nfrom pyspark.ml import Pipeline\r\n\r\npipeline = Pipeline(stages=[indexer, assembler, decision_tree)\r\nDTmodel = pipeline.fit(train)\r\nva = dtModel.stages[-2]\r\ntree = DTmodel.stages[-1]\r\n\r\ndisplay(tree) #visualize the decision tree model\r\nprint(tree.toDebugString) #print the nodes of the decision tree model\r\n\r\nlist(zip(va.getInputCols(), tree.featureImportances)) You can also tune a tree-based model using a cross validator in the last stage of the pipeline. To visualize the decision tree and print the feature importance levels, you extract the bestModel from the CrossValidator object: %python\r\n\r\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\r\n\r\ncv = CrossValidator(estimator=decision_tree, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\r\npipelineCV = Pipeline(stages=[indexer, assembler, cv)\r\nDTmodelCV = pipelineCV.fit(train)\r\nva = DTmodelCV.stages[-2]\r\ntreeCV = DTmodelCV.stages[-1].bestModel\r\n\r\ndisplay(treeCV) #visualize the best decision tree model\r\nprint(treeCV.toDebugString) #print the nodes of the decision tree model\r\n\r\nlist(zip(va.getInputCols(), treeCV.featureImportances)) The display function visualizes decision tree models only. See Machine learning visualizations (AWS | Azure | GCP). Was this article helpful? (14) (11) Additional Informations Related Articles Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... Related Articles Download artifacts from MLflow By default, the MLflow client saves artifacts to an artifact store URI during an ... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to extract feature information for tree-based Apache SparkML pipeline models",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/extract-feature-info"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/get-notebooks-deleted-user",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks Access notebooks owned by a deleted user Access notebooks owned by a deleted user How to access Databricks notebooks owned by a deleted user. Written by John.Lourdu Last published at: May 16th, 2022 When you remove a user (AWS | Azure) from Databricks, a special backup folder is created in the workspace. This backup folder contains all of the deleted user’s content. Backup folders appear in the workspace as <deleted username>-backup-#. Delete Info Only an admin user can access a backup folder. To access a backup folder: Log into Databricks as an admin user. Select Workspace from the sidebar. Select Users. Select the backup folder. You can delete the backup folder once it is no longer required. Was this article helpful? (7) (21) Additional Informations Related Articles How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... JSON reader parses values as null Problem You are attempting to read a JSON file. You know the file has data in it,... Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... Related Articles How to check if a spark property is modifiable in a notebook Problem You can tune applications by setting various configurations. Some configu... JSON reader parses values as null Problem You are attempting to read a JSON file. You know the file has data in it,... Cannot use IAM roles with table ACL Problem You want to use IAM roles when table ACLs are enabled, but you get an err... Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Access notebooks owned by a deleted user",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/get-notebooks-deleted-user"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/cloud/custom-dns-routing",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Cloud infrastructure Configure custom DNS settings using dnsmasq Configure custom DNS settings using dnsmasq Learn how to configure custom DNS settings using dnsmasq. Written by Adam Pavlacka Last published at: February 25th, 2022 dnsmasq is a tool for installing and configuring DNS routing rules for cluster nodes. You can use it to set up routing between your Databricks environment and your on-premise network. Delete Warning If you use your own DNS server and it goes down, you will experience an outage and will not be able to create clusters. Use the following cluster-scoped init script to configure dnsmasq for a cluster node. Use netcat (nc) to test connectivity from the notebook environment to your on-premise network. nc -vz <on-premise-ip> 53 Create the base directory you want to store the init script in if it does not already exist. dbutils.fs.mkdirs(\"dbfs:/databricks/<init-script-folder>/\") Create the script. AWS Scala example dbutils.fs.put(\"/databricks/<init-script-folder>/dns-masq.sh\";,\"\"\"\r\n#!/bin/bash\r\n########################################\r\nConfigure on-prem dns access.\r\n########################################\r\n\r\nsudo apt-get update -y\r\nsudo apt-get install dnsmasq -y --force-yes\r\n\r\n## Add dns entries for internal your-company.net name servers\r\necho server=/databricks.net/<dns-server-ip> | sudo tee --append /etc/dnsmasq.conf\r\n\r\n## Find the default DNS settings for the EC2 instance and use them as the default DNS route\r\n\r\nec2_dns=cat /etc/resolv.conf | grep \"nameserver\"; | cut -d' ' -f 2\r\necho \"Old dns in resolv.conf $ec2_dns\"\r\n\r\necho \"server=$ec2_dns\" | sudo tee --append /etc/dnsmasq.conf\r\n\r\n## configure resolv.conf to point to dnsmasq service instead of static resolv.conf file\r\nmv /etc/resolv.conf /etc/resolv.conf.orig\r\necho nameserver 127.0.0.1 | sudo tee --append /etc/resolv.conf\r\nsudo systemctl disable --now systemd-resolved\r\nsudo systemctl enable --now dnsmasq\r\n\"\"\", true) Delete Azure Scala example dbutils.fs.put(\"/databricks/<init-script-folder>/dns-masq.sh\";,\"\"\"\r\n#!/bin/bash\r\nsudo apt-get update -y\r\nsudo apt-get install dnsmasq -y --force-yes\r\n\r\n## Add dns entries for internal nameservers\r\necho server=/databricks.net/<dns-server-ip> | sudo tee --append /etc/dnsmasq.conf\r\n   \r\n## Find the default DNS settings for the instance and use them as the default DNS route\r\nazvm_dns=cat /etc/resolv.conf | grep \"nameserver\"; | cut -d' ' -f 2\r\necho \"Old dns in resolv.conf $azvm_dns\"\r\necho \"server=$azvm_dns\" | sudo tee --append /etc/dnsmasq.conf\r\n    \r\n## configure resolv.conf to point to dnsmasq service instead of static resolv.conf file\r\nmv /etc/resolv.conf /etc/resolv.conf.orig\r\necho nameserver 127.0.0.1 | sudo tee --append /etc/resolv.conf\r\nsudo systemctl disable --now systemd-resolved\r\nsudo systemctl enable --now dnsmasq\r\n\"\"\", true) Delete Check that the script exists. display(dbutils.fs.ls(\"dbfs:/databricks/<init-script-folder>/dns-masq.sh\")) Configure the init script that you just created as a cluster-scoped init script. You will need the full path to the location of the script (dbfs:/databricks/<init-script-folder>/dns-masq.sh). Launch a zero-node cluster to confirm that you can create clusters. Was this article helpful? (11) (13) Additional Informations Related Articles AWS services fail with No region provided error Problem Your code snippets that use AWS services fail with a java.lang.IllegalArg... Vulnerability scan shows vulnerabilities in Databricks EC2 instances Problem The Corporate Information Security (CIS) Vulnerability Management team id... Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... Related Articles AWS services fail with No region provided error Problem Your code snippets that use AWS services fail with a java.lang.IllegalArg... Vulnerability scan shows vulnerabilities in Databricks EC2 instances Problem The Corporate Information Security (CIS) Vulnerability Management team id... Troubleshooting Amazon Redshift connection problems Problem You created a VPC peering connection and configured an Amazon Redshift cl... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Configure custom DNS settings using dnsmasq",
          "view_href" : "https://kb.databricks.com/en_US/cloud/custom-dns-routing"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/null-empty-strings",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management Nulls and empty strings in a partitioned column save as nulls Nulls and empty strings in a partitioned column save as nulls Learn why nulls and empty strings in a partitioned column save as nulls in Databricks. Written by Adam Pavlacka Last published at: May 31st, 2022 Problem If you save data containing both empty strings and null values in a column on which the table is partitioned, both values become null after writing and reading the table. To illustrate this, create a simple DataFrame: %scala\r\n\r\nimport org.apache.spark.sql.types._\r\nimport org.apache.spark.sql.catalyst.encoders.RowEncoder\r\nval data = Seq(Row(1, \"\"), Row(2, \"\"), Row(3, \"\"), Row(4, \"hello\"), Row(5, null))\r\nval schema = new StructType().add(\"a\", IntegerType).add(\"b\", StringType)\r\nval df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema) At this point, if you display the contents of df, it appears unchanged: Write df, read it again, and display it. The empty strings are replaced by null values: Cause This is the expected behavior. It is inherited from Apache Hive. Solution In general, you shouldn’t use both null and empty strings as values in a partitioned column. Was this article helpful? (13) (13) Additional Informations Related Articles Revoke all user privileges When user permissions are explicitly granted for individual tables and views, the... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... No USAGE permission on database Problem You are using a cluster running Databricks Runtime 7.3 LTS and above. You... Related Articles Revoke all user privileges When user permissions are explicitly granted for individual tables and views, the... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... How to dump tables in CSV, JSON, XML, text, or HTML format You want to send results of your computations in Databricks outside Databricks. Y... No USAGE permission on database Problem You are using a cluster running Databricks Runtime 7.3 LTS and above. You... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Nulls and empty strings in a partitioned column save as nulls",
          "view_href" : "https://kb.databricks.com/en_US/data/null-empty-strings"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/scala/convert-flat-df-to-nested-json",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Convert flattened DataFrame to nested JSON Convert flattened DataFrame to nested JSON How to convert a flattened DataFrame to nested JSON using a nested case class. Written by Adam Pavlacka Last published at: May 20th, 2022 This article explains how to convert a flattened DataFrame to a nested structure, by nesting a case class within another case class. You can use this technique to build a JSON file, that can then be sent to an external API. Define nested schema We’ll start with a flattened DataFrame. Using this example DataFrame, we define a custom nested schema using case classes. %scala\r\n\r\ncase class empId(id:String)\r\ncase class depId(dep_id:String)\r\ncase class details(id:empId,name:String,position:String,depId:depId)\r\ncase class code(manager_id:String)\r\ncase class reporting(reporting:Array[code])\r\ncase class hireDate(hire_date:String)\r\ncase class emp_record(emp_details:details,incrementDate:String,commission:String,country:String,hireDate:hireDate,reports_to:reporting) You can see that the case classes nest different data types within one another. Convert flattened DataFrame to a nested structure Use DF.map to pass every row object to the corresponding case class. %scala\r\n\r\nimport spark.implicits._\r\nval nestedDF= DF.map(r=>{\r\nval empID_1= empId(r.getString(0))\r\nval depId_1 = depId(r.getString(7))\r\nval details_1=details(empID_1,r.getString(1),r.getString(2),depId_1)\r\nval code_1=code(r.getString(3))\r\nval reporting_1 = reporting(Array(code_1))\r\nval hireDate_1 = hireDate(r.getString(4))\r\nemp_record(details_1,r.getString(8),r.getString(6),r.getString(9),hireDate_1,reporting_1)\r\n\r\n}\r\n) This creates a nested DataFrame. Write out nested DataFrame as a JSON file Use the repartition().write.option function to write the nested DataFrame to a JSON file. %scala\r\n\r\nnestedDF.repartition(1).write.option(\"multiLine\",\"true\").json(\"dbfs:/tmp/test/json1/\") Example notebook Review the DataFrame to nested JSON example notebook to see each of these steps performed. Was this article helpful? (11) (13) Additional Informations Related Articles Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Related Articles Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... Apache Spark UI is not in sync with job Problem The status of your Spark jobs is not correctly shown in the Spark UI (AWS... Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Convert flattened DataFrame to nested JSON",
          "view_href" : "https://kb.databricks.com/en_US/scala/convert-flat-df-to-nested-json"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/dev-tools/dbconnect-incompatible-dbr64",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Developer tools Databricks Connect reports version error with Databricks Runtime 6.4 Databricks Connect reports version error with Databricks Runtime 6.4 Written by rakesh.parija Last published at: May 9th, 2022 Problem You are using the Databricks Connect client with Databricks Runtime 6.4 and receive an error message which states that the client does not support the cluster. Caused by: java.lang.IllegalArgumentException: The cluster is running server version `dbr-6.4` but this client only supports Set(dbr-5.5). You can find a list of client releases at https://pypi.org/project/databricks-connect/#history, and install the right client version with `pip install -U databricks-connect==<version>`. For example, to install the latest 5.1 release, use `pip install -U databricks-connect==5.1.*`. To ignore this error and continue, set DEBUG_IGNORE_VERSION_MISMATCH=1. Cause Improvements were made to Databricks Runtime 6.4 which are incompatible with the Databricks Connect client 6.4.1 and below. Solution Upgrade the Databricks Connect client to 6.4.2. Follow the documentation to set up the client (AWS | Azure) on your local workstation, making sure to set the databricks-connect value to 6.4.2. pip install databricks-connect==6.4.2 Delete Warning Setting DEBUG_IGNORE_VERSION_MISMATCH=1 is not recommended, as it does not resolve the underlying compatibility issues. Was this article helpful? (11) (12) Additional Informations Related Articles Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... Related Articles Apache Spark session is null in DBConnect Problem You are trying to run your code using Databricks Connect ( AWS | Azure ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Databricks Connect reports version error with Databricks Runtime 6.4",
          "view_href" : "https://kb.databricks.com/en_US/dev-tools/dbconnect-incompatible-dbr64"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/speed-up-cross-validation",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning How to speed up cross-validation How to speed up cross-validation Learn how to improve cross-validation performance in SparkML with Databricks. Written by Adam Pavlacka Last published at: May 16th, 2022 Hyperparameter tuning of Apache SparkML models takes a very long time, depending on the size of the parameter grid. You can improve the performance of the cross-validation step in SparkML to speed things up: Cache the data before running any feature transformations or modeling steps, including cross-validation. Processes that refer to the data multiple times benefit from a cache. Remember to call an action on the DataFrame for the cache to take effect. Increase the parallelism parameter inside the CrossValidator, which sets the number of threads to use when running parallel algorithms. The default setting is 1. See the CrossValidator documentation for more information. Don’t use the pipeline as the estimator inside the CrossValidator specification. In some cases where the featurizers are being tuned along with the model, running the whole pipeline inside the CrossValidator makes sense. However, this executes the entire pipeline for every parameter combination and fold. Therefore, if only the model is being tuned, set the model specification as the estimator inside the CrossValidator. Delete Info CrossValidator can be set as the final stage inside the pipeline after the featurizers. The best model identified by the CrossValidator is output. Was this article helpful? (7) (13) Additional Informations Related Articles How to extract feature information for tree-based Apache SparkML pipeline models When you are fitting a tree-based model, such as a decision tree, random forest, ... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... Fitting an Apache SparkML model throws error Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.... MLflow project fails to access an Apache Hive table Problem You have an MLflow project that fails to access a Hive table and returns ... Related Articles How to extract feature information for tree-based Apache SparkML pipeline models When you are fitting a tree-based model, such as a decision tree, random forest, ... Conda fails to download packages from Anaconda Problem You are attempting to download packages from the Anaconda repository and ... Fitting an Apache SparkML model throws error Problem Databricks throws an error when fitting a SparkML model or Pipeline: org.... MLflow project fails to access an Apache Hive table Problem You have an MLflow project that fails to access a Hive table and returns ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to speed up cross-validation",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/speed-up-cross-validation"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/gen-unique-increasing-values",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark Generate unique increasing numeric values Generate unique increasing numeric values Use Apache Spark functions to generate unique and increasing numbers in a column in a table in a file or DataFrame. Written by ram.sankarasubramanian Last published at: May 23rd, 2022 This article shows you how to use Apache Spark functions to generate unique increasing numeric values in a column. We review three different methods to use. You should select the method that works best with your use case. Use zipWithIndex() in a Resilient Distributed Dataset (RDD) The zipWithIndex() function is only available within RDDs. You cannot use it directly on a DataFrame. Convert your DataFrame to a RDD, apply zipWithIndex() to your data, and then convert the RDD back to a DataFrame. We are going to use the following example code to add unique id numbers to a basic table with two entries. %python\r\n\r\ndf = spark.createDataFrame(\r\n    [\r\n        ('Alice','10'),('Susan','12')\r\n    ],\r\n    ['Name','Age']\r\n)\r\n\r\n\r\ndf1=df.rdd.zipWithIndex().toDF()\r\ndf2=df1.select(col(\"_1.*\"),col(\"_2\").alias('increasing_id'))\r\ndf2.show() Run the example code and we get the following results: +-----+---+-------------+\r\n| Name|Age|increasing_id|\r\n+-----+---+-------------+\r\n|Alice| 10|            0|\r\n|Susan| 12|            1|\r\n+-----+---+-------------+ Use monotonically_increasing_id() for unique, but not consecutive numbers The monotonically_increasing_id() function generates monotonically increasing 64-bit integers. The generated id numbers are guaranteed to be increasing and unique, but they are not guaranteed to be consecutive. We are going to use the following example code to add monotonically increasing id numbers to a basic table with two entries. %python\r\n\r\nfrom pyspark.sql.functions import *\r\n\r\ndf_with_increasing_id = df.withColumn(\"monotonically_increasing_id\", monotonically_increasing_id())\r\ndf_with_increasing_id.show() Run the example code and we get the following results: +-----+---+---------------------------+\r\n| Name|Age|monotonically_increasing_id|\r\n+-----+---+---------------------------+\r\n|Alice| 10|                 8589934592|\r\n|Susan| 12|                25769803776|\r\n+-----+---+---------------------------+ Combine monotonically_increasing_id() with row_number() for two columns The row_number() function generates numbers that are consecutive. Combine this with monotonically_increasing_id() to generate two columns of numbers that can be used to identify data entries. We are going to use the following example code to add monotonically increasing id numbers and row numbers to a basic table with two entries. %python\r\n\r\nfrom pyspark.sql.functions import *\r\nfrom pyspark.sql.window import *\r\n\r\nwindow = Window.orderBy(col('monotonically_increasing_id'))\r\ndf_with_consecutive_increasing_id = df_with_increasing_id.withColumn('increasing_id', row_number().over(window))\r\ndf_with_consecutive_increasing_id.show() Run the example code and we get the following results: +-----+---+---------------------------+-------------+\r\n| Name|Age|monotonically_increasing_id|increasing_id|\r\n+-----+---+---------------------------+-------------+\r\n|Alice| 10|                 8589934592|            1|\r\n|Susan| 12|                25769803776|            2|\r\n+-----+---+---------------------------+-------------+ If you need to increment based on the last updated maximum value, you can define a previous maximum value and then start counting from there. We’re going to build on the example code that we just ran. First, we need to define the value of previous_max_value. You would normally do this by fetching the value from your existing output table. For this example, we are going to define it as 1000. %python\r\n\r\nprevious_max_value = 1000\r\ndf_with_consecutive_increasing_id.withColumn(\"cnsecutiv_increase\", col(\"increasing_id\") + lit(previous_max_value)).show() When this is combined with the previous example code and run, we get the following results: +-----+---+---------------------------+-------------+------------------+\r\n| Name|Age|monotonically_increasing_id|increasing_id|cnsecutiv_increase|\r\n+-----+---+---------------------------+-------------+------------------+\r\n|Alice| 10|                 8589934592|            1|              1001|\r\n|Susan| 12|                25769803776|            2|              1002|\r\n+-----+---+---------------------------+-------------+------------------+ Was this article helpful? (13) (45) Additional Informations Related Articles Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Date functions only accept int values in Apache Spark 3.0 Problem You are attempting to use the date_add() or date_sub() functions in Spark... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Duplicate columns in the metadata error Problem Your Apache Spark job is processing a Delta table when the job fails with... Related Articles Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Date functions only accept int values in Apache Spark 3.0 Problem You are attempting to use the date_add() or date_sub() functions in Spark... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Duplicate columns in the metadata error Problem Your Apache Spark job is processing a Delta table when the job fails with... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Generate unique increasing numeric values",
          "view_href" : "https://kb.databricks.com/en_US/sql/gen-unique-increasing-values"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/no-use-perms-db",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management No USAGE permission on database No USAGE permission on database User does not have USAGE permission on the database. Written by rakesh.parija Last published at: May 31st, 2022 Problem You are using a cluster running Databricks Runtime 7.3 LTS and above. You have enabled table access control for your workspace (AWS | Azure | GCP) as the admin user, and granted the SELECT privilege to a standard user-group that needs to access the tables. A user tries to access an object in the database and gets a SecurityException error message. Error in SQL statement: SecurityException: User does not have permission USAGE on database <databasename> Cause A new USAGE privilege was added to the available data access privileges. This privilege is enforced on clusters running Databricks Runtime 7.3 LTS and above. Solution Grant the USAGE privilege to the user-group. Login to the workspace as an admin user. Open a notebook. Run the following command: %sql\r\n\r\nGRANT USAGE ON DATABASE <databasename> TO <user-group>; Review the USAGE privilege (AWS | Azure | GCP) documentation for more information. Was this article helpful? (8) (14) Additional Informations Related Articles Hive UDFs This article shows how to create a Hive UDF, register it in Spark, and use it in ... How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... Prevent duplicated columns when joining two DataFrames If you perform a join in Spark and don’t specify your join correctly you’ll end u... Related Articles Hive UDFs This article shows how to create a Hive UDF, register it in Spark, and use it in ... How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... Prevent duplicated columns when joining two DataFrames If you perform a join in Spark and don’t specify your join correctly you’ll end u... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "No USAGE permission on database",
          "view_href" : "https://kb.databricks.com/en_US/data/no-use-perms-db"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/sql/find-size-of-table",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles SQL with Apache Spark Find the size of a table Find the size of a table How to find the size of a table. Written by mathan.pillai Last published at: May 23rd, 2022 This article explains how to find the size of a table. The command used depends on if you are trying to find the size of a delta table or a non-delta table. Size of a delta table To find the size of a delta table, you can use a Apache Spark SQL command. %scala\r\n\r\nimport com.databricks.sql.transaction.tahoe._\r\nval deltaLog = DeltaLog.forTable(spark, \"dbfs:/<path-to-delta-table>\")\r\nval snapshot = deltaLog.snapshot               // the current delta table snapshot\r\nprintln(s\"Total file size (bytes): ${deltaLog.snapshot.sizeInBytes}\") Size of a non-delta table You can determine the size of a non-delta table by calculating the total sum of the individual files within the underlying directory. You can also use queryExecution.analyzed.stats to return the size. %scala\r\n\r\nspark.read.table(\"<non-delta-table-name>\").queryExecution.analyzed.stats Was this article helpful? (5) (17) Additional Informations Related Articles Error when downloading full results after join Problem You are working with two tables in a notebook. You perform a join. You ca... Disable broadcast when query plan has BroadcastNestedLoopJoin This article explains how to disable broadcast when the query plan has BroadcastN... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... Related Articles Error when downloading full results after join Problem You are working with two tables in a notebook. You perform a join. You ca... Disable broadcast when query plan has BroadcastNestedLoopJoin This article explains how to disable broadcast when the query plan has BroadcastN... Broadcast join exceeds threshold, returns out of memory error Problem You are attempting to join two large tables, projecting selected columns ... Cannot grow BufferHolder; exceeds size limitation Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Find the size of a table",
          "view_href" : "https://kb.databricks.com/en_US/sql/find-size-of-table"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/persist-metrics-csv-sink-dbfs",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Persist Apache Spark CSV metrics to a DBFS location Persist Apache Spark CSV metrics to a DBFS location Persist Spark CSV metrics to a sink in a DBFS location. Written by Adam Pavlacka Last published at: March 4th, 2022 Spark has a configurable metrics system that supports a number of sinks, including CSV files. In this article, we are going to show you how to configure a Databricks cluster to use a CSV sink and persist those metrics to a DBFS location. Create an init script All of the configuration is done in an init script. The init script does the following three things: Configures the cluster to generate CSV metrics on both the driver and the worker. Writes the CSV metrics to a temporary, local folder. Uploads the CSV metrics from the temporary, local folder to the chosen DBFS location. Delete Note The CSV metrics are saved locally before being uploaded to the DBFS location because DBFS is not designed for a large number of random writes. Customize the sample code and then run it in a notebook to create an init script on your cluster. Sample code to create an init script: %python\r\n\r\ndbutils.fs.put(\"/<init-path>/metrics.sh\",\"\"\"\r\n#!/bin/bash\r\nmkdir /tmp/csv\r\nsudo bash -c \"cat <<EOF >> /databricks/spark/dbconf/log4j/master-worker/metrics.properties\r\n*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\r\nspark.metrics.staticSources.enabled true\r\nspark.metrics.executorMetricsSource.enabled true\r\nspark.executor.processTreeMetrics.enabled true\r\nspark.sql.streaming.metricsEnabled true\r\nmaster.source.jvm.class org.apache.spark.metrics.source.JvmSource\r\nworker.source.jvm.class org.apache.spark.metrics.source.JvmSource\r\n*.sink.csv.period 5\r\n*.sink.csv.unit seconds\r\n*.sink.csv.directory /tmp/csv/\r\nworker.sink.csv.period 5\r\nworker.sink.csv.unit seconds\r\nEOF\"\r\n\r\nsudo bash -c \"cat <<EOF >> /databricks/spark/conf/metrics.properties\r\n*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\r\nspark.metrics.staticSources.enabled true\r\nspark.metrics.executorMetricsSource.enabled true\r\nspark.executor.processTreeMetrics.enabled true\r\nspark.sql.streaming.metricsEnabled true\r\ndriver.source.jvm.class org.apache.spark.metrics.source.JvmSource\r\nexecutor.source.jvm.class org.apache.spark.metrics.source.JvmSource\r\n*.sink.csv.period 5\r\n*.sink.csv.unit seconds\r\n*.sink.csv.directory /tmp/csv/\r\nworker.sink.csv.period 5\r\nworker.sink.csv.unit seconds\r\nEOF\"\r\n\r\ncat <<'EOF' >> /tmp/asynccode.sh\r\n#!/bin/bash\r\nDB_CLUSTER_ID=$(echo $HOSTNAME | awk -F '-' '{print$1\"-\"$2\"-\"$3}')\r\nMYIP=$(hostname -I)\r\nif [[ ! -d /dbfs/<metrics-path>/${DB_CLUSTER_ID}/metrics-${MYIP} ]] ; then\r\nsudo mkdir -p /dbfs/<metrics-path>/${DB_CLUSTER_ID}/metrics-${MYIP}\r\nfi\r\nwhile true; do\r\n    if [ -d \"/tmp/csv\" ]; then\r\n        sudo cp -r /tmp/csv/* /dbfs/<metrics-path>/$DB_CLUSTER_ID/metrics-$MYIP\r\n  fi\r\n  sleep 5\r\ndone\r\nEOF\r\nchmod a+x /tmp/asynccode.sh\r\n/tmp/asynccode.sh & disown\r\n\"\"\", True) Replace <init-path> with the DBFS location you want to use to save the init script. Replace <metrics-path> with the DBFS location you want to use to save the CSV metrics. Cluster-scoped init script Once you have created the init script on your cluster, you must configure it as a cluster-scoped init script. Verify that CSV metrics are correctly written Restart your cluster and run a sample job. Check the DBFS location that you configured for CSV metrics and verify that they were correctly written. Was this article helpful? (6) (15) Additional Informations Related Articles Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Cluster fails to start with dummy does not exist error Problem You try to start a cluster, but it fails to start. You get an Apache Spar... How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... Related Articles Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... Launch fails with Client.InternalError Problem You deploy a new E2 workspace, but you get cluster launch failures with t... Cluster fails to start with dummy does not exist error Problem You try to start a cluster, but it fails to start. You get an Apache Spar... How to calculate the number of cores in a cluster You can view the number of cores in a Databricks cluster in the Workspace UI usin... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Persist Apache Spark CSV metrics to a DBFS location",
          "view_href" : "https://kb.databricks.com/en_US/clusters/persist-metrics-csv-sink-dbfs"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/clusters/cluster-restart-fails-admin-user",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Clusters Admin user cannot restart cluster to run job Admin user cannot restart cluster to run job Learn how to re-grant privileges to Databricks Admin users. Written by Adam Pavlacka Last published at: March 4th, 2022 Problem When a user who has permission to start a cluster, such as a Databricks Admin user, submits a job that is owned by a different user, the job fails with the following message: Message: Run executed on existing cluster ID <cluster id> failed because of insufficient permissions. The error received from the cluster manager was: 'You are not authorized to restart this cluster. Please contact your administrator or the cluster creator.' Cause This error can occur when the job owner’s privilege to start the cluster is revoked. In this scenario, the job will fail even if it is submitted by an Admin user. Solution Re-grant the privilege to start the cluster (known as Can Manage) to the job owner. Change the job owner to a user or group that has the cluster start privilege. You can change it by navigating to your job page in Jobs, then to Advanced > Permissions > Edit. Was this article helpful? (7) (19) Additional Informations Related Articles Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Cluster failed to launch This article describes several scenarios in which a cluster fails to launch, and ... IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... Related Articles Cannot apply updated cluster policy Problem You are attempting to update an existing cluster policy, however the upda... Enable OpenJSSE and TLS 1.3 Queries and transformations are encrypted before being send to your clusters. By ... Cluster failed to launch This article describes several scenarios in which a cluster fails to launch, and ... IP access list update returns INVALID_STATE Problem You are trying to update an IP access list and you get an INVALID_STATE e... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Admin user cannot restart cluster to run job",
          "view_href" : "https://kb.databricks.com/en_US/clusters/cluster-restart-fails-admin-user"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/data/dump-table",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Data management How to dump tables in CSV, JSON, XML, text, or HTML format How to dump tables in CSV, JSON, XML, text, or HTML format Learn how to output tables from Databricks in CSV, JSON, XML, text, or HTML format. Written by Adam Pavlacka Last published at: May 25th, 2022 You want to send results of your computations in Databricks outside Databricks. You can use BI tools to connect to your cluster via JDBC and export results from the BI tools, or save your tables in DBFS or blob storage and copy the data via REST API. This article introduces JSpark, a simple console tool for executing SQL queries using JDBC on Spark clusters to dump remote tables to local disk in CSV, JSON, XML, Text, and HTML format. For example: %sh\r\n\r\njava -Dconfig.file=mycluster.conf -jar jspark.jar -q \"select id, type, priority, status from tickets limit 5\" returns: +----+--------+--------+------+\r\n|  id|type    |priority|status|\r\n+----+--------+--------+------+\r\n|9120|problem |urgent  |closed|\r\n|9121|question|normal  |hold  |\r\n|9122|incident|normal  |closed|\r\n|9123|question|normal  |open  |\r\n|9124|incident|normal  |solved|\r\n+----+--------+--------+------+ Instructions for use, example usage, source code, and a link to the assembled JAR is available at the JSpark GitHub repo. You can specify the parameters of JDBC connection using arguments or using a config file, for example: mycluster.conf. To check or troubleshoot JDBC connections, download the fat JAR jspark.jar and launch it as a regular JAR. It includes hive-jdbc 1.2.1 and all required dependencies. Was this article helpful? (12) (41) Additional Informations Related Articles How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... Related Articles How to handle blob data contained in an XML file If you log events in XML format, then every XML event is recorded as a base64 str... Append to a DataFrame To append to a DataFrame, use the union method. %scala val firstDF = spark.range(... How to improve performance with bucketing Bucketing is an optimization technique in Apache Spark SQL. Data is allocated amo... Simplify chained transformations Sometimes you may need to perform multiple transformations on your DataFrame: %sc... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "How to dump tables in CSV, JSON, XML, text, or HTML format",
          "view_href" : "https://kb.databricks.com/en_US/data/dump-table"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/notebooks/cannot-use-iam-with-table-acl",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Notebooks Cannot use IAM roles with table ACL Cannot use IAM roles with table ACL You cannot obtain IAM credentials when table ACLs are enabled because access to the EC2 instance metadata service is blocked. Written by Adam Pavlacka Last published at: May 16th, 2022 Problem You want to use IAM roles when table ACLs are enabled, but you get an error saying credentials cannot be located. NoCredentialsError: Unable to locate credentials Cause When a table ACL is enabled, access to the EC2 instance metadata service is blocked. This is a security measure that prevents users from obtaining IAM access credentials. Solution You can explicitly provide AWS credentials in your notebook by using boto3. You need to supply the values for aws_access_key_id and aws_secret_access_key. %python\r\n\r\nimport logging\r\nimport boto3\r\nfrom botocore.exceptions import ClientError\r\n\r\n# Get a list objects in bucket\r\ntry:\r\n  s3 = boto3.client(\r\n    's3',\r\n    aws_access_key_id='<access-key-id>',\r\n    aws_secret_access_key='<secret-access-key>'\r\n  )\r\n\r\n  response = s3.list_objects_v2(Bucket='<aws-bucket-name>')\r\n\r\n  # Output the object in the bucket\r\n  keys = []\r\n  print('Object existing in bucket:')\r\n  for obj in response['Contents']:\r\n    keys.append(obj['Key'])\r\n  print(keys)\r\nexcept ClientError as e:\r\n  logging.error(e) Delete Warning You should not disable process isolation in your Spark config as this can be a security risk. Was this article helpful? (14) (17) Additional Informations Related Articles Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... Related Articles Access S3 with temporary session credentials You can use IAM session tokens with Hadoop config support to access S3 storage in... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cannot use IAM roles with table ACL",
          "view_href" : "https://kb.databricks.com/en_US/notebooks/cannot-use-iam-with-table-acl"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/cannot-delete-data-jdbc-works-nb",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Cannot delete data using JDBC in Eclipse Cannot delete data using JDBC in Eclipse Delete operations on a Delta table fail with a SparkJDBCDriver error when using JDBC in an Eclipse local environment. Written by annapurna.hiriyur Last published at: May 10th, 2022 Problem You cannot delete data from a Delta table using JDBC from your local Eclipse environment. The same delete operation works when run in a notebook. You can connect to Databricks from your local environment. A select query works. Integer count = namedlocalJdbcTemplate.queryForObject(\"SELECT COUNT(*) FROM <table-name> \", new MapSqlParameterSource(), Integer.class); You attempt a delete operation and it fails. namedlocalJdbcTemplate.update(\"DELETE FROM <table-name> WHERE id = :Id\", parameters);\r\n\r\nwhere parameter is\r\nMapSqlParameterSource params = new MapSqlParameterSource();\r\nparams.addValue(\"Id\", “001”); You receive a SparkJDBCDriver error message. org.springframework.jdbc.UncategorizedSQLException: PreparedStatementCallback; uncategorized SQLException for SQL [DELETE FROM <table-name> WHERE Id = ?]; SQL state [HY000]; error code [500051]; [Simba][SparkJDBCDriver](500051) ERROR processing query/statement. Error Code: 0, SQL state: Error running query: org.apache.spark.sql.AnalysisException: cannot resolve '`<table-name>.Id`' given input columns: []; line 1 pos 41;\r\n'DeleteCommandEdge Delta[version=0, s3://<path-to-database>], ('<table-name>.Id = 001) Cause This error can occur when UseNativeQuery has a value of 0. The ODBC driver parses the query and does some rewriting when UseNativeQuery has a value of 0. Some tools, like PowerBI, depend on this translation. For example, PowerBI generates standard ANSI SQL which needs translation to Spark SQL. It has the potential to add unnecessary overheard if the application already generates Spark SQL. Solution Set UseNativeQuery to a value of 1 in the JDBC connection string in your IDE. This forces the ODBC driver to pass the submitted query without rewriting it. Delete Info UseNativeQuery=0 takes the submitted query as ANSI SQL-92 and rewrites it into Spark SQL. UseNativeQuery=1 takes the submitted query verbatim and submits it as-is. String CONNECTION_URL = \"jdbc:spark://<server-hostname>:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/0/xxxx-xxxxxx-xxxxxxxx;AuthMech=3;UID=token;PWD=<personal-access-token>;UseNativeQuery=1\"\r\nConnection connection = null;\r\nconnection = DriverManager.getConnection(CONNECTION_URL); Review the Initializing the Driver Class Simba documentation for more information on establishing a JDBC connection. After you set UseNativeQuery to a value of 1, you must rewrite your insert query so that it does not specify column names. For example, INSERT INTO <table-name> VALUES (<?>,<?>,<?>,<?>,<?>,<?>,<?>,<?>). This is because Spark SQL does not natively support column names when using the insert command. Was this article helpful? (16) (32) Additional Informations Related Articles A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Converting from Parquet to Delta Lake fails Problem You are attempting to convert a Parquet file to a Delta Lake file. The di... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... Related Articles A file referenced in the transaction log cannot be found Problem Your job fails with an error message: A file referenced in the transactio... Converting from Parquet to Delta Lake fails Problem You are attempting to convert a Parquet file to a Delta Lake file. The di... Compare two versions of a Delta table Delta Lake supports time travel, which allows you to query an older snapshot of a... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Cannot delete data using JDBC in Eclipse",
          "view_href" : "https://kb.databricks.com/en_US/delta/cannot-delete-data-jdbc-works-nb"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/machine-learning/runs-not-nested-sparktrials-hyperopt",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Machine learning Runs are not nested when SparkTrials is enabled in Hyperopt Runs are not nested when SparkTrials is enabled in Hyperopt When SparkTrials is enabled in Hyperopt, MLflow runs are not nested under the parent run. Written by pradeepkumar.palaniswamy Last published at: May 16th, 2022 Problem SparkTrials is an extension of Hyperopt, which allows runs to be distributed to Spark workers. When you start an MLflow run with nested=True in the worker function, the results are supposed to be nested under the parent run. Sometimes the results are not correctly nested under the parent run, even though you ran SparkTrials with nested=True in the worker function. For example: %python\r\n\r\nfrom hyperopt import fmin, tpe, hp, Trials, STATUS_OK\r\n\r\ndef train(params):\r\n  \"\"\"\r\n  An example train method that computes the square of the input.\r\n  This method will be passed to `hyperopt.fmin()`.\r\n\r\n  :param params: hyperparameters. Its structure is consistent with how search space is defined. See below.\r\n  :return: dict with fields 'loss' (scalar loss) and 'status' (success/failure status of run)\r\n  \"\"\"\r\n  with mlflow.start_run(run_name='inner_run', nested=True) as run:\r\n\r\n    x, = params\r\n  return {'loss': x ** 2, 'status': STATUS_OK}\r\n\r\nwith mlflow.start_run(run_name='outer_run_with_sparktrials'):\r\n  spark_trials_run_id = mlflow.active_run().info.run_id\r\n  argmin = fmin(\r\n    fn=train,\r\n    space=search_space,\r\n    algo=algo,\r\n    max_evals=16,\r\n    trials=spark_trials\r\n  ) Expected results: Actual results: Cause The open source version of Hyperopt does not support the required features necessary to properly nest SparkTrials MLflow runs on Databricks. Solution Databricks Runtime for Machine Learning includes an internal fork of Hyperopt with additional features. If you want to use SparkTrials, you should use Databricks Runtime for Machine Learning instead of installing Hyperopt manually from open-source repositories. Was this article helpful? (10) (11) Additional Informations Related Articles How to perform group K-fold cross validation with Apache Spark Cross validation randomly splits the training data into a specified number of fol... Experiment warning when legacy artifact storage location is used Problem A new icon appears on the MLflow Experiments page with the following open... H2O.ai Sparkling Water cluster not reachable Problem You are trying to initialize H2O.ai’s Sparkling Water on Databricks Runti... OSError when accessing MLflow experiment artifacts Problem You get an OSError: No such file or directory error message when trying t... Related Articles How to perform group K-fold cross validation with Apache Spark Cross validation randomly splits the training data into a specified number of fol... Experiment warning when legacy artifact storage location is used Problem A new icon appears on the MLflow Experiments page with the following open... H2O.ai Sparkling Water cluster not reachable Problem You are trying to initialize H2O.ai’s Sparkling Water on Databricks Runti... OSError when accessing MLflow experiment artifacts Problem You get an OSError: No such file or directory error message when trying t... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Runs are not nested when SparkTrials is enabled in Hyperopt",
          "view_href" : "https://kb.databricks.com/en_US/machine-learning/runs-not-nested-sparktrials-hyperopt"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/delta/zordering-ineffective-column-stats",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Delta Lake Z-Ordering will be ineffective, not collecting stats Z-Ordering will be ineffective, not collecting stats Z-Ordering is ineffective, error about not collecting stats. Reorder table so the columns you want to optimize on are within the first 32 columns. Written by mathan.pillai Last published at: May 10th, 2022 Problem You are trying to optimize a Delta table by Z-Ordering and receive an error about not collecting stats for the columns. AnalysisException: Z-Ordering on [col1, col2] will be ineffective, because we currently do not collect stats for these columns. Delete Info Please review Z-Ordering (multi-dimensional clustering) (AWS | Azure | GCP) for more information on data skipping and z-ordering. Cause Delta Lake collects statistics on the first 32 columns defined in your table schema. If the columns you are attempting to Z-Order are not in the first 32 columns, no statistics are collected for those columns. Solution Reorder the columns in your table, so the columns you are attempting to Z-Order are in the first 32 columns in your table. You can use an ALTER TABLE statement to reorder the columns. %sql\r\n\r\nALTER TABLE table_name CHANGE [COLUMN] col_name col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name] For example, this statement brings the column with <column-name> to the first column in the table. %sql\r\n\r\nALTER TABLE <delta-table-name> CHANGE COLUMN <column-name> <column-name> <data-type> FIRST Recompute the statistics after you have reordered the columns in the table. %scala\r\n\r\nimport com.databricks.sql.transaction.tahoe._\r\nimport org.apache.spark.sql.catalyst.TableIdentifier\r\nimport com.databricks.sql.transaction.tahoe.stats.StatisticsCollection\r\n\r\nval tableName = \"<name of table>\"\r\nval deltaLog = DeltaLog.forTable(spark, TableIdentifier(tableName))\r\n\r\nStatisticsCollection.recompute(spark, deltaLog) Rerun the Z-Order on the table and it should complete successfully. Was this article helpful? (8) (13) Additional Informations Related Articles Best practices for dropping a managed Delta Lake table Regardless of how you drop a managed table, it can take a significant amount of t... How Delta cache behaves on an autoscaling cluster This article is about how Delta cache (AWS | Azure | GCP) behaves on an auto-scal... How to populate or update columns in an existing Delta table Problem You have an existing Delta table, with a few empty columns. You need to p... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... Related Articles Best practices for dropping a managed Delta Lake table Regardless of how you drop a managed table, it can take a significant amount of t... How Delta cache behaves on an autoscaling cluster This article is about how Delta cache (AWS | Azure | GCP) behaves on an auto-scal... How to populate or update columns in an existing Delta table Problem You have an existing Delta table, with a few empty columns. You need to p... Delta Merge cannot resolve nested field Problem You are attempting a Delta Merge with automatic schema evolution, but it ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Z-Ordering will be ineffective, not collecting stats",
          "view_href" : "https://kb.databricks.com/en_US/delta/zordering-ineffective-column-stats"
        }
      },
      {
        "_index" : "aws_kb",
        "_type" : "aws kb",
        "_id" : "https://kb.databricks.com/en_US/python/convert-datetime-to-string",
        "_score" : 1.0,
        "_source" : {
          "body" : "Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Python with Apache Spark Convert Python datetime object to string Convert Python datetime object to string Display date and time values in a column, as a datetime object, and as a string. Written by Adam Pavlacka Last published at: May 19th, 2022 There are multiple ways to display date and time values with Python, however not all of them are easy to read. For example, when you collect a timestamp column from a DataFrame and save it as a Python variable, the value is stored as a datetime object. If you are not familiar with the datetime object format, it is not as easy to read as the common YYYY-MM-DD HH:MM:SS format. If you wanted to print the date and time, or maybe use it for timestamp validation, you can convert the datetime object to a string. This automatically converts the datetime object into a common time format. In this article, we show you how to display the timestamp as a column value, before converting it to a datetime object, and finally, a string value. Display timestamp as a column value To display the current timestamp as a column value, you should call current_timestamp(). This provides the date and time as of the moment it is called. %python\r\n\r\nfrom pyspark.sql.functions import *\r\ndisplay(spark.range(1).withColumn(\"date\",current_timestamp()).select(\"date\")) Sample output: Assign timestamp to datetime object Instead of displaying the date and time in a column, you can assign it to a variable. %python\r\n\r\nmydate = spark.range(1).withColumn(\"date\",current_timestamp()).select(\"date\").collect()[0][0] Once this assignment is made, you can call the variable to display the stored date and time value as a datetime object. %python\r\n\r\nmydate Sample output: datetime.datetime(2021, 6, 25, 11, 0, 56, 813000) Delete Info The date and time is current as of the moment it is assigned to the variable as a datetime object, but the datetime object value is static unless a new value is assigned. Convert to string You can convert the datetime object to a string by calling str() on the variable. Calling str() just converts the datetime object to a string. It does not update the value with the current date and time. %python\r\n\r\nstr(mydate) Sample output: '2021-06-25 11:00:56.813000' Was this article helpful? (12) (10) Additional Informations Related Articles AttributeError: ‘function’ object has no attribute Problem You are selecting columns from a DataFrame and you get an error message. ... Related Articles AttributeError: ‘function’ object has no attribute Problem You are selecting columns from a DataFrame and you get an error message. ... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",
          "title" : "Convert Python datetime object to string",
          "view_href" : "https://kb.databricks.com/en_US/python/convert-datetime-to-string"
        }
      }
    ]
  }
}
